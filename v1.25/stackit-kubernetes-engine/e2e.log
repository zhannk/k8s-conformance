I0406 10:17:52.634210      22 e2e.go:116] Starting e2e run "29d2c07a-ee25-4bc9-9950-c07bb92b6fe9" on Ginkgo node 1
Apr  6 10:17:52.662: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1680776272 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Apr  6 10:17:52.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:17:52.890: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr  6 10:17:53.116: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr  6 10:17:53.210: INFO: 40 / 40 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr  6 10:17:53.210: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Apr  6 10:17:53.210: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-pool-5srtzxdh8p-v1.25.8' (0 seconds elapsed)
Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Apr  6 10:17:53.222: INFO: e2e test version: v1.25.8
Apr  6 10:17:53.225: INFO: kube-apiserver version: v1.25.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Apr  6 10:17:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:17:53.232: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.350 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Apr  6 10:17:52.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:17:52.890: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Apr  6 10:17:53.116: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Apr  6 10:17:53.210: INFO: 40 / 40 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Apr  6 10:17:53.210: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
    Apr  6 10:17:53.210: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
    Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
    Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-pool-5srtzxdh8p-v1.25.8' (0 seconds elapsed)
    Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Apr  6 10:17:53.222: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Apr  6 10:17:53.222: INFO: e2e test version: v1.25.8
    Apr  6 10:17:53.225: INFO: kube-apiserver version: v1.25.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Apr  6 10:17:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:17:53.232: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:17:53.273
Apr  6 10:17:53.273: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 10:17:53.274
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:17:53.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:17:53.305
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 in namespace container-probe-2454 04/06/23 10:17:53.312
Apr  6 10:17:53.327: INFO: Waiting up to 5m0s for pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495" in namespace "container-probe-2454" to be "not pending"
Apr  6 10:17:53.335: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Pending", Reason="", readiness=false. Elapsed: 6.53177ms
Apr  6 10:17:55.349: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02069455s
Apr  6 10:17:57.342: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014334975s
Apr  6 10:17:59.360: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Running", Reason="", readiness=true. Elapsed: 6.031476467s
Apr  6 10:17:59.360: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495" satisfied condition "not pending"
Apr  6 10:17:59.363: INFO: Started pod liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 in namespace container-probe-2454
STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 10:17:59.363
Apr  6 10:17:59.371: INFO: Initial restart count of pod liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is 0
Apr  6 10:18:17.490: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 1 (18.118811686s elapsed)
Apr  6 10:18:35.569: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 2 (36.197971138s elapsed)
Apr  6 10:18:55.664: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 3 (56.292906721s elapsed)
Apr  6 10:19:15.752: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 4 (1m16.38045024s elapsed)
Apr  6 10:20:16.085: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 5 (2m16.713487151s elapsed)
STEP: deleting the pod 04/06/23 10:20:16.085
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 10:20:16.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2454" for this suite. 04/06/23 10:20:16.11
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":1,"skipped":5,"failed":0}
------------------------------
â€¢ [SLOW TEST] [142.846 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:17:53.273
    Apr  6 10:17:53.273: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 10:17:53.274
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:17:53.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:17:53.305
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 in namespace container-probe-2454 04/06/23 10:17:53.312
    Apr  6 10:17:53.327: INFO: Waiting up to 5m0s for pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495" in namespace "container-probe-2454" to be "not pending"
    Apr  6 10:17:53.335: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Pending", Reason="", readiness=false. Elapsed: 6.53177ms
    Apr  6 10:17:55.349: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02069455s
    Apr  6 10:17:57.342: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014334975s
    Apr  6 10:17:59.360: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495": Phase="Running", Reason="", readiness=true. Elapsed: 6.031476467s
    Apr  6 10:17:59.360: INFO: Pod "liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495" satisfied condition "not pending"
    Apr  6 10:17:59.363: INFO: Started pod liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 in namespace container-probe-2454
    STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 10:17:59.363
    Apr  6 10:17:59.371: INFO: Initial restart count of pod liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is 0
    Apr  6 10:18:17.490: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 1 (18.118811686s elapsed)
    Apr  6 10:18:35.569: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 2 (36.197971138s elapsed)
    Apr  6 10:18:55.664: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 3 (56.292906721s elapsed)
    Apr  6 10:19:15.752: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 4 (1m16.38045024s elapsed)
    Apr  6 10:20:16.085: INFO: Restart count of pod container-probe-2454/liveness-e8806797-dcc1-4db3-991e-42cc4b6ad495 is now 5 (2m16.713487151s elapsed)
    STEP: deleting the pod 04/06/23 10:20:16.085
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 10:20:16.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2454" for this suite. 04/06/23 10:20:16.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:20:16.121
Apr  6 10:20:16.121: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename gc 04/06/23 10:20:16.123
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:16.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:16.149
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 04/06/23 10:20:16.155
STEP: Wait for the Deployment to create new ReplicaSet 04/06/23 10:20:16.163
STEP: delete the deployment 04/06/23 10:20:16.683
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/06/23 10:20:16.707
STEP: Gathering metrics 04/06/23 10:20:17.24
W0406 10:20:17.288091      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 10:20:17.288: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Apr  6 10:20:17.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2458" for this suite. 04/06/23 10:20:17.299
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":2,"skipped":46,"failed":0}
------------------------------
â€¢ [1.187 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:20:16.121
    Apr  6 10:20:16.121: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename gc 04/06/23 10:20:16.123
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:16.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:16.149
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 04/06/23 10:20:16.155
    STEP: Wait for the Deployment to create new ReplicaSet 04/06/23 10:20:16.163
    STEP: delete the deployment 04/06/23 10:20:16.683
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/06/23 10:20:16.707
    STEP: Gathering metrics 04/06/23 10:20:17.24
    W0406 10:20:17.288091      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr  6 10:20:17.288: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Apr  6 10:20:17.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2458" for this suite. 04/06/23 10:20:17.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:20:17.309
Apr  6 10:20:17.310: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 10:20:17.311
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:17.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:17.345
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/06/23 10:20:17.355
Apr  6 10:20:17.377: INFO: Waiting up to 5m0s for pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2" in namespace "emptydir-492" to be "Succeeded or Failed"
Apr  6 10:20:17.386: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.369134ms
Apr  6 10:20:19.397: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019356534s
Apr  6 10:20:21.393: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015539649s
STEP: Saw pod success 04/06/23 10:20:21.393
Apr  6 10:20:21.393: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2" satisfied condition "Succeeded or Failed"
Apr  6 10:20:21.406: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-a94ae266-224d-4562-96fc-f6376830ebb2 container test-container: <nil>
STEP: delete the pod 04/06/23 10:20:21.425
Apr  6 10:20:21.452: INFO: Waiting for pod pod-a94ae266-224d-4562-96fc-f6376830ebb2 to disappear
Apr  6 10:20:21.457: INFO: Pod pod-a94ae266-224d-4562-96fc-f6376830ebb2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 10:20:21.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-492" for this suite. 04/06/23 10:20:21.484
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":3,"skipped":51,"failed":0}
------------------------------
â€¢ [4.184 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:20:17.309
    Apr  6 10:20:17.310: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 10:20:17.311
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:17.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:17.345
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/06/23 10:20:17.355
    Apr  6 10:20:17.377: INFO: Waiting up to 5m0s for pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2" in namespace "emptydir-492" to be "Succeeded or Failed"
    Apr  6 10:20:17.386: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.369134ms
    Apr  6 10:20:19.397: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019356534s
    Apr  6 10:20:21.393: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015539649s
    STEP: Saw pod success 04/06/23 10:20:21.393
    Apr  6 10:20:21.393: INFO: Pod "pod-a94ae266-224d-4562-96fc-f6376830ebb2" satisfied condition "Succeeded or Failed"
    Apr  6 10:20:21.406: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-a94ae266-224d-4562-96fc-f6376830ebb2 container test-container: <nil>
    STEP: delete the pod 04/06/23 10:20:21.425
    Apr  6 10:20:21.452: INFO: Waiting for pod pod-a94ae266-224d-4562-96fc-f6376830ebb2 to disappear
    Apr  6 10:20:21.457: INFO: Pod pod-a94ae266-224d-4562-96fc-f6376830ebb2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 10:20:21.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-492" for this suite. 04/06/23 10:20:21.484
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:20:21.494
Apr  6 10:20:21.494: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:20:21.496
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:21.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:21.545
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-178aeab3-de1e-43cb-801e-825bca82278b 04/06/23 10:20:21.552
STEP: Creating a pod to test consume secrets 04/06/23 10:20:21.56
Apr  6 10:20:21.574: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80" in namespace "projected-8393" to be "Succeeded or Failed"
Apr  6 10:20:21.587: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.482167ms
Apr  6 10:20:23.595: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020177198s
Apr  6 10:20:25.597: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022715074s
Apr  6 10:20:27.596: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02122847s
Apr  6 10:20:29.596: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02148518s
STEP: Saw pod success 04/06/23 10:20:29.596
Apr  6 10:20:29.596: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80" satisfied condition "Succeeded or Failed"
Apr  6 10:20:29.602: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/06/23 10:20:29.648
Apr  6 10:20:29.662: INFO: Waiting for pod pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80 to disappear
Apr  6 10:20:29.669: INFO: Pod pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Apr  6 10:20:29.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8393" for this suite. 04/06/23 10:20:29.679
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":4,"skipped":52,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.192 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:20:21.494
    Apr  6 10:20:21.494: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:20:21.496
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:21.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:21.545
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-178aeab3-de1e-43cb-801e-825bca82278b 04/06/23 10:20:21.552
    STEP: Creating a pod to test consume secrets 04/06/23 10:20:21.56
    Apr  6 10:20:21.574: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80" in namespace "projected-8393" to be "Succeeded or Failed"
    Apr  6 10:20:21.587: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.482167ms
    Apr  6 10:20:23.595: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020177198s
    Apr  6 10:20:25.597: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022715074s
    Apr  6 10:20:27.596: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02122847s
    Apr  6 10:20:29.596: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02148518s
    STEP: Saw pod success 04/06/23 10:20:29.596
    Apr  6 10:20:29.596: INFO: Pod "pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80" satisfied condition "Succeeded or Failed"
    Apr  6 10:20:29.602: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:20:29.648
    Apr  6 10:20:29.662: INFO: Waiting for pod pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80 to disappear
    Apr  6 10:20:29.669: INFO: Pod pod-projected-secrets-ef218aba-dd3c-4449-adf6-d87f756d1b80 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Apr  6 10:20:29.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8393" for this suite. 04/06/23 10:20:29.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:20:29.697
Apr  6 10:20:29.697: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:20:29.699
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:29.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:29.743
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Apr  6 10:20:29.755: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/06/23 10:20:33.082
Apr  6 10:20:33.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
Apr  6 10:20:34.193: INFO: stderr: ""
Apr  6 10:20:34.193: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr  6 10:20:34.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 delete e2e-test-crd-publish-openapi-2635-crds test-foo'
Apr  6 10:20:34.438: INFO: stderr: ""
Apr  6 10:20:34.438: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr  6 10:20:34.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 apply -f -'
Apr  6 10:20:34.784: INFO: stderr: ""
Apr  6 10:20:34.784: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr  6 10:20:34.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 delete e2e-test-crd-publish-openapi-2635-crds test-foo'
Apr  6 10:20:34.941: INFO: stderr: ""
Apr  6 10:20:34.941: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/06/23 10:20:34.941
Apr  6 10:20:34.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
Apr  6 10:20:35.730: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/06/23 10:20:35.734
Apr  6 10:20:35.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
Apr  6 10:20:36.173: INFO: rc: 1
Apr  6 10:20:36.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 apply -f -'
Apr  6 10:20:36.566: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/06/23 10:20:36.566
Apr  6 10:20:36.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
Apr  6 10:20:36.923: INFO: rc: 1
Apr  6 10:20:36.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 apply -f -'
Apr  6 10:20:37.259: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 04/06/23 10:20:37.259
Apr  6 10:20:37.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds'
Apr  6 10:20:37.655: INFO: stderr: ""
Apr  6 10:20:37.655: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 04/06/23 10:20:37.656
Apr  6 10:20:37.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.metadata'
Apr  6 10:20:38.028: INFO: stderr: ""
Apr  6 10:20:38.028: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr  6 10:20:38.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.spec'
Apr  6 10:20:38.442: INFO: stderr: ""
Apr  6 10:20:38.442: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr  6 10:20:38.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.spec.bars'
Apr  6 10:20:38.849: INFO: stderr: ""
Apr  6 10:20:38.858: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/06/23 10:20:38.859
Apr  6 10:20:38.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.spec.bars2'
Apr  6 10:20:39.251: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:20:44.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-477" for this suite. 04/06/23 10:20:44.189
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":5,"skipped":58,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.499 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:20:29.697
    Apr  6 10:20:29.697: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:20:29.699
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:29.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:29.743
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Apr  6 10:20:29.755: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/06/23 10:20:33.082
    Apr  6 10:20:33.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
    Apr  6 10:20:34.193: INFO: stderr: ""
    Apr  6 10:20:34.193: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr  6 10:20:34.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 delete e2e-test-crd-publish-openapi-2635-crds test-foo'
    Apr  6 10:20:34.438: INFO: stderr: ""
    Apr  6 10:20:34.438: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Apr  6 10:20:34.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 apply -f -'
    Apr  6 10:20:34.784: INFO: stderr: ""
    Apr  6 10:20:34.784: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr  6 10:20:34.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 delete e2e-test-crd-publish-openapi-2635-crds test-foo'
    Apr  6 10:20:34.941: INFO: stderr: ""
    Apr  6 10:20:34.941: INFO: stdout: "e2e-test-crd-publish-openapi-2635-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/06/23 10:20:34.941
    Apr  6 10:20:34.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
    Apr  6 10:20:35.730: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/06/23 10:20:35.734
    Apr  6 10:20:35.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
    Apr  6 10:20:36.173: INFO: rc: 1
    Apr  6 10:20:36.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 apply -f -'
    Apr  6 10:20:36.566: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/06/23 10:20:36.566
    Apr  6 10:20:36.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 create -f -'
    Apr  6 10:20:36.923: INFO: rc: 1
    Apr  6 10:20:36.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 --namespace=crd-publish-openapi-477 apply -f -'
    Apr  6 10:20:37.259: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 04/06/23 10:20:37.259
    Apr  6 10:20:37.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds'
    Apr  6 10:20:37.655: INFO: stderr: ""
    Apr  6 10:20:37.655: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 04/06/23 10:20:37.656
    Apr  6 10:20:37.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.metadata'
    Apr  6 10:20:38.028: INFO: stderr: ""
    Apr  6 10:20:38.028: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Apr  6 10:20:38.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.spec'
    Apr  6 10:20:38.442: INFO: stderr: ""
    Apr  6 10:20:38.442: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Apr  6 10:20:38.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.spec.bars'
    Apr  6 10:20:38.849: INFO: stderr: ""
    Apr  6 10:20:38.858: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2635-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/06/23 10:20:38.859
    Apr  6 10:20:38.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-477 explain e2e-test-crd-publish-openapi-2635-crds.spec.bars2'
    Apr  6 10:20:39.251: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:20:44.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-477" for this suite. 04/06/23 10:20:44.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:20:44.199
Apr  6 10:20:44.199: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:20:44.2
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:44.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:44.235
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 04/06/23 10:20:44.252
STEP: watching for the Service to be added 04/06/23 10:20:44.267
Apr  6 10:20:44.270: INFO: Found Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Apr  6 10:20:44.270: INFO: Service test-service-swdbd created
STEP: Getting /status 04/06/23 10:20:44.27
Apr  6 10:20:44.275: INFO: Service test-service-swdbd has LoadBalancer: {[]}
STEP: patching the ServiceStatus 04/06/23 10:20:44.275
STEP: watching for the Service to be patched 04/06/23 10:20:44.285
Apr  6 10:20:44.300: INFO: observed Service test-service-swdbd in namespace services-4450 with annotations: map[] & LoadBalancer: {[]}
Apr  6 10:20:44.300: INFO: Found Service test-service-swdbd in namespace services-4450 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Apr  6 10:20:44.300: INFO: Service test-service-swdbd has service status patched
STEP: updating the ServiceStatus 04/06/23 10:20:44.3
Apr  6 10:20:44.320: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 04/06/23 10:20:44.32
Apr  6 10:20:44.326: INFO: Observed Service test-service-swdbd in namespace services-4450 with annotations: map[] & Conditions: {[]}
Apr  6 10:20:44.326: INFO: Observed event: &Service{ObjectMeta:{test-service-swdbd  services-4450  6a0d3558-1133-40d3-a45a-60c21bfd09a0 7955 0 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.68.194.44,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.68.194.44],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Apr  6 10:20:44.327: INFO: Observed event: &Service{ObjectMeta:{test-service-swdbd  services-4450  6a0d3558-1133-40d3-a45a-60c21bfd09a0 7957 0 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.68.194.44,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.68.194.44],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
Apr  6 10:20:44.327: INFO: Found Service test-service-swdbd in namespace services-4450 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 10:20:44.327: INFO: Service test-service-swdbd has service status updated
STEP: patching the service 04/06/23 10:20:44.327
STEP: watching for the Service to be patched 04/06/23 10:20:44.345
Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
Apr  6 10:20:44.349: INFO: Found Service test-service-swdbd in namespace services-4450 with labels: map[test-service:patched test-service-static:true]
Apr  6 10:20:44.349: INFO: Service test-service-swdbd patched
STEP: deleting the service 04/06/23 10:20:44.349
STEP: watching for the Service to be deleted 04/06/23 10:20:44.361
Apr  6 10:20:44.365: INFO: Observed event: ADDED
Apr  6 10:20:44.365: INFO: Observed event: MODIFIED
Apr  6 10:20:44.366: INFO: Observed event: MODIFIED
Apr  6 10:20:44.366: INFO: Observed event: MODIFIED
Apr  6 10:20:44.366: INFO: Observed event: MODIFIED
Apr  6 10:20:44.366: INFO: Found Service test-service-swdbd in namespace services-4450 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Apr  6 10:20:44.366: INFO: Service test-service-swdbd deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:20:44.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4450" for this suite. 04/06/23 10:20:44.375
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":6,"skipped":106,"failed":0}
------------------------------
â€¢ [0.199 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:20:44.199
    Apr  6 10:20:44.199: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:20:44.2
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:44.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:44.235
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 04/06/23 10:20:44.252
    STEP: watching for the Service to be added 04/06/23 10:20:44.267
    Apr  6 10:20:44.270: INFO: Found Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Apr  6 10:20:44.270: INFO: Service test-service-swdbd created
    STEP: Getting /status 04/06/23 10:20:44.27
    Apr  6 10:20:44.275: INFO: Service test-service-swdbd has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 04/06/23 10:20:44.275
    STEP: watching for the Service to be patched 04/06/23 10:20:44.285
    Apr  6 10:20:44.300: INFO: observed Service test-service-swdbd in namespace services-4450 with annotations: map[] & LoadBalancer: {[]}
    Apr  6 10:20:44.300: INFO: Found Service test-service-swdbd in namespace services-4450 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Apr  6 10:20:44.300: INFO: Service test-service-swdbd has service status patched
    STEP: updating the ServiceStatus 04/06/23 10:20:44.3
    Apr  6 10:20:44.320: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 04/06/23 10:20:44.32
    Apr  6 10:20:44.326: INFO: Observed Service test-service-swdbd in namespace services-4450 with annotations: map[] & Conditions: {[]}
    Apr  6 10:20:44.326: INFO: Observed event: &Service{ObjectMeta:{test-service-swdbd  services-4450  6a0d3558-1133-40d3-a45a-60c21bfd09a0 7955 0 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.68.194.44,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.68.194.44],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Apr  6 10:20:44.327: INFO: Observed event: &Service{ObjectMeta:{test-service-swdbd  services-4450  6a0d3558-1133-40d3-a45a-60c21bfd09a0 7957 0 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-06 10:20:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.68.194.44,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.68.194.44],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
    Apr  6 10:20:44.327: INFO: Found Service test-service-swdbd in namespace services-4450 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr  6 10:20:44.327: INFO: Service test-service-swdbd has service status updated
    STEP: patching the service 04/06/23 10:20:44.327
    STEP: watching for the Service to be patched 04/06/23 10:20:44.345
    Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
    Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
    Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
    Apr  6 10:20:44.349: INFO: observed Service test-service-swdbd in namespace services-4450 with labels: map[test-service-static:true]
    Apr  6 10:20:44.349: INFO: Found Service test-service-swdbd in namespace services-4450 with labels: map[test-service:patched test-service-static:true]
    Apr  6 10:20:44.349: INFO: Service test-service-swdbd patched
    STEP: deleting the service 04/06/23 10:20:44.349
    STEP: watching for the Service to be deleted 04/06/23 10:20:44.361
    Apr  6 10:20:44.365: INFO: Observed event: ADDED
    Apr  6 10:20:44.365: INFO: Observed event: MODIFIED
    Apr  6 10:20:44.366: INFO: Observed event: MODIFIED
    Apr  6 10:20:44.366: INFO: Observed event: MODIFIED
    Apr  6 10:20:44.366: INFO: Observed event: MODIFIED
    Apr  6 10:20:44.366: INFO: Found Service test-service-swdbd in namespace services-4450 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Apr  6 10:20:44.366: INFO: Service test-service-swdbd deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:20:44.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4450" for this suite. 04/06/23 10:20:44.375
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:20:44.399
Apr  6 10:20:44.399: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 10:20:44.401
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:44.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:44.441
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Apr  6 10:20:44.447: INFO: Creating deployment "test-recreate-deployment"
Apr  6 10:20:44.454: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr  6 10:20:44.466: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr  6 10:20:46.478: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr  6 10:20:46.484: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr  6 10:20:46.501: INFO: Updating deployment test-recreate-deployment
Apr  6 10:20:46.501: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 10:20:46.572: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9298  508067a0-0bfb-46b3-80d7-745fbba073fe 8004 2 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e5b638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-06 10:20:46 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-04-06 10:20:46 +0000 UTC,LastTransitionTime:2023-04-06 10:20:44 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr  6 10:20:46.582: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9298  23c49f87-202f-4ad2-a19a-6bfbe824f2f7 8003 1 2023-04-06 10:20:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 508067a0-0bfb-46b3-80d7-745fbba073fe 0xc003e5be80 0xc003e5be81}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508067a0-0bfb-46b3-80d7-745fbba073fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e5bfb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 10:20:46.582: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr  6 10:20:46.582: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9298  8cc07223-792a-4fa5-bf65-4834fc9f5ca6 7996 2 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 508067a0-0bfb-46b3-80d7-745fbba073fe 0xc003e5bcd7 0xc003e5bcd8}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508067a0-0bfb-46b3-80d7-745fbba073fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e5bdf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 10:20:46.588: INFO: Pod "test-recreate-deployment-9d58999df-r8jl6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-r8jl6 test-recreate-deployment-9d58999df- deployment-9298  30e5a662-8382-4844-b7c5-9351a449bf54 8005 0 2023-04-06 10:20:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 23c49f87-202f-4ad2-a19a-6bfbe824f2f7 0xc003f05a80 0xc003f05a81}] [] [{kube-controller-manager Update v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23c49f87-202f-4ad2-a19a-6bfbe824f2f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5dr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5dr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:,StartTime:2023-04-06 10:20:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 10:20:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9298" for this suite. 04/06/23 10:20:46.599
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":7,"skipped":106,"failed":0}
------------------------------
â€¢ [2.211 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:20:44.399
    Apr  6 10:20:44.399: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 10:20:44.401
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:44.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:44.441
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Apr  6 10:20:44.447: INFO: Creating deployment "test-recreate-deployment"
    Apr  6 10:20:44.454: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Apr  6 10:20:44.466: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Apr  6 10:20:46.478: INFO: Waiting deployment "test-recreate-deployment" to complete
    Apr  6 10:20:46.484: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Apr  6 10:20:46.501: INFO: Updating deployment test-recreate-deployment
    Apr  6 10:20:46.501: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 10:20:46.572: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9298  508067a0-0bfb-46b3-80d7-745fbba073fe 8004 2 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e5b638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-06 10:20:46 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-04-06 10:20:46 +0000 UTC,LastTransitionTime:2023-04-06 10:20:44 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Apr  6 10:20:46.582: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9298  23c49f87-202f-4ad2-a19a-6bfbe824f2f7 8003 1 2023-04-06 10:20:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 508067a0-0bfb-46b3-80d7-745fbba073fe 0xc003e5be80 0xc003e5be81}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508067a0-0bfb-46b3-80d7-745fbba073fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e5bfb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 10:20:46.582: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Apr  6 10:20:46.582: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9298  8cc07223-792a-4fa5-bf65-4834fc9f5ca6 7996 2 2023-04-06 10:20:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 508067a0-0bfb-46b3-80d7-745fbba073fe 0xc003e5bcd7 0xc003e5bcd8}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508067a0-0bfb-46b3-80d7-745fbba073fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e5bdf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 10:20:46.588: INFO: Pod "test-recreate-deployment-9d58999df-r8jl6" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-r8jl6 test-recreate-deployment-9d58999df- deployment-9298  30e5a662-8382-4844-b7c5-9351a449bf54 8005 0 2023-04-06 10:20:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 23c49f87-202f-4ad2-a19a-6bfbe824f2f7 0xc003f05a80 0xc003f05a81}] [] [{kube-controller-manager Update v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23c49f87-202f-4ad2-a19a-6bfbe824f2f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 10:20:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5dr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5dr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:20:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:,StartTime:2023-04-06 10:20:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 10:20:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9298" for this suite. 04/06/23 10:20:46.599
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:20:46.615
Apr  6 10:20:46.615: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-pred 04/06/23 10:20:46.616
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:46.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:46.646
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Apr  6 10:20:46.652: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 10:20:46.666: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 10:20:46.671: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
Apr  6 10:20:46.698: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:20:46.698: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:20:46.698: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:20:46.698: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:20:46.698: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:20:46.698: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:20:46.698: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:20:46.698: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:20:46.698: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:20:46.698: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:20:46.698: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 10:20:46.698: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container e2e ready: true, restart count 0
Apr  6 10:20:46.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:20:46.698: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:20:46.698: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:20:46.698: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
Apr  6 10:20:46.717: INFO: test-recreate-deployment-9d58999df-r8jl6 from deployment-9298 started at 2023-04-06 10:20:46 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container httpd ready: false, restart count 0
Apr  6 10:20:46.717: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:20:46.717: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:20:46.717: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:20:46.717: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:20:46.717: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:20:46.717: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:20:46.717: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:20:46.717: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:20:46.717: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:20:46.717: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:20:46.717: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:20:46.717: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:20:46.717: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
Apr  6 10:20:46.754: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.755: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:20:46.755: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:20:46.755: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.755: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 10:20:46.755: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.755: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:20:46.755: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.755: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 10:20:46.756: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.756: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 10:20:46.756: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.756: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 10:20:46.756: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.756: INFO: 	Container coredns ready: true, restart count 0
Apr  6 10:20:46.756: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.756: INFO: 	Container coredns ready: true, restart count 0
Apr  6 10:20:46.757: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
Apr  6 10:20:46.757: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:20:46.757: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:20:46.757: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:20:46.757: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.757: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:20:46.757: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:20:46.757: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.758: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 10:20:46.758: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.758: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 10:20:46.758: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.758: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:20:46.758: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.758: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:20:46.758: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.759: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr  6 10:20:46.759: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:20:46.759: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:20:46.759: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
Apr  6 10:20:46.781: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.781: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:20:46.781: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:20:46.781: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.781: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:20:46.781: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
Apr  6 10:20:46.781: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:20:46.781: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:20:46.781: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:20:46.781: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.781: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:20:46.781: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:20:46.781: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.781: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:20:46.781: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.781: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:20:46.781: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:20:46.781: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:20:46.781: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
Apr  6 10:20:46.813: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:20:46.813: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:20:46.813: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:20:46.813: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 10:20:46.813: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:20:46.813: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:20:46.813: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:20:46.813: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:20:46.813: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:20:46.813: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:20:46.813: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:20:46.813: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:20:46.813: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:20:46.813: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/06/23 10:20:46.813
Apr  6 10:20:46.834: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6744" to be "running"
Apr  6 10:20:46.839: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.758772ms
Apr  6 10:20:48.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011806263s
Apr  6 10:20:50.847: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.01352408s
Apr  6 10:20:50.848: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/06/23 10:20:50.856
STEP: Trying to apply a random label on the found node. 04/06/23 10:20:50.873
STEP: verifying the node has the label kubernetes.io/e2e-b06e00b0-8998-4cc8-9ff3-7906dcb27bac 95 04/06/23 10:20:50.895
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/06/23 10:20:50.9
Apr  6 10:20:50.911: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6744" to be "not pending"
Apr  6 10:20:50.915: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666709ms
Apr  6 10:20:52.924: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012872176s
Apr  6 10:20:52.924: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.1.148 on the node which pod4 resides and expect not scheduled 04/06/23 10:20:52.924
Apr  6 10:20:52.942: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6744" to be "not pending"
Apr  6 10:20:52.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.986015ms
Apr  6 10:20:54.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011472189s
Apr  6 10:20:56.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01254002s
Apr  6 10:20:58.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011168366s
Apr  6 10:21:00.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01164674s
Apr  6 10:21:02.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011753959s
Apr  6 10:21:04.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010754622s
Apr  6 10:21:06.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011827976s
Apr  6 10:21:08.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015230921s
Apr  6 10:21:10.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013236657s
Apr  6 10:21:12.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016181561s
Apr  6 10:21:14.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012359174s
Apr  6 10:21:16.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012400344s
Apr  6 10:21:18.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01423215s
Apr  6 10:21:20.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.014018985s
Apr  6 10:21:22.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016409857s
Apr  6 10:21:24.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012011534s
Apr  6 10:21:26.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012835733s
Apr  6 10:21:28.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012537242s
Apr  6 10:21:30.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.015224991s
Apr  6 10:21:32.963: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.02126414s
Apr  6 10:21:34.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01136288s
Apr  6 10:21:36.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01289284s
Apr  6 10:21:38.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012122519s
Apr  6 10:21:40.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015227613s
Apr  6 10:21:42.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013709833s
Apr  6 10:21:44.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010305219s
Apr  6 10:21:46.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.013064536s
Apr  6 10:21:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01267722s
Apr  6 10:21:50.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012494346s
Apr  6 10:21:52.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011323752s
Apr  6 10:21:54.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023038219s
Apr  6 10:21:56.968: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.026463036s
Apr  6 10:21:58.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011995713s
Apr  6 10:22:00.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.015919387s
Apr  6 10:22:02.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012473105s
Apr  6 10:22:04.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013237443s
Apr  6 10:22:06.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.029774749s
Apr  6 10:22:08.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015128714s
Apr  6 10:22:10.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013651082s
Apr  6 10:22:12.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011127484s
Apr  6 10:22:14.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015910314s
Apr  6 10:22:16.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011977546s
Apr  6 10:22:18.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014462122s
Apr  6 10:22:20.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011159097s
Apr  6 10:22:22.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011576567s
Apr  6 10:22:24.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011469291s
Apr  6 10:22:26.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01579726s
Apr  6 10:22:28.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011737909s
Apr  6 10:22:30.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013677578s
Apr  6 10:22:32.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011504911s
Apr  6 10:22:34.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012549029s
Apr  6 10:22:36.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014307335s
Apr  6 10:22:38.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011001655s
Apr  6 10:22:40.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012019056s
Apr  6 10:22:42.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013831124s
Apr  6 10:22:44.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011753786s
Apr  6 10:22:46.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015983141s
Apr  6 10:22:48.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012026184s
Apr  6 10:22:50.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013139052s
Apr  6 10:22:52.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016013734s
Apr  6 10:22:54.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.011436478s
Apr  6 10:22:56.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013366187s
Apr  6 10:22:58.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011465217s
Apr  6 10:23:00.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011736019s
Apr  6 10:23:02.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011880205s
Apr  6 10:23:04.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.014358737s
Apr  6 10:23:06.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.011964599s
Apr  6 10:23:08.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.013117606s
Apr  6 10:23:10.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.01265527s
Apr  6 10:23:12.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.013158986s
Apr  6 10:23:14.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.013274329s
Apr  6 10:23:16.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.013323803s
Apr  6 10:23:18.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.011328265s
Apr  6 10:23:20.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.013124937s
Apr  6 10:23:22.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012372644s
Apr  6 10:23:24.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011669343s
Apr  6 10:23:26.959: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.017285642s
Apr  6 10:23:28.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.014895098s
Apr  6 10:23:30.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012712276s
Apr  6 10:23:32.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.014253425s
Apr  6 10:23:34.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012801422s
Apr  6 10:23:36.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.013066589s
Apr  6 10:23:38.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010629917s
Apr  6 10:23:40.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012463672s
Apr  6 10:23:42.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015556986s
Apr  6 10:23:44.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.014366292s
Apr  6 10:23:46.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013650639s
Apr  6 10:23:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.013303744s
Apr  6 10:23:50.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.018073169s
Apr  6 10:23:52.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011780224s
Apr  6 10:23:54.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.011549838s
Apr  6 10:23:56.962: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.020236641s
Apr  6 10:23:58.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.015431891s
Apr  6 10:24:00.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011050233s
Apr  6 10:24:02.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013152271s
Apr  6 10:24:04.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010429178s
Apr  6 10:24:06.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.014761774s
Apr  6 10:24:08.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.011044992s
Apr  6 10:24:10.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.012580578s
Apr  6 10:24:12.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011613324s
Apr  6 10:24:14.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012612662s
Apr  6 10:24:16.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.011773245s
Apr  6 10:24:18.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011662753s
Apr  6 10:24:20.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.012374994s
Apr  6 10:24:22.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.014116436s
Apr  6 10:24:24.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.011432008s
Apr  6 10:24:26.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013364878s
Apr  6 10:24:28.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.01303552s
Apr  6 10:24:30.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.015452408s
Apr  6 10:24:32.962: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019684633s
Apr  6 10:24:34.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011914817s
Apr  6 10:24:36.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.014650535s
Apr  6 10:24:38.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.017876783s
Apr  6 10:24:40.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.01367805s
Apr  6 10:24:42.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014823193s
Apr  6 10:24:44.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.01121519s
Apr  6 10:24:46.964: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022057914s
Apr  6 10:24:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012817001s
Apr  6 10:24:50.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.014186451s
Apr  6 10:24:52.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.012150213s
Apr  6 10:24:54.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.012624718s
Apr  6 10:24:56.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.01804248s
Apr  6 10:24:58.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013084841s
Apr  6 10:25:00.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012719494s
Apr  6 10:25:02.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.015762773s
Apr  6 10:25:04.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011632788s
Apr  6 10:25:06.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.015423647s
Apr  6 10:25:08.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.013804231s
Apr  6 10:25:10.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.013405227s
Apr  6 10:25:12.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.011192459s
Apr  6 10:25:14.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.013713658s
Apr  6 10:25:16.964: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.021821425s
Apr  6 10:25:18.959: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.017183198s
Apr  6 10:25:20.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016478969s
Apr  6 10:25:22.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.012707903s
Apr  6 10:25:24.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.010061416s
Apr  6 10:25:26.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013450874s
Apr  6 10:25:28.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.011685382s
Apr  6 10:25:30.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.011777569s
Apr  6 10:25:32.959: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.017406324s
Apr  6 10:25:34.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.011409104s
Apr  6 10:25:36.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.01771776s
Apr  6 10:25:38.962: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.019853567s
Apr  6 10:25:40.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.013441035s
Apr  6 10:25:42.961: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.019220406s
Apr  6 10:25:44.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011685937s
Apr  6 10:25:46.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.011936426s
Apr  6 10:25:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013595039s
Apr  6 10:25:50.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012746292s
Apr  6 10:25:52.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011293759s
Apr  6 10:25:52.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015857793s
STEP: removing the label kubernetes.io/e2e-b06e00b0-8998-4cc8-9ff3-7906dcb27bac off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 10:25:52.961
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b06e00b0-8998-4cc8-9ff3-7906dcb27bac 04/06/23 10:25:52.976
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:25:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6744" for this suite. 04/06/23 10:25:52.996
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":8,"skipped":106,"failed":0}
------------------------------
â€¢ [SLOW TEST] [306.391 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:20:46.615
    Apr  6 10:20:46.615: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-pred 04/06/23 10:20:46.616
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:20:46.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:20:46.646
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Apr  6 10:20:46.652: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr  6 10:20:46.666: INFO: Waiting for terminating namespaces to be deleted...
    Apr  6 10:20:46.671: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
    Apr  6 10:20:46.698: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container e2e ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:20:46.698: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
    Apr  6 10:20:46.717: INFO: test-recreate-deployment-9d58999df-r8jl6 from deployment-9298 started at 2023-04-06 10:20:46 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container httpd ready: false, restart count 0
    Apr  6 10:20:46.717: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:20:46.717: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
    Apr  6 10:20:46.754: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.755: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:20:46.755: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:20:46.755: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.755: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Apr  6 10:20:46.755: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.755: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:20:46.755: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.755: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 10:20:46.756: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.756: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 10:20:46.756: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.756: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 10:20:46.756: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.756: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 10:20:46.756: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.756: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 10:20:46.757: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
    Apr  6 10:20:46.757: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:20:46.757: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:20:46.757: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:20:46.757: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.757: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:20:46.757: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:20:46.757: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.758: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 10:20:46.758: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.758: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 10:20:46.758: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.758: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:20:46.758: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.758: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:20:46.758: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.759: INFO: 	Container vpn-shoot ready: true, restart count 0
    Apr  6 10:20:46.759: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:20:46.759: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:20:46.759: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
    Apr  6 10:20:46.781: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.781: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.781: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
    Apr  6 10:20:46.781: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.781: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.781: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.781: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:20:46.781: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
    Apr  6 10:20:46.813: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container calico-typha ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:20:46.813: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:20:46.813: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/06/23 10:20:46.813
    Apr  6 10:20:46.834: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6744" to be "running"
    Apr  6 10:20:46.839: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.758772ms
    Apr  6 10:20:48.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011806263s
    Apr  6 10:20:50.847: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.01352408s
    Apr  6 10:20:50.848: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/06/23 10:20:50.856
    STEP: Trying to apply a random label on the found node. 04/06/23 10:20:50.873
    STEP: verifying the node has the label kubernetes.io/e2e-b06e00b0-8998-4cc8-9ff3-7906dcb27bac 95 04/06/23 10:20:50.895
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/06/23 10:20:50.9
    Apr  6 10:20:50.911: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6744" to be "not pending"
    Apr  6 10:20:50.915: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666709ms
    Apr  6 10:20:52.924: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012872176s
    Apr  6 10:20:52.924: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.1.148 on the node which pod4 resides and expect not scheduled 04/06/23 10:20:52.924
    Apr  6 10:20:52.942: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6744" to be "not pending"
    Apr  6 10:20:52.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.986015ms
    Apr  6 10:20:54.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011472189s
    Apr  6 10:20:56.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01254002s
    Apr  6 10:20:58.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011168366s
    Apr  6 10:21:00.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01164674s
    Apr  6 10:21:02.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011753959s
    Apr  6 10:21:04.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010754622s
    Apr  6 10:21:06.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011827976s
    Apr  6 10:21:08.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015230921s
    Apr  6 10:21:10.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013236657s
    Apr  6 10:21:12.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016181561s
    Apr  6 10:21:14.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012359174s
    Apr  6 10:21:16.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012400344s
    Apr  6 10:21:18.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01423215s
    Apr  6 10:21:20.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.014018985s
    Apr  6 10:21:22.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016409857s
    Apr  6 10:21:24.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012011534s
    Apr  6 10:21:26.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012835733s
    Apr  6 10:21:28.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012537242s
    Apr  6 10:21:30.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.015224991s
    Apr  6 10:21:32.963: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.02126414s
    Apr  6 10:21:34.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01136288s
    Apr  6 10:21:36.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01289284s
    Apr  6 10:21:38.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012122519s
    Apr  6 10:21:40.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015227613s
    Apr  6 10:21:42.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013709833s
    Apr  6 10:21:44.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010305219s
    Apr  6 10:21:46.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.013064536s
    Apr  6 10:21:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01267722s
    Apr  6 10:21:50.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012494346s
    Apr  6 10:21:52.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011323752s
    Apr  6 10:21:54.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023038219s
    Apr  6 10:21:56.968: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.026463036s
    Apr  6 10:21:58.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011995713s
    Apr  6 10:22:00.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.015919387s
    Apr  6 10:22:02.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012473105s
    Apr  6 10:22:04.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013237443s
    Apr  6 10:22:06.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.029774749s
    Apr  6 10:22:08.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015128714s
    Apr  6 10:22:10.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.013651082s
    Apr  6 10:22:12.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011127484s
    Apr  6 10:22:14.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015910314s
    Apr  6 10:22:16.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011977546s
    Apr  6 10:22:18.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014462122s
    Apr  6 10:22:20.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011159097s
    Apr  6 10:22:22.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011576567s
    Apr  6 10:22:24.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011469291s
    Apr  6 10:22:26.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01579726s
    Apr  6 10:22:28.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011737909s
    Apr  6 10:22:30.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013677578s
    Apr  6 10:22:32.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011504911s
    Apr  6 10:22:34.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012549029s
    Apr  6 10:22:36.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014307335s
    Apr  6 10:22:38.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011001655s
    Apr  6 10:22:40.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012019056s
    Apr  6 10:22:42.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013831124s
    Apr  6 10:22:44.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011753786s
    Apr  6 10:22:46.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015983141s
    Apr  6 10:22:48.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012026184s
    Apr  6 10:22:50.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013139052s
    Apr  6 10:22:52.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016013734s
    Apr  6 10:22:54.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.011436478s
    Apr  6 10:22:56.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013366187s
    Apr  6 10:22:58.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011465217s
    Apr  6 10:23:00.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011736019s
    Apr  6 10:23:02.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011880205s
    Apr  6 10:23:04.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.014358737s
    Apr  6 10:23:06.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.011964599s
    Apr  6 10:23:08.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.013117606s
    Apr  6 10:23:10.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.01265527s
    Apr  6 10:23:12.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.013158986s
    Apr  6 10:23:14.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.013274329s
    Apr  6 10:23:16.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.013323803s
    Apr  6 10:23:18.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.011328265s
    Apr  6 10:23:20.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.013124937s
    Apr  6 10:23:22.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012372644s
    Apr  6 10:23:24.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011669343s
    Apr  6 10:23:26.959: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.017285642s
    Apr  6 10:23:28.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.014895098s
    Apr  6 10:23:30.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012712276s
    Apr  6 10:23:32.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.014253425s
    Apr  6 10:23:34.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012801422s
    Apr  6 10:23:36.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.013066589s
    Apr  6 10:23:38.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010629917s
    Apr  6 10:23:40.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012463672s
    Apr  6 10:23:42.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015556986s
    Apr  6 10:23:44.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.014366292s
    Apr  6 10:23:46.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013650639s
    Apr  6 10:23:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.013303744s
    Apr  6 10:23:50.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.018073169s
    Apr  6 10:23:52.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011780224s
    Apr  6 10:23:54.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.011549838s
    Apr  6 10:23:56.962: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.020236641s
    Apr  6 10:23:58.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.015431891s
    Apr  6 10:24:00.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011050233s
    Apr  6 10:24:02.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013152271s
    Apr  6 10:24:04.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010429178s
    Apr  6 10:24:06.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.014761774s
    Apr  6 10:24:08.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.011044992s
    Apr  6 10:24:10.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.012580578s
    Apr  6 10:24:12.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011613324s
    Apr  6 10:24:14.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012612662s
    Apr  6 10:24:16.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.011773245s
    Apr  6 10:24:18.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011662753s
    Apr  6 10:24:20.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.012374994s
    Apr  6 10:24:22.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.014116436s
    Apr  6 10:24:24.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.011432008s
    Apr  6 10:24:26.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013364878s
    Apr  6 10:24:28.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.01303552s
    Apr  6 10:24:30.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.015452408s
    Apr  6 10:24:32.962: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019684633s
    Apr  6 10:24:34.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011914817s
    Apr  6 10:24:36.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.014650535s
    Apr  6 10:24:38.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.017876783s
    Apr  6 10:24:40.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.01367805s
    Apr  6 10:24:42.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014823193s
    Apr  6 10:24:44.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.01121519s
    Apr  6 10:24:46.964: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022057914s
    Apr  6 10:24:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012817001s
    Apr  6 10:24:50.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.014186451s
    Apr  6 10:24:52.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.012150213s
    Apr  6 10:24:54.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.012624718s
    Apr  6 10:24:56.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.01804248s
    Apr  6 10:24:58.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013084841s
    Apr  6 10:25:00.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012719494s
    Apr  6 10:25:02.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.015762773s
    Apr  6 10:25:04.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011632788s
    Apr  6 10:25:06.957: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.015423647s
    Apr  6 10:25:08.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.013804231s
    Apr  6 10:25:10.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.013405227s
    Apr  6 10:25:12.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.011192459s
    Apr  6 10:25:14.956: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.013713658s
    Apr  6 10:25:16.964: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.021821425s
    Apr  6 10:25:18.959: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.017183198s
    Apr  6 10:25:20.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016478969s
    Apr  6 10:25:22.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.012707903s
    Apr  6 10:25:24.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.010061416s
    Apr  6 10:25:26.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013450874s
    Apr  6 10:25:28.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.011685382s
    Apr  6 10:25:30.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.011777569s
    Apr  6 10:25:32.959: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.017406324s
    Apr  6 10:25:34.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.011409104s
    Apr  6 10:25:36.960: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.01771776s
    Apr  6 10:25:38.962: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.019853567s
    Apr  6 10:25:40.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.013441035s
    Apr  6 10:25:42.961: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.019220406s
    Apr  6 10:25:44.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011685937s
    Apr  6 10:25:46.954: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.011936426s
    Apr  6 10:25:48.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013595039s
    Apr  6 10:25:50.955: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012746292s
    Apr  6 10:25:52.953: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011293759s
    Apr  6 10:25:52.958: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015857793s
    STEP: removing the label kubernetes.io/e2e-b06e00b0-8998-4cc8-9ff3-7906dcb27bac off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 10:25:52.961
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b06e00b0-8998-4cc8-9ff3-7906dcb27bac 04/06/23 10:25:52.976
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:25:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6744" for this suite. 04/06/23 10:25:52.996
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:25:53.007
Apr  6 10:25:53.008: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:25:53.009
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:25:53.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:25:53.05
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:25:53.092
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:25:53.806
STEP: Deploying the webhook pod 04/06/23 10:25:53.817
STEP: Wait for the deployment to be ready 04/06/23 10:25:53.831
Apr  6 10:25:53.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:25:55.865
STEP: Verifying the service has paired with the endpoint 04/06/23 10:25:55.881
Apr  6 10:25:56.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 04/06/23 10:25:56.989
STEP: Creating a configMap that should be mutated 04/06/23 10:25:57.13
STEP: Deleting the collection of validation webhooks 04/06/23 10:25:58.012
STEP: Creating a configMap that should not be mutated 04/06/23 10:25:58.059
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:25:58.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1387" for this suite. 04/06/23 10:25:58.08
STEP: Destroying namespace "webhook-1387-markers" for this suite. 04/06/23 10:25:58.086
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":9,"skipped":106,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.126 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:25:53.007
    Apr  6 10:25:53.008: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:25:53.009
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:25:53.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:25:53.05
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:25:53.092
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:25:53.806
    STEP: Deploying the webhook pod 04/06/23 10:25:53.817
    STEP: Wait for the deployment to be ready 04/06/23 10:25:53.831
    Apr  6 10:25:53.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:25:55.865
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:25:55.881
    Apr  6 10:25:56.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 04/06/23 10:25:56.989
    STEP: Creating a configMap that should be mutated 04/06/23 10:25:57.13
    STEP: Deleting the collection of validation webhooks 04/06/23 10:25:58.012
    STEP: Creating a configMap that should not be mutated 04/06/23 10:25:58.059
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:25:58.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1387" for this suite. 04/06/23 10:25:58.08
    STEP: Destroying namespace "webhook-1387-markers" for this suite. 04/06/23 10:25:58.086
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:25:58.134
Apr  6 10:25:58.134: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:25:58.135
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:25:58.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:25:58.169
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 10:25:58.176
Apr  6 10:25:58.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7036 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr  6 10:25:58.499: INFO: stderr: ""
Apr  6 10:25:58.499: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 04/06/23 10:25:58.499
Apr  6 10:25:58.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7036 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Apr  6 10:25:59.540: INFO: stderr: ""
Apr  6 10:25:59.540: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 10:25:59.54
Apr  6 10:25:59.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7036 delete pods e2e-test-httpd-pod'
Apr  6 10:26:06.239: INFO: stderr: ""
Apr  6 10:26:06.239: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:26:06.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7036" for this suite. 04/06/23 10:26:06.249
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":10,"skipped":124,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.122 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:25:58.134
    Apr  6 10:25:58.134: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:25:58.135
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:25:58.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:25:58.169
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 10:25:58.176
    Apr  6 10:25:58.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7036 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr  6 10:25:58.499: INFO: stderr: ""
    Apr  6 10:25:58.499: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 04/06/23 10:25:58.499
    Apr  6 10:25:58.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7036 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Apr  6 10:25:59.540: INFO: stderr: ""
    Apr  6 10:25:59.540: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 10:25:59.54
    Apr  6 10:25:59.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7036 delete pods e2e-test-httpd-pod'
    Apr  6 10:26:06.239: INFO: stderr: ""
    Apr  6 10:26:06.239: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:26:06.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7036" for this suite. 04/06/23 10:26:06.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:06.258
Apr  6 10:26:06.258: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 10:26:06.259
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:06.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:06.281
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-2e701e01-34fa-428f-9420-2bfd96cf78d6 04/06/23 10:26:06.287
STEP: Creating a pod to test consume secrets 04/06/23 10:26:06.291
Apr  6 10:26:06.313: INFO: Waiting up to 5m0s for pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7" in namespace "secrets-8605" to be "Succeeded or Failed"
Apr  6 10:26:06.318: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650322ms
Apr  6 10:26:08.324: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01072907s
Apr  6 10:26:10.325: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011916356s
STEP: Saw pod success 04/06/23 10:26:10.325
Apr  6 10:26:10.325: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7" satisfied condition "Succeeded or Failed"
Apr  6 10:26:10.329: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7 container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 10:26:10.383
Apr  6 10:26:10.392: INFO: Waiting for pod pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7 to disappear
Apr  6 10:26:10.408: INFO: Pod pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 10:26:10.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8605" for this suite. 04/06/23 10:26:10.415
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":11,"skipped":151,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:06.258
    Apr  6 10:26:06.258: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 10:26:06.259
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:06.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:06.281
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-2e701e01-34fa-428f-9420-2bfd96cf78d6 04/06/23 10:26:06.287
    STEP: Creating a pod to test consume secrets 04/06/23 10:26:06.291
    Apr  6 10:26:06.313: INFO: Waiting up to 5m0s for pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7" in namespace "secrets-8605" to be "Succeeded or Failed"
    Apr  6 10:26:06.318: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650322ms
    Apr  6 10:26:08.324: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01072907s
    Apr  6 10:26:10.325: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011916356s
    STEP: Saw pod success 04/06/23 10:26:10.325
    Apr  6 10:26:10.325: INFO: Pod "pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7" satisfied condition "Succeeded or Failed"
    Apr  6 10:26:10.329: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7 container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:26:10.383
    Apr  6 10:26:10.392: INFO: Waiting for pod pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7 to disappear
    Apr  6 10:26:10.408: INFO: Pod pod-secrets-fff0f2bb-0c09-460d-9272-9c7aadef3cc7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 10:26:10.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8605" for this suite. 04/06/23 10:26:10.415
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:10.425
Apr  6 10:26:10.425: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:26:10.426
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:10.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:10.447
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 04/06/23 10:26:10.456
STEP: fetching the ConfigMap 04/06/23 10:26:10.461
STEP: patching the ConfigMap 04/06/23 10:26:10.47
STEP: listing all ConfigMaps in all namespaces with a label selector 04/06/23 10:26:10.48
STEP: deleting the ConfigMap by collection with a label selector 04/06/23 10:26:10.49
STEP: listing all ConfigMaps in test namespace 04/06/23 10:26:10.502
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:26:10.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7900" for this suite. 04/06/23 10:26:10.521
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":12,"skipped":154,"failed":0}
------------------------------
â€¢ [0.113 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:10.425
    Apr  6 10:26:10.425: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:26:10.426
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:10.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:10.447
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 04/06/23 10:26:10.456
    STEP: fetching the ConfigMap 04/06/23 10:26:10.461
    STEP: patching the ConfigMap 04/06/23 10:26:10.47
    STEP: listing all ConfigMaps in all namespaces with a label selector 04/06/23 10:26:10.48
    STEP: deleting the ConfigMap by collection with a label selector 04/06/23 10:26:10.49
    STEP: listing all ConfigMaps in test namespace 04/06/23 10:26:10.502
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:26:10.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7900" for this suite. 04/06/23 10:26:10.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:10.539
Apr  6 10:26:10.539: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename subpath 04/06/23 10:26:10.54
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:10.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:10.56
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/06/23 10:26:10.566
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-nvjh 04/06/23 10:26:10.578
STEP: Creating a pod to test atomic-volume-subpath 04/06/23 10:26:10.579
Apr  6 10:26:10.588: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nvjh" in namespace "subpath-718" to be "Succeeded or Failed"
Apr  6 10:26:10.591: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.790641ms
Apr  6 10:26:12.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 2.011887192s
Apr  6 10:26:14.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.012191744s
Apr  6 10:26:16.598: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.010392088s
Apr  6 10:26:18.598: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.010619352s
Apr  6 10:26:20.609: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.021829889s
Apr  6 10:26:22.609: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.020882874s
Apr  6 10:26:24.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.012429477s
Apr  6 10:26:26.605: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.017643729s
Apr  6 10:26:28.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.012188801s
Apr  6 10:26:30.608: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.020769474s
Apr  6 10:26:32.601: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=false. Elapsed: 22.01340171s
Apr  6 10:26:34.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012489322s
STEP: Saw pod success 04/06/23 10:26:34.6
Apr  6 10:26:34.600: INFO: Pod "pod-subpath-test-secret-nvjh" satisfied condition "Succeeded or Failed"
Apr  6 10:26:34.605: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-subpath-test-secret-nvjh container test-container-subpath-secret-nvjh: <nil>
STEP: delete the pod 04/06/23 10:26:34.63
Apr  6 10:26:34.640: INFO: Waiting for pod pod-subpath-test-secret-nvjh to disappear
Apr  6 10:26:34.645: INFO: Pod pod-subpath-test-secret-nvjh no longer exists
STEP: Deleting pod pod-subpath-test-secret-nvjh 04/06/23 10:26:34.648
Apr  6 10:26:34.648: INFO: Deleting pod "pod-subpath-test-secret-nvjh" in namespace "subpath-718"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Apr  6 10:26:34.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-718" for this suite. 04/06/23 10:26:34.661
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":13,"skipped":168,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.129 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:10.539
    Apr  6 10:26:10.539: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename subpath 04/06/23 10:26:10.54
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:10.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:10.56
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/06/23 10:26:10.566
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-nvjh 04/06/23 10:26:10.578
    STEP: Creating a pod to test atomic-volume-subpath 04/06/23 10:26:10.579
    Apr  6 10:26:10.588: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nvjh" in namespace "subpath-718" to be "Succeeded or Failed"
    Apr  6 10:26:10.591: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.790641ms
    Apr  6 10:26:12.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 2.011887192s
    Apr  6 10:26:14.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.012191744s
    Apr  6 10:26:16.598: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.010392088s
    Apr  6 10:26:18.598: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.010619352s
    Apr  6 10:26:20.609: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.021829889s
    Apr  6 10:26:22.609: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.020882874s
    Apr  6 10:26:24.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.012429477s
    Apr  6 10:26:26.605: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.017643729s
    Apr  6 10:26:28.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.012188801s
    Apr  6 10:26:30.608: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.020769474s
    Apr  6 10:26:32.601: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Running", Reason="", readiness=false. Elapsed: 22.01340171s
    Apr  6 10:26:34.600: INFO: Pod "pod-subpath-test-secret-nvjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012489322s
    STEP: Saw pod success 04/06/23 10:26:34.6
    Apr  6 10:26:34.600: INFO: Pod "pod-subpath-test-secret-nvjh" satisfied condition "Succeeded or Failed"
    Apr  6 10:26:34.605: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-subpath-test-secret-nvjh container test-container-subpath-secret-nvjh: <nil>
    STEP: delete the pod 04/06/23 10:26:34.63
    Apr  6 10:26:34.640: INFO: Waiting for pod pod-subpath-test-secret-nvjh to disappear
    Apr  6 10:26:34.645: INFO: Pod pod-subpath-test-secret-nvjh no longer exists
    STEP: Deleting pod pod-subpath-test-secret-nvjh 04/06/23 10:26:34.648
    Apr  6 10:26:34.648: INFO: Deleting pod "pod-subpath-test-secret-nvjh" in namespace "subpath-718"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Apr  6 10:26:34.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-718" for this suite. 04/06/23 10:26:34.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:34.675
Apr  6 10:26:34.675: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:26:34.678
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:34.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:34.713
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-c19d652f-d443-4b54-b32e-f10789c9203d 04/06/23 10:26:34.728
STEP: Creating secret with name s-test-opt-upd-2a8fb667-be00-4f55-900f-8e78617f5847 04/06/23 10:26:34.74
STEP: Creating the pod 04/06/23 10:26:34.745
Apr  6 10:26:34.759: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4" in namespace "projected-5238" to be "running and ready"
Apr  6 10:26:34.764: INFO: Pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062349ms
Apr  6 10:26:34.764: INFO: The phase of Pod pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:26:36.773: INFO: Pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013563846s
Apr  6 10:26:36.773: INFO: The phase of Pod pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4 is Running (Ready = true)
Apr  6 10:26:36.773: INFO: Pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c19d652f-d443-4b54-b32e-f10789c9203d 04/06/23 10:26:36.971
STEP: Updating secret s-test-opt-upd-2a8fb667-be00-4f55-900f-8e78617f5847 04/06/23 10:26:36.978
STEP: Creating secret with name s-test-opt-create-b03d705f-9c0e-4eca-a349-c225b12b25f1 04/06/23 10:26:36.984
STEP: waiting to observe update in volume 04/06/23 10:26:36.989
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Apr  6 10:26:39.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5238" for this suite. 04/06/23 10:26:39.257
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":14,"skipped":250,"failed":0}
------------------------------
â€¢ [4.589 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:34.675
    Apr  6 10:26:34.675: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:26:34.678
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:34.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:34.713
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-c19d652f-d443-4b54-b32e-f10789c9203d 04/06/23 10:26:34.728
    STEP: Creating secret with name s-test-opt-upd-2a8fb667-be00-4f55-900f-8e78617f5847 04/06/23 10:26:34.74
    STEP: Creating the pod 04/06/23 10:26:34.745
    Apr  6 10:26:34.759: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4" in namespace "projected-5238" to be "running and ready"
    Apr  6 10:26:34.764: INFO: Pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062349ms
    Apr  6 10:26:34.764: INFO: The phase of Pod pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:26:36.773: INFO: Pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013563846s
    Apr  6 10:26:36.773: INFO: The phase of Pod pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4 is Running (Ready = true)
    Apr  6 10:26:36.773: INFO: Pod "pod-projected-secrets-a2356e87-f61c-4397-bfdc-12af62b532d4" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c19d652f-d443-4b54-b32e-f10789c9203d 04/06/23 10:26:36.971
    STEP: Updating secret s-test-opt-upd-2a8fb667-be00-4f55-900f-8e78617f5847 04/06/23 10:26:36.978
    STEP: Creating secret with name s-test-opt-create-b03d705f-9c0e-4eca-a349-c225b12b25f1 04/06/23 10:26:36.984
    STEP: waiting to observe update in volume 04/06/23 10:26:36.989
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Apr  6 10:26:39.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5238" for this suite. 04/06/23 10:26:39.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:39.265
Apr  6 10:26:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:26:39.266
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:39.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:39.293
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:26:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7604" for this suite. 04/06/23 10:26:39.364
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":15,"skipped":255,"failed":0}
------------------------------
â€¢ [0.107 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:39.265
    Apr  6 10:26:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:26:39.266
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:39.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:39.293
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:26:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7604" for this suite. 04/06/23 10:26:39.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:39.373
Apr  6 10:26:39.373: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replicaset 04/06/23 10:26:39.374
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:39.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:39.404
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Apr  6 10:26:39.411: INFO: Creating ReplicaSet my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01
Apr  6 10:26:39.428: INFO: Pod name my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01: Found 0 pods out of 1
Apr  6 10:26:44.439: INFO: Pod name my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01: Found 1 pods out of 1
Apr  6 10:26:44.439: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01" is running
Apr  6 10:26:44.439: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77" in namespace "replicaset-3383" to be "running"
Apr  6 10:26:44.444: INFO: Pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77": Phase="Running", Reason="", readiness=true. Elapsed: 5.754728ms
Apr  6 10:26:44.445: INFO: Pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77" satisfied condition "running"
Apr  6 10:26:44.445: INFO: Pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:39 +0000 UTC Reason: Message:}])
Apr  6 10:26:44.445: INFO: Trying to dial the pod
Apr  6 10:26:49.573: INFO: Controller my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01: Got expected result from replica 1 [my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77]: "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Apr  6 10:26:49.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3383" for this suite. 04/06/23 10:26:49.583
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":16,"skipped":265,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.220 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:39.373
    Apr  6 10:26:39.373: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replicaset 04/06/23 10:26:39.374
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:39.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:39.404
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Apr  6 10:26:39.411: INFO: Creating ReplicaSet my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01
    Apr  6 10:26:39.428: INFO: Pod name my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01: Found 0 pods out of 1
    Apr  6 10:26:44.439: INFO: Pod name my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01: Found 1 pods out of 1
    Apr  6 10:26:44.439: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01" is running
    Apr  6 10:26:44.439: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77" in namespace "replicaset-3383" to be "running"
    Apr  6 10:26:44.444: INFO: Pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77": Phase="Running", Reason="", readiness=true. Elapsed: 5.754728ms
    Apr  6 10:26:44.445: INFO: Pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77" satisfied condition "running"
    Apr  6 10:26:44.445: INFO: Pod "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:26:39 +0000 UTC Reason: Message:}])
    Apr  6 10:26:44.445: INFO: Trying to dial the pod
    Apr  6 10:26:49.573: INFO: Controller my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01: Got expected result from replica 1 [my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77]: "my-hostname-basic-4f39d4df-9d7b-4661-a895-11077d77fd01-p7w77", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Apr  6 10:26:49.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3383" for this suite. 04/06/23 10:26:49.583
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:49.593
Apr  6 10:26:49.593: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 10:26:49.594
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:49.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:49.636
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 04/06/23 10:26:49.649
Apr  6 10:26:49.649: INFO: Creating simple deployment test-deployment-cvhzv
Apr  6 10:26:49.665: INFO: deployment "test-deployment-cvhzv" doesn't have the required revision set
Apr  6 10:26:51.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-cvhzv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 10:26:53.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-cvhzv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 04/06/23 10:26:55.742
Apr  6 10:26:55.755: INFO: Deployment test-deployment-cvhzv has Conditions: [{Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 04/06/23 10:26:55.756
Apr  6 10:26:55.777: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-cvhzv-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 04/06/23 10:26:55.777
Apr  6 10:26:55.783: INFO: Observed &Deployment event: ADDED
Apr  6 10:26:55.784: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
Apr  6 10:26:55.792: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 10:26:55.794: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvhzv-777898ffcc" is progressing.}
Apr  6 10:26:55.801: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.801: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 10:26:55.801: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
Apr  6 10:26:55.801: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.801: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 10:26:55.802: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
Apr  6 10:26:55.802: INFO: Found Deployment test-deployment-cvhzv in namespace deployment-2225 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 10:26:55.802: INFO: Deployment test-deployment-cvhzv has an updated status
STEP: patching the Statefulset Status 04/06/23 10:26:55.802
Apr  6 10:26:55.802: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr  6 10:26:55.813: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 04/06/23 10:26:55.813
Apr  6 10:26:55.819: INFO: Observed &Deployment event: ADDED
Apr  6 10:26:55.819: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
Apr  6 10:26:55.820: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 10:26:55.820: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvhzv-777898ffcc" is progressing.}
Apr  6 10:26:55.820: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
Apr  6 10:26:55.821: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.821: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr  6 10:26:55.821: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
Apr  6 10:26:55.821: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 10:26:55.821: INFO: Observed &Deployment event: MODIFIED
Apr  6 10:26:55.821: INFO: Found deployment test-deployment-cvhzv in namespace deployment-2225 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Apr  6 10:26:55.821: INFO: Deployment test-deployment-cvhzv has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 10:26:55.836: INFO: Deployment "test-deployment-cvhzv":
&Deployment{ObjectMeta:{test-deployment-cvhzv  deployment-2225  1ff6e1fc-1960-4323-b0fc-417af834ea45 9650 1 2023-04-06 10:26:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-06 10:26:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-06 10:26:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-06 10:26:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025d6ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-cvhzv-777898ffcc",LastUpdateTime:2023-04-06 10:26:55 +0000 UTC,LastTransitionTime:2023-04-06 10:26:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 10:26:55.851: INFO: New ReplicaSet "test-deployment-cvhzv-777898ffcc" of Deployment "test-deployment-cvhzv":
&ReplicaSet{ObjectMeta:{test-deployment-cvhzv-777898ffcc  deployment-2225  37733099-6ac0-45b1-ac40-398f135c809f 9634 1 2023-04-06 10:26:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-cvhzv 1ff6e1fc-1960-4323-b0fc-417af834ea45 0xc0025d7257 0xc0025d7258}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:26:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ff6e1fc-1960-4323-b0fc-417af834ea45\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:26:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025d7308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 10:26:55.857: INFO: Pod "test-deployment-cvhzv-777898ffcc-hhfmz" is available:
&Pod{ObjectMeta:{test-deployment-cvhzv-777898ffcc-hhfmz test-deployment-cvhzv-777898ffcc- deployment-2225  b1216e93-336e-4314-a716-5b8d235f068b 9633 0 2023-04-06 10:26:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:c7d6f88fc03535380113a1b7480a58df4b3f73f89809d5168a4887000229e017 cni.projectcalico.org/podIP:10.96.1.10/32 cni.projectcalico.org/podIPs:10.96.1.10/32] [{apps/v1 ReplicaSet test-deployment-cvhzv-777898ffcc 37733099-6ac0-45b1-ac40-398f135c809f 0xc0025d76b7 0xc0025d76b8}] [] [{kube-controller-manager Update v1 2023-04-06 10:26:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37733099-6ac0-45b1-ac40-398f135c809f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 10:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 10:26:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmhfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmhfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.10,StartTime:2023-04-06 10:26:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 10:26:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c0112f92380fef6faa360a74d52404406c5f5ae79260e2a17da0d58fbbde51e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 10:26:55.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2225" for this suite. 04/06/23 10:26:55.867
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":17,"skipped":268,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.288 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:49.593
    Apr  6 10:26:49.593: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 10:26:49.594
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:49.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:49.636
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 04/06/23 10:26:49.649
    Apr  6 10:26:49.649: INFO: Creating simple deployment test-deployment-cvhzv
    Apr  6 10:26:49.665: INFO: deployment "test-deployment-cvhzv" doesn't have the required revision set
    Apr  6 10:26:51.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-cvhzv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 10:26:53.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-cvhzv-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 04/06/23 10:26:55.742
    Apr  6 10:26:55.755: INFO: Deployment test-deployment-cvhzv has Conditions: [{Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 04/06/23 10:26:55.756
    Apr  6 10:26:55.777: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 26, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 26, 49, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-cvhzv-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 04/06/23 10:26:55.777
    Apr  6 10:26:55.783: INFO: Observed &Deployment event: ADDED
    Apr  6 10:26:55.784: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
    Apr  6 10:26:55.792: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
    Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr  6 10:26:55.794: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr  6 10:26:55.794: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvhzv-777898ffcc" is progressing.}
    Apr  6 10:26:55.801: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.801: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr  6 10:26:55.801: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
    Apr  6 10:26:55.801: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.801: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr  6 10:26:55.802: INFO: Observed Deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
    Apr  6 10:26:55.802: INFO: Found Deployment test-deployment-cvhzv in namespace deployment-2225 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr  6 10:26:55.802: INFO: Deployment test-deployment-cvhzv has an updated status
    STEP: patching the Statefulset Status 04/06/23 10:26:55.802
    Apr  6 10:26:55.802: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr  6 10:26:55.813: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 04/06/23 10:26:55.813
    Apr  6 10:26:55.819: INFO: Observed &Deployment event: ADDED
    Apr  6 10:26:55.819: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
    Apr  6 10:26:55.820: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvhzv-777898ffcc"}
    Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr  6 10:26:55.820: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:49 +0000 UTC 2023-04-06 10:26:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvhzv-777898ffcc" is progressing.}
    Apr  6 10:26:55.820: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr  6 10:26:55.820: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
    Apr  6 10:26:55.821: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.821: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr  6 10:26:55.821: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-06 10:26:54 +0000 UTC 2023-04-06 10:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvhzv-777898ffcc" has successfully progressed.}
    Apr  6 10:26:55.821: INFO: Observed deployment test-deployment-cvhzv in namespace deployment-2225 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr  6 10:26:55.821: INFO: Observed &Deployment event: MODIFIED
    Apr  6 10:26:55.821: INFO: Found deployment test-deployment-cvhzv in namespace deployment-2225 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Apr  6 10:26:55.821: INFO: Deployment test-deployment-cvhzv has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 10:26:55.836: INFO: Deployment "test-deployment-cvhzv":
    &Deployment{ObjectMeta:{test-deployment-cvhzv  deployment-2225  1ff6e1fc-1960-4323-b0fc-417af834ea45 9650 1 2023-04-06 10:26:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-06 10:26:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-06 10:26:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-06 10:26:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025d6ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-cvhzv-777898ffcc",LastUpdateTime:2023-04-06 10:26:55 +0000 UTC,LastTransitionTime:2023-04-06 10:26:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr  6 10:26:55.851: INFO: New ReplicaSet "test-deployment-cvhzv-777898ffcc" of Deployment "test-deployment-cvhzv":
    &ReplicaSet{ObjectMeta:{test-deployment-cvhzv-777898ffcc  deployment-2225  37733099-6ac0-45b1-ac40-398f135c809f 9634 1 2023-04-06 10:26:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-cvhzv 1ff6e1fc-1960-4323-b0fc-417af834ea45 0xc0025d7257 0xc0025d7258}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:26:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ff6e1fc-1960-4323-b0fc-417af834ea45\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:26:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025d7308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 10:26:55.857: INFO: Pod "test-deployment-cvhzv-777898ffcc-hhfmz" is available:
    &Pod{ObjectMeta:{test-deployment-cvhzv-777898ffcc-hhfmz test-deployment-cvhzv-777898ffcc- deployment-2225  b1216e93-336e-4314-a716-5b8d235f068b 9633 0 2023-04-06 10:26:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:c7d6f88fc03535380113a1b7480a58df4b3f73f89809d5168a4887000229e017 cni.projectcalico.org/podIP:10.96.1.10/32 cni.projectcalico.org/podIPs:10.96.1.10/32] [{apps/v1 ReplicaSet test-deployment-cvhzv-777898ffcc 37733099-6ac0-45b1-ac40-398f135c809f 0xc0025d76b7 0xc0025d76b8}] [] [{kube-controller-manager Update v1 2023-04-06 10:26:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37733099-6ac0-45b1-ac40-398f135c809f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 10:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 10:26:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmhfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmhfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:26:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.10,StartTime:2023-04-06 10:26:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 10:26:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c0112f92380fef6faa360a74d52404406c5f5ae79260e2a17da0d58fbbde51e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 10:26:55.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2225" for this suite. 04/06/23 10:26:55.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:26:55.883
Apr  6 10:26:55.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 10:26:55.884
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:55.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:55.918
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/06/23 10:26:55.926
Apr  6 10:26:55.944: INFO: Waiting up to 5m0s for pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321" in namespace "emptydir-1637" to be "Succeeded or Failed"
Apr  6 10:26:55.949: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321": Phase="Pending", Reason="", readiness=false. Elapsed: 5.410494ms
Apr  6 10:26:57.956: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01173869s
Apr  6 10:26:59.957: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012600636s
STEP: Saw pod success 04/06/23 10:26:59.962
Apr  6 10:26:59.963: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321" satisfied condition "Succeeded or Failed"
Apr  6 10:26:59.971: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321 container test-container: <nil>
STEP: delete the pod 04/06/23 10:27:00.023
Apr  6 10:27:00.036: INFO: Waiting for pod pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321 to disappear
Apr  6 10:27:00.041: INFO: Pod pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 10:27:00.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1637" for this suite. 04/06/23 10:27:00.049
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":18,"skipped":335,"failed":0}
------------------------------
â€¢ [4.173 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:26:55.883
    Apr  6 10:26:55.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 10:26:55.884
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:26:55.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:26:55.918
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/06/23 10:26:55.926
    Apr  6 10:26:55.944: INFO: Waiting up to 5m0s for pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321" in namespace "emptydir-1637" to be "Succeeded or Failed"
    Apr  6 10:26:55.949: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321": Phase="Pending", Reason="", readiness=false. Elapsed: 5.410494ms
    Apr  6 10:26:57.956: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01173869s
    Apr  6 10:26:59.957: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012600636s
    STEP: Saw pod success 04/06/23 10:26:59.962
    Apr  6 10:26:59.963: INFO: Pod "pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321" satisfied condition "Succeeded or Failed"
    Apr  6 10:26:59.971: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321 container test-container: <nil>
    STEP: delete the pod 04/06/23 10:27:00.023
    Apr  6 10:27:00.036: INFO: Waiting for pod pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321 to disappear
    Apr  6 10:27:00.041: INFO: Pod pod-f1db42b6-ac87-4b54-9d1b-dea5260d5321 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 10:27:00.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1637" for this suite. 04/06/23 10:27:00.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:27:00.057
Apr  6 10:27:00.057: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:27:00.059
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:27:00.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:27:00.085
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-0e9c7538-681c-4f17-92c1-00e9a92620f3 04/06/23 10:27:00.1
STEP: Creating configMap with name cm-test-opt-upd-43ed0ccb-a48e-44c0-a203-39be7ebeb09f 04/06/23 10:27:00.107
STEP: Creating the pod 04/06/23 10:27:00.113
Apr  6 10:27:00.127: INFO: Waiting up to 5m0s for pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7" in namespace "configmap-8377" to be "running and ready"
Apr  6 10:27:00.134: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.639378ms
Apr  6 10:27:00.134: INFO: The phase of Pod pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:27:02.153: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025744133s
Apr  6 10:27:02.153: INFO: The phase of Pod pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:27:04.141: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7": Phase="Running", Reason="", readiness=true. Elapsed: 4.013291382s
Apr  6 10:27:04.141: INFO: The phase of Pod pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7 is Running (Ready = true)
Apr  6 10:27:04.141: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0e9c7538-681c-4f17-92c1-00e9a92620f3 04/06/23 10:27:04.343
STEP: Updating configmap cm-test-opt-upd-43ed0ccb-a48e-44c0-a203-39be7ebeb09f 04/06/23 10:27:04.35
STEP: Creating configMap with name cm-test-opt-create-12719409-9bed-4bdf-afd1-362f17b9d716 04/06/23 10:27:04.354
STEP: waiting to observe update in volume 04/06/23 10:27:04.358
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:28:17.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8377" for this suite. 04/06/23 10:28:17.362
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":19,"skipped":347,"failed":0}
------------------------------
â€¢ [SLOW TEST] [77.312 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:27:00.057
    Apr  6 10:27:00.057: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:27:00.059
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:27:00.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:27:00.085
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-0e9c7538-681c-4f17-92c1-00e9a92620f3 04/06/23 10:27:00.1
    STEP: Creating configMap with name cm-test-opt-upd-43ed0ccb-a48e-44c0-a203-39be7ebeb09f 04/06/23 10:27:00.107
    STEP: Creating the pod 04/06/23 10:27:00.113
    Apr  6 10:27:00.127: INFO: Waiting up to 5m0s for pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7" in namespace "configmap-8377" to be "running and ready"
    Apr  6 10:27:00.134: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.639378ms
    Apr  6 10:27:00.134: INFO: The phase of Pod pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:27:02.153: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025744133s
    Apr  6 10:27:02.153: INFO: The phase of Pod pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:27:04.141: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7": Phase="Running", Reason="", readiness=true. Elapsed: 4.013291382s
    Apr  6 10:27:04.141: INFO: The phase of Pod pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7 is Running (Ready = true)
    Apr  6 10:27:04.141: INFO: Pod "pod-configmaps-bbf1014f-d3bf-435f-bb01-28812544c7c7" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0e9c7538-681c-4f17-92c1-00e9a92620f3 04/06/23 10:27:04.343
    STEP: Updating configmap cm-test-opt-upd-43ed0ccb-a48e-44c0-a203-39be7ebeb09f 04/06/23 10:27:04.35
    STEP: Creating configMap with name cm-test-opt-create-12719409-9bed-4bdf-afd1-362f17b9d716 04/06/23 10:27:04.354
    STEP: waiting to observe update in volume 04/06/23 10:27:04.358
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:28:17.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8377" for this suite. 04/06/23 10:28:17.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:28:17.37
Apr  6 10:28:17.370: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 10:28:17.373
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:28:17.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:28:17.399
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Apr  6 10:28:17.410: INFO: Creating simple deployment test-new-deployment
Apr  6 10:28:17.432: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 04/06/23 10:28:19.472
STEP: updating a scale subresource 04/06/23 10:28:19.477
STEP: verifying the deployment Spec.Replicas was modified 04/06/23 10:28:19.485
STEP: Patch a scale subresource 04/06/23 10:28:19.492
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 10:28:19.524: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4907  98098fc1-b1c2-4ef7-bf77-4824e2629e2b 10024 3 2023-04-06 10:28:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-06 10:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401e1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-04-06 10:28:18 +0000 UTC,LastTransitionTime:2023-04-06 10:28:17 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-06 10:28:19 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 10:28:19.531: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-4907  b680a077-03f8-4836-b298-f11e0c813c84 10027 3 2023-04-06 10:28:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 98098fc1-b1c2-4ef7-bf77-4824e2629e2b 0xc003fb0897 0xc003fb0898}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98098fc1-b1c2-4ef7-bf77-4824e2629e2b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fb0928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 10:28:19.540: INFO: Pod "test-new-deployment-845c8977d9-7x5h9" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-7x5h9 test-new-deployment-845c8977d9- deployment-4907  3f17991e-8c60-47ce-a788-706f1b1f3188 10026 0 2023-04-06 10:28:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401e5e7 0xc00401e5e8}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blfkn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blfkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:,StartTime:2023-04-06 10:28:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 10:28:19.541: INFO: Pod "test-new-deployment-845c8977d9-bppdw" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-bppdw test-new-deployment-845c8977d9- deployment-4907  d8314956-d25d-4287-848c-601a3dcd8309 10032 0 2023-04-06 10:28:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401e7a7 0xc00401e7a8}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sfpk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sfpk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 10:28:19.541: INFO: Pod "test-new-deployment-845c8977d9-bsrdg" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-bsrdg test-new-deployment-845c8977d9- deployment-4907  0b34c73b-7bf1-4adb-80a2-96d2d2e258d9 10011 0 2023-04-06 10:28:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2a8af556983e17970da969eedb98712bbb76575a9405bc9cac0893f7979af646 cni.projectcalico.org/podIP:10.96.1.11/32 cni.projectcalico.org/podIPs:10.96.1.11/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401e977 0xc00401e978}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 10:28:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 10:28:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g45vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g45vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.11,StartTime:2023-04-06 10:28:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 10:28:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9f55775557b983caa6a2228814a7363eb3ad268d547571d90c3342fa0a571312,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 10:28:19.541: INFO: Pod "test-new-deployment-845c8977d9-qw4v2" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-qw4v2 test-new-deployment-845c8977d9- deployment-4907  479273a3-c343-412d-8cb5-c8677176d5d2 10031 0 2023-04-06 10:28:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401ef50 0xc00401ef51}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-488w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-488w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 10:28:19.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4907" for this suite. 04/06/23 10:28:19.553
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":20,"skipped":366,"failed":0}
------------------------------
â€¢ [2.188 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:28:17.37
    Apr  6 10:28:17.370: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 10:28:17.373
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:28:17.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:28:17.399
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Apr  6 10:28:17.410: INFO: Creating simple deployment test-new-deployment
    Apr  6 10:28:17.432: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 04/06/23 10:28:19.472
    STEP: updating a scale subresource 04/06/23 10:28:19.477
    STEP: verifying the deployment Spec.Replicas was modified 04/06/23 10:28:19.485
    STEP: Patch a scale subresource 04/06/23 10:28:19.492
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 10:28:19.524: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4907  98098fc1-b1c2-4ef7-bf77-4824e2629e2b 10024 3 2023-04-06 10:28:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-06 10:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401e1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-04-06 10:28:18 +0000 UTC,LastTransitionTime:2023-04-06 10:28:17 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-06 10:28:19 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr  6 10:28:19.531: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-4907  b680a077-03f8-4836-b298-f11e0c813c84 10027 3 2023-04-06 10:28:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 98098fc1-b1c2-4ef7-bf77-4824e2629e2b 0xc003fb0897 0xc003fb0898}] [] [{kube-controller-manager Update apps/v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98098fc1-b1c2-4ef7-bf77-4824e2629e2b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fb0928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 10:28:19.540: INFO: Pod "test-new-deployment-845c8977d9-7x5h9" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-7x5h9 test-new-deployment-845c8977d9- deployment-4907  3f17991e-8c60-47ce-a788-706f1b1f3188 10026 0 2023-04-06 10:28:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401e5e7 0xc00401e5e8}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blfkn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blfkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:,StartTime:2023-04-06 10:28:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 10:28:19.541: INFO: Pod "test-new-deployment-845c8977d9-bppdw" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-bppdw test-new-deployment-845c8977d9- deployment-4907  d8314956-d25d-4287-848c-601a3dcd8309 10032 0 2023-04-06 10:28:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401e7a7 0xc00401e7a8}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sfpk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sfpk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 10:28:19.541: INFO: Pod "test-new-deployment-845c8977d9-bsrdg" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-bsrdg test-new-deployment-845c8977d9- deployment-4907  0b34c73b-7bf1-4adb-80a2-96d2d2e258d9 10011 0 2023-04-06 10:28:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2a8af556983e17970da969eedb98712bbb76575a9405bc9cac0893f7979af646 cni.projectcalico.org/podIP:10.96.1.11/32 cni.projectcalico.org/podIPs:10.96.1.11/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401e977 0xc00401e978}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 10:28:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 10:28:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g45vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g45vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.11,StartTime:2023-04-06 10:28:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 10:28:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9f55775557b983caa6a2228814a7363eb3ad268d547571d90c3342fa0a571312,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 10:28:19.541: INFO: Pod "test-new-deployment-845c8977d9-qw4v2" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-qw4v2 test-new-deployment-845c8977d9- deployment-4907  479273a3-c343-412d-8cb5-c8677176d5d2 10031 0 2023-04-06 10:28:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b680a077-03f8-4836-b298-f11e0c813c84 0xc00401ef50 0xc00401ef51}] [] [{kube-controller-manager Update v1 2023-04-06 10:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b680a077-03f8-4836-b298-f11e0c813c84\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-488w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-488w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 10:28:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 10:28:19.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4907" for this suite. 04/06/23 10:28:19.553
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:28:19.559
Apr  6 10:28:19.559: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename proxy 04/06/23 10:28:19.56
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:28:19.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:28:19.586
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 04/06/23 10:28:19.606
STEP: creating replication controller proxy-service-928jp in namespace proxy-8768 04/06/23 10:28:19.606
I0406 10:28:19.615273      22 runners.go:193] Created replication controller with name: proxy-service-928jp, namespace: proxy-8768, replica count: 1
I0406 10:28:20.667270      22 runners.go:193] proxy-service-928jp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 10:28:21.668374      22 runners.go:193] proxy-service-928jp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0406 10:28:22.668622      22 runners.go:193] proxy-service-928jp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 10:28:22.675: INFO: setup took 3.08346569s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/06/23 10:28:22.675
Apr  6 10:28:22.717: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 41.248014ms)
Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 41.342667ms)
Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 41.158003ms)
Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 41.386502ms)
Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 42.212117ms)
Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 41.475791ms)
Apr  6 10:28:22.722: INFO: (0) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 45.125646ms)
Apr  6 10:28:22.722: INFO: (0) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 45.768583ms)
Apr  6 10:28:22.724: INFO: (0) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 48.103279ms)
Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 47.862693ms)
Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 48.394452ms)
Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 49.320472ms)
Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 48.886672ms)
Apr  6 10:28:22.727: INFO: (0) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 51.120081ms)
Apr  6 10:28:22.729: INFO: (0) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 52.630301ms)
Apr  6 10:28:22.729: INFO: (0) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 52.925191ms)
Apr  6 10:28:22.738: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 8.733786ms)
Apr  6 10:28:22.739: INFO: (1) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 9.575678ms)
Apr  6 10:28:22.739: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.871143ms)
Apr  6 10:28:22.739: INFO: (1) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.11986ms)
Apr  6 10:28:22.740: INFO: (1) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 9.379543ms)
Apr  6 10:28:22.740: INFO: (1) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 10.270797ms)
Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 19.140225ms)
Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 18.350662ms)
Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 18.468886ms)
Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 18.585581ms)
Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 18.676122ms)
Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 19.134006ms)
Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 19.409915ms)
Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.171677ms)
Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 18.73374ms)
Apr  6 10:28:22.754: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 24.301723ms)
Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 10.446632ms)
Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 10.582792ms)
Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.111084ms)
Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.250253ms)
Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.113427ms)
Apr  6 10:28:22.768: INFO: (2) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 12.539048ms)
Apr  6 10:28:22.768: INFO: (2) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 14.147205ms)
Apr  6 10:28:22.769: INFO: (2) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 13.008965ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 27.260764ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 27.755599ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 28.881254ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 29.020677ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 29.415676ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 28.421574ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 28.355538ms)
Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 28.048625ms)
Apr  6 10:28:22.793: INFO: (3) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 9.784385ms)
Apr  6 10:28:22.803: INFO: (3) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 18.349485ms)
Apr  6 10:28:22.805: INFO: (3) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 20.059906ms)
Apr  6 10:28:22.805: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 20.348668ms)
Apr  6 10:28:22.805: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 20.944439ms)
Apr  6 10:28:22.812: INFO: (3) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 26.680317ms)
Apr  6 10:28:22.812: INFO: (3) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 27.490172ms)
Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 27.587302ms)
Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 27.747217ms)
Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 27.946877ms)
Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 28.973949ms)
Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 27.743072ms)
Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 29.250536ms)
Apr  6 10:28:22.816: INFO: (3) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 31.105736ms)
Apr  6 10:28:22.818: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 33.188635ms)
Apr  6 10:28:22.820: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 35.713742ms)
Apr  6 10:28:22.829: INFO: (4) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 9.141805ms)
Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 12.533573ms)
Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 11.484946ms)
Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 11.731226ms)
Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 11.574544ms)
Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.840906ms)
Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.909919ms)
Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 13.965076ms)
Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 13.872639ms)
Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 14.517303ms)
Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 14.923572ms)
Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 14.808198ms)
Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 16.72249ms)
Apr  6 10:28:22.842: INFO: (4) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 20.609626ms)
Apr  6 10:28:22.842: INFO: (4) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 21.094118ms)
Apr  6 10:28:22.842: INFO: (4) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.994788ms)
Apr  6 10:28:22.856: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 13.789523ms)
Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.594331ms)
Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 14.433523ms)
Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.562718ms)
Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.365331ms)
Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 14.435759ms)
Apr  6 10:28:22.865: INFO: (5) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 22.341349ms)
Apr  6 10:28:22.865: INFO: (5) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 22.384016ms)
Apr  6 10:28:22.865: INFO: (5) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 22.494419ms)
Apr  6 10:28:22.866: INFO: (5) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 23.742864ms)
Apr  6 10:28:22.866: INFO: (5) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 23.113078ms)
Apr  6 10:28:22.876: INFO: (5) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 33.250945ms)
Apr  6 10:28:22.877: INFO: (5) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 34.696621ms)
Apr  6 10:28:22.878: INFO: (5) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 35.566831ms)
Apr  6 10:28:22.899: INFO: (5) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 55.968499ms)
Apr  6 10:28:22.899: INFO: (5) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 56.342794ms)
Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.448666ms)
Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 12.033777ms)
Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 11.955798ms)
Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 12.343613ms)
Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 12.501685ms)
Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.338362ms)
Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 12.97161ms)
Apr  6 10:28:22.917: INFO: (6) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 16.873155ms)
Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 17.250842ms)
Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.549008ms)
Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 17.430282ms)
Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 17.700871ms)
Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 18.142124ms)
Apr  6 10:28:22.920: INFO: (6) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 19.758506ms)
Apr  6 10:28:22.920: INFO: (6) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.748081ms)
Apr  6 10:28:22.920: INFO: (6) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 19.875708ms)
Apr  6 10:28:22.929: INFO: (7) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 8.276722ms)
Apr  6 10:28:22.931: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 10.515425ms)
Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 16.460206ms)
Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 15.6418ms)
Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 16.393096ms)
Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 16.978919ms)
Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.867536ms)
Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.900192ms)
Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 16.860797ms)
Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 17.304121ms)
Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 17.703183ms)
Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 16.78715ms)
Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 18.881684ms)
Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 19.610565ms)
Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.424375ms)
Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 18.40196ms)
Apr  6 10:28:22.952: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 11.045752ms)
Apr  6 10:28:22.952: INFO: (8) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.499654ms)
Apr  6 10:28:22.952: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.941258ms)
Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 11.276929ms)
Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.323115ms)
Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 11.812618ms)
Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 11.496354ms)
Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 11.438794ms)
Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 12.782918ms)
Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 13.070074ms)
Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 13.505815ms)
Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 12.77516ms)
Apr  6 10:28:22.955: INFO: (8) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 14.420102ms)
Apr  6 10:28:22.955: INFO: (8) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 13.794681ms)
Apr  6 10:28:22.957: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 15.657145ms)
Apr  6 10:28:22.966: INFO: (8) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 24.41205ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 14.601683ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 14.384345ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 14.833468ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 15.19517ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 14.096634ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 14.03247ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.697984ms)
Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 15.132709ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 17.570751ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 17.617364ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 17.803961ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 17.910808ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 17.857012ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 18.324521ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.698047ms)
Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.783711ms)
Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 12.320667ms)
Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 11.723571ms)
Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.452748ms)
Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 12.086957ms)
Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 12.706787ms)
Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.399061ms)
Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 12.731013ms)
Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 17.319295ms)
Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 17.382892ms)
Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 17.830977ms)
Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 17.661499ms)
Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 17.309975ms)
Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.519122ms)
Apr  6 10:28:23.010: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 20.434997ms)
Apr  6 10:28:23.011: INFO: (10) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 21.295313ms)
Apr  6 10:28:23.013: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 22.977768ms)
Apr  6 10:28:23.028: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 13.70283ms)
Apr  6 10:28:23.028: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.754964ms)
Apr  6 10:28:23.030: INFO: (11) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 17.124018ms)
Apr  6 10:28:23.030: INFO: (11) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 15.97263ms)
Apr  6 10:28:23.030: INFO: (11) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 15.483629ms)
Apr  6 10:28:23.032: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 16.980685ms)
Apr  6 10:28:23.033: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 19.123378ms)
Apr  6 10:28:23.033: INFO: (11) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 19.836749ms)
Apr  6 10:28:23.033: INFO: (11) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 20.006899ms)
Apr  6 10:28:23.034: INFO: (11) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 20.809856ms)
Apr  6 10:28:23.034: INFO: (11) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 19.797575ms)
Apr  6 10:28:23.034: INFO: (11) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 18.66441ms)
Apr  6 10:28:23.037: INFO: (11) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 21.075924ms)
Apr  6 10:28:23.037: INFO: (11) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 21.440661ms)
Apr  6 10:28:23.037: INFO: (11) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 24.055996ms)
Apr  6 10:28:23.040: INFO: (11) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 25.028763ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 15.610113ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 17.728958ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.578692ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 17.537476ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 18.826039ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 16.541566ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 18.494617ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 15.938691ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.463097ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 16.363988ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 15.525107ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 16.24586ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 16.913995ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 18.479532ms)
Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 15.292266ms)
Apr  6 10:28:23.061: INFO: (12) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 19.136903ms)
Apr  6 10:28:23.070: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.060274ms)
Apr  6 10:28:23.070: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 9.101288ms)
Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 9.223058ms)
Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.779047ms)
Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 9.524464ms)
Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 9.636935ms)
Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 9.656463ms)
Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 12.596549ms)
Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 12.654847ms)
Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 12.642466ms)
Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 12.720858ms)
Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 12.736895ms)
Apr  6 10:28:23.077: INFO: (13) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 15.272284ms)
Apr  6 10:28:23.077: INFO: (13) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 15.439206ms)
Apr  6 10:28:23.077: INFO: (13) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 15.426605ms)
Apr  6 10:28:23.078: INFO: (13) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 17.412479ms)
Apr  6 10:28:23.089: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.893389ms)
Apr  6 10:28:23.090: INFO: (14) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 10.357786ms)
Apr  6 10:28:23.090: INFO: (14) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 10.539469ms)
Apr  6 10:28:23.090: INFO: (14) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 10.030605ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 11.116504ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 11.86215ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 10.225146ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 10.794873ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.432698ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 11.786687ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 12.181896ms)
Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 9.995538ms)
Apr  6 10:28:23.092: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 11.970111ms)
Apr  6 10:28:23.097: INFO: (14) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.289201ms)
Apr  6 10:28:23.097: INFO: (14) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 16.407355ms)
Apr  6 10:28:23.097: INFO: (14) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 17.104926ms)
Apr  6 10:28:23.109: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 10.028019ms)
Apr  6 10:28:23.111: INFO: (15) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.88805ms)
Apr  6 10:28:23.111: INFO: (15) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 12.663709ms)
Apr  6 10:28:23.111: INFO: (15) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 12.784667ms)
Apr  6 10:28:23.114: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 14.476497ms)
Apr  6 10:28:23.114: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 14.283588ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.748116ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 17.799844ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 15.057457ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 15.373059ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 15.478253ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 18.09328ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 14.954157ms)
Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 15.921349ms)
Apr  6 10:28:23.120: INFO: (15) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 19.947341ms)
Apr  6 10:28:23.120: INFO: (15) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 20.416335ms)
Apr  6 10:28:23.130: INFO: (16) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 10.011287ms)
Apr  6 10:28:23.130: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 9.364359ms)
Apr  6 10:28:23.136: INFO: (16) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 15.093751ms)
Apr  6 10:28:23.136: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 15.019969ms)
Apr  6 10:28:23.137: INFO: (16) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 15.918846ms)
Apr  6 10:28:23.137: INFO: (16) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 16.049142ms)
Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 16.352487ms)
Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 16.600365ms)
Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 16.656311ms)
Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 16.709915ms)
Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 16.581432ms)
Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 16.950164ms)
Apr  6 10:28:23.140: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 18.987661ms)
Apr  6 10:28:23.141: INFO: (16) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.169233ms)
Apr  6 10:28:23.160: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 38.68626ms)
Apr  6 10:28:23.160: INFO: (16) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 38.703303ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.437919ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 9.831676ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 10.522331ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 10.701988ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.289845ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.458032ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.706219ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.158243ms)
Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 10.275183ms)
Apr  6 10:28:23.175: INFO: (17) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 13.573983ms)
Apr  6 10:28:23.175: INFO: (17) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 12.534042ms)
Apr  6 10:28:23.175: INFO: (17) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 13.10803ms)
Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 14.789418ms)
Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 16.38864ms)
Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 15.015947ms)
Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 16.853084ms)
Apr  6 10:28:23.187: INFO: (18) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 9.832233ms)
Apr  6 10:28:23.188: INFO: (18) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 10.084347ms)
Apr  6 10:28:23.188: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 10.686943ms)
Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 13.037639ms)
Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 13.386203ms)
Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 13.629342ms)
Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 13.766855ms)
Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 13.334456ms)
Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.55317ms)
Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.607586ms)
Apr  6 10:28:23.192: INFO: (18) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 13.60921ms)
Apr  6 10:28:23.192: INFO: (18) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.963207ms)
Apr  6 10:28:23.192: INFO: (18) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 14.71817ms)
Apr  6 10:28:23.198: INFO: (18) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 19.84325ms)
Apr  6 10:28:23.198: INFO: (18) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 20.08721ms)
Apr  6 10:28:23.198: INFO: (18) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.47819ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 18.646086ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.880011ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 18.859853ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 21.685147ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 21.729227ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 18.20407ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 22.208352ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 18.617143ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 21.891061ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 19.191458ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.4038ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 21.762475ms)
Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 18.882398ms)
Apr  6 10:28:23.222: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 23.203237ms)
Apr  6 10:28:23.224: INFO: (19) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 20.065318ms)
Apr  6 10:28:23.224: INFO: (19) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 22.348902ms)
STEP: deleting ReplicationController proxy-service-928jp in namespace proxy-8768, will wait for the garbage collector to delete the pods 04/06/23 10:28:23.224
Apr  6 10:28:23.286: INFO: Deleting ReplicationController proxy-service-928jp took: 7.077792ms
Apr  6 10:28:23.386: INFO: Terminating ReplicationController proxy-service-928jp pods took: 100.514527ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Apr  6 10:28:24.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8768" for this suite. 04/06/23 10:28:24.902
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":21,"skipped":370,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.351 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:28:19.559
    Apr  6 10:28:19.559: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename proxy 04/06/23 10:28:19.56
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:28:19.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:28:19.586
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 04/06/23 10:28:19.606
    STEP: creating replication controller proxy-service-928jp in namespace proxy-8768 04/06/23 10:28:19.606
    I0406 10:28:19.615273      22 runners.go:193] Created replication controller with name: proxy-service-928jp, namespace: proxy-8768, replica count: 1
    I0406 10:28:20.667270      22 runners.go:193] proxy-service-928jp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0406 10:28:21.668374      22 runners.go:193] proxy-service-928jp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0406 10:28:22.668622      22 runners.go:193] proxy-service-928jp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 10:28:22.675: INFO: setup took 3.08346569s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/06/23 10:28:22.675
    Apr  6 10:28:22.717: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 41.248014ms)
    Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 41.342667ms)
    Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 41.158003ms)
    Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 41.386502ms)
    Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 42.212117ms)
    Apr  6 10:28:22.718: INFO: (0) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 41.475791ms)
    Apr  6 10:28:22.722: INFO: (0) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 45.125646ms)
    Apr  6 10:28:22.722: INFO: (0) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 45.768583ms)
    Apr  6 10:28:22.724: INFO: (0) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 48.103279ms)
    Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 47.862693ms)
    Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 48.394452ms)
    Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 49.320472ms)
    Apr  6 10:28:22.725: INFO: (0) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 48.886672ms)
    Apr  6 10:28:22.727: INFO: (0) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 51.120081ms)
    Apr  6 10:28:22.729: INFO: (0) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 52.630301ms)
    Apr  6 10:28:22.729: INFO: (0) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 52.925191ms)
    Apr  6 10:28:22.738: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 8.733786ms)
    Apr  6 10:28:22.739: INFO: (1) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 9.575678ms)
    Apr  6 10:28:22.739: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.871143ms)
    Apr  6 10:28:22.739: INFO: (1) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.11986ms)
    Apr  6 10:28:22.740: INFO: (1) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 9.379543ms)
    Apr  6 10:28:22.740: INFO: (1) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 10.270797ms)
    Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 19.140225ms)
    Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 18.350662ms)
    Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 18.468886ms)
    Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 18.585581ms)
    Apr  6 10:28:22.748: INFO: (1) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 18.676122ms)
    Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 19.134006ms)
    Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 19.409915ms)
    Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.171677ms)
    Apr  6 10:28:22.749: INFO: (1) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 18.73374ms)
    Apr  6 10:28:22.754: INFO: (1) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 24.301723ms)
    Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 10.446632ms)
    Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 10.582792ms)
    Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.111084ms)
    Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.250253ms)
    Apr  6 10:28:22.766: INFO: (2) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.113427ms)
    Apr  6 10:28:22.768: INFO: (2) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 12.539048ms)
    Apr  6 10:28:22.768: INFO: (2) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 14.147205ms)
    Apr  6 10:28:22.769: INFO: (2) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 13.008965ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 27.260764ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 27.755599ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 28.881254ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 29.020677ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 29.415676ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 28.421574ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 28.355538ms)
    Apr  6 10:28:22.783: INFO: (2) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 28.048625ms)
    Apr  6 10:28:22.793: INFO: (3) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 9.784385ms)
    Apr  6 10:28:22.803: INFO: (3) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 18.349485ms)
    Apr  6 10:28:22.805: INFO: (3) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 20.059906ms)
    Apr  6 10:28:22.805: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 20.348668ms)
    Apr  6 10:28:22.805: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 20.944439ms)
    Apr  6 10:28:22.812: INFO: (3) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 26.680317ms)
    Apr  6 10:28:22.812: INFO: (3) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 27.490172ms)
    Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 27.587302ms)
    Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 27.747217ms)
    Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 27.946877ms)
    Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 28.973949ms)
    Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 27.743072ms)
    Apr  6 10:28:22.813: INFO: (3) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 29.250536ms)
    Apr  6 10:28:22.816: INFO: (3) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 31.105736ms)
    Apr  6 10:28:22.818: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 33.188635ms)
    Apr  6 10:28:22.820: INFO: (3) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 35.713742ms)
    Apr  6 10:28:22.829: INFO: (4) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 9.141805ms)
    Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 12.533573ms)
    Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 11.484946ms)
    Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 11.731226ms)
    Apr  6 10:28:22.833: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 11.574544ms)
    Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.840906ms)
    Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.909919ms)
    Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 13.965076ms)
    Apr  6 10:28:22.835: INFO: (4) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 13.872639ms)
    Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 14.517303ms)
    Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 14.923572ms)
    Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 14.808198ms)
    Apr  6 10:28:22.836: INFO: (4) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 16.72249ms)
    Apr  6 10:28:22.842: INFO: (4) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 20.609626ms)
    Apr  6 10:28:22.842: INFO: (4) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 21.094118ms)
    Apr  6 10:28:22.842: INFO: (4) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.994788ms)
    Apr  6 10:28:22.856: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 13.789523ms)
    Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.594331ms)
    Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 14.433523ms)
    Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.562718ms)
    Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.365331ms)
    Apr  6 10:28:22.857: INFO: (5) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 14.435759ms)
    Apr  6 10:28:22.865: INFO: (5) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 22.341349ms)
    Apr  6 10:28:22.865: INFO: (5) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 22.384016ms)
    Apr  6 10:28:22.865: INFO: (5) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 22.494419ms)
    Apr  6 10:28:22.866: INFO: (5) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 23.742864ms)
    Apr  6 10:28:22.866: INFO: (5) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 23.113078ms)
    Apr  6 10:28:22.876: INFO: (5) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 33.250945ms)
    Apr  6 10:28:22.877: INFO: (5) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 34.696621ms)
    Apr  6 10:28:22.878: INFO: (5) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 35.566831ms)
    Apr  6 10:28:22.899: INFO: (5) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 55.968499ms)
    Apr  6 10:28:22.899: INFO: (5) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 56.342794ms)
    Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.448666ms)
    Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 12.033777ms)
    Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 11.955798ms)
    Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 12.343613ms)
    Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 12.501685ms)
    Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.338362ms)
    Apr  6 10:28:22.912: INFO: (6) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 12.97161ms)
    Apr  6 10:28:22.917: INFO: (6) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 16.873155ms)
    Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 17.250842ms)
    Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.549008ms)
    Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 17.430282ms)
    Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 17.700871ms)
    Apr  6 10:28:22.918: INFO: (6) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 18.142124ms)
    Apr  6 10:28:22.920: INFO: (6) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 19.758506ms)
    Apr  6 10:28:22.920: INFO: (6) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.748081ms)
    Apr  6 10:28:22.920: INFO: (6) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 19.875708ms)
    Apr  6 10:28:22.929: INFO: (7) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 8.276722ms)
    Apr  6 10:28:22.931: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 10.515425ms)
    Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 16.460206ms)
    Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 15.6418ms)
    Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 16.393096ms)
    Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 16.978919ms)
    Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.867536ms)
    Apr  6 10:28:22.937: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 14.900192ms)
    Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 16.860797ms)
    Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 17.304121ms)
    Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 17.703183ms)
    Apr  6 10:28:22.939: INFO: (7) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 16.78715ms)
    Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 18.881684ms)
    Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 19.610565ms)
    Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.424375ms)
    Apr  6 10:28:22.940: INFO: (7) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 18.40196ms)
    Apr  6 10:28:22.952: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 11.045752ms)
    Apr  6 10:28:22.952: INFO: (8) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.499654ms)
    Apr  6 10:28:22.952: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.941258ms)
    Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 11.276929ms)
    Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.323115ms)
    Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 11.812618ms)
    Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 11.496354ms)
    Apr  6 10:28:22.953: INFO: (8) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 11.438794ms)
    Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 12.782918ms)
    Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 13.070074ms)
    Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 13.505815ms)
    Apr  6 10:28:22.954: INFO: (8) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 12.77516ms)
    Apr  6 10:28:22.955: INFO: (8) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 14.420102ms)
    Apr  6 10:28:22.955: INFO: (8) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 13.794681ms)
    Apr  6 10:28:22.957: INFO: (8) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 15.657145ms)
    Apr  6 10:28:22.966: INFO: (8) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 24.41205ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 14.601683ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 14.384345ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 14.833468ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 15.19517ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 14.096634ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 14.03247ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.697984ms)
    Apr  6 10:28:22.981: INFO: (9) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 15.132709ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 17.570751ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 17.617364ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 17.803961ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 17.910808ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 17.857012ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 18.324521ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.698047ms)
    Apr  6 10:28:22.985: INFO: (9) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.783711ms)
    Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 12.320667ms)
    Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 11.723571ms)
    Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.452748ms)
    Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 12.086957ms)
    Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 12.706787ms)
    Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.399061ms)
    Apr  6 10:28:23.002: INFO: (10) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 12.731013ms)
    Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 17.319295ms)
    Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 17.382892ms)
    Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 17.830977ms)
    Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 17.661499ms)
    Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 17.309975ms)
    Apr  6 10:28:23.008: INFO: (10) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.519122ms)
    Apr  6 10:28:23.010: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 20.434997ms)
    Apr  6 10:28:23.011: INFO: (10) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 21.295313ms)
    Apr  6 10:28:23.013: INFO: (10) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 22.977768ms)
    Apr  6 10:28:23.028: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 13.70283ms)
    Apr  6 10:28:23.028: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.754964ms)
    Apr  6 10:28:23.030: INFO: (11) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 17.124018ms)
    Apr  6 10:28:23.030: INFO: (11) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 15.97263ms)
    Apr  6 10:28:23.030: INFO: (11) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 15.483629ms)
    Apr  6 10:28:23.032: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 16.980685ms)
    Apr  6 10:28:23.033: INFO: (11) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 19.123378ms)
    Apr  6 10:28:23.033: INFO: (11) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 19.836749ms)
    Apr  6 10:28:23.033: INFO: (11) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 20.006899ms)
    Apr  6 10:28:23.034: INFO: (11) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 20.809856ms)
    Apr  6 10:28:23.034: INFO: (11) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 19.797575ms)
    Apr  6 10:28:23.034: INFO: (11) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 18.66441ms)
    Apr  6 10:28:23.037: INFO: (11) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 21.075924ms)
    Apr  6 10:28:23.037: INFO: (11) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 21.440661ms)
    Apr  6 10:28:23.037: INFO: (11) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 24.055996ms)
    Apr  6 10:28:23.040: INFO: (11) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 25.028763ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 15.610113ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 17.728958ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.578692ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 17.537476ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 18.826039ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 16.541566ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 18.494617ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 15.938691ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.463097ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 16.363988ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 15.525107ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 16.24586ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 16.913995ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 18.479532ms)
    Apr  6 10:28:23.059: INFO: (12) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 15.292266ms)
    Apr  6 10:28:23.061: INFO: (12) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 19.136903ms)
    Apr  6 10:28:23.070: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.060274ms)
    Apr  6 10:28:23.070: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 9.101288ms)
    Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 9.223058ms)
    Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.779047ms)
    Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 9.524464ms)
    Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 9.636935ms)
    Apr  6 10:28:23.071: INFO: (13) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 9.656463ms)
    Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 12.596549ms)
    Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 12.654847ms)
    Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 12.642466ms)
    Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 12.720858ms)
    Apr  6 10:28:23.074: INFO: (13) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 12.736895ms)
    Apr  6 10:28:23.077: INFO: (13) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 15.272284ms)
    Apr  6 10:28:23.077: INFO: (13) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 15.439206ms)
    Apr  6 10:28:23.077: INFO: (13) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 15.426605ms)
    Apr  6 10:28:23.078: INFO: (13) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 17.412479ms)
    Apr  6 10:28:23.089: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 9.893389ms)
    Apr  6 10:28:23.090: INFO: (14) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 10.357786ms)
    Apr  6 10:28:23.090: INFO: (14) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 10.539469ms)
    Apr  6 10:28:23.090: INFO: (14) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 10.030605ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 11.116504ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 11.86215ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 10.225146ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 10.794873ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 11.432698ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 11.786687ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 12.181896ms)
    Apr  6 10:28:23.091: INFO: (14) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 9.995538ms)
    Apr  6 10:28:23.092: INFO: (14) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 11.970111ms)
    Apr  6 10:28:23.097: INFO: (14) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.289201ms)
    Apr  6 10:28:23.097: INFO: (14) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 16.407355ms)
    Apr  6 10:28:23.097: INFO: (14) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 17.104926ms)
    Apr  6 10:28:23.109: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 10.028019ms)
    Apr  6 10:28:23.111: INFO: (15) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.88805ms)
    Apr  6 10:28:23.111: INFO: (15) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 12.663709ms)
    Apr  6 10:28:23.111: INFO: (15) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 12.784667ms)
    Apr  6 10:28:23.114: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 14.476497ms)
    Apr  6 10:28:23.114: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 14.283588ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 14.748116ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 17.799844ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 15.057457ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 15.373059ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 15.478253ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 18.09328ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 14.954157ms)
    Apr  6 10:28:23.115: INFO: (15) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 15.921349ms)
    Apr  6 10:28:23.120: INFO: (15) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 19.947341ms)
    Apr  6 10:28:23.120: INFO: (15) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 20.416335ms)
    Apr  6 10:28:23.130: INFO: (16) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 10.011287ms)
    Apr  6 10:28:23.130: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 9.364359ms)
    Apr  6 10:28:23.136: INFO: (16) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 15.093751ms)
    Apr  6 10:28:23.136: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 15.019969ms)
    Apr  6 10:28:23.137: INFO: (16) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 15.918846ms)
    Apr  6 10:28:23.137: INFO: (16) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 16.049142ms)
    Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 16.352487ms)
    Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 16.600365ms)
    Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 16.656311ms)
    Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 16.709915ms)
    Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 16.581432ms)
    Apr  6 10:28:23.138: INFO: (16) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 16.950164ms)
    Apr  6 10:28:23.140: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 18.987661ms)
    Apr  6 10:28:23.141: INFO: (16) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.169233ms)
    Apr  6 10:28:23.160: INFO: (16) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 38.68626ms)
    Apr  6 10:28:23.160: INFO: (16) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 38.703303ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.437919ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 9.831676ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 10.522331ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 10.701988ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.289845ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 11.458032ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 12.706219ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.158243ms)
    Apr  6 10:28:23.173: INFO: (17) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 10.275183ms)
    Apr  6 10:28:23.175: INFO: (17) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 13.573983ms)
    Apr  6 10:28:23.175: INFO: (17) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 12.534042ms)
    Apr  6 10:28:23.175: INFO: (17) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 13.10803ms)
    Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 14.789418ms)
    Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 16.38864ms)
    Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 15.015947ms)
    Apr  6 10:28:23.177: INFO: (17) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 16.853084ms)
    Apr  6 10:28:23.187: INFO: (18) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 9.832233ms)
    Apr  6 10:28:23.188: INFO: (18) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 10.084347ms)
    Apr  6 10:28:23.188: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 10.686943ms)
    Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 13.037639ms)
    Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 13.386203ms)
    Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 13.629342ms)
    Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 13.766855ms)
    Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 13.334456ms)
    Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.55317ms)
    Apr  6 10:28:23.191: INFO: (18) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 13.607586ms)
    Apr  6 10:28:23.192: INFO: (18) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 13.60921ms)
    Apr  6 10:28:23.192: INFO: (18) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 13.963207ms)
    Apr  6 10:28:23.192: INFO: (18) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 14.71817ms)
    Apr  6 10:28:23.198: INFO: (18) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 19.84325ms)
    Apr  6 10:28:23.198: INFO: (18) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 20.08721ms)
    Apr  6 10:28:23.198: INFO: (18) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 20.47819ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:443/proxy/tlsrewritem... (200; 18.646086ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:162/proxy/: bar (200; 17.880011ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:1080/proxy/rewriteme">... (200; 18.859853ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname2/proxy/: bar (200; 21.685147ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9/proxy/rewriteme">test</a> (200; 21.729227ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname1/proxy/: tls baz (200; 18.20407ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:460/proxy/: tls baz (200; 22.208352ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:160/proxy/: foo (200; 18.617143ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/http:proxy-service-928jp-dm9x9:160/proxy/: foo (200; 21.891061ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/https:proxy-service-928jp-dm9x9:462/proxy/: tls qux (200; 19.191458ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname2/proxy/: bar (200; 18.4038ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/services/https:proxy-service-928jp:tlsportname2/proxy/: tls qux (200; 21.762475ms)
    Apr  6 10:28:23.220: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:162/proxy/: bar (200; 18.882398ms)
    Apr  6 10:28:23.222: INFO: (19) /api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8768/pods/proxy-service-928jp-dm9x9:1080/proxy/rewriteme">test<... (200; 23.203237ms)
    Apr  6 10:28:23.224: INFO: (19) /api/v1/namespaces/proxy-8768/services/proxy-service-928jp:portname1/proxy/: foo (200; 20.065318ms)
    Apr  6 10:28:23.224: INFO: (19) /api/v1/namespaces/proxy-8768/services/http:proxy-service-928jp:portname1/proxy/: foo (200; 22.348902ms)
    STEP: deleting ReplicationController proxy-service-928jp in namespace proxy-8768, will wait for the garbage collector to delete the pods 04/06/23 10:28:23.224
    Apr  6 10:28:23.286: INFO: Deleting ReplicationController proxy-service-928jp took: 7.077792ms
    Apr  6 10:28:23.386: INFO: Terminating ReplicationController proxy-service-928jp pods took: 100.514527ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Apr  6 10:28:24.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8768" for this suite. 04/06/23 10:28:24.902
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:28:24.911
Apr  6 10:28:24.911: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 10:28:24.912
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:28:24.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:28:24.956
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 04/06/23 10:28:24.968
STEP: waiting for pod running 04/06/23 10:28:24.999
Apr  6 10:28:24.999: INFO: Waiting up to 2m0s for pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" in namespace "var-expansion-8418" to be "running"
Apr  6 10:28:25.009: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085702ms
Apr  6 10:28:27.016: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014": Phase="Running", Reason="", readiness=true. Elapsed: 2.016559196s
Apr  6 10:28:27.016: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" satisfied condition "running"
STEP: creating a file in subpath 04/06/23 10:28:27.016
Apr  6 10:28:27.022: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8418 PodName:var-expansion-ad20ee28-8055-4621-8811-a25a97d00014 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:28:27.022: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:28:27.023: INFO: ExecWithOptions: Clientset creation
Apr  6 10:28:27.023: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/var-expansion-8418/pods/var-expansion-ad20ee28-8055-4621-8811-a25a97d00014/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 04/06/23 10:28:27.56
Apr  6 10:28:27.567: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8418 PodName:var-expansion-ad20ee28-8055-4621-8811-a25a97d00014 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:28:27.567: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:28:27.568: INFO: ExecWithOptions: Clientset creation
Apr  6 10:28:27.568: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/var-expansion-8418/pods/var-expansion-ad20ee28-8055-4621-8811-a25a97d00014/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 04/06/23 10:28:27.953
Apr  6 10:28:28.473: INFO: Successfully updated pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014"
STEP: waiting for annotated pod running 04/06/23 10:28:28.473
Apr  6 10:28:28.473: INFO: Waiting up to 2m0s for pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" in namespace "var-expansion-8418" to be "running"
Apr  6 10:28:28.479: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014": Phase="Running", Reason="", readiness=true. Elapsed: 6.487118ms
Apr  6 10:28:28.479: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" satisfied condition "running"
STEP: deleting the pod gracefully 04/06/23 10:28:28.479
Apr  6 10:28:28.480: INFO: Deleting pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" in namespace "var-expansion-8418"
Apr  6 10:28:28.488: INFO: Wait up to 5m0s for pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 10:29:02.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8418" for this suite. 04/06/23 10:29:02.514
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":22,"skipped":373,"failed":0}
------------------------------
â€¢ [SLOW TEST] [37.611 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:28:24.911
    Apr  6 10:28:24.911: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 10:28:24.912
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:28:24.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:28:24.956
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 04/06/23 10:28:24.968
    STEP: waiting for pod running 04/06/23 10:28:24.999
    Apr  6 10:28:24.999: INFO: Waiting up to 2m0s for pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" in namespace "var-expansion-8418" to be "running"
    Apr  6 10:28:25.009: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085702ms
    Apr  6 10:28:27.016: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014": Phase="Running", Reason="", readiness=true. Elapsed: 2.016559196s
    Apr  6 10:28:27.016: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" satisfied condition "running"
    STEP: creating a file in subpath 04/06/23 10:28:27.016
    Apr  6 10:28:27.022: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8418 PodName:var-expansion-ad20ee28-8055-4621-8811-a25a97d00014 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:28:27.022: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:28:27.023: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:28:27.023: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/var-expansion-8418/pods/var-expansion-ad20ee28-8055-4621-8811-a25a97d00014/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 04/06/23 10:28:27.56
    Apr  6 10:28:27.567: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8418 PodName:var-expansion-ad20ee28-8055-4621-8811-a25a97d00014 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:28:27.567: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:28:27.568: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:28:27.568: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/var-expansion-8418/pods/var-expansion-ad20ee28-8055-4621-8811-a25a97d00014/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 04/06/23 10:28:27.953
    Apr  6 10:28:28.473: INFO: Successfully updated pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014"
    STEP: waiting for annotated pod running 04/06/23 10:28:28.473
    Apr  6 10:28:28.473: INFO: Waiting up to 2m0s for pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" in namespace "var-expansion-8418" to be "running"
    Apr  6 10:28:28.479: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014": Phase="Running", Reason="", readiness=true. Elapsed: 6.487118ms
    Apr  6 10:28:28.479: INFO: Pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" satisfied condition "running"
    STEP: deleting the pod gracefully 04/06/23 10:28:28.479
    Apr  6 10:28:28.480: INFO: Deleting pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" in namespace "var-expansion-8418"
    Apr  6 10:28:28.488: INFO: Wait up to 5m0s for pod "var-expansion-ad20ee28-8055-4621-8811-a25a97d00014" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 10:29:02.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8418" for this suite. 04/06/23 10:29:02.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:29:02.536
Apr  6 10:29:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 10:29:02.538
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:02.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:02.569
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 04/06/23 10:29:02.587
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 04/06/23 10:29:02.595
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 04/06/23 10:29:02.595
STEP: creating a pod to probe DNS 04/06/23 10:29:02.595
STEP: submitting the pod to kubernetes 04/06/23 10:29:02.595
Apr  6 10:29:02.614: INFO: Waiting up to 15m0s for pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939" in namespace "dns-2328" to be "running"
Apr  6 10:29:02.620: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 4.849454ms
Apr  6 10:29:04.628: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013232412s
Apr  6 10:29:06.625: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010204391s
Apr  6 10:29:08.628: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013767897s
Apr  6 10:29:10.627: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012308506s
Apr  6 10:29:12.626: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011073885s
Apr  6 10:29:14.631: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Running", Reason="", readiness=true. Elapsed: 12.01620335s
Apr  6 10:29:14.631: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939" satisfied condition "running"
STEP: retrieving the pod 04/06/23 10:29:14.631
STEP: looking for the results for each expected name from probers 04/06/23 10:29:14.637
Apr  6 10:29:14.826: INFO: DNS probes using dns-2328/dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939 succeeded

STEP: deleting the pod 04/06/23 10:29:14.826
STEP: deleting the test headless service 04/06/23 10:29:14.836
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 10:29:14.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2328" for this suite. 04/06/23 10:29:14.849
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":23,"skipped":425,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.318 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:29:02.536
    Apr  6 10:29:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 10:29:02.538
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:02.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:02.569
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 04/06/23 10:29:02.587
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     04/06/23 10:29:02.595
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2328.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     04/06/23 10:29:02.595
    STEP: creating a pod to probe DNS 04/06/23 10:29:02.595
    STEP: submitting the pod to kubernetes 04/06/23 10:29:02.595
    Apr  6 10:29:02.614: INFO: Waiting up to 15m0s for pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939" in namespace "dns-2328" to be "running"
    Apr  6 10:29:02.620: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 4.849454ms
    Apr  6 10:29:04.628: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013232412s
    Apr  6 10:29:06.625: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010204391s
    Apr  6 10:29:08.628: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013767897s
    Apr  6 10:29:10.627: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012308506s
    Apr  6 10:29:12.626: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011073885s
    Apr  6 10:29:14.631: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939": Phase="Running", Reason="", readiness=true. Elapsed: 12.01620335s
    Apr  6 10:29:14.631: INFO: Pod "dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 10:29:14.631
    STEP: looking for the results for each expected name from probers 04/06/23 10:29:14.637
    Apr  6 10:29:14.826: INFO: DNS probes using dns-2328/dns-test-bb86d71b-aa2e-4cbd-8f79-8dd9886c0939 succeeded

    STEP: deleting the pod 04/06/23 10:29:14.826
    STEP: deleting the test headless service 04/06/23 10:29:14.836
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 10:29:14.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2328" for this suite. 04/06/23 10:29:14.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:29:14.855
Apr  6 10:29:14.855: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:29:14.856
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:14.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:14.904
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Apr  6 10:29:14.910: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:29:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9908" for this suite. 04/06/23 10:29:17.68
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":24,"skipped":447,"failed":0}
------------------------------
â€¢ [2.831 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:29:14.855
    Apr  6 10:29:14.855: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:29:14.856
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:14.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:14.904
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Apr  6 10:29:14.910: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:29:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9908" for this suite. 04/06/23 10:29:17.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:29:17.69
Apr  6 10:29:17.691: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:29:17.692
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:17.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:17.725
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:29:17.738
Apr  6 10:29:17.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df" in namespace "downward-api-8014" to be "Succeeded or Failed"
Apr  6 10:29:17.754: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.404767ms
Apr  6 10:29:19.761: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01181668s
Apr  6 10:29:21.763: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013347135s
STEP: Saw pod success 04/06/23 10:29:21.763
Apr  6 10:29:21.763: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df" satisfied condition "Succeeded or Failed"
Apr  6 10:29:21.768: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df container client-container: <nil>
STEP: delete the pod 04/06/23 10:29:21.782
Apr  6 10:29:21.791: INFO: Waiting for pod downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df to disappear
Apr  6 10:29:21.796: INFO: Pod downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 10:29:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8014" for this suite. 04/06/23 10:29:21.805
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":25,"skipped":528,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:29:17.69
    Apr  6 10:29:17.691: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:29:17.692
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:17.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:17.725
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:29:17.738
    Apr  6 10:29:17.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df" in namespace "downward-api-8014" to be "Succeeded or Failed"
    Apr  6 10:29:17.754: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.404767ms
    Apr  6 10:29:19.761: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01181668s
    Apr  6 10:29:21.763: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013347135s
    STEP: Saw pod success 04/06/23 10:29:21.763
    Apr  6 10:29:21.763: INFO: Pod "downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df" satisfied condition "Succeeded or Failed"
    Apr  6 10:29:21.768: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df container client-container: <nil>
    STEP: delete the pod 04/06/23 10:29:21.782
    Apr  6 10:29:21.791: INFO: Waiting for pod downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df to disappear
    Apr  6 10:29:21.796: INFO: Pod downwardapi-volume-625eda81-84e9-40d0-abd9-e6a19802f6df no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 10:29:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8014" for this suite. 04/06/23 10:29:21.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:29:21.815
Apr  6 10:29:21.815: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:29:21.816
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:21.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:21.843
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:29:21.866
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:29:22.389
STEP: Deploying the webhook pod 04/06/23 10:29:22.404
STEP: Wait for the deployment to be ready 04/06/23 10:29:22.419
Apr  6 10:29:22.428: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:29:24.445
STEP: Verifying the service has paired with the endpoint 04/06/23 10:29:24.458
Apr  6 10:29:25.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 04/06/23 10:29:25.465
STEP: create a pod 04/06/23 10:29:25.593
Apr  6 10:29:25.608: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8250" to be "running"
Apr  6 10:29:25.616: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.150108ms
Apr  6 10:29:27.626: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018551987s
Apr  6 10:29:27.627: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 04/06/23 10:29:27.627
Apr  6 10:29:27.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=webhook-8250 attach --namespace=webhook-8250 to-be-attached-pod -i -c=container1'
Apr  6 10:29:27.946: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:29:27.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8250" for this suite. 04/06/23 10:29:27.964
STEP: Destroying namespace "webhook-8250-markers" for this suite. 04/06/23 10:29:27.972
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":26,"skipped":568,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.223 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:29:21.815
    Apr  6 10:29:21.815: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:29:21.816
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:21.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:21.843
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:29:21.866
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:29:22.389
    STEP: Deploying the webhook pod 04/06/23 10:29:22.404
    STEP: Wait for the deployment to be ready 04/06/23 10:29:22.419
    Apr  6 10:29:22.428: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:29:24.445
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:29:24.458
    Apr  6 10:29:25.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 04/06/23 10:29:25.465
    STEP: create a pod 04/06/23 10:29:25.593
    Apr  6 10:29:25.608: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8250" to be "running"
    Apr  6 10:29:25.616: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.150108ms
    Apr  6 10:29:27.626: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018551987s
    Apr  6 10:29:27.627: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 04/06/23 10:29:27.627
    Apr  6 10:29:27.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=webhook-8250 attach --namespace=webhook-8250 to-be-attached-pod -i -c=container1'
    Apr  6 10:29:27.946: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:29:27.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8250" for this suite. 04/06/23 10:29:27.964
    STEP: Destroying namespace "webhook-8250-markers" for this suite. 04/06/23 10:29:27.972
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:29:28.038
Apr  6 10:29:28.038: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 10:29:28.039
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:28.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:28.082
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 04/06/23 10:29:28.092
STEP: setting up watch 04/06/23 10:29:28.093
STEP: submitting the pod to kubernetes 04/06/23 10:29:28.199
STEP: verifying the pod is in kubernetes 04/06/23 10:29:28.219
STEP: verifying pod creation was observed 04/06/23 10:29:28.232
Apr  6 10:29:28.232: INFO: Waiting up to 5m0s for pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8" in namespace "pods-9461" to be "running"
Apr  6 10:29:28.237: INFO: Pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773528ms
Apr  6 10:29:30.243: INFO: Pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011045887s
Apr  6 10:29:30.243: INFO: Pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8" satisfied condition "running"
STEP: deleting the pod gracefully 04/06/23 10:29:30.252
STEP: verifying pod deletion was observed 04/06/23 10:29:30.26
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 10:29:32.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9461" for this suite. 04/06/23 10:29:32.199
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":27,"skipped":572,"failed":0}
------------------------------
â€¢ [4.170 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:29:28.038
    Apr  6 10:29:28.038: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 10:29:28.039
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:28.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:28.082
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 04/06/23 10:29:28.092
    STEP: setting up watch 04/06/23 10:29:28.093
    STEP: submitting the pod to kubernetes 04/06/23 10:29:28.199
    STEP: verifying the pod is in kubernetes 04/06/23 10:29:28.219
    STEP: verifying pod creation was observed 04/06/23 10:29:28.232
    Apr  6 10:29:28.232: INFO: Waiting up to 5m0s for pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8" in namespace "pods-9461" to be "running"
    Apr  6 10:29:28.237: INFO: Pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773528ms
    Apr  6 10:29:30.243: INFO: Pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011045887s
    Apr  6 10:29:30.243: INFO: Pod "pod-submit-remove-297b0877-9e9f-48c4-9912-f99d60173eb8" satisfied condition "running"
    STEP: deleting the pod gracefully 04/06/23 10:29:30.252
    STEP: verifying pod deletion was observed 04/06/23 10:29:30.26
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 10:29:32.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9461" for this suite. 04/06/23 10:29:32.199
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:29:32.211
Apr  6 10:29:32.211: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:29:32.212
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:32.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:32.236
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-14821525-33b8-41c5-85f3-e4761a9ede7c 04/06/23 10:29:32.26
STEP: Creating configMap with name cm-test-opt-upd-e9c6edb9-d253-40a4-8d28-5d766cc67aa3 04/06/23 10:29:32.266
STEP: Creating the pod 04/06/23 10:29:32.272
Apr  6 10:29:32.290: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8" in namespace "projected-6475" to be "running and ready"
Apr  6 10:29:32.295: INFO: Pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.013647ms
Apr  6 10:29:32.295: INFO: The phase of Pod pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:29:34.304: INFO: Pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013630119s
Apr  6 10:29:34.304: INFO: The phase of Pod pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8 is Running (Ready = true)
Apr  6 10:29:34.304: INFO: Pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-14821525-33b8-41c5-85f3-e4761a9ede7c 04/06/23 10:29:34.534
STEP: Updating configmap cm-test-opt-upd-e9c6edb9-d253-40a4-8d28-5d766cc67aa3 04/06/23 10:29:34.541
STEP: Creating configMap with name cm-test-opt-create-cda42ae7-177b-4020-9833-a68c1e76e499 04/06/23 10:29:34.557
STEP: waiting to observe update in volume 04/06/23 10:29:34.567
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 10:30:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6475" for this suite. 04/06/23 10:30:55.612
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":28,"skipped":575,"failed":0}
------------------------------
â€¢ [SLOW TEST] [83.412 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:29:32.211
    Apr  6 10:29:32.211: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:29:32.212
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:29:32.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:29:32.236
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-14821525-33b8-41c5-85f3-e4761a9ede7c 04/06/23 10:29:32.26
    STEP: Creating configMap with name cm-test-opt-upd-e9c6edb9-d253-40a4-8d28-5d766cc67aa3 04/06/23 10:29:32.266
    STEP: Creating the pod 04/06/23 10:29:32.272
    Apr  6 10:29:32.290: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8" in namespace "projected-6475" to be "running and ready"
    Apr  6 10:29:32.295: INFO: Pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.013647ms
    Apr  6 10:29:32.295: INFO: The phase of Pod pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:29:34.304: INFO: Pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013630119s
    Apr  6 10:29:34.304: INFO: The phase of Pod pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8 is Running (Ready = true)
    Apr  6 10:29:34.304: INFO: Pod "pod-projected-configmaps-d4b7966b-0a90-453d-9de6-23b1e61518c8" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-14821525-33b8-41c5-85f3-e4761a9ede7c 04/06/23 10:29:34.534
    STEP: Updating configmap cm-test-opt-upd-e9c6edb9-d253-40a4-8d28-5d766cc67aa3 04/06/23 10:29:34.541
    STEP: Creating configMap with name cm-test-opt-create-cda42ae7-177b-4020-9833-a68c1e76e499 04/06/23 10:29:34.557
    STEP: waiting to observe update in volume 04/06/23 10:29:34.567
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 10:30:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6475" for this suite. 04/06/23 10:30:55.612
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:30:55.623
Apr  6 10:30:55.623: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubelet-test 04/06/23 10:30:55.625
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:30:55.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:30:55.657
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Apr  6 10:30:55.690: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f" in namespace "kubelet-test-7207" to be "running and ready"
Apr  6 10:30:55.696: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.450665ms
Apr  6 10:30:55.696: INFO: The phase of Pod busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:30:57.703: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012487197s
Apr  6 10:30:57.703: INFO: The phase of Pod busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:30:59.706: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f": Phase="Running", Reason="", readiness=true. Elapsed: 4.015693989s
Apr  6 10:30:59.710: INFO: The phase of Pod busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f is Running (Ready = true)
Apr  6 10:30:59.710: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Apr  6 10:30:59.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7207" for this suite. 04/06/23 10:30:59.79
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":29,"skipped":577,"failed":0}
------------------------------
â€¢ [4.175 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:30:55.623
    Apr  6 10:30:55.623: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubelet-test 04/06/23 10:30:55.625
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:30:55.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:30:55.657
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Apr  6 10:30:55.690: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f" in namespace "kubelet-test-7207" to be "running and ready"
    Apr  6 10:30:55.696: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.450665ms
    Apr  6 10:30:55.696: INFO: The phase of Pod busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:30:57.703: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012487197s
    Apr  6 10:30:57.703: INFO: The phase of Pod busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:30:59.706: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f": Phase="Running", Reason="", readiness=true. Elapsed: 4.015693989s
    Apr  6 10:30:59.710: INFO: The phase of Pod busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f is Running (Ready = true)
    Apr  6 10:30:59.710: INFO: Pod "busybox-readonly-fs6f6dac81-0b85-47d9-ae49-a80c0512615f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Apr  6 10:30:59.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7207" for this suite. 04/06/23 10:30:59.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:30:59.801
Apr  6 10:30:59.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:30:59.818
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:30:59.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:30:59.861
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-c21c3e11-8ab2-4746-955f-56281ec4491e 04/06/23 10:30:59.87
STEP: Creating a pod to test consume configMaps 04/06/23 10:30:59.877
Apr  6 10:30:59.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516" in namespace "projected-6420" to be "Succeeded or Failed"
Apr  6 10:30:59.898: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Pending", Reason="", readiness=false. Elapsed: 5.541308ms
Apr  6 10:31:01.914: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Running", Reason="", readiness=true. Elapsed: 2.021730974s
Apr  6 10:31:03.912: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Running", Reason="", readiness=false. Elapsed: 4.01904735s
Apr  6 10:31:05.909: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016236923s
STEP: Saw pod success 04/06/23 10:31:05.909
Apr  6 10:31:05.909: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516" satisfied condition "Succeeded or Failed"
Apr  6 10:31:05.914: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:31:05.934
Apr  6 10:31:05.945: INFO: Waiting for pod pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516 to disappear
Apr  6 10:31:05.951: INFO: Pod pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 10:31:05.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6420" for this suite. 04/06/23 10:31:05.974
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":30,"skipped":594,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.180 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:30:59.801
    Apr  6 10:30:59.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:30:59.818
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:30:59.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:30:59.861
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-c21c3e11-8ab2-4746-955f-56281ec4491e 04/06/23 10:30:59.87
    STEP: Creating a pod to test consume configMaps 04/06/23 10:30:59.877
    Apr  6 10:30:59.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516" in namespace "projected-6420" to be "Succeeded or Failed"
    Apr  6 10:30:59.898: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Pending", Reason="", readiness=false. Elapsed: 5.541308ms
    Apr  6 10:31:01.914: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Running", Reason="", readiness=true. Elapsed: 2.021730974s
    Apr  6 10:31:03.912: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Running", Reason="", readiness=false. Elapsed: 4.01904735s
    Apr  6 10:31:05.909: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016236923s
    STEP: Saw pod success 04/06/23 10:31:05.909
    Apr  6 10:31:05.909: INFO: Pod "pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516" satisfied condition "Succeeded or Failed"
    Apr  6 10:31:05.914: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:31:05.934
    Apr  6 10:31:05.945: INFO: Waiting for pod pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516 to disappear
    Apr  6 10:31:05.951: INFO: Pod pod-projected-configmaps-6bbdaf1e-edd2-4a94-97b6-0f09bc249516 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 10:31:05.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6420" for this suite. 04/06/23 10:31:05.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:31:05.983
Apr  6 10:31:05.984: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:31:05.985
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:06.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:06.031
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 04/06/23 10:31:06.038
Apr  6 10:31:06.038: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr  6 10:31:06.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
Apr  6 10:31:07.121: INFO: stderr: ""
Apr  6 10:31:07.121: INFO: stdout: "service/agnhost-replica created\n"
Apr  6 10:31:07.121: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr  6 10:31:07.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
Apr  6 10:31:08.239: INFO: stderr: ""
Apr  6 10:31:08.239: INFO: stdout: "service/agnhost-primary created\n"
Apr  6 10:31:08.239: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr  6 10:31:08.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
Apr  6 10:31:08.677: INFO: stderr: ""
Apr  6 10:31:08.677: INFO: stdout: "service/frontend created\n"
Apr  6 10:31:08.677: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr  6 10:31:08.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
Apr  6 10:31:09.052: INFO: stderr: ""
Apr  6 10:31:09.052: INFO: stdout: "deployment.apps/frontend created\n"
Apr  6 10:31:09.052: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  6 10:31:09.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
Apr  6 10:31:09.485: INFO: stderr: ""
Apr  6 10:31:09.485: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr  6 10:31:09.485: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  6 10:31:09.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
Apr  6 10:31:09.991: INFO: stderr: ""
Apr  6 10:31:09.991: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 04/06/23 10:31:09.991
Apr  6 10:31:09.991: INFO: Waiting for all frontend pods to be Running.
Apr  6 10:31:20.044: INFO: Waiting for frontend to serve content.
Apr  6 10:31:20.142: INFO: Trying to add a new entry to the guestbook.
Apr  6 10:31:20.253: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 04/06/23 10:31:20.383
Apr  6 10:31:20.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
Apr  6 10:31:20.577: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:31:20.578: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 04/06/23 10:31:20.578
Apr  6 10:31:20.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
Apr  6 10:31:20.761: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:31:20.761: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/06/23 10:31:20.761
Apr  6 10:31:20.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
Apr  6 10:31:20.945: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:31:20.945: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/06/23 10:31:20.945
Apr  6 10:31:20.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
Apr  6 10:31:21.160: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:31:21.160: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/06/23 10:31:21.16
Apr  6 10:31:21.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
Apr  6 10:31:21.339: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:31:21.339: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/06/23 10:31:21.339
Apr  6 10:31:21.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
Apr  6 10:31:21.482: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:31:21.482: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:31:21.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1922" for this suite. 04/06/23 10:31:21.491
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":31,"skipped":624,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.515 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:31:05.983
    Apr  6 10:31:05.984: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:31:05.985
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:06.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:06.031
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 04/06/23 10:31:06.038
    Apr  6 10:31:06.038: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Apr  6 10:31:06.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
    Apr  6 10:31:07.121: INFO: stderr: ""
    Apr  6 10:31:07.121: INFO: stdout: "service/agnhost-replica created\n"
    Apr  6 10:31:07.121: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Apr  6 10:31:07.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
    Apr  6 10:31:08.239: INFO: stderr: ""
    Apr  6 10:31:08.239: INFO: stdout: "service/agnhost-primary created\n"
    Apr  6 10:31:08.239: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Apr  6 10:31:08.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
    Apr  6 10:31:08.677: INFO: stderr: ""
    Apr  6 10:31:08.677: INFO: stdout: "service/frontend created\n"
    Apr  6 10:31:08.677: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Apr  6 10:31:08.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
    Apr  6 10:31:09.052: INFO: stderr: ""
    Apr  6 10:31:09.052: INFO: stdout: "deployment.apps/frontend created\n"
    Apr  6 10:31:09.052: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr  6 10:31:09.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
    Apr  6 10:31:09.485: INFO: stderr: ""
    Apr  6 10:31:09.485: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Apr  6 10:31:09.485: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr  6 10:31:09.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 create -f -'
    Apr  6 10:31:09.991: INFO: stderr: ""
    Apr  6 10:31:09.991: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 04/06/23 10:31:09.991
    Apr  6 10:31:09.991: INFO: Waiting for all frontend pods to be Running.
    Apr  6 10:31:20.044: INFO: Waiting for frontend to serve content.
    Apr  6 10:31:20.142: INFO: Trying to add a new entry to the guestbook.
    Apr  6 10:31:20.253: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 04/06/23 10:31:20.383
    Apr  6 10:31:20.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
    Apr  6 10:31:20.577: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:31:20.578: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 04/06/23 10:31:20.578
    Apr  6 10:31:20.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
    Apr  6 10:31:20.761: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:31:20.761: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/06/23 10:31:20.761
    Apr  6 10:31:20.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
    Apr  6 10:31:20.945: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:31:20.945: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/06/23 10:31:20.945
    Apr  6 10:31:20.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
    Apr  6 10:31:21.160: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:31:21.160: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/06/23 10:31:21.16
    Apr  6 10:31:21.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
    Apr  6 10:31:21.339: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:31:21.339: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/06/23 10:31:21.339
    Apr  6 10:31:21.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1922 delete --grace-period=0 --force -f -'
    Apr  6 10:31:21.482: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:31:21.482: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:31:21.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1922" for this suite. 04/06/23 10:31:21.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:31:21.505
Apr  6 10:31:21.505: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename cronjob 04/06/23 10:31:21.507
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:21.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:21.537
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 04/06/23 10:31:21.544
STEP: creating 04/06/23 10:31:21.544
STEP: getting 04/06/23 10:31:21.552
STEP: listing 04/06/23 10:31:21.556
STEP: watching 04/06/23 10:31:21.561
Apr  6 10:31:21.561: INFO: starting watch
STEP: cluster-wide listing 04/06/23 10:31:21.563
STEP: cluster-wide watching 04/06/23 10:31:21.569
Apr  6 10:31:21.569: INFO: starting watch
STEP: patching 04/06/23 10:31:21.572
STEP: updating 04/06/23 10:31:21.582
Apr  6 10:31:21.595: INFO: waiting for watch events with expected annotations
Apr  6 10:31:21.595: INFO: saw patched and updated annotations
STEP: patching /status 04/06/23 10:31:21.595
STEP: updating /status 04/06/23 10:31:21.605
STEP: get /status 04/06/23 10:31:21.62
STEP: deleting 04/06/23 10:31:21.629
STEP: deleting a collection 04/06/23 10:31:21.648
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Apr  6 10:31:21.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-85" for this suite. 04/06/23 10:31:21.675
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":32,"skipped":651,"failed":0}
------------------------------
â€¢ [0.175 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:31:21.505
    Apr  6 10:31:21.505: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename cronjob 04/06/23 10:31:21.507
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:21.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:21.537
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 04/06/23 10:31:21.544
    STEP: creating 04/06/23 10:31:21.544
    STEP: getting 04/06/23 10:31:21.552
    STEP: listing 04/06/23 10:31:21.556
    STEP: watching 04/06/23 10:31:21.561
    Apr  6 10:31:21.561: INFO: starting watch
    STEP: cluster-wide listing 04/06/23 10:31:21.563
    STEP: cluster-wide watching 04/06/23 10:31:21.569
    Apr  6 10:31:21.569: INFO: starting watch
    STEP: patching 04/06/23 10:31:21.572
    STEP: updating 04/06/23 10:31:21.582
    Apr  6 10:31:21.595: INFO: waiting for watch events with expected annotations
    Apr  6 10:31:21.595: INFO: saw patched and updated annotations
    STEP: patching /status 04/06/23 10:31:21.595
    STEP: updating /status 04/06/23 10:31:21.605
    STEP: get /status 04/06/23 10:31:21.62
    STEP: deleting 04/06/23 10:31:21.629
    STEP: deleting a collection 04/06/23 10:31:21.648
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Apr  6 10:31:21.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-85" for this suite. 04/06/23 10:31:21.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:31:21.681
Apr  6 10:31:21.681: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename namespaces 04/06/23 10:31:21.682
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:21.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:21.722
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 04/06/23 10:31:21.73
Apr  6 10:31:21.740: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 04/06/23 10:31:21.74
Apr  6 10:31:21.748: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 04/06/23 10:31:21.748
Apr  6 10:31:21.758: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:31:21.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5603" for this suite. 04/06/23 10:31:21.766
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":33,"skipped":657,"failed":0}
------------------------------
â€¢ [0.094 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:31:21.681
    Apr  6 10:31:21.681: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename namespaces 04/06/23 10:31:21.682
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:21.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:21.722
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 04/06/23 10:31:21.73
    Apr  6 10:31:21.740: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 04/06/23 10:31:21.74
    Apr  6 10:31:21.748: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 04/06/23 10:31:21.748
    Apr  6 10:31:21.758: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:31:21.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5603" for this suite. 04/06/23 10:31:21.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:31:21.776
Apr  6 10:31:21.776: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename subpath 04/06/23 10:31:21.778
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:21.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:21.816
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/06/23 10:31:21.834
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-dhpf 04/06/23 10:31:21.846
STEP: Creating a pod to test atomic-volume-subpath 04/06/23 10:31:21.846
Apr  6 10:31:21.863: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dhpf" in namespace "subpath-2417" to be "Succeeded or Failed"
Apr  6 10:31:21.872: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.248825ms
Apr  6 10:31:23.880: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 2.016868888s
Apr  6 10:31:25.881: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 4.017314877s
Apr  6 10:31:27.879: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015149203s
Apr  6 10:31:29.881: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 8.017278749s
Apr  6 10:31:31.881: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 10.017565493s
Apr  6 10:31:33.889: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 12.025120283s
Apr  6 10:31:35.880: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 14.016130222s
Apr  6 10:31:37.879: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 16.015986553s
Apr  6 10:31:39.886: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 18.022382886s
Apr  6 10:31:41.879: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 20.015375493s
Apr  6 10:31:43.880: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=false. Elapsed: 22.016134572s
Apr  6 10:31:45.885: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.021141754s
STEP: Saw pod success 04/06/23 10:31:45.885
Apr  6 10:31:45.886: INFO: Pod "pod-subpath-test-configmap-dhpf" satisfied condition "Succeeded or Failed"
Apr  6 10:31:45.896: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-subpath-test-configmap-dhpf container test-container-subpath-configmap-dhpf: <nil>
STEP: delete the pod 04/06/23 10:31:45.938
Apr  6 10:31:45.949: INFO: Waiting for pod pod-subpath-test-configmap-dhpf to disappear
Apr  6 10:31:45.956: INFO: Pod pod-subpath-test-configmap-dhpf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dhpf 04/06/23 10:31:45.957
Apr  6 10:31:45.958: INFO: Deleting pod "pod-subpath-test-configmap-dhpf" in namespace "subpath-2417"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Apr  6 10:31:45.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2417" for this suite. 04/06/23 10:31:46.003
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":34,"skipped":667,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.240 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:31:21.776
    Apr  6 10:31:21.776: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename subpath 04/06/23 10:31:21.778
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:21.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:21.816
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/06/23 10:31:21.834
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-dhpf 04/06/23 10:31:21.846
    STEP: Creating a pod to test atomic-volume-subpath 04/06/23 10:31:21.846
    Apr  6 10:31:21.863: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dhpf" in namespace "subpath-2417" to be "Succeeded or Failed"
    Apr  6 10:31:21.872: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.248825ms
    Apr  6 10:31:23.880: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 2.016868888s
    Apr  6 10:31:25.881: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 4.017314877s
    Apr  6 10:31:27.879: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015149203s
    Apr  6 10:31:29.881: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 8.017278749s
    Apr  6 10:31:31.881: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 10.017565493s
    Apr  6 10:31:33.889: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 12.025120283s
    Apr  6 10:31:35.880: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 14.016130222s
    Apr  6 10:31:37.879: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 16.015986553s
    Apr  6 10:31:39.886: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 18.022382886s
    Apr  6 10:31:41.879: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=true. Elapsed: 20.015375493s
    Apr  6 10:31:43.880: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Running", Reason="", readiness=false. Elapsed: 22.016134572s
    Apr  6 10:31:45.885: INFO: Pod "pod-subpath-test-configmap-dhpf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.021141754s
    STEP: Saw pod success 04/06/23 10:31:45.885
    Apr  6 10:31:45.886: INFO: Pod "pod-subpath-test-configmap-dhpf" satisfied condition "Succeeded or Failed"
    Apr  6 10:31:45.896: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-subpath-test-configmap-dhpf container test-container-subpath-configmap-dhpf: <nil>
    STEP: delete the pod 04/06/23 10:31:45.938
    Apr  6 10:31:45.949: INFO: Waiting for pod pod-subpath-test-configmap-dhpf to disappear
    Apr  6 10:31:45.956: INFO: Pod pod-subpath-test-configmap-dhpf no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-dhpf 04/06/23 10:31:45.957
    Apr  6 10:31:45.958: INFO: Deleting pod "pod-subpath-test-configmap-dhpf" in namespace "subpath-2417"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Apr  6 10:31:45.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2417" for this suite. 04/06/23 10:31:46.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:31:46.022
Apr  6 10:31:46.022: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename daemonsets 04/06/23 10:31:46.025
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:46.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:46.072
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Apr  6 10:31:46.131: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 04/06/23 10:31:46.141
Apr  6 10:31:46.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:46.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 04/06/23 10:31:46.146
Apr  6 10:31:46.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:46.196: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 10:31:47.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:47.203: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 10:31:48.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr  6 10:31:48.208: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 04/06/23 10:31:48.213
Apr  6 10:31:48.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr  6 10:31:48.257: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Apr  6 10:31:49.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:49.266: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/06/23 10:31:49.266
Apr  6 10:31:49.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:49.284: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 10:31:50.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:50.310: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 10:31:51.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:51.291: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 10:31:52.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:52.291: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 10:31:53.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr  6 10:31:53.293: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 04/06/23 10:31:53.31
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7867, will wait for the garbage collector to delete the pods 04/06/23 10:31:53.31
Apr  6 10:31:53.374: INFO: Deleting DaemonSet.extensions daemon-set took: 7.770418ms
Apr  6 10:31:53.575: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.105421ms
Apr  6 10:31:55.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:31:55.489: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr  6 10:31:55.500: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11372"},"items":null}

Apr  6 10:31:55.509: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11372"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:31:55.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7867" for this suite. 04/06/23 10:31:55.592
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":35,"skipped":709,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.579 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:31:46.022
    Apr  6 10:31:46.022: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename daemonsets 04/06/23 10:31:46.025
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:46.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:46.072
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Apr  6 10:31:46.131: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 04/06/23 10:31:46.141
    Apr  6 10:31:46.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:46.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 04/06/23 10:31:46.146
    Apr  6 10:31:46.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:46.196: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 10:31:47.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:47.203: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 10:31:48.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr  6 10:31:48.208: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 04/06/23 10:31:48.213
    Apr  6 10:31:48.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr  6 10:31:48.257: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Apr  6 10:31:49.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:49.266: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/06/23 10:31:49.266
    Apr  6 10:31:49.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:49.284: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 10:31:50.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:50.310: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 10:31:51.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:51.291: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 10:31:52.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:52.291: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 10:31:53.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr  6 10:31:53.293: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 04/06/23 10:31:53.31
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7867, will wait for the garbage collector to delete the pods 04/06/23 10:31:53.31
    Apr  6 10:31:53.374: INFO: Deleting DaemonSet.extensions daemon-set took: 7.770418ms
    Apr  6 10:31:53.575: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.105421ms
    Apr  6 10:31:55.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:31:55.489: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr  6 10:31:55.500: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11372"},"items":null}

    Apr  6 10:31:55.509: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11372"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:31:55.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7867" for this suite. 04/06/23 10:31:55.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:31:55.601
Apr  6 10:31:55.601: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:31:55.605
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:55.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:55.642
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:31:55.65
Apr  6 10:31:55.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5" in namespace "downward-api-3917" to be "Succeeded or Failed"
Apr  6 10:31:55.683: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.008057ms
Apr  6 10:31:57.689: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010111842s
Apr  6 10:31:59.690: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011921824s
STEP: Saw pod success 04/06/23 10:31:59.69
Apr  6 10:31:59.691: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5" satisfied condition "Succeeded or Failed"
Apr  6 10:31:59.696: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5 container client-container: <nil>
STEP: delete the pod 04/06/23 10:31:59.753
Apr  6 10:31:59.768: INFO: Waiting for pod downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5 to disappear
Apr  6 10:31:59.772: INFO: Pod downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 10:31:59.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3917" for this suite. 04/06/23 10:31:59.79
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":36,"skipped":714,"failed":0}
------------------------------
â€¢ [4.196 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:31:55.601
    Apr  6 10:31:55.601: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:31:55.605
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:55.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:55.642
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:31:55.65
    Apr  6 10:31:55.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5" in namespace "downward-api-3917" to be "Succeeded or Failed"
    Apr  6 10:31:55.683: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.008057ms
    Apr  6 10:31:57.689: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010111842s
    Apr  6 10:31:59.690: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011921824s
    STEP: Saw pod success 04/06/23 10:31:59.69
    Apr  6 10:31:59.691: INFO: Pod "downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5" satisfied condition "Succeeded or Failed"
    Apr  6 10:31:59.696: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:31:59.753
    Apr  6 10:31:59.768: INFO: Waiting for pod downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5 to disappear
    Apr  6 10:31:59.772: INFO: Pod downwardapi-volume-c9f61fba-42ed-4dc2-bf29-9b0f06ccb2f5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 10:31:59.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3917" for this suite. 04/06/23 10:31:59.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:31:59.801
Apr  6 10:31:59.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pod-network-test 04/06/23 10:31:59.802
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:59.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:59.839
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9099 04/06/23 10:31:59.846
STEP: creating a selector 04/06/23 10:31:59.846
STEP: Creating the service pods in kubernetes 04/06/23 10:31:59.846
Apr  6 10:31:59.847: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 10:31:59.954: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9099" to be "running and ready"
Apr  6 10:31:59.958: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.6422ms
Apr  6 10:31:59.958: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:32:01.965: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011335532s
Apr  6 10:32:01.965: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:32:03.966: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012357022s
Apr  6 10:32:03.971: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:32:05.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01250704s
Apr  6 10:32:05.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:07.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010836078s
Apr  6 10:32:07.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:09.964: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010494731s
Apr  6 10:32:09.964: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:11.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014308926s
Apr  6 10:32:11.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:13.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011420049s
Apr  6 10:32:13.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:15.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013589431s
Apr  6 10:32:15.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:17.964: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010553483s
Apr  6 10:32:17.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:19.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014300994s
Apr  6 10:32:19.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:32:21.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014346124s
Apr  6 10:32:21.968: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr  6 10:32:21.968: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr  6 10:32:21.973: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9099" to be "running and ready"
Apr  6 10:32:21.985: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.958368ms
Apr  6 10:32:21.985: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr  6 10:32:21.985: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr  6 10:32:22.009: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9099" to be "running and ready"
Apr  6 10:32:22.015: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.285951ms
Apr  6 10:32:22.015: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr  6 10:32:22.015: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr  6 10:32:22.022: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9099" to be "running and ready"
Apr  6 10:32:22.025: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.755058ms
Apr  6 10:32:22.025: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr  6 10:32:22.026: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr  6 10:32:22.030: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-9099" to be "running and ready"
Apr  6 10:32:22.042: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 11.953374ms
Apr  6 10:32:22.043: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr  6 10:32:22.043: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 04/06/23 10:32:22.049
Apr  6 10:32:22.067: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9099" to be "running"
Apr  6 10:32:22.072: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901667ms
Apr  6 10:32:24.078: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010689216s
Apr  6 10:32:24.078: INFO: Pod "test-container-pod" satisfied condition "running"
Apr  6 10:32:24.082: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9099" to be "running"
Apr  6 10:32:24.086: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.145242ms
Apr  6 10:32:24.086: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr  6 10:32:24.092: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Apr  6 10:32:24.092: INFO: Going to poll 10.96.3.5 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:32:24.096: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.3.5:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:32:24.096: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:32:24.097: INFO: ExecWithOptions: Clientset creation
Apr  6 10:32:24.097: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.3.5%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:32:24.533: INFO: Found all 1 expected endpoints: [netserver-0]
Apr  6 10:32:24.533: INFO: Going to poll 10.96.2.23 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:32:24.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.2.23:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:32:24.539: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:32:24.540: INFO: ExecWithOptions: Clientset creation
Apr  6 10:32:24.540: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.2.23%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:32:25.035: INFO: Found all 1 expected endpoints: [netserver-1]
Apr  6 10:32:25.035: INFO: Going to poll 10.96.0.12 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:32:25.040: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.0.12:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:32:25.040: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:32:25.041: INFO: ExecWithOptions: Clientset creation
Apr  6 10:32:25.041: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.0.12%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:32:25.608: INFO: Found all 1 expected endpoints: [netserver-2]
Apr  6 10:32:25.609: INFO: Going to poll 10.96.1.20 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:32:25.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.1.20:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:32:25.616: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:32:25.617: INFO: ExecWithOptions: Clientset creation
Apr  6 10:32:25.618: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.1.20%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:32:26.149: INFO: Found all 1 expected endpoints: [netserver-3]
Apr  6 10:32:26.149: INFO: Going to poll 10.96.4.4 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:32:26.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.4.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:32:26.160: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:32:26.160: INFO: ExecWithOptions: Clientset creation
Apr  6 10:32:26.161: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.4.4%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:32:26.692: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Apr  6 10:32:26.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9099" for this suite. 04/06/23 10:32:26.705
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":37,"skipped":731,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.913 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:31:59.801
    Apr  6 10:31:59.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pod-network-test 04/06/23 10:31:59.802
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:31:59.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:31:59.839
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9099 04/06/23 10:31:59.846
    STEP: creating a selector 04/06/23 10:31:59.846
    STEP: Creating the service pods in kubernetes 04/06/23 10:31:59.846
    Apr  6 10:31:59.847: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr  6 10:31:59.954: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9099" to be "running and ready"
    Apr  6 10:31:59.958: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.6422ms
    Apr  6 10:31:59.958: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:32:01.965: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011335532s
    Apr  6 10:32:01.965: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:32:03.966: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012357022s
    Apr  6 10:32:03.971: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:32:05.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01250704s
    Apr  6 10:32:05.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:07.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010836078s
    Apr  6 10:32:07.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:09.964: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010494731s
    Apr  6 10:32:09.964: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:11.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014308926s
    Apr  6 10:32:11.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:13.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011420049s
    Apr  6 10:32:13.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:15.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013589431s
    Apr  6 10:32:15.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:17.964: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010553483s
    Apr  6 10:32:17.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:19.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014300994s
    Apr  6 10:32:19.968: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:32:21.968: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014346124s
    Apr  6 10:32:21.968: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr  6 10:32:21.968: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr  6 10:32:21.973: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9099" to be "running and ready"
    Apr  6 10:32:21.985: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.958368ms
    Apr  6 10:32:21.985: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr  6 10:32:21.985: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr  6 10:32:22.009: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9099" to be "running and ready"
    Apr  6 10:32:22.015: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.285951ms
    Apr  6 10:32:22.015: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr  6 10:32:22.015: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr  6 10:32:22.022: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9099" to be "running and ready"
    Apr  6 10:32:22.025: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.755058ms
    Apr  6 10:32:22.025: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr  6 10:32:22.026: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr  6 10:32:22.030: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-9099" to be "running and ready"
    Apr  6 10:32:22.042: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 11.953374ms
    Apr  6 10:32:22.043: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr  6 10:32:22.043: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 04/06/23 10:32:22.049
    Apr  6 10:32:22.067: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9099" to be "running"
    Apr  6 10:32:22.072: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901667ms
    Apr  6 10:32:24.078: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010689216s
    Apr  6 10:32:24.078: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr  6 10:32:24.082: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9099" to be "running"
    Apr  6 10:32:24.086: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.145242ms
    Apr  6 10:32:24.086: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr  6 10:32:24.092: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Apr  6 10:32:24.092: INFO: Going to poll 10.96.3.5 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:32:24.096: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.3.5:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:32:24.096: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:32:24.097: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:32:24.097: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.3.5%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:32:24.533: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr  6 10:32:24.533: INFO: Going to poll 10.96.2.23 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:32:24.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.2.23:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:32:24.539: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:32:24.540: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:32:24.540: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.2.23%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:32:25.035: INFO: Found all 1 expected endpoints: [netserver-1]
    Apr  6 10:32:25.035: INFO: Going to poll 10.96.0.12 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:32:25.040: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.0.12:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:32:25.040: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:32:25.041: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:32:25.041: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.0.12%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:32:25.608: INFO: Found all 1 expected endpoints: [netserver-2]
    Apr  6 10:32:25.609: INFO: Going to poll 10.96.1.20 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:32:25.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.1.20:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:32:25.616: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:32:25.617: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:32:25.618: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.1.20%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:32:26.149: INFO: Found all 1 expected endpoints: [netserver-3]
    Apr  6 10:32:26.149: INFO: Going to poll 10.96.4.4 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:32:26.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.96.4.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9099 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:32:26.160: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:32:26.160: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:32:26.161: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-9099/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.96.4.4%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:32:26.692: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Apr  6 10:32:26.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9099" for this suite. 04/06/23 10:32:26.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:32:26.715
Apr  6 10:32:26.715: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replication-controller 04/06/23 10:32:26.717
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:26.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:26.766
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb 04/06/23 10:32:26.775
Apr  6 10:32:26.795: INFO: Pod name my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb: Found 0 pods out of 1
Apr  6 10:32:31.801: INFO: Pod name my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb: Found 1 pods out of 1
Apr  6 10:32:31.801: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb" are running
Apr  6 10:32:31.801: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l" in namespace "replication-controller-8093" to be "running"
Apr  6 10:32:31.808: INFO: Pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l": Phase="Running", Reason="", readiness=true. Elapsed: 7.384482ms
Apr  6 10:32:31.808: INFO: Pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l" satisfied condition "running"
Apr  6 10:32:31.808: INFO: Pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:26 +0000 UTC Reason: Message:}])
Apr  6 10:32:31.808: INFO: Trying to dial the pod
Apr  6 10:32:36.943: INFO: Controller my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb: Got expected result from replica 1 [my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l]: "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Apr  6 10:32:36.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8093" for this suite. 04/06/23 10:32:36.96
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":38,"skipped":738,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.253 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:32:26.715
    Apr  6 10:32:26.715: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replication-controller 04/06/23 10:32:26.717
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:26.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:26.766
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb 04/06/23 10:32:26.775
    Apr  6 10:32:26.795: INFO: Pod name my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb: Found 0 pods out of 1
    Apr  6 10:32:31.801: INFO: Pod name my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb: Found 1 pods out of 1
    Apr  6 10:32:31.801: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb" are running
    Apr  6 10:32:31.801: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l" in namespace "replication-controller-8093" to be "running"
    Apr  6 10:32:31.808: INFO: Pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l": Phase="Running", Reason="", readiness=true. Elapsed: 7.384482ms
    Apr  6 10:32:31.808: INFO: Pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l" satisfied condition "running"
    Apr  6 10:32:31.808: INFO: Pod "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-06 10:32:26 +0000 UTC Reason: Message:}])
    Apr  6 10:32:31.808: INFO: Trying to dial the pod
    Apr  6 10:32:36.943: INFO: Controller my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb: Got expected result from replica 1 [my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l]: "my-hostname-basic-5870551b-8f6d-44b4-a376-8bbcb0f01eeb-jvp6l", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Apr  6 10:32:36.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8093" for this suite. 04/06/23 10:32:36.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:32:36.972
Apr  6 10:32:36.972: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:32:36.983
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:37.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:37.042
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:32:37.07
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:32:38.201
STEP: Deploying the webhook pod 04/06/23 10:32:38.22
STEP: Wait for the deployment to be ready 04/06/23 10:32:38.234
Apr  6 10:32:38.244: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:32:40.266
STEP: Verifying the service has paired with the endpoint 04/06/23 10:32:40.277
Apr  6 10:32:41.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 04/06/23 10:32:41.289
STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:32:41.425
STEP: Updating a validating webhook configuration's rules to not include the create operation 04/06/23 10:32:41.544
STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:32:41.56
STEP: Patching a validating webhook configuration's rules to include the create operation 04/06/23 10:32:41.577
STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:32:41.59
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:32:41.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1092" for this suite. 04/06/23 10:32:41.626
STEP: Destroying namespace "webhook-1092-markers" for this suite. 04/06/23 10:32:41.632
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":39,"skipped":799,"failed":0}
------------------------------
â€¢ [4.721 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:32:36.972
    Apr  6 10:32:36.972: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:32:36.983
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:37.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:37.042
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:32:37.07
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:32:38.201
    STEP: Deploying the webhook pod 04/06/23 10:32:38.22
    STEP: Wait for the deployment to be ready 04/06/23 10:32:38.234
    Apr  6 10:32:38.244: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:32:40.266
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:32:40.277
    Apr  6 10:32:41.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 04/06/23 10:32:41.289
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:32:41.425
    STEP: Updating a validating webhook configuration's rules to not include the create operation 04/06/23 10:32:41.544
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:32:41.56
    STEP: Patching a validating webhook configuration's rules to include the create operation 04/06/23 10:32:41.577
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:32:41.59
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:32:41.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1092" for this suite. 04/06/23 10:32:41.626
    STEP: Destroying namespace "webhook-1092-markers" for this suite. 04/06/23 10:32:41.632
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:32:41.694
Apr  6 10:32:41.694: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:32:41.696
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:41.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:41.724
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-88ddb862-956c-466d-a9d9-96a6c7e39129 04/06/23 10:32:41.729
STEP: Creating a pod to test consume configMaps 04/06/23 10:32:41.737
Apr  6 10:32:41.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95" in namespace "configmap-4937" to be "Succeeded or Failed"
Apr  6 10:32:41.756: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102645ms
Apr  6 10:32:43.764: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012455175s
Apr  6 10:32:45.766: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014630413s
STEP: Saw pod success 04/06/23 10:32:45.766
Apr  6 10:32:45.767: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95" satisfied condition "Succeeded or Failed"
Apr  6 10:32:45.777: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:32:45.834
Apr  6 10:32:45.847: INFO: Waiting for pod pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95 to disappear
Apr  6 10:32:45.857: INFO: Pod pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:32:45.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4937" for this suite. 04/06/23 10:32:45.867
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":40,"skipped":806,"failed":0}
------------------------------
â€¢ [4.188 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:32:41.694
    Apr  6 10:32:41.694: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:32:41.696
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:41.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:41.724
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-88ddb862-956c-466d-a9d9-96a6c7e39129 04/06/23 10:32:41.729
    STEP: Creating a pod to test consume configMaps 04/06/23 10:32:41.737
    Apr  6 10:32:41.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95" in namespace "configmap-4937" to be "Succeeded or Failed"
    Apr  6 10:32:41.756: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102645ms
    Apr  6 10:32:43.764: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012455175s
    Apr  6 10:32:45.766: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014630413s
    STEP: Saw pod success 04/06/23 10:32:45.766
    Apr  6 10:32:45.767: INFO: Pod "pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95" satisfied condition "Succeeded or Failed"
    Apr  6 10:32:45.777: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:32:45.834
    Apr  6 10:32:45.847: INFO: Waiting for pod pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95 to disappear
    Apr  6 10:32:45.857: INFO: Pod pod-configmaps-53da2df4-7fab-4a48-826e-48e5927f4b95 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:32:45.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4937" for this suite. 04/06/23 10:32:45.867
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:32:45.883
Apr  6 10:32:45.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename gc 04/06/23 10:32:45.89
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:45.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:45.919
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 04/06/23 10:32:45.925
STEP: delete the rc 04/06/23 10:32:50.94
STEP: wait for all pods to be garbage collected 04/06/23 10:32:50.948
STEP: Gathering metrics 04/06/23 10:32:55.961
W0406 10:32:55.995876      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 10:32:55.995: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Apr  6 10:32:55.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6420" for this suite. 04/06/23 10:32:56.008
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":41,"skipped":808,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.135 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:32:45.883
    Apr  6 10:32:45.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename gc 04/06/23 10:32:45.89
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:45.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:45.919
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 04/06/23 10:32:45.925
    STEP: delete the rc 04/06/23 10:32:50.94
    STEP: wait for all pods to be garbage collected 04/06/23 10:32:50.948
    STEP: Gathering metrics 04/06/23 10:32:55.961
    W0406 10:32:55.995876      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr  6 10:32:55.995: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Apr  6 10:32:55.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6420" for this suite. 04/06/23 10:32:56.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:32:56.026
Apr  6 10:32:56.027: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:32:56.028
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:56.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:56.063
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:32:56.07
Apr  6 10:32:56.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b" in namespace "downward-api-8985" to be "Succeeded or Failed"
Apr  6 10:32:56.091: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.914832ms
Apr  6 10:32:58.097: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b": Phase="Running", Reason="", readiness=false. Elapsed: 2.012577437s
Apr  6 10:33:00.098: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013724802s
STEP: Saw pod success 04/06/23 10:33:00.098
Apr  6 10:33:00.099: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b" satisfied condition "Succeeded or Failed"
Apr  6 10:33:00.103: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b container client-container: <nil>
STEP: delete the pod 04/06/23 10:33:00.183
Apr  6 10:33:00.193: INFO: Waiting for pod downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b to disappear
Apr  6 10:33:00.197: INFO: Pod downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 10:33:00.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8985" for this suite. 04/06/23 10:33:00.205
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":42,"skipped":836,"failed":0}
------------------------------
â€¢ [4.184 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:32:56.026
    Apr  6 10:32:56.027: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:32:56.028
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:32:56.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:32:56.063
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:32:56.07
    Apr  6 10:32:56.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b" in namespace "downward-api-8985" to be "Succeeded or Failed"
    Apr  6 10:32:56.091: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.914832ms
    Apr  6 10:32:58.097: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b": Phase="Running", Reason="", readiness=false. Elapsed: 2.012577437s
    Apr  6 10:33:00.098: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013724802s
    STEP: Saw pod success 04/06/23 10:33:00.098
    Apr  6 10:33:00.099: INFO: Pod "downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b" satisfied condition "Succeeded or Failed"
    Apr  6 10:33:00.103: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b container client-container: <nil>
    STEP: delete the pod 04/06/23 10:33:00.183
    Apr  6 10:33:00.193: INFO: Waiting for pod downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b to disappear
    Apr  6 10:33:00.197: INFO: Pod downwardapi-volume-4f39effd-9c14-4f97-b0bc-74507c0af27b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 10:33:00.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8985" for this suite. 04/06/23 10:33:00.205
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:00.21
Apr  6 10:33:00.210: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 10:33:00.211
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:00.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:00.229
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 04/06/23 10:33:00.234
STEP: Getting a ResourceQuota 04/06/23 10:33:00.239
STEP: Listing all ResourceQuotas with LabelSelector 04/06/23 10:33:00.243
STEP: Patching the ResourceQuota 04/06/23 10:33:00.247
STEP: Deleting a Collection of ResourceQuotas 04/06/23 10:33:00.253
STEP: Verifying the deleted ResourceQuota 04/06/23 10:33:00.26
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 10:33:00.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9752" for this suite. 04/06/23 10:33:00.271
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":43,"skipped":836,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:00.21
    Apr  6 10:33:00.210: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 10:33:00.211
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:00.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:00.229
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 04/06/23 10:33:00.234
    STEP: Getting a ResourceQuota 04/06/23 10:33:00.239
    STEP: Listing all ResourceQuotas with LabelSelector 04/06/23 10:33:00.243
    STEP: Patching the ResourceQuota 04/06/23 10:33:00.247
    STEP: Deleting a Collection of ResourceQuotas 04/06/23 10:33:00.253
    STEP: Verifying the deleted ResourceQuota 04/06/23 10:33:00.26
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 10:33:00.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9752" for this suite. 04/06/23 10:33:00.271
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:00.277
Apr  6 10:33:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:33:00.278
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:00.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:00.32
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:33:00.326
Apr  6 10:33:00.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3" in namespace "projected-2810" to be "Succeeded or Failed"
Apr  6 10:33:00.344: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.512905ms
Apr  6 10:33:02.365: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026896401s
Apr  6 10:33:04.352: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013710403s
STEP: Saw pod success 04/06/23 10:33:04.352
Apr  6 10:33:04.352: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3" satisfied condition "Succeeded or Failed"
Apr  6 10:33:04.361: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3 container client-container: <nil>
STEP: delete the pod 04/06/23 10:33:04.376
Apr  6 10:33:04.389: INFO: Waiting for pod downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3 to disappear
Apr  6 10:33:04.394: INFO: Pod downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 10:33:04.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2810" for this suite. 04/06/23 10:33:04.404
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":44,"skipped":837,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:00.277
    Apr  6 10:33:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:33:00.278
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:00.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:00.32
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:33:00.326
    Apr  6 10:33:00.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3" in namespace "projected-2810" to be "Succeeded or Failed"
    Apr  6 10:33:00.344: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.512905ms
    Apr  6 10:33:02.365: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026896401s
    Apr  6 10:33:04.352: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013710403s
    STEP: Saw pod success 04/06/23 10:33:04.352
    Apr  6 10:33:04.352: INFO: Pod "downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3" satisfied condition "Succeeded or Failed"
    Apr  6 10:33:04.361: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:33:04.376
    Apr  6 10:33:04.389: INFO: Waiting for pod downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3 to disappear
    Apr  6 10:33:04.394: INFO: Pod downwardapi-volume-afe2ed28-c4de-472a-83d9-b52217ae84b3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 10:33:04.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2810" for this suite. 04/06/23 10:33:04.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:04.412
Apr  6 10:33:04.412: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:33:04.413
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:04.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:04.439
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 04/06/23 10:33:04.447
Apr  6 10:33:04.464: INFO: Waiting up to 5m0s for pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf" in namespace "downward-api-6366" to be "Succeeded or Failed"
Apr  6 10:33:04.468: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.707321ms
Apr  6 10:33:06.487: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023356531s
Apr  6 10:33:08.475: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010879887s
STEP: Saw pod success 04/06/23 10:33:08.475
Apr  6 10:33:08.475: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf" satisfied condition "Succeeded or Failed"
Apr  6 10:33:08.480: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf container dapi-container: <nil>
STEP: delete the pod 04/06/23 10:33:08.513
Apr  6 10:33:08.526: INFO: Waiting for pod downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf to disappear
Apr  6 10:33:08.530: INFO: Pod downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Apr  6 10:33:08.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6366" for this suite. 04/06/23 10:33:08.539
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":45,"skipped":852,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:04.412
    Apr  6 10:33:04.412: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:33:04.413
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:04.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:04.439
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 04/06/23 10:33:04.447
    Apr  6 10:33:04.464: INFO: Waiting up to 5m0s for pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf" in namespace "downward-api-6366" to be "Succeeded or Failed"
    Apr  6 10:33:04.468: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.707321ms
    Apr  6 10:33:06.487: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023356531s
    Apr  6 10:33:08.475: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010879887s
    STEP: Saw pod success 04/06/23 10:33:08.475
    Apr  6 10:33:08.475: INFO: Pod "downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf" satisfied condition "Succeeded or Failed"
    Apr  6 10:33:08.480: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf container dapi-container: <nil>
    STEP: delete the pod 04/06/23 10:33:08.513
    Apr  6 10:33:08.526: INFO: Waiting for pod downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf to disappear
    Apr  6 10:33:08.530: INFO: Pod downward-api-e80e3ba3-a2b5-4f43-9d59-19102a8458cf no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Apr  6 10:33:08.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6366" for this suite. 04/06/23 10:33:08.539
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:08.547
Apr  6 10:33:08.547: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:33:08.548
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:08.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:08.59
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/06/23 10:33:08.601
Apr  6 10:33:08.602: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:33:11.891: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:33:27.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8751" for this suite. 04/06/23 10:33:27.577
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":46,"skipped":852,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.040 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:08.547
    Apr  6 10:33:08.547: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:33:08.548
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:08.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:08.59
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/06/23 10:33:08.601
    Apr  6 10:33:08.602: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:33:11.891: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:33:27.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8751" for this suite. 04/06/23 10:33:27.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:27.606
Apr  6 10:33:27.606: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:33:27.607
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:27.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:27.634
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Apr  6 10:33:27.677: INFO: created pod pod-service-account-defaultsa
Apr  6 10:33:27.678: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr  6 10:33:27.693: INFO: created pod pod-service-account-mountsa
Apr  6 10:33:27.693: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr  6 10:33:27.705: INFO: created pod pod-service-account-nomountsa
Apr  6 10:33:27.705: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr  6 10:33:27.712: INFO: created pod pod-service-account-defaultsa-mountspec
Apr  6 10:33:27.712: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr  6 10:33:27.720: INFO: created pod pod-service-account-mountsa-mountspec
Apr  6 10:33:27.720: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr  6 10:33:27.729: INFO: created pod pod-service-account-nomountsa-mountspec
Apr  6 10:33:27.729: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr  6 10:33:27.736: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr  6 10:33:27.736: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr  6 10:33:27.742: INFO: created pod pod-service-account-mountsa-nomountspec
Apr  6 10:33:27.742: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr  6 10:33:27.749: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr  6 10:33:27.749: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Apr  6 10:33:27.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9635" for this suite. 04/06/23 10:33:27.757
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":47,"skipped":955,"failed":0}
------------------------------
â€¢ [0.157 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:27.606
    Apr  6 10:33:27.606: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:33:27.607
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:27.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:27.634
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Apr  6 10:33:27.677: INFO: created pod pod-service-account-defaultsa
    Apr  6 10:33:27.678: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Apr  6 10:33:27.693: INFO: created pod pod-service-account-mountsa
    Apr  6 10:33:27.693: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Apr  6 10:33:27.705: INFO: created pod pod-service-account-nomountsa
    Apr  6 10:33:27.705: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Apr  6 10:33:27.712: INFO: created pod pod-service-account-defaultsa-mountspec
    Apr  6 10:33:27.712: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Apr  6 10:33:27.720: INFO: created pod pod-service-account-mountsa-mountspec
    Apr  6 10:33:27.720: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Apr  6 10:33:27.729: INFO: created pod pod-service-account-nomountsa-mountspec
    Apr  6 10:33:27.729: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Apr  6 10:33:27.736: INFO: created pod pod-service-account-defaultsa-nomountspec
    Apr  6 10:33:27.736: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Apr  6 10:33:27.742: INFO: created pod pod-service-account-mountsa-nomountspec
    Apr  6 10:33:27.742: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Apr  6 10:33:27.749: INFO: created pod pod-service-account-nomountsa-nomountspec
    Apr  6 10:33:27.749: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Apr  6 10:33:27.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9635" for this suite. 04/06/23 10:33:27.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:27.764
Apr  6 10:33:27.764: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replicaset 04/06/23 10:33:27.765
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:27.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:27.808
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/06/23 10:33:27.83
Apr  6 10:33:27.843: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-349" to be "running and ready"
Apr  6 10:33:27.847: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025342ms
Apr  6 10:33:27.848: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:33:29.857: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014470468s
Apr  6 10:33:29.857: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:33:31.855: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012779935s
Apr  6 10:33:31.855: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:33:33.859: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 6.01667799s
Apr  6 10:33:33.859: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Apr  6 10:33:33.859: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 04/06/23 10:33:33.867
STEP: Then the orphan pod is adopted 04/06/23 10:33:33.876
STEP: When the matched label of one of its pods change 04/06/23 10:33:34.892
Apr  6 10:33:34.897: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 04/06/23 10:33:34.91
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Apr  6 10:33:35.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-349" for this suite. 04/06/23 10:33:35.934
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":48,"skipped":974,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.181 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:27.764
    Apr  6 10:33:27.764: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replicaset 04/06/23 10:33:27.765
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:27.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:27.808
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/06/23 10:33:27.83
    Apr  6 10:33:27.843: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-349" to be "running and ready"
    Apr  6 10:33:27.847: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025342ms
    Apr  6 10:33:27.848: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:33:29.857: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014470468s
    Apr  6 10:33:29.857: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:33:31.855: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012779935s
    Apr  6 10:33:31.855: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:33:33.859: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 6.01667799s
    Apr  6 10:33:33.859: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Apr  6 10:33:33.859: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 04/06/23 10:33:33.867
    STEP: Then the orphan pod is adopted 04/06/23 10:33:33.876
    STEP: When the matched label of one of its pods change 04/06/23 10:33:34.892
    Apr  6 10:33:34.897: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/06/23 10:33:34.91
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Apr  6 10:33:35.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-349" for this suite. 04/06/23 10:33:35.934
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:35.945
Apr  6 10:33:35.945: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:33:35.948
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:36.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:36.015
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-28ce1be0-2b2b-44f5-9006-527fd74d63b7 04/06/23 10:33:36.023
STEP: Creating a pod to test consume secrets 04/06/23 10:33:36.03
Apr  6 10:33:36.049: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27" in namespace "projected-1294" to be "Succeeded or Failed"
Apr  6 10:33:36.054: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177502ms
Apr  6 10:33:38.062: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013487743s
Apr  6 10:33:40.065: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016649898s
STEP: Saw pod success 04/06/23 10:33:40.067
Apr  6 10:33:40.068: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27" satisfied condition "Succeeded or Failed"
Apr  6 10:33:40.075: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st pod pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/06/23 10:33:40.101
Apr  6 10:33:40.110: INFO: Waiting for pod pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27 to disappear
Apr  6 10:33:40.113: INFO: Pod pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Apr  6 10:33:40.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1294" for this suite. 04/06/23 10:33:40.121
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":49,"skipped":974,"failed":0}
------------------------------
â€¢ [4.181 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:35.945
    Apr  6 10:33:35.945: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:33:35.948
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:36.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:36.015
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-28ce1be0-2b2b-44f5-9006-527fd74d63b7 04/06/23 10:33:36.023
    STEP: Creating a pod to test consume secrets 04/06/23 10:33:36.03
    Apr  6 10:33:36.049: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27" in namespace "projected-1294" to be "Succeeded or Failed"
    Apr  6 10:33:36.054: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177502ms
    Apr  6 10:33:38.062: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013487743s
    Apr  6 10:33:40.065: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016649898s
    STEP: Saw pod success 04/06/23 10:33:40.067
    Apr  6 10:33:40.068: INFO: Pod "pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27" satisfied condition "Succeeded or Failed"
    Apr  6 10:33:40.075: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st pod pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:33:40.101
    Apr  6 10:33:40.110: INFO: Waiting for pod pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27 to disappear
    Apr  6 10:33:40.113: INFO: Pod pod-projected-secrets-4da07e57-f9f9-4848-9894-111f1c5f6d27 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Apr  6 10:33:40.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1294" for this suite. 04/06/23 10:33:40.121
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:40.126
Apr  6 10:33:40.126: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:33:40.128
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:40.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:40.152
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:33:40.174
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:33:40.577
STEP: Deploying the webhook pod 04/06/23 10:33:40.592
STEP: Wait for the deployment to be ready 04/06/23 10:33:40.606
Apr  6 10:33:40.627: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:33:42.649
STEP: Verifying the service has paired with the endpoint 04/06/23 10:33:42.663
Apr  6 10:33:43.663: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Apr  6 10:33:43.669: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/06/23 10:33:44.194
STEP: Creating a custom resource that should be denied by the webhook 04/06/23 10:33:44.344
STEP: Creating a custom resource whose deletion would be denied by the webhook 04/06/23 10:33:46.507
STEP: Updating the custom resource with disallowed data should be denied 04/06/23 10:33:46.56
STEP: Deleting the custom resource should be denied 04/06/23 10:33:46.667
STEP: Remove the offending key and value from the custom resource data 04/06/23 10:33:46.782
STEP: Deleting the updated custom resource should be successful 04/06/23 10:33:46.847
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:33:47.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8831" for this suite. 04/06/23 10:33:47.484
STEP: Destroying namespace "webhook-8831-markers" for this suite. 04/06/23 10:33:47.493
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":50,"skipped":976,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.436 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:40.126
    Apr  6 10:33:40.126: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:33:40.128
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:40.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:40.152
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:33:40.174
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:33:40.577
    STEP: Deploying the webhook pod 04/06/23 10:33:40.592
    STEP: Wait for the deployment to be ready 04/06/23 10:33:40.606
    Apr  6 10:33:40.627: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:33:42.649
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:33:42.663
    Apr  6 10:33:43.663: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Apr  6 10:33:43.669: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/06/23 10:33:44.194
    STEP: Creating a custom resource that should be denied by the webhook 04/06/23 10:33:44.344
    STEP: Creating a custom resource whose deletion would be denied by the webhook 04/06/23 10:33:46.507
    STEP: Updating the custom resource with disallowed data should be denied 04/06/23 10:33:46.56
    STEP: Deleting the custom resource should be denied 04/06/23 10:33:46.667
    STEP: Remove the offending key and value from the custom resource data 04/06/23 10:33:46.782
    STEP: Deleting the updated custom resource should be successful 04/06/23 10:33:46.847
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:33:47.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8831" for this suite. 04/06/23 10:33:47.484
    STEP: Destroying namespace "webhook-8831-markers" for this suite. 04/06/23 10:33:47.493
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:47.563
Apr  6 10:33:47.563: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename events 04/06/23 10:33:47.565
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:47.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:47.63
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 04/06/23 10:33:47.643
STEP: listing events in all namespaces 04/06/23 10:33:47.65
STEP: listing events in test namespace 04/06/23 10:33:47.666
STEP: listing events with field selection filtering on source 04/06/23 10:33:47.676
STEP: listing events with field selection filtering on reportingController 04/06/23 10:33:47.685
STEP: getting the test event 04/06/23 10:33:47.69
STEP: patching the test event 04/06/23 10:33:47.693
STEP: getting the test event 04/06/23 10:33:47.702
STEP: updating the test event 04/06/23 10:33:47.706
STEP: getting the test event 04/06/23 10:33:47.717
STEP: deleting the test event 04/06/23 10:33:47.733
STEP: listing events in all namespaces 04/06/23 10:33:47.741
STEP: listing events in test namespace 04/06/23 10:33:47.765
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Apr  6 10:33:47.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5783" for this suite. 04/06/23 10:33:47.781
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":51,"skipped":977,"failed":0}
------------------------------
â€¢ [0.247 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:47.563
    Apr  6 10:33:47.563: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename events 04/06/23 10:33:47.565
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:47.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:47.63
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 04/06/23 10:33:47.643
    STEP: listing events in all namespaces 04/06/23 10:33:47.65
    STEP: listing events in test namespace 04/06/23 10:33:47.666
    STEP: listing events with field selection filtering on source 04/06/23 10:33:47.676
    STEP: listing events with field selection filtering on reportingController 04/06/23 10:33:47.685
    STEP: getting the test event 04/06/23 10:33:47.69
    STEP: patching the test event 04/06/23 10:33:47.693
    STEP: getting the test event 04/06/23 10:33:47.702
    STEP: updating the test event 04/06/23 10:33:47.706
    STEP: getting the test event 04/06/23 10:33:47.717
    STEP: deleting the test event 04/06/23 10:33:47.733
    STEP: listing events in all namespaces 04/06/23 10:33:47.741
    STEP: listing events in test namespace 04/06/23 10:33:47.765
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Apr  6 10:33:47.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5783" for this suite. 04/06/23 10:33:47.781
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:47.81
Apr  6 10:33:47.810: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename endpointslicemirroring 04/06/23 10:33:47.811
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:47.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:47.841
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 04/06/23 10:33:47.864
Apr  6 10:33:47.874: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 04/06/23 10:33:49.883
STEP: mirroring deletion of a custom Endpoint 04/06/23 10:33:49.893
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Apr  6 10:33:49.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3292" for this suite. 04/06/23 10:33:49.919
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":52,"skipped":977,"failed":0}
------------------------------
â€¢ [2.118 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:47.81
    Apr  6 10:33:47.810: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename endpointslicemirroring 04/06/23 10:33:47.811
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:47.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:47.841
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 04/06/23 10:33:47.864
    Apr  6 10:33:47.874: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 04/06/23 10:33:49.883
    STEP: mirroring deletion of a custom Endpoint 04/06/23 10:33:49.893
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Apr  6 10:33:49.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-3292" for this suite. 04/06/23 10:33:49.919
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:49.93
Apr  6 10:33:49.930: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 10:33:49.931
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:49.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:49.969
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 10:33:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6810" for this suite. 04/06/23 10:33:50.077
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":53,"skipped":981,"failed":0}
------------------------------
â€¢ [0.161 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:49.93
    Apr  6 10:33:49.930: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 10:33:49.931
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:49.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:49.969
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 10:33:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6810" for this suite. 04/06/23 10:33:50.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:50.091
Apr  6 10:33:50.092: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:33:50.093
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:50.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:50.115
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 04/06/23 10:33:50.121
Apr  6 10:33:50.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1891 create -f -'
Apr  6 10:33:51.330: INFO: stderr: ""
Apr  6 10:33:51.330: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 04/06/23 10:33:51.33
Apr  6 10:33:51.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1891 diff -f -'
Apr  6 10:33:52.423: INFO: rc: 1
Apr  6 10:33:52.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1891 delete -f -'
Apr  6 10:33:52.660: INFO: stderr: ""
Apr  6 10:33:52.660: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:33:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1891" for this suite. 04/06/23 10:33:52.672
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":54,"skipped":996,"failed":0}
------------------------------
â€¢ [2.588 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:50.091
    Apr  6 10:33:50.092: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:33:50.093
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:50.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:50.115
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 04/06/23 10:33:50.121
    Apr  6 10:33:50.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1891 create -f -'
    Apr  6 10:33:51.330: INFO: stderr: ""
    Apr  6 10:33:51.330: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 04/06/23 10:33:51.33
    Apr  6 10:33:51.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1891 diff -f -'
    Apr  6 10:33:52.423: INFO: rc: 1
    Apr  6 10:33:52.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1891 delete -f -'
    Apr  6 10:33:52.660: INFO: stderr: ""
    Apr  6 10:33:52.660: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:33:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1891" for this suite. 04/06/23 10:33:52.672
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:52.68
Apr  6 10:33:52.680: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename containers 04/06/23 10:33:52.681
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:52.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:52.71
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 04/06/23 10:33:52.717
Apr  6 10:33:52.732: INFO: Waiting up to 5m0s for pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3" in namespace "containers-9120" to be "Succeeded or Failed"
Apr  6 10:33:52.737: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.502475ms
Apr  6 10:33:54.744: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011390963s
Apr  6 10:33:56.744: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012037368s
STEP: Saw pod success 04/06/23 10:33:56.744
Apr  6 10:33:56.745: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3" satisfied condition "Succeeded or Failed"
Apr  6 10:33:56.750: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:33:56.765
Apr  6 10:33:56.775: INFO: Waiting for pod client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3 to disappear
Apr  6 10:33:56.779: INFO: Pod client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Apr  6 10:33:56.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9120" for this suite. 04/06/23 10:33:56.788
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":55,"skipped":997,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:52.68
    Apr  6 10:33:52.680: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename containers 04/06/23 10:33:52.681
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:52.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:52.71
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 04/06/23 10:33:52.717
    Apr  6 10:33:52.732: INFO: Waiting up to 5m0s for pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3" in namespace "containers-9120" to be "Succeeded or Failed"
    Apr  6 10:33:52.737: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.502475ms
    Apr  6 10:33:54.744: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011390963s
    Apr  6 10:33:56.744: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012037368s
    STEP: Saw pod success 04/06/23 10:33:56.744
    Apr  6 10:33:56.745: INFO: Pod "client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3" satisfied condition "Succeeded or Failed"
    Apr  6 10:33:56.750: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:33:56.765
    Apr  6 10:33:56.775: INFO: Waiting for pod client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3 to disappear
    Apr  6 10:33:56.779: INFO: Pod client-containers-068a5c5b-1eab-4882-bfb0-6401687985e3 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Apr  6 10:33:56.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9120" for this suite. 04/06/23 10:33:56.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:33:56.796
Apr  6 10:33:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:33:56.798
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:56.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:56.83
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:33:56.855
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:33:57.114
STEP: Deploying the webhook pod 04/06/23 10:33:57.122
STEP: Wait for the deployment to be ready 04/06/23 10:33:57.136
Apr  6 10:33:57.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:33:59.172
STEP: Verifying the service has paired with the endpoint 04/06/23 10:33:59.187
Apr  6 10:34:00.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/06/23 10:34:00.195
STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:00.196
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/06/23 10:34:00.291
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/06/23 10:34:01.305
STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:01.305
STEP: Having no error when timeout is longer than webhook latency 04/06/23 10:34:02.347
STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:02.347
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/06/23 10:34:07.558
STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:07.559
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:34:12.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5532" for this suite. 04/06/23 10:34:12.646
STEP: Destroying namespace "webhook-5532-markers" for this suite. 04/06/23 10:34:12.656
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":56,"skipped":1010,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.917 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:33:56.796
    Apr  6 10:33:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:33:56.798
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:33:56.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:33:56.83
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:33:56.855
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:33:57.114
    STEP: Deploying the webhook pod 04/06/23 10:33:57.122
    STEP: Wait for the deployment to be ready 04/06/23 10:33:57.136
    Apr  6 10:33:57.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:33:59.172
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:33:59.187
    Apr  6 10:34:00.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/06/23 10:34:00.195
    STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:00.196
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/06/23 10:34:00.291
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/06/23 10:34:01.305
    STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:01.305
    STEP: Having no error when timeout is longer than webhook latency 04/06/23 10:34:02.347
    STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:02.347
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/06/23 10:34:07.558
    STEP: Registering slow webhook via the AdmissionRegistration API 04/06/23 10:34:07.559
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:34:12.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5532" for this suite. 04/06/23 10:34:12.646
    STEP: Destroying namespace "webhook-5532-markers" for this suite. 04/06/23 10:34:12.656
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:34:12.715
Apr  6 10:34:12.715: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:34:12.716
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:34:12.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:34:12.745
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-3982 04/06/23 10:34:12.753
STEP: creating service affinity-nodeport in namespace services-3982 04/06/23 10:34:12.753
STEP: creating replication controller affinity-nodeport in namespace services-3982 04/06/23 10:34:12.774
I0406 10:34:12.785945      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3982, replica count: 3
I0406 10:34:15.836434      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 10:34:15.860: INFO: Creating new exec pod
Apr  6 10:34:15.874: INFO: Waiting up to 5m0s for pod "execpod-affinityj4rfc" in namespace "services-3982" to be "running"
Apr  6 10:34:15.879: INFO: Pod "execpod-affinityj4rfc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.072973ms
Apr  6 10:34:17.887: INFO: Pod "execpod-affinityj4rfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012558033s
Apr  6 10:34:17.890: INFO: Pod "execpod-affinityj4rfc" satisfied condition "running"
Apr  6 10:34:18.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Apr  6 10:34:19.535: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr  6 10:34:19.536: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:34:19.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.51.61 80'
Apr  6 10:34:20.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.71.51.61 80\nConnection to 10.71.51.61 80 port [tcp/http] succeeded!\n"
Apr  6 10:34:20.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:34:20.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 32191'
Apr  6 10:34:20.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.236 32191\nConnection to 10.250.2.236 32191 port [tcp/*] succeeded!\n"
Apr  6 10:34:20.622: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:34:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.68 32191'
Apr  6 10:34:21.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.68 32191\nConnection to 10.250.0.68 32191 port [tcp/*] succeeded!\n"
Apr  6 10:34:21.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:34:21.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:32191/ ; done'
Apr  6 10:34:22.015: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n"
Apr  6 10:34:22.015: INFO: stdout: "\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862"
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
Apr  6 10:34:22.015: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3982, will wait for the garbage collector to delete the pods 04/06/23 10:34:22.026
Apr  6 10:34:22.090: INFO: Deleting ReplicationController affinity-nodeport took: 8.432317ms
Apr  6 10:34:22.190: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.453788ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:34:24.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3982" for this suite. 04/06/23 10:34:24.64
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":57,"skipped":1018,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.930 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:34:12.715
    Apr  6 10:34:12.715: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:34:12.716
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:34:12.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:34:12.745
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-3982 04/06/23 10:34:12.753
    STEP: creating service affinity-nodeport in namespace services-3982 04/06/23 10:34:12.753
    STEP: creating replication controller affinity-nodeport in namespace services-3982 04/06/23 10:34:12.774
    I0406 10:34:12.785945      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3982, replica count: 3
    I0406 10:34:15.836434      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 10:34:15.860: INFO: Creating new exec pod
    Apr  6 10:34:15.874: INFO: Waiting up to 5m0s for pod "execpod-affinityj4rfc" in namespace "services-3982" to be "running"
    Apr  6 10:34:15.879: INFO: Pod "execpod-affinityj4rfc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.072973ms
    Apr  6 10:34:17.887: INFO: Pod "execpod-affinityj4rfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012558033s
    Apr  6 10:34:17.890: INFO: Pod "execpod-affinityj4rfc" satisfied condition "running"
    Apr  6 10:34:18.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Apr  6 10:34:19.535: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Apr  6 10:34:19.536: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:34:19.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.51.61 80'
    Apr  6 10:34:20.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.71.51.61 80\nConnection to 10.71.51.61 80 port [tcp/http] succeeded!\n"
    Apr  6 10:34:20.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:34:20.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 32191'
    Apr  6 10:34:20.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.236 32191\nConnection to 10.250.2.236 32191 port [tcp/*] succeeded!\n"
    Apr  6 10:34:20.622: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:34:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.68 32191'
    Apr  6 10:34:21.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.68 32191\nConnection to 10.250.0.68 32191 port [tcp/*] succeeded!\n"
    Apr  6 10:34:21.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:34:21.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-3982 exec execpod-affinityj4rfc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:32191/ ; done'
    Apr  6 10:34:22.015: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:32191/\n"
    Apr  6 10:34:22.015: INFO: stdout: "\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862\naffinity-nodeport-82862"
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Received response from host: affinity-nodeport-82862
    Apr  6 10:34:22.015: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3982, will wait for the garbage collector to delete the pods 04/06/23 10:34:22.026
    Apr  6 10:34:22.090: INFO: Deleting ReplicationController affinity-nodeport took: 8.432317ms
    Apr  6 10:34:22.190: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.453788ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:34:24.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3982" for this suite. 04/06/23 10:34:24.64
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:34:24.646
Apr  6 10:34:24.646: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-runtime 04/06/23 10:34:24.648
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:34:24.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:34:24.677
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 04/06/23 10:34:24.686
STEP: wait for the container to reach Succeeded 04/06/23 10:34:24.709
STEP: get the container status 04/06/23 10:34:28.763
STEP: the container should be terminated 04/06/23 10:34:28.769
STEP: the termination message should be set 04/06/23 10:34:28.769
Apr  6 10:34:28.769: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/06/23 10:34:28.769
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Apr  6 10:34:28.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1751" for this suite. 04/06/23 10:34:28.807
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":58,"skipped":1023,"failed":0}
------------------------------
â€¢ [4.168 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:34:24.646
    Apr  6 10:34:24.646: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-runtime 04/06/23 10:34:24.648
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:34:24.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:34:24.677
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 04/06/23 10:34:24.686
    STEP: wait for the container to reach Succeeded 04/06/23 10:34:24.709
    STEP: get the container status 04/06/23 10:34:28.763
    STEP: the container should be terminated 04/06/23 10:34:28.769
    STEP: the termination message should be set 04/06/23 10:34:28.769
    Apr  6 10:34:28.769: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/06/23 10:34:28.769
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Apr  6 10:34:28.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1751" for this suite. 04/06/23 10:34:28.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:34:28.814
Apr  6 10:34:28.815: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-watch 04/06/23 10:34:28.816
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:34:28.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:34:28.848
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Apr  6 10:34:28.856: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Creating first CR  04/06/23 10:34:31.453
Apr  6 10:34:31.461: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:31Z]] name:name1 resourceVersion:12717 uid:22e25a1c-93a4-4354-bfce-83483ee34c2f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 04/06/23 10:34:41.462
Apr  6 10:34:41.472: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:41Z]] name:name2 resourceVersion:12761 uid:82ef9fb4-ebc0-47c9-81ae-c1b1cd6f4cb8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 04/06/23 10:34:51.476
Apr  6 10:34:51.495: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:51Z]] name:name1 resourceVersion:12799 uid:22e25a1c-93a4-4354-bfce-83483ee34c2f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 04/06/23 10:35:01.499
Apr  6 10:35:01.514: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:35:01Z]] name:name2 resourceVersion:12837 uid:82ef9fb4-ebc0-47c9-81ae-c1b1cd6f4cb8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 04/06/23 10:35:11.514
Apr  6 10:35:11.530: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:51Z]] name:name1 resourceVersion:12875 uid:22e25a1c-93a4-4354-bfce-83483ee34c2f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 04/06/23 10:35:21.533
Apr  6 10:35:21.545: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:35:01Z]] name:name2 resourceVersion:12913 uid:82ef9fb4-ebc0-47c9-81ae-c1b1cd6f4cb8] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:35:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7540" for this suite. 04/06/23 10:35:32.101
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":59,"skipped":1031,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.293 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:34:28.814
    Apr  6 10:34:28.815: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-watch 04/06/23 10:34:28.816
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:34:28.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:34:28.848
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Apr  6 10:34:28.856: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Creating first CR  04/06/23 10:34:31.453
    Apr  6 10:34:31.461: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:31Z]] name:name1 resourceVersion:12717 uid:22e25a1c-93a4-4354-bfce-83483ee34c2f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 04/06/23 10:34:41.462
    Apr  6 10:34:41.472: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:41Z]] name:name2 resourceVersion:12761 uid:82ef9fb4-ebc0-47c9-81ae-c1b1cd6f4cb8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 04/06/23 10:34:51.476
    Apr  6 10:34:51.495: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:51Z]] name:name1 resourceVersion:12799 uid:22e25a1c-93a4-4354-bfce-83483ee34c2f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 04/06/23 10:35:01.499
    Apr  6 10:35:01.514: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:35:01Z]] name:name2 resourceVersion:12837 uid:82ef9fb4-ebc0-47c9-81ae-c1b1cd6f4cb8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 04/06/23 10:35:11.514
    Apr  6 10:35:11.530: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:34:51Z]] name:name1 resourceVersion:12875 uid:22e25a1c-93a4-4354-bfce-83483ee34c2f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 04/06/23 10:35:21.533
    Apr  6 10:35:21.545: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-06T10:34:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-06T10:35:01Z]] name:name2 resourceVersion:12913 uid:82ef9fb4-ebc0-47c9-81ae-c1b1cd6f4cb8] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:35:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7540" for this suite. 04/06/23 10:35:32.101
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:35:32.108
Apr  6 10:35:32.108: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-runtime 04/06/23 10:35:32.11
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:32.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:32.14
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 04/06/23 10:35:32.146
STEP: wait for the container to reach Succeeded 04/06/23 10:35:32.162
STEP: get the container status 04/06/23 10:35:36.209
STEP: the container should be terminated 04/06/23 10:35:36.215
STEP: the termination message should be set 04/06/23 10:35:36.216
Apr  6 10:35:36.216: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 04/06/23 10:35:36.217
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Apr  6 10:35:36.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-405" for this suite. 04/06/23 10:35:36.249
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":60,"skipped":1034,"failed":0}
------------------------------
â€¢ [4.148 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:35:32.108
    Apr  6 10:35:32.108: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-runtime 04/06/23 10:35:32.11
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:32.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:32.14
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 04/06/23 10:35:32.146
    STEP: wait for the container to reach Succeeded 04/06/23 10:35:32.162
    STEP: get the container status 04/06/23 10:35:36.209
    STEP: the container should be terminated 04/06/23 10:35:36.215
    STEP: the termination message should be set 04/06/23 10:35:36.216
    Apr  6 10:35:36.216: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 04/06/23 10:35:36.217
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Apr  6 10:35:36.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-405" for this suite. 04/06/23 10:35:36.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:35:36.257
Apr  6 10:35:36.257: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename podtemplate 04/06/23 10:35:36.258
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:36.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:36.327
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 04/06/23 10:35:36.334
Apr  6 10:35:36.344: INFO: created test-podtemplate-1
Apr  6 10:35:36.351: INFO: created test-podtemplate-2
Apr  6 10:35:36.356: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 04/06/23 10:35:36.356
STEP: delete collection of pod templates 04/06/23 10:35:36.371
Apr  6 10:35:36.371: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 04/06/23 10:35:36.391
Apr  6 10:35:36.391: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Apr  6 10:35:36.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1517" for this suite. 04/06/23 10:35:36.418
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":61,"skipped":1045,"failed":0}
------------------------------
â€¢ [0.173 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:35:36.257
    Apr  6 10:35:36.257: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename podtemplate 04/06/23 10:35:36.258
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:36.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:36.327
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 04/06/23 10:35:36.334
    Apr  6 10:35:36.344: INFO: created test-podtemplate-1
    Apr  6 10:35:36.351: INFO: created test-podtemplate-2
    Apr  6 10:35:36.356: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 04/06/23 10:35:36.356
    STEP: delete collection of pod templates 04/06/23 10:35:36.371
    Apr  6 10:35:36.371: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 04/06/23 10:35:36.391
    Apr  6 10:35:36.391: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Apr  6 10:35:36.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1517" for this suite. 04/06/23 10:35:36.418
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:35:36.434
Apr  6 10:35:36.434: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:35:36.435
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:36.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:36.477
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 04/06/23 10:35:36.484
Apr  6 10:35:36.502: INFO: Waiting up to 5m0s for pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c" in namespace "projected-6962" to be "running and ready"
Apr  6 10:35:36.507: INFO: Pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878624ms
Apr  6 10:35:36.507: INFO: The phase of Pod annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:35:38.518: INFO: Pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015849579s
Apr  6 10:35:38.518: INFO: The phase of Pod annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c is Running (Ready = true)
Apr  6 10:35:38.518: INFO: Pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c" satisfied condition "running and ready"
Apr  6 10:35:39.118: INFO: Successfully updated pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 10:35:43.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6962" for this suite. 04/06/23 10:35:43.197
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":62,"skipped":1046,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.771 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:35:36.434
    Apr  6 10:35:36.434: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:35:36.435
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:36.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:36.477
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 04/06/23 10:35:36.484
    Apr  6 10:35:36.502: INFO: Waiting up to 5m0s for pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c" in namespace "projected-6962" to be "running and ready"
    Apr  6 10:35:36.507: INFO: Pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878624ms
    Apr  6 10:35:36.507: INFO: The phase of Pod annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:35:38.518: INFO: Pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015849579s
    Apr  6 10:35:38.518: INFO: The phase of Pod annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c is Running (Ready = true)
    Apr  6 10:35:38.518: INFO: Pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c" satisfied condition "running and ready"
    Apr  6 10:35:39.118: INFO: Successfully updated pod "annotationupdatecd99447b-6ae9-4e4f-8ae0-611e62cf171c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 10:35:43.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6962" for this suite. 04/06/23 10:35:43.197
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:35:43.215
Apr  6 10:35:43.215: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:35:43.217
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:43.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:43.24
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7046 04/06/23 10:35:43.248
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/06/23 10:35:43.264
STEP: creating service externalsvc in namespace services-7046 04/06/23 10:35:43.264
STEP: creating replication controller externalsvc in namespace services-7046 04/06/23 10:35:43.284
I0406 10:35:43.295850      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7046, replica count: 2
I0406 10:35:46.347267      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 04/06/23 10:35:46.353
Apr  6 10:35:46.372: INFO: Creating new exec pod
Apr  6 10:35:46.405: INFO: Waiting up to 5m0s for pod "execpodlrqzg" in namespace "services-7046" to be "running"
Apr  6 10:35:46.411: INFO: Pod "execpodlrqzg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.199248ms
Apr  6 10:35:48.419: INFO: Pod "execpodlrqzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.012874829s
Apr  6 10:35:48.419: INFO: Pod "execpodlrqzg" satisfied condition "running"
Apr  6 10:35:48.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-7046 exec execpodlrqzg -- /bin/sh -x -c nslookup clusterip-service.services-7046.svc.cluster.local'
Apr  6 10:35:49.185: INFO: stderr: "+ nslookup clusterip-service.services-7046.svc.cluster.local\n"
Apr  6 10:35:49.185: INFO: stdout: "Server:\t\t10.64.0.10\nAddress:\t10.64.0.10#53\n\nclusterip-service.services-7046.svc.cluster.local\tcanonical name = externalsvc.services-7046.svc.cluster.local.\nName:\texternalsvc.services-7046.svc.cluster.local\nAddress: 10.64.33.29\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7046, will wait for the garbage collector to delete the pods 04/06/23 10:35:49.185
Apr  6 10:35:49.249: INFO: Deleting ReplicationController externalsvc took: 7.748151ms
Apr  6 10:35:49.349: INFO: Terminating ReplicationController externalsvc pods took: 100.466495ms
Apr  6 10:35:51.869: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:35:51.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7046" for this suite. 04/06/23 10:35:51.895
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":63,"skipped":1050,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.689 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:35:43.215
    Apr  6 10:35:43.215: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:35:43.217
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:43.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:43.24
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7046 04/06/23 10:35:43.248
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/06/23 10:35:43.264
    STEP: creating service externalsvc in namespace services-7046 04/06/23 10:35:43.264
    STEP: creating replication controller externalsvc in namespace services-7046 04/06/23 10:35:43.284
    I0406 10:35:43.295850      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7046, replica count: 2
    I0406 10:35:46.347267      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 04/06/23 10:35:46.353
    Apr  6 10:35:46.372: INFO: Creating new exec pod
    Apr  6 10:35:46.405: INFO: Waiting up to 5m0s for pod "execpodlrqzg" in namespace "services-7046" to be "running"
    Apr  6 10:35:46.411: INFO: Pod "execpodlrqzg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.199248ms
    Apr  6 10:35:48.419: INFO: Pod "execpodlrqzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.012874829s
    Apr  6 10:35:48.419: INFO: Pod "execpodlrqzg" satisfied condition "running"
    Apr  6 10:35:48.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-7046 exec execpodlrqzg -- /bin/sh -x -c nslookup clusterip-service.services-7046.svc.cluster.local'
    Apr  6 10:35:49.185: INFO: stderr: "+ nslookup clusterip-service.services-7046.svc.cluster.local\n"
    Apr  6 10:35:49.185: INFO: stdout: "Server:\t\t10.64.0.10\nAddress:\t10.64.0.10#53\n\nclusterip-service.services-7046.svc.cluster.local\tcanonical name = externalsvc.services-7046.svc.cluster.local.\nName:\texternalsvc.services-7046.svc.cluster.local\nAddress: 10.64.33.29\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7046, will wait for the garbage collector to delete the pods 04/06/23 10:35:49.185
    Apr  6 10:35:49.249: INFO: Deleting ReplicationController externalsvc took: 7.748151ms
    Apr  6 10:35:49.349: INFO: Terminating ReplicationController externalsvc pods took: 100.466495ms
    Apr  6 10:35:51.869: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:35:51.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7046" for this suite. 04/06/23 10:35:51.895
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:35:51.907
Apr  6 10:35:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 10:35:51.914
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:51.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:51.941
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 04/06/23 10:35:51.957
Apr  6 10:35:51.972: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-53" to be "running and ready"
Apr  6 10:35:51.978: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037654ms
Apr  6 10:35:51.978: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:35:53.985: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012573732s
Apr  6 10:35:53.985: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr  6 10:35:53.985: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 04/06/23 10:35:53.989
Apr  6 10:35:53.996: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-53" to be "running and ready"
Apr  6 10:35:54.000: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.690723ms
Apr  6 10:35:54.000: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:35:56.020: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023985315s
Apr  6 10:35:56.020: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:35:58.008: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012083285s
Apr  6 10:35:58.008: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Apr  6 10:35:58.008: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/06/23 10:35:58.024
STEP: delete the pod with lifecycle hook 04/06/23 10:35:58.083
Apr  6 10:35:58.091: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 10:35:58.097: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 10:36:00.099: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 10:36:00.106: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Apr  6 10:36:00.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-53" for this suite. 04/06/23 10:36:00.116
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":64,"skipped":1079,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.216 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:35:51.907
    Apr  6 10:35:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 10:35:51.914
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:35:51.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:35:51.941
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 04/06/23 10:35:51.957
    Apr  6 10:35:51.972: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-53" to be "running and ready"
    Apr  6 10:35:51.978: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037654ms
    Apr  6 10:35:51.978: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:35:53.985: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012573732s
    Apr  6 10:35:53.985: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr  6 10:35:53.985: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 04/06/23 10:35:53.989
    Apr  6 10:35:53.996: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-53" to be "running and ready"
    Apr  6 10:35:54.000: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.690723ms
    Apr  6 10:35:54.000: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:35:56.020: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023985315s
    Apr  6 10:35:56.020: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:35:58.008: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012083285s
    Apr  6 10:35:58.008: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Apr  6 10:35:58.008: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/06/23 10:35:58.024
    STEP: delete the pod with lifecycle hook 04/06/23 10:35:58.083
    Apr  6 10:35:58.091: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr  6 10:35:58.097: INFO: Pod pod-with-poststart-exec-hook still exists
    Apr  6 10:36:00.099: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr  6 10:36:00.106: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Apr  6 10:36:00.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-53" for this suite. 04/06/23 10:36:00.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:36:00.125
Apr  6 10:36:00.125: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 10:36:00.127
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:00.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:00.145
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 04/06/23 10:36:00.152
Apr  6 10:36:00.171: INFO: Waiting up to 5m0s for pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652" in namespace "emptydir-2134" to be "Succeeded or Failed"
Apr  6 10:36:00.201: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652": Phase="Pending", Reason="", readiness=false. Elapsed: 20.447039ms
Apr  6 10:36:02.208: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026778076s
Apr  6 10:36:04.209: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027821521s
STEP: Saw pod success 04/06/23 10:36:04.209
Apr  6 10:36:04.209: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652" satisfied condition "Succeeded or Failed"
Apr  6 10:36:04.219: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-71bae343-6f9e-4942-b2bc-068ed2ae3652 container test-container: <nil>
STEP: delete the pod 04/06/23 10:36:04.231
Apr  6 10:36:04.242: INFO: Waiting for pod pod-71bae343-6f9e-4942-b2bc-068ed2ae3652 to disappear
Apr  6 10:36:04.246: INFO: Pod pod-71bae343-6f9e-4942-b2bc-068ed2ae3652 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 10:36:04.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2134" for this suite. 04/06/23 10:36:04.254
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":65,"skipped":1095,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:36:00.125
    Apr  6 10:36:00.125: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 10:36:00.127
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:00.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:00.145
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 04/06/23 10:36:00.152
    Apr  6 10:36:00.171: INFO: Waiting up to 5m0s for pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652" in namespace "emptydir-2134" to be "Succeeded or Failed"
    Apr  6 10:36:00.201: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652": Phase="Pending", Reason="", readiness=false. Elapsed: 20.447039ms
    Apr  6 10:36:02.208: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026778076s
    Apr  6 10:36:04.209: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027821521s
    STEP: Saw pod success 04/06/23 10:36:04.209
    Apr  6 10:36:04.209: INFO: Pod "pod-71bae343-6f9e-4942-b2bc-068ed2ae3652" satisfied condition "Succeeded or Failed"
    Apr  6 10:36:04.219: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-71bae343-6f9e-4942-b2bc-068ed2ae3652 container test-container: <nil>
    STEP: delete the pod 04/06/23 10:36:04.231
    Apr  6 10:36:04.242: INFO: Waiting for pod pod-71bae343-6f9e-4942-b2bc-068ed2ae3652 to disappear
    Apr  6 10:36:04.246: INFO: Pod pod-71bae343-6f9e-4942-b2bc-068ed2ae3652 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 10:36:04.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2134" for this suite. 04/06/23 10:36:04.254
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:36:04.264
Apr  6 10:36:04.264: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename watch 04/06/23 10:36:04.265
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:04.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:04.309
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 04/06/23 10:36:04.324
STEP: starting a background goroutine to produce watch events 04/06/23 10:36:04.329
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/06/23 10:36:04.329
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Apr  6 10:36:07.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3316" for this suite. 04/06/23 10:36:07.126
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":66,"skipped":1098,"failed":0}
------------------------------
â€¢ [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:36:04.264
    Apr  6 10:36:04.264: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename watch 04/06/23 10:36:04.265
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:04.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:04.309
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 04/06/23 10:36:04.324
    STEP: starting a background goroutine to produce watch events 04/06/23 10:36:04.329
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/06/23 10:36:04.329
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Apr  6 10:36:07.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3316" for this suite. 04/06/23 10:36:07.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:36:07.175
Apr  6 10:36:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename endpointslice 04/06/23 10:36:07.176
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:07.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:07.206
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Apr  6 10:36:07.233: INFO: Endpoints addresses: [172.20.85.61] , ports: [443]
Apr  6 10:36:07.233: INFO: EndpointSlices addresses: [172.20.85.61] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Apr  6 10:36:07.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7529" for this suite. 04/06/23 10:36:07.242
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":67,"skipped":1128,"failed":0}
------------------------------
â€¢ [0.072 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:36:07.175
    Apr  6 10:36:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename endpointslice 04/06/23 10:36:07.176
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:07.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:07.206
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Apr  6 10:36:07.233: INFO: Endpoints addresses: [172.20.85.61] , ports: [443]
    Apr  6 10:36:07.233: INFO: EndpointSlices addresses: [172.20.85.61] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Apr  6 10:36:07.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7529" for this suite. 04/06/23 10:36:07.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:36:07.248
Apr  6 10:36:07.249: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 10:36:07.249
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:07.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:07.277
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 04/06/23 10:36:07.29
Apr  6 10:36:07.305: INFO: Waiting up to 2m0s for pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" in namespace "var-expansion-1697" to be "running"
Apr  6 10:36:07.309: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340628ms
Apr  6 10:36:09.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011084317s
Apr  6 10:36:11.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012830968s
Apr  6 10:36:13.335: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029461476s
Apr  6 10:36:15.331: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026394777s
Apr  6 10:36:17.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011174903s
Apr  6 10:36:19.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012893038s
Apr  6 10:36:21.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011821523s
Apr  6 10:36:23.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010634173s
Apr  6 10:36:25.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011466996s
Apr  6 10:36:27.325: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01986739s
Apr  6 10:36:29.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012101556s
Apr  6 10:36:31.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010844454s
Apr  6 10:36:33.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012174713s
Apr  6 10:36:35.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01232142s
Apr  6 10:36:37.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011075314s
Apr  6 10:36:39.321: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 32.015821959s
Apr  6 10:36:41.321: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015796106s
Apr  6 10:36:43.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011379122s
Apr  6 10:36:45.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011631196s
Apr  6 10:36:47.322: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01714526s
Apr  6 10:36:49.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01209853s
Apr  6 10:36:51.319: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013880221s
Apr  6 10:36:53.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012396602s
Apr  6 10:36:55.324: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01898009s
Apr  6 10:36:57.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01183013s
Apr  6 10:36:59.324: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01874094s
Apr  6 10:37:01.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014945782s
Apr  6 10:37:03.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013297349s
Apr  6 10:37:05.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014626626s
Apr  6 10:37:07.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012632617s
Apr  6 10:37:09.322: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.016526772s
Apr  6 10:37:11.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011829163s
Apr  6 10:37:13.327: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021500001s
Apr  6 10:37:15.321: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.015564182s
Apr  6 10:37:17.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010844973s
Apr  6 10:37:19.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012011879s
Apr  6 10:37:21.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012189865s
Apr  6 10:37:23.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011441659s
Apr  6 10:37:25.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011873352s
Apr  6 10:37:27.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013314687s
Apr  6 10:37:29.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011189684s
Apr  6 10:37:31.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013272121s
Apr  6 10:37:33.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011357772s
Apr  6 10:37:35.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012790748s
Apr  6 10:37:37.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012298701s
Apr  6 10:37:39.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012478261s
Apr  6 10:37:41.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013040314s
Apr  6 10:37:43.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012061992s
Apr  6 10:37:45.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014566311s
Apr  6 10:37:47.329: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024324625s
Apr  6 10:37:49.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.015121851s
Apr  6 10:37:51.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012655352s
Apr  6 10:37:53.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011149877s
Apr  6 10:37:55.325: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019703904s
Apr  6 10:37:57.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011024687s
Apr  6 10:37:59.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012540952s
Apr  6 10:38:01.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011590595s
Apr  6 10:38:03.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011293783s
Apr  6 10:38:05.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011980501s
Apr  6 10:38:07.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013399865s
Apr  6 10:38:07.324: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018662988s
STEP: updating the pod 04/06/23 10:38:07.324
Apr  6 10:38:07.857: INFO: Successfully updated pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6"
STEP: waiting for pod running 04/06/23 10:38:07.857
Apr  6 10:38:07.857: INFO: Waiting up to 2m0s for pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" in namespace "var-expansion-1697" to be "running"
Apr  6 10:38:07.863: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.383633ms
Apr  6 10:38:09.878: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020934265s
Apr  6 10:38:09.878: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" satisfied condition "running"
STEP: deleting the pod gracefully 04/06/23 10:38:09.878
Apr  6 10:38:09.878: INFO: Deleting pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" in namespace "var-expansion-1697"
Apr  6 10:38:09.888: INFO: Wait up to 5m0s for pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 10:38:41.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1697" for this suite. 04/06/23 10:38:41.92
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":68,"skipped":1148,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.685 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:36:07.248
    Apr  6 10:36:07.249: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 10:36:07.249
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:36:07.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:36:07.277
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 04/06/23 10:36:07.29
    Apr  6 10:36:07.305: INFO: Waiting up to 2m0s for pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" in namespace "var-expansion-1697" to be "running"
    Apr  6 10:36:07.309: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340628ms
    Apr  6 10:36:09.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011084317s
    Apr  6 10:36:11.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012830968s
    Apr  6 10:36:13.335: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029461476s
    Apr  6 10:36:15.331: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026394777s
    Apr  6 10:36:17.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011174903s
    Apr  6 10:36:19.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012893038s
    Apr  6 10:36:21.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011821523s
    Apr  6 10:36:23.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010634173s
    Apr  6 10:36:25.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011466996s
    Apr  6 10:36:27.325: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01986739s
    Apr  6 10:36:29.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012101556s
    Apr  6 10:36:31.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010844454s
    Apr  6 10:36:33.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012174713s
    Apr  6 10:36:35.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01232142s
    Apr  6 10:36:37.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011075314s
    Apr  6 10:36:39.321: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 32.015821959s
    Apr  6 10:36:41.321: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015796106s
    Apr  6 10:36:43.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011379122s
    Apr  6 10:36:45.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011631196s
    Apr  6 10:36:47.322: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01714526s
    Apr  6 10:36:49.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01209853s
    Apr  6 10:36:51.319: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013880221s
    Apr  6 10:36:53.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012396602s
    Apr  6 10:36:55.324: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01898009s
    Apr  6 10:36:57.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01183013s
    Apr  6 10:36:59.324: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01874094s
    Apr  6 10:37:01.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014945782s
    Apr  6 10:37:03.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013297349s
    Apr  6 10:37:05.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014626626s
    Apr  6 10:37:07.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012632617s
    Apr  6 10:37:09.322: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.016526772s
    Apr  6 10:37:11.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011829163s
    Apr  6 10:37:13.327: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021500001s
    Apr  6 10:37:15.321: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.015564182s
    Apr  6 10:37:17.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010844973s
    Apr  6 10:37:19.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012011879s
    Apr  6 10:37:21.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012189865s
    Apr  6 10:37:23.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011441659s
    Apr  6 10:37:25.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011873352s
    Apr  6 10:37:27.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013314687s
    Apr  6 10:37:29.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011189684s
    Apr  6 10:37:31.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013272121s
    Apr  6 10:37:33.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011357772s
    Apr  6 10:37:35.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012790748s
    Apr  6 10:37:37.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012298701s
    Apr  6 10:37:39.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012478261s
    Apr  6 10:37:41.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013040314s
    Apr  6 10:37:43.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012061992s
    Apr  6 10:37:45.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.014566311s
    Apr  6 10:37:47.329: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024324625s
    Apr  6 10:37:49.320: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.015121851s
    Apr  6 10:37:51.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012655352s
    Apr  6 10:37:53.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011149877s
    Apr  6 10:37:55.325: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019703904s
    Apr  6 10:37:57.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011024687s
    Apr  6 10:37:59.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012540952s
    Apr  6 10:38:01.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011590595s
    Apr  6 10:38:03.316: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011293783s
    Apr  6 10:38:05.317: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011980501s
    Apr  6 10:38:07.318: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013399865s
    Apr  6 10:38:07.324: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018662988s
    STEP: updating the pod 04/06/23 10:38:07.324
    Apr  6 10:38:07.857: INFO: Successfully updated pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6"
    STEP: waiting for pod running 04/06/23 10:38:07.857
    Apr  6 10:38:07.857: INFO: Waiting up to 2m0s for pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" in namespace "var-expansion-1697" to be "running"
    Apr  6 10:38:07.863: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.383633ms
    Apr  6 10:38:09.878: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020934265s
    Apr  6 10:38:09.878: INFO: Pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" satisfied condition "running"
    STEP: deleting the pod gracefully 04/06/23 10:38:09.878
    Apr  6 10:38:09.878: INFO: Deleting pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" in namespace "var-expansion-1697"
    Apr  6 10:38:09.888: INFO: Wait up to 5m0s for pod "var-expansion-2b8a5a9e-484b-4d78-8e17-d44e125cd3b6" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 10:38:41.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1697" for this suite. 04/06/23 10:38:41.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:38:41.935
Apr  6 10:38:41.935: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename security-context 04/06/23 10:38:41.937
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:38:41.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:38:41.973
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/06/23 10:38:41.988
Apr  6 10:38:42.019: INFO: Waiting up to 5m0s for pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108" in namespace "security-context-602" to be "Succeeded or Failed"
Apr  6 10:38:42.023: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.372394ms
Apr  6 10:38:44.029: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010667776s
Apr  6 10:38:46.031: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012558984s
Apr  6 10:38:48.029: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010156502s
STEP: Saw pod success 04/06/23 10:38:48.029
Apr  6 10:38:48.029: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108" satisfied condition "Succeeded or Failed"
Apr  6 10:38:48.034: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108 container test-container: <nil>
STEP: delete the pod 04/06/23 10:38:48.058
Apr  6 10:38:48.069: INFO: Waiting for pod security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108 to disappear
Apr  6 10:38:48.073: INFO: Pod security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Apr  6 10:38:48.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-602" for this suite. 04/06/23 10:38:48.082
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":69,"skipped":1162,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.154 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:38:41.935
    Apr  6 10:38:41.935: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename security-context 04/06/23 10:38:41.937
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:38:41.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:38:41.973
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/06/23 10:38:41.988
    Apr  6 10:38:42.019: INFO: Waiting up to 5m0s for pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108" in namespace "security-context-602" to be "Succeeded or Failed"
    Apr  6 10:38:42.023: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.372394ms
    Apr  6 10:38:44.029: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010667776s
    Apr  6 10:38:46.031: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012558984s
    Apr  6 10:38:48.029: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010156502s
    STEP: Saw pod success 04/06/23 10:38:48.029
    Apr  6 10:38:48.029: INFO: Pod "security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108" satisfied condition "Succeeded or Failed"
    Apr  6 10:38:48.034: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108 container test-container: <nil>
    STEP: delete the pod 04/06/23 10:38:48.058
    Apr  6 10:38:48.069: INFO: Waiting for pod security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108 to disappear
    Apr  6 10:38:48.073: INFO: Pod security-context-5b4146be-33af-4a3b-a3ea-ef8e57bb1108 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Apr  6 10:38:48.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-602" for this suite. 04/06/23 10:38:48.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:38:48.089
Apr  6 10:38:48.090: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:38:48.091
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:38:48.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:38:48.114
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Apr  6 10:38:48.139: INFO: created pod
Apr  6 10:38:48.139: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5715" to be "Succeeded or Failed"
Apr  6 10:38:48.144: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0516ms
Apr  6 10:38:50.150: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010281229s
Apr  6 10:38:52.154: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014373012s
STEP: Saw pod success 04/06/23 10:38:52.154
Apr  6 10:38:52.154: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Apr  6 10:39:22.154: INFO: polling logs
Apr  6 10:39:22.179: INFO: Pod logs: 
I0406 10:38:49.190845       1 log.go:195] OK: Got token
I0406 10:38:49.190919       1 log.go:195] validating with in-cluster discovery
I0406 10:38:49.191552       1 log.go:195] OK: got issuer https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud
I0406 10:38:49.191618       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", Subject:"system:serviceaccount:svcaccounts-5715:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680778128, NotBefore:1680777528, IssuedAt:1680777528, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5715", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"46fbf064-9a9d-4009-8b09-19920bb0932d"}}}
I0406 10:38:49.218462       1 log.go:195] OK: Constructed OIDC provider for issuer https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud
I0406 10:38:49.221985       1 log.go:195] OK: Validated signature on JWT
I0406 10:38:49.222158       1 log.go:195] OK: Got valid claims from token!
I0406 10:38:49.222182       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", Subject:"system:serviceaccount:svcaccounts-5715:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680778128, NotBefore:1680777528, IssuedAt:1680777528, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5715", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"46fbf064-9a9d-4009-8b09-19920bb0932d"}}}

Apr  6 10:39:22.179: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Apr  6 10:39:22.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5715" for this suite. 04/06/23 10:39:22.2
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":70,"skipped":1170,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.118 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:38:48.089
    Apr  6 10:38:48.090: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:38:48.091
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:38:48.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:38:48.114
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Apr  6 10:38:48.139: INFO: created pod
    Apr  6 10:38:48.139: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5715" to be "Succeeded or Failed"
    Apr  6 10:38:48.144: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0516ms
    Apr  6 10:38:50.150: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010281229s
    Apr  6 10:38:52.154: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014373012s
    STEP: Saw pod success 04/06/23 10:38:52.154
    Apr  6 10:38:52.154: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Apr  6 10:39:22.154: INFO: polling logs
    Apr  6 10:39:22.179: INFO: Pod logs: 
    I0406 10:38:49.190845       1 log.go:195] OK: Got token
    I0406 10:38:49.190919       1 log.go:195] validating with in-cluster discovery
    I0406 10:38:49.191552       1 log.go:195] OK: got issuer https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud
    I0406 10:38:49.191618       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", Subject:"system:serviceaccount:svcaccounts-5715:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680778128, NotBefore:1680777528, IssuedAt:1680777528, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5715", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"46fbf064-9a9d-4009-8b09-19920bb0932d"}}}
    I0406 10:38:49.218462       1 log.go:195] OK: Constructed OIDC provider for issuer https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud
    I0406 10:38:49.221985       1 log.go:195] OK: Validated signature on JWT
    I0406 10:38:49.222158       1 log.go:195] OK: Got valid claims from token!
    I0406 10:38:49.222182       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", Subject:"system:serviceaccount:svcaccounts-5715:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1680778128, NotBefore:1680777528, IssuedAt:1680777528, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5715", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"46fbf064-9a9d-4009-8b09-19920bb0932d"}}}

    Apr  6 10:39:22.179: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Apr  6 10:39:22.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5715" for this suite. 04/06/23 10:39:22.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:22.219
Apr  6 10:39:22.219: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 10:39:22.22
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:22.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:22.248
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 04/06/23 10:39:22.256
STEP: listing secrets in all namespaces to ensure that there are more than zero 04/06/23 10:39:22.262
STEP: patching the secret 04/06/23 10:39:22.273
STEP: deleting the secret using a LabelSelector 04/06/23 10:39:22.294
STEP: listing secrets in all namespaces, searching for label name and value in patch 04/06/23 10:39:22.305
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Apr  6 10:39:22.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8896" for this suite. 04/06/23 10:39:22.321
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":71,"skipped":1192,"failed":0}
------------------------------
â€¢ [0.109 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:22.219
    Apr  6 10:39:22.219: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 10:39:22.22
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:22.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:22.248
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 04/06/23 10:39:22.256
    STEP: listing secrets in all namespaces to ensure that there are more than zero 04/06/23 10:39:22.262
    STEP: patching the secret 04/06/23 10:39:22.273
    STEP: deleting the secret using a LabelSelector 04/06/23 10:39:22.294
    STEP: listing secrets in all namespaces, searching for label name and value in patch 04/06/23 10:39:22.305
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 10:39:22.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8896" for this suite. 04/06/23 10:39:22.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:22.331
Apr  6 10:39:22.331: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 10:39:22.333
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:22.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:22.361
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 04/06/23 10:39:22.367
STEP: submitting the pod to kubernetes 04/06/23 10:39:22.367
Apr  6 10:39:22.380: INFO: Waiting up to 5m0s for pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" in namespace "pods-1740" to be "running and ready"
Apr  6 10:39:22.390: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078": Phase="Pending", Reason="", readiness=false. Elapsed: 10.783652ms
Apr  6 10:39:22.390: INFO: The phase of Pod pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:39:24.402: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078": Phase="Running", Reason="", readiness=true. Elapsed: 2.02273882s
Apr  6 10:39:24.402: INFO: The phase of Pod pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078 is Running (Ready = true)
Apr  6 10:39:24.402: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/06/23 10:39:24.407
STEP: updating the pod 04/06/23 10:39:24.416
Apr  6 10:39:24.934: INFO: Successfully updated pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078"
Apr  6 10:39:24.934: INFO: Waiting up to 5m0s for pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" in namespace "pods-1740" to be "running"
Apr  6 10:39:24.940: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078": Phase="Running", Reason="", readiness=true. Elapsed: 5.772621ms
Apr  6 10:39:24.940: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 04/06/23 10:39:24.94
Apr  6 10:39:24.945: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 10:39:24.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1740" for this suite. 04/06/23 10:39:24.952
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":72,"skipped":1212,"failed":0}
------------------------------
â€¢ [2.628 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:22.331
    Apr  6 10:39:22.331: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 10:39:22.333
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:22.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:22.361
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 04/06/23 10:39:22.367
    STEP: submitting the pod to kubernetes 04/06/23 10:39:22.367
    Apr  6 10:39:22.380: INFO: Waiting up to 5m0s for pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" in namespace "pods-1740" to be "running and ready"
    Apr  6 10:39:22.390: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078": Phase="Pending", Reason="", readiness=false. Elapsed: 10.783652ms
    Apr  6 10:39:22.390: INFO: The phase of Pod pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:39:24.402: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078": Phase="Running", Reason="", readiness=true. Elapsed: 2.02273882s
    Apr  6 10:39:24.402: INFO: The phase of Pod pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078 is Running (Ready = true)
    Apr  6 10:39:24.402: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/06/23 10:39:24.407
    STEP: updating the pod 04/06/23 10:39:24.416
    Apr  6 10:39:24.934: INFO: Successfully updated pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078"
    Apr  6 10:39:24.934: INFO: Waiting up to 5m0s for pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" in namespace "pods-1740" to be "running"
    Apr  6 10:39:24.940: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078": Phase="Running", Reason="", readiness=true. Elapsed: 5.772621ms
    Apr  6 10:39:24.940: INFO: Pod "pod-update-efe1d48c-6ec5-4400-a1f6-340431a6b078" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 04/06/23 10:39:24.94
    Apr  6 10:39:24.945: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 10:39:24.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1740" for this suite. 04/06/23 10:39:24.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:24.962
Apr  6 10:39:24.962: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:39:24.963
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:24.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:25.006
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-7338/configmap-test-1f1bb807-cd71-4bd0-9ecd-df0b797b5e2e 04/06/23 10:39:25.014
STEP: Creating a pod to test consume configMaps 04/06/23 10:39:25.021
Apr  6 10:39:25.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839" in namespace "configmap-7338" to be "Succeeded or Failed"
Apr  6 10:39:25.058: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Pending", Reason="", readiness=false. Elapsed: 4.946936ms
Apr  6 10:39:27.065: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Running", Reason="", readiness=true. Elapsed: 2.011765654s
Apr  6 10:39:29.066: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Running", Reason="", readiness=false. Elapsed: 4.013051463s
Apr  6 10:39:31.068: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014155925s
STEP: Saw pod success 04/06/23 10:39:31.068
Apr  6 10:39:31.068: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839" satisfied condition "Succeeded or Failed"
Apr  6 10:39:31.074: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839 container env-test: <nil>
STEP: delete the pod 04/06/23 10:39:31.126
Apr  6 10:39:31.136: INFO: Waiting for pod pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839 to disappear
Apr  6 10:39:31.139: INFO: Pod pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:39:31.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7338" for this suite. 04/06/23 10:39:31.148
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":73,"skipped":1264,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.191 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:24.962
    Apr  6 10:39:24.962: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:39:24.963
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:24.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:25.006
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-7338/configmap-test-1f1bb807-cd71-4bd0-9ecd-df0b797b5e2e 04/06/23 10:39:25.014
    STEP: Creating a pod to test consume configMaps 04/06/23 10:39:25.021
    Apr  6 10:39:25.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839" in namespace "configmap-7338" to be "Succeeded or Failed"
    Apr  6 10:39:25.058: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Pending", Reason="", readiness=false. Elapsed: 4.946936ms
    Apr  6 10:39:27.065: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Running", Reason="", readiness=true. Elapsed: 2.011765654s
    Apr  6 10:39:29.066: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Running", Reason="", readiness=false. Elapsed: 4.013051463s
    Apr  6 10:39:31.068: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014155925s
    STEP: Saw pod success 04/06/23 10:39:31.068
    Apr  6 10:39:31.068: INFO: Pod "pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839" satisfied condition "Succeeded or Failed"
    Apr  6 10:39:31.074: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839 container env-test: <nil>
    STEP: delete the pod 04/06/23 10:39:31.126
    Apr  6 10:39:31.136: INFO: Waiting for pod pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839 to disappear
    Apr  6 10:39:31.139: INFO: Pod pod-configmaps-97e37afb-1099-4745-9fee-90ec531f2839 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:39:31.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7338" for this suite. 04/06/23 10:39:31.148
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:31.156
Apr  6 10:39:31.156: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 10:39:31.157
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:31.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:31.179
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 04/06/23 10:39:31.187
Apr  6 10:39:31.204: INFO: Waiting up to 5m0s for pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768" in namespace "var-expansion-2576" to be "Succeeded or Failed"
Apr  6 10:39:31.211: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Pending", Reason="", readiness=false. Elapsed: 7.045727ms
Apr  6 10:39:33.222: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Running", Reason="", readiness=true. Elapsed: 2.018506873s
Apr  6 10:39:35.226: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Running", Reason="", readiness=false. Elapsed: 4.02247195s
Apr  6 10:39:37.218: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014509232s
STEP: Saw pod success 04/06/23 10:39:37.218
Apr  6 10:39:37.218: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768" satisfied condition "Succeeded or Failed"
Apr  6 10:39:37.224: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768 container dapi-container: <nil>
STEP: delete the pod 04/06/23 10:39:37.27
Apr  6 10:39:37.281: INFO: Waiting for pod var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768 to disappear
Apr  6 10:39:37.288: INFO: Pod var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 10:39:37.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2576" for this suite. 04/06/23 10:39:37.297
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":74,"skipped":1266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.147 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:31.156
    Apr  6 10:39:31.156: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 10:39:31.157
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:31.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:31.179
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 04/06/23 10:39:31.187
    Apr  6 10:39:31.204: INFO: Waiting up to 5m0s for pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768" in namespace "var-expansion-2576" to be "Succeeded or Failed"
    Apr  6 10:39:31.211: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Pending", Reason="", readiness=false. Elapsed: 7.045727ms
    Apr  6 10:39:33.222: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Running", Reason="", readiness=true. Elapsed: 2.018506873s
    Apr  6 10:39:35.226: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Running", Reason="", readiness=false. Elapsed: 4.02247195s
    Apr  6 10:39:37.218: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014509232s
    STEP: Saw pod success 04/06/23 10:39:37.218
    Apr  6 10:39:37.218: INFO: Pod "var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768" satisfied condition "Succeeded or Failed"
    Apr  6 10:39:37.224: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768 container dapi-container: <nil>
    STEP: delete the pod 04/06/23 10:39:37.27
    Apr  6 10:39:37.281: INFO: Waiting for pod var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768 to disappear
    Apr  6 10:39:37.288: INFO: Pod var-expansion-52e71bf9-583e-4377-ae68-0fdfd4612768 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 10:39:37.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2576" for this suite. 04/06/23 10:39:37.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:37.305
Apr  6 10:39:37.305: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sysctl 04/06/23 10:39:37.308
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:37.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:37.345
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 04/06/23 10:39:37.355
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Apr  6 10:39:37.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2456" for this suite. 04/06/23 10:39:37.382
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":75,"skipped":1308,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:37.305
    Apr  6 10:39:37.305: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sysctl 04/06/23 10:39:37.308
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:37.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:37.345
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 04/06/23 10:39:37.355
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Apr  6 10:39:37.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2456" for this suite. 04/06/23 10:39:37.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:37.389
Apr  6 10:39:37.389: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:39:37.39
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:37.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:37.408
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 04/06/23 10:39:37.415
STEP: watching for the ServiceAccount to be added 04/06/23 10:39:37.432
STEP: patching the ServiceAccount 04/06/23 10:39:37.435
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/06/23 10:39:37.445
STEP: deleting the ServiceAccount 04/06/23 10:39:37.452
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Apr  6 10:39:37.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7117" for this suite. 04/06/23 10:39:37.475
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":76,"skipped":1318,"failed":0}
------------------------------
â€¢ [0.092 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:37.389
    Apr  6 10:39:37.389: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:39:37.39
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:37.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:37.408
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 04/06/23 10:39:37.415
    STEP: watching for the ServiceAccount to be added 04/06/23 10:39:37.432
    STEP: patching the ServiceAccount 04/06/23 10:39:37.435
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/06/23 10:39:37.445
    STEP: deleting the ServiceAccount 04/06/23 10:39:37.452
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Apr  6 10:39:37.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7117" for this suite. 04/06/23 10:39:37.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:37.483
Apr  6 10:39:37.483: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:39:37.485
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:37.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:37.533
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-0637e4d7-e0bf-43b7-9e7d-64a70a66c603 04/06/23 10:39:37.54
STEP: Creating a pod to test consume configMaps 04/06/23 10:39:37.546
Apr  6 10:39:37.558: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e" in namespace "projected-7588" to be "Succeeded or Failed"
Apr  6 10:39:37.562: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293198ms
Apr  6 10:39:39.580: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02221922s
Apr  6 10:39:41.568: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01062644s
STEP: Saw pod success 04/06/23 10:39:41.568
Apr  6 10:39:41.569: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e" satisfied condition "Succeeded or Failed"
Apr  6 10:39:41.574: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:39:41.593
Apr  6 10:39:41.611: INFO: Waiting for pod pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e to disappear
Apr  6 10:39:41.616: INFO: Pod pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 10:39:41.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7588" for this suite. 04/06/23 10:39:41.627
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":77,"skipped":1328,"failed":0}
------------------------------
â€¢ [4.150 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:37.483
    Apr  6 10:39:37.483: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:39:37.485
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:37.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:37.533
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-0637e4d7-e0bf-43b7-9e7d-64a70a66c603 04/06/23 10:39:37.54
    STEP: Creating a pod to test consume configMaps 04/06/23 10:39:37.546
    Apr  6 10:39:37.558: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e" in namespace "projected-7588" to be "Succeeded or Failed"
    Apr  6 10:39:37.562: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293198ms
    Apr  6 10:39:39.580: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02221922s
    Apr  6 10:39:41.568: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01062644s
    STEP: Saw pod success 04/06/23 10:39:41.568
    Apr  6 10:39:41.569: INFO: Pod "pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e" satisfied condition "Succeeded or Failed"
    Apr  6 10:39:41.574: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:39:41.593
    Apr  6 10:39:41.611: INFO: Waiting for pod pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e to disappear
    Apr  6 10:39:41.616: INFO: Pod pod-projected-configmaps-1ac9973d-ceb8-4558-b728-1bf93eb0281e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 10:39:41.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7588" for this suite. 04/06/23 10:39:41.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:41.638
Apr  6 10:39:41.638: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 10:39:41.64
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:41.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:41.667
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 04/06/23 10:39:41.676
Apr  6 10:39:41.700: INFO: Waiting up to 5m0s for pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b" in namespace "emptydir-4401" to be "Succeeded or Failed"
Apr  6 10:39:41.707: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178007ms
Apr  6 10:39:43.715: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014978281s
Apr  6 10:39:45.715: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014292576s
STEP: Saw pod success 04/06/23 10:39:45.715
Apr  6 10:39:45.715: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b" satisfied condition "Succeeded or Failed"
Apr  6 10:39:45.721: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b container test-container: <nil>
STEP: delete the pod 04/06/23 10:39:45.739
Apr  6 10:39:45.760: INFO: Waiting for pod pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b to disappear
Apr  6 10:39:45.769: INFO: Pod pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 10:39:45.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4401" for this suite. 04/06/23 10:39:45.791
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":78,"skipped":1351,"failed":0}
------------------------------
â€¢ [4.165 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:41.638
    Apr  6 10:39:41.638: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 10:39:41.64
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:41.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:41.667
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/06/23 10:39:41.676
    Apr  6 10:39:41.700: INFO: Waiting up to 5m0s for pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b" in namespace "emptydir-4401" to be "Succeeded or Failed"
    Apr  6 10:39:41.707: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178007ms
    Apr  6 10:39:43.715: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014978281s
    Apr  6 10:39:45.715: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014292576s
    STEP: Saw pod success 04/06/23 10:39:45.715
    Apr  6 10:39:45.715: INFO: Pod "pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b" satisfied condition "Succeeded or Failed"
    Apr  6 10:39:45.721: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b container test-container: <nil>
    STEP: delete the pod 04/06/23 10:39:45.739
    Apr  6 10:39:45.760: INFO: Waiting for pod pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b to disappear
    Apr  6 10:39:45.769: INFO: Pod pod-c0f2fe35-3bc5-4320-aeda-c76f33c72b7b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 10:39:45.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4401" for this suite. 04/06/23 10:39:45.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:45.807
Apr  6 10:39:45.807: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename server-version 04/06/23 10:39:45.811
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:45.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:45.83
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 04/06/23 10:39:45.837
STEP: Confirm major version 04/06/23 10:39:45.839
Apr  6 10:39:45.840: INFO: Major version: 1
STEP: Confirm minor version 04/06/23 10:39:45.84
Apr  6 10:39:45.840: INFO: cleanMinorVersion: 25
Apr  6 10:39:45.840: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Apr  6 10:39:45.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4052" for this suite. 04/06/23 10:39:45.851
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":79,"skipped":1403,"failed":0}
------------------------------
â€¢ [0.065 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:45.807
    Apr  6 10:39:45.807: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename server-version 04/06/23 10:39:45.811
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:45.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:45.83
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 04/06/23 10:39:45.837
    STEP: Confirm major version 04/06/23 10:39:45.839
    Apr  6 10:39:45.840: INFO: Major version: 1
    STEP: Confirm minor version 04/06/23 10:39:45.84
    Apr  6 10:39:45.840: INFO: cleanMinorVersion: 25
    Apr  6 10:39:45.840: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Apr  6 10:39:45.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-4052" for this suite. 04/06/23 10:39:45.851
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:45.873
Apr  6 10:39:45.874: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:39:45.875
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:45.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:45.915
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Apr  6 10:39:45.924: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:39:51.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7305" for this suite. 04/06/23 10:39:51.799
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":80,"skipped":1406,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.942 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:45.873
    Apr  6 10:39:45.874: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:39:45.875
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:45.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:45.915
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Apr  6 10:39:45.924: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:39:51.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7305" for this suite. 04/06/23 10:39:51.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:39:51.827
Apr  6 10:39:51.827: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 10:39:51.829
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:51.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:51.853
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 in namespace container-probe-8711 04/06/23 10:39:51.86
Apr  6 10:39:51.884: INFO: Waiting up to 5m0s for pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364" in namespace "container-probe-8711" to be "not pending"
Apr  6 10:39:51.891: INFO: Pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124445ms
Apr  6 10:39:53.911: INFO: Pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364": Phase="Running", Reason="", readiness=true. Elapsed: 2.027166638s
Apr  6 10:39:53.911: INFO: Pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364" satisfied condition "not pending"
Apr  6 10:39:53.912: INFO: Started pod liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 in namespace container-probe-8711
STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 10:39:53.912
Apr  6 10:39:53.918: INFO: Initial restart count of pod liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 is 0
Apr  6 10:40:14.012: INFO: Restart count of pod container-probe-8711/liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 is now 1 (20.093827665s elapsed)
STEP: deleting the pod 04/06/23 10:40:14.012
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 10:40:14.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8711" for this suite. 04/06/23 10:40:14.027
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":81,"skipped":1439,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.209 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:39:51.827
    Apr  6 10:39:51.827: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 10:39:51.829
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:39:51.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:39:51.853
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 in namespace container-probe-8711 04/06/23 10:39:51.86
    Apr  6 10:39:51.884: INFO: Waiting up to 5m0s for pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364" in namespace "container-probe-8711" to be "not pending"
    Apr  6 10:39:51.891: INFO: Pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124445ms
    Apr  6 10:39:53.911: INFO: Pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364": Phase="Running", Reason="", readiness=true. Elapsed: 2.027166638s
    Apr  6 10:39:53.911: INFO: Pod "liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364" satisfied condition "not pending"
    Apr  6 10:39:53.912: INFO: Started pod liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 in namespace container-probe-8711
    STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 10:39:53.912
    Apr  6 10:39:53.918: INFO: Initial restart count of pod liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 is 0
    Apr  6 10:40:14.012: INFO: Restart count of pod container-probe-8711/liveness-5d0ac978-c5e3-40bb-a2b5-eee7836d1364 is now 1 (20.093827665s elapsed)
    STEP: deleting the pod 04/06/23 10:40:14.012
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 10:40:14.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8711" for this suite. 04/06/23 10:40:14.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:14.037
Apr  6 10:40:14.037: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 10:40:14.038
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:14.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:14.062
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 04/06/23 10:40:14.068
Apr  6 10:40:14.079: INFO: Waiting up to 5m0s for pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e" in namespace "var-expansion-1652" to be "Succeeded or Failed"
Apr  6 10:40:14.082: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408016ms
Apr  6 10:40:16.090: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010745738s
Apr  6 10:40:18.089: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009821766s
STEP: Saw pod success 04/06/23 10:40:18.089
Apr  6 10:40:18.089: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e" satisfied condition "Succeeded or Failed"
Apr  6 10:40:18.094: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e container dapi-container: <nil>
STEP: delete the pod 04/06/23 10:40:18.105
Apr  6 10:40:18.114: INFO: Waiting for pod var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e to disappear
Apr  6 10:40:18.118: INFO: Pod var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 10:40:18.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1652" for this suite. 04/06/23 10:40:18.135
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":82,"skipped":1451,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:14.037
    Apr  6 10:40:14.037: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 10:40:14.038
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:14.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:14.062
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 04/06/23 10:40:14.068
    Apr  6 10:40:14.079: INFO: Waiting up to 5m0s for pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e" in namespace "var-expansion-1652" to be "Succeeded or Failed"
    Apr  6 10:40:14.082: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408016ms
    Apr  6 10:40:16.090: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010745738s
    Apr  6 10:40:18.089: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009821766s
    STEP: Saw pod success 04/06/23 10:40:18.089
    Apr  6 10:40:18.089: INFO: Pod "var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e" satisfied condition "Succeeded or Failed"
    Apr  6 10:40:18.094: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e container dapi-container: <nil>
    STEP: delete the pod 04/06/23 10:40:18.105
    Apr  6 10:40:18.114: INFO: Waiting for pod var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e to disappear
    Apr  6 10:40:18.118: INFO: Pod var-expansion-e04a69a1-0f7b-42f1-92f3-167166fa6b5e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 10:40:18.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1652" for this suite. 04/06/23 10:40:18.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:18.143
Apr  6 10:40:18.143: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename security-context 04/06/23 10:40:18.144
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:18.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:18.164
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/06/23 10:40:18.172
Apr  6 10:40:18.186: INFO: Waiting up to 5m0s for pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697" in namespace "security-context-4408" to be "Succeeded or Failed"
Apr  6 10:40:18.196: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697": Phase="Pending", Reason="", readiness=false. Elapsed: 9.845713ms
Apr  6 10:40:20.204: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01800052s
Apr  6 10:40:22.203: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017623099s
STEP: Saw pod success 04/06/23 10:40:22.203
Apr  6 10:40:22.203: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697" satisfied condition "Succeeded or Failed"
Apr  6 10:40:22.208: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697 container test-container: <nil>
STEP: delete the pod 04/06/23 10:40:22.221
Apr  6 10:40:22.230: INFO: Waiting for pod security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697 to disappear
Apr  6 10:40:22.233: INFO: Pod security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Apr  6 10:40:22.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4408" for this suite. 04/06/23 10:40:22.249
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":83,"skipped":1477,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:18.143
    Apr  6 10:40:18.143: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename security-context 04/06/23 10:40:18.144
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:18.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:18.164
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/06/23 10:40:18.172
    Apr  6 10:40:18.186: INFO: Waiting up to 5m0s for pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697" in namespace "security-context-4408" to be "Succeeded or Failed"
    Apr  6 10:40:18.196: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697": Phase="Pending", Reason="", readiness=false. Elapsed: 9.845713ms
    Apr  6 10:40:20.204: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01800052s
    Apr  6 10:40:22.203: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017623099s
    STEP: Saw pod success 04/06/23 10:40:22.203
    Apr  6 10:40:22.203: INFO: Pod "security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697" satisfied condition "Succeeded or Failed"
    Apr  6 10:40:22.208: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697 container test-container: <nil>
    STEP: delete the pod 04/06/23 10:40:22.221
    Apr  6 10:40:22.230: INFO: Waiting for pod security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697 to disappear
    Apr  6 10:40:22.233: INFO: Pod security-context-48a966ae-356d-4a43-8e6c-ff4eaf1ae697 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Apr  6 10:40:22.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4408" for this suite. 04/06/23 10:40:22.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:22.264
Apr  6 10:40:22.265: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:40:22.266
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:22.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:22.293
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-79ffe1a2-40ed-4fea-8503-e7d600e8e7fe 04/06/23 10:40:22.299
STEP: Creating a pod to test consume configMaps 04/06/23 10:40:22.308
Apr  6 10:40:22.326: INFO: Waiting up to 5m0s for pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161" in namespace "configmap-9072" to be "Succeeded or Failed"
Apr  6 10:40:22.331: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161": Phase="Pending", Reason="", readiness=false. Elapsed: 4.932809ms
Apr  6 10:40:24.340: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014034763s
Apr  6 10:40:26.338: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01161918s
STEP: Saw pod success 04/06/23 10:40:26.338
Apr  6 10:40:26.338: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161" satisfied condition "Succeeded or Failed"
Apr  6 10:40:26.343: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:40:26.356
Apr  6 10:40:26.367: INFO: Waiting for pod pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161 to disappear
Apr  6 10:40:26.375: INFO: Pod pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:40:26.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9072" for this suite. 04/06/23 10:40:26.383
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":84,"skipped":1498,"failed":0}
------------------------------
â€¢ [4.124 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:22.264
    Apr  6 10:40:22.265: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:40:22.266
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:22.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:22.293
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-79ffe1a2-40ed-4fea-8503-e7d600e8e7fe 04/06/23 10:40:22.299
    STEP: Creating a pod to test consume configMaps 04/06/23 10:40:22.308
    Apr  6 10:40:22.326: INFO: Waiting up to 5m0s for pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161" in namespace "configmap-9072" to be "Succeeded or Failed"
    Apr  6 10:40:22.331: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161": Phase="Pending", Reason="", readiness=false. Elapsed: 4.932809ms
    Apr  6 10:40:24.340: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014034763s
    Apr  6 10:40:26.338: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01161918s
    STEP: Saw pod success 04/06/23 10:40:26.338
    Apr  6 10:40:26.338: INFO: Pod "pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161" satisfied condition "Succeeded or Failed"
    Apr  6 10:40:26.343: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:40:26.356
    Apr  6 10:40:26.367: INFO: Waiting for pod pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161 to disappear
    Apr  6 10:40:26.375: INFO: Pod pod-configmaps-39d3f933-fceb-46b4-8bcf-7d17dace8161 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:40:26.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9072" for this suite. 04/06/23 10:40:26.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:26.393
Apr  6 10:40:26.393: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 10:40:26.398
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:26.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:26.423
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/06/23 10:40:26.43
Apr  6 10:40:26.446: INFO: Waiting up to 5m0s for pod "pod-89870773-746a-45b9-a4ab-0f48716411df" in namespace "emptydir-962" to be "Succeeded or Failed"
Apr  6 10:40:26.459: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df": Phase="Pending", Reason="", readiness=false. Elapsed: 13.04969ms
Apr  6 10:40:28.475: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028676961s
Apr  6 10:40:30.476: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030270698s
STEP: Saw pod success 04/06/23 10:40:30.477
Apr  6 10:40:30.477: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df" satisfied condition "Succeeded or Failed"
Apr  6 10:40:30.482: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-89870773-746a-45b9-a4ab-0f48716411df container test-container: <nil>
STEP: delete the pod 04/06/23 10:40:30.542
Apr  6 10:40:30.564: INFO: Waiting for pod pod-89870773-746a-45b9-a4ab-0f48716411df to disappear
Apr  6 10:40:30.568: INFO: Pod pod-89870773-746a-45b9-a4ab-0f48716411df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 10:40:30.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-962" for this suite. 04/06/23 10:40:30.581
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":85,"skipped":1518,"failed":0}
------------------------------
â€¢ [4.196 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:26.393
    Apr  6 10:40:26.393: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 10:40:26.398
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:26.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:26.423
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/06/23 10:40:26.43
    Apr  6 10:40:26.446: INFO: Waiting up to 5m0s for pod "pod-89870773-746a-45b9-a4ab-0f48716411df" in namespace "emptydir-962" to be "Succeeded or Failed"
    Apr  6 10:40:26.459: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df": Phase="Pending", Reason="", readiness=false. Elapsed: 13.04969ms
    Apr  6 10:40:28.475: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028676961s
    Apr  6 10:40:30.476: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030270698s
    STEP: Saw pod success 04/06/23 10:40:30.477
    Apr  6 10:40:30.477: INFO: Pod "pod-89870773-746a-45b9-a4ab-0f48716411df" satisfied condition "Succeeded or Failed"
    Apr  6 10:40:30.482: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-89870773-746a-45b9-a4ab-0f48716411df container test-container: <nil>
    STEP: delete the pod 04/06/23 10:40:30.542
    Apr  6 10:40:30.564: INFO: Waiting for pod pod-89870773-746a-45b9-a4ab-0f48716411df to disappear
    Apr  6 10:40:30.568: INFO: Pod pod-89870773-746a-45b9-a4ab-0f48716411df no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 10:40:30.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-962" for this suite. 04/06/23 10:40:30.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:30.593
Apr  6 10:40:30.594: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:40:30.609
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:30.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:30.645
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-584fd0e2-0933-4b78-920b-6389d0eadd27 04/06/23 10:40:30.651
STEP: Creating a pod to test consume configMaps 04/06/23 10:40:30.656
Apr  6 10:40:30.668: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa" in namespace "projected-8171" to be "Succeeded or Failed"
Apr  6 10:40:30.685: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.814243ms
Apr  6 10:40:32.693: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0247582s
Apr  6 10:40:34.700: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032458378s
STEP: Saw pod success 04/06/23 10:40:34.7
Apr  6 10:40:34.701: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa" satisfied condition "Succeeded or Failed"
Apr  6 10:40:34.706: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:40:34.72
Apr  6 10:40:34.731: INFO: Waiting for pod pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa to disappear
Apr  6 10:40:34.735: INFO: Pod pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 10:40:34.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8171" for this suite. 04/06/23 10:40:34.743
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":86,"skipped":1551,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:30.593
    Apr  6 10:40:30.594: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:40:30.609
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:30.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:30.645
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-584fd0e2-0933-4b78-920b-6389d0eadd27 04/06/23 10:40:30.651
    STEP: Creating a pod to test consume configMaps 04/06/23 10:40:30.656
    Apr  6 10:40:30.668: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa" in namespace "projected-8171" to be "Succeeded or Failed"
    Apr  6 10:40:30.685: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.814243ms
    Apr  6 10:40:32.693: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0247582s
    Apr  6 10:40:34.700: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032458378s
    STEP: Saw pod success 04/06/23 10:40:34.7
    Apr  6 10:40:34.701: INFO: Pod "pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa" satisfied condition "Succeeded or Failed"
    Apr  6 10:40:34.706: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:40:34.72
    Apr  6 10:40:34.731: INFO: Waiting for pod pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa to disappear
    Apr  6 10:40:34.735: INFO: Pod pod-projected-configmaps-dc36d493-c93a-4369-b657-1d48294735fa no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 10:40:34.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8171" for this suite. 04/06/23 10:40:34.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:34.75
Apr  6 10:40:34.750: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:40:34.752
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:34.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:34.772
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-464f3b1f-7269-49f2-9c1a-37301c588d16 04/06/23 10:40:34.777
STEP: Creating a pod to test consume secrets 04/06/23 10:40:34.782
Apr  6 10:40:34.793: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b" in namespace "projected-881" to be "Succeeded or Failed"
Apr  6 10:40:34.798: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.861372ms
Apr  6 10:40:36.811: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Running", Reason="", readiness=true. Elapsed: 2.018183096s
Apr  6 10:40:38.828: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Running", Reason="", readiness=false. Elapsed: 4.035085579s
Apr  6 10:40:40.808: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014969955s
STEP: Saw pod success 04/06/23 10:40:40.808
Apr  6 10:40:40.808: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b" satisfied condition "Succeeded or Failed"
Apr  6 10:40:40.815: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b container projected-secret-volume-test: <nil>
STEP: delete the pod 04/06/23 10:40:40.874
Apr  6 10:40:40.885: INFO: Waiting for pod pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b to disappear
Apr  6 10:40:40.889: INFO: Pod pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Apr  6 10:40:40.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-881" for this suite. 04/06/23 10:40:40.901
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":87,"skipped":1593,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.170 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:34.75
    Apr  6 10:40:34.750: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:40:34.752
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:34.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:34.772
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-464f3b1f-7269-49f2-9c1a-37301c588d16 04/06/23 10:40:34.777
    STEP: Creating a pod to test consume secrets 04/06/23 10:40:34.782
    Apr  6 10:40:34.793: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b" in namespace "projected-881" to be "Succeeded or Failed"
    Apr  6 10:40:34.798: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.861372ms
    Apr  6 10:40:36.811: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Running", Reason="", readiness=true. Elapsed: 2.018183096s
    Apr  6 10:40:38.828: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Running", Reason="", readiness=false. Elapsed: 4.035085579s
    Apr  6 10:40:40.808: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014969955s
    STEP: Saw pod success 04/06/23 10:40:40.808
    Apr  6 10:40:40.808: INFO: Pod "pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b" satisfied condition "Succeeded or Failed"
    Apr  6 10:40:40.815: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:40:40.874
    Apr  6 10:40:40.885: INFO: Waiting for pod pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b to disappear
    Apr  6 10:40:40.889: INFO: Pod pod-projected-secrets-5af4ecc7-f22d-4f4b-ac1a-9350e55ba08b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Apr  6 10:40:40.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-881" for this suite. 04/06/23 10:40:40.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:40.924
Apr  6 10:40:40.924: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:40:40.925
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:40.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:40.951
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-e83fc2a8-ec68-4b41-bafa-581771f5fdef 04/06/23 10:40:40.957
STEP: Creating a pod to test consume configMaps 04/06/23 10:40:40.962
Apr  6 10:40:40.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba" in namespace "configmap-707" to be "Succeeded or Failed"
Apr  6 10:40:40.978: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.947617ms
Apr  6 10:40:42.985: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011303331s
Apr  6 10:40:44.984: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009881011s
STEP: Saw pod success 04/06/23 10:40:44.984
Apr  6 10:40:44.984: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba" satisfied condition "Succeeded or Failed"
Apr  6 10:40:44.992: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:40:45.016
Apr  6 10:40:45.030: INFO: Waiting for pod pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba to disappear
Apr  6 10:40:45.035: INFO: Pod pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:40:45.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-707" for this suite. 04/06/23 10:40:45.044
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":88,"skipped":1668,"failed":0}
------------------------------
â€¢ [4.127 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:40.924
    Apr  6 10:40:40.924: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:40:40.925
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:40.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:40.951
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-e83fc2a8-ec68-4b41-bafa-581771f5fdef 04/06/23 10:40:40.957
    STEP: Creating a pod to test consume configMaps 04/06/23 10:40:40.962
    Apr  6 10:40:40.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba" in namespace "configmap-707" to be "Succeeded or Failed"
    Apr  6 10:40:40.978: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.947617ms
    Apr  6 10:40:42.985: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011303331s
    Apr  6 10:40:44.984: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009881011s
    STEP: Saw pod success 04/06/23 10:40:44.984
    Apr  6 10:40:44.984: INFO: Pod "pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba" satisfied condition "Succeeded or Failed"
    Apr  6 10:40:44.992: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:40:45.016
    Apr  6 10:40:45.030: INFO: Waiting for pod pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba to disappear
    Apr  6 10:40:45.035: INFO: Pod pod-configmaps-f940e3a3-efbd-406f-9808-dc8630033cba no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:40:45.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-707" for this suite. 04/06/23 10:40:45.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:45.06
Apr  6 10:40:45.060: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename init-container 04/06/23 10:40:45.062
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:45.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:45.114
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 04/06/23 10:40:45.133
Apr  6 10:40:45.134: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Apr  6 10:40:48.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1081" for this suite. 04/06/23 10:40:48.961
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":89,"skipped":1732,"failed":0}
------------------------------
â€¢ [3.908 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:45.06
    Apr  6 10:40:45.060: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename init-container 04/06/23 10:40:45.062
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:45.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:45.114
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 04/06/23 10:40:45.133
    Apr  6 10:40:45.134: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Apr  6 10:40:48.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1081" for this suite. 04/06/23 10:40:48.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:48.971
Apr  6 10:40:48.971: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-pred 04/06/23 10:40:48.973
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:49.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:49.036
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Apr  6 10:40:49.046: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 10:40:49.071: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 10:40:49.078: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
Apr  6 10:40:49.098: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.098: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:40:49.098: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:40:49.098: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.098: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:40:49.098: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
Apr  6 10:40:49.098: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:40:49.098: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:40:49.098: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:40:49.098: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.098: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:40:49.098: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:40:49.098: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.098: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:40:49.098: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.098: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:40:49.099: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.099: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 10:40:49.099: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.099: INFO: 	Container e2e ready: true, restart count 0
Apr  6 10:40:49.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:40:49.099: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:40:49.099: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:40:49.099: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
Apr  6 10:40:49.124: INFO: pod-init-94873c01-f674-48de-887f-f969b32b6318 from init-container-1081 started at 2023-04-06 10:40:45 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container run1 ready: true, restart count 0
Apr  6 10:40:49.124: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:40:49.124: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:40:49.124: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:40:49.124: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:40:49.124: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:40:49.124: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:40:49.124: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:40:49.124: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:40:49.124: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:40:49.124: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:40:49.124: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:40:49.124: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:40:49.124: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
Apr  6 10:40:49.159: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:40:49.159: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:40:49.159: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 10:40:49.159: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:40:49.159: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 10:40:49.159: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 10:40:49.159: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 10:40:49.159: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container coredns ready: true, restart count 0
Apr  6 10:40:49.159: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container coredns ready: true, restart count 0
Apr  6 10:40:49.159: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
Apr  6 10:40:49.159: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:40:49.159: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:40:49.160: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:40:49.160: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.160: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:40:49.160: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:40:49.160: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.160: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 10:40:49.160: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.160: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 10:40:49.160: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.160: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:40:49.160: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.160: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:40:49.160: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.160: INFO: 	Container vpn-shoot ready: true, restart count 1
Apr  6 10:40:49.160: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.160: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:40:49.160: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:40:49.160: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
Apr  6 10:40:49.178: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.178: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:40:49.178: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:40:49.178: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.178: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:40:49.178: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
Apr  6 10:40:49.179: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:40:49.179: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:40:49.179: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:40:49.179: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.179: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:40:49.179: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:40:49.179: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.179: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:40:49.179: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.179: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:40:49.179: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:40:49.179: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 10:40:49.180: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
Apr  6 10:40:49.212: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container proxy ready: true, restart count 0
Apr  6 10:40:49.212: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 10:40:49.212: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 10:40:49.212: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 10:40:49.212: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 10:40:49.212: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 10:40:49.212: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 10:40:49.212: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 10:40:49.212: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 10:40:49.212: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 10:40:49.212: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 10:40:49.212: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 10:40:49.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 10:40:49.212: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/06/23 10:40:49.212
Apr  6 10:40:49.222: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4357" to be "running"
Apr  6 10:40:49.227: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250475ms
Apr  6 10:40:51.234: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011451413s
Apr  6 10:40:51.234: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/06/23 10:40:51.238
STEP: Trying to apply a random label on the found node. 04/06/23 10:40:51.249
STEP: verifying the node has the label kubernetes.io/e2e-c11f7af1-db03-40d3-bf66-ada355daa0ca 42 04/06/23 10:40:51.261
STEP: Trying to relaunch the pod, now with labels. 04/06/23 10:40:51.266
Apr  6 10:40:51.273: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4357" to be "not pending"
Apr  6 10:40:51.277: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.723161ms
Apr  6 10:40:53.284: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011491139s
Apr  6 10:40:53.284: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-c11f7af1-db03-40d3-bf66-ada355daa0ca off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 10:40:53.289
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c11f7af1-db03-40d3-bf66-ada355daa0ca 04/06/23 10:40:53.319
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:40:53.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4357" for this suite. 04/06/23 10:40:53.336
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":90,"skipped":1738,"failed":0}
------------------------------
â€¢ [4.374 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:48.971
    Apr  6 10:40:48.971: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-pred 04/06/23 10:40:48.973
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:49.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:49.036
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Apr  6 10:40:49.046: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr  6 10:40:49.071: INFO: Waiting for terminating namespaces to be deleted...
    Apr  6 10:40:49.078: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
    Apr  6 10:40:49.098: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.098: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.098: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
    Apr  6 10:40:49.098: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.098: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.098: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:40:49.098: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.098: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:40:49.099: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.099: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr  6 10:40:49.099: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.099: INFO: 	Container e2e ready: true, restart count 0
    Apr  6 10:40:49.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:40:49.099: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:40:49.099: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:40:49.099: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
    Apr  6 10:40:49.124: INFO: pod-init-94873c01-f674-48de-887f-f969b32b6318 from init-container-1081 started at 2023-04-06 10:40:45 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container run1 ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:40:49.124: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
    Apr  6 10:40:49.159: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
    Apr  6 10:40:49.159: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:40:49.159: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.160: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.160: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.160: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.160: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.160: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.160: INFO: 	Container vpn-shoot ready: true, restart count 1
    Apr  6 10:40:49.160: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.160: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:40:49.160: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
    Apr  6 10:40:49.178: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.178: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:40:49.178: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:40:49.178: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.178: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:40:49.178: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
    Apr  6 10:40:49.179: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.179: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.179: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.179: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:40:49.179: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 10:40:49.180: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
    Apr  6 10:40:49.212: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container calico-typha ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 10:40:49.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 10:40:49.212: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/06/23 10:40:49.212
    Apr  6 10:40:49.222: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4357" to be "running"
    Apr  6 10:40:49.227: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250475ms
    Apr  6 10:40:51.234: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011451413s
    Apr  6 10:40:51.234: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/06/23 10:40:51.238
    STEP: Trying to apply a random label on the found node. 04/06/23 10:40:51.249
    STEP: verifying the node has the label kubernetes.io/e2e-c11f7af1-db03-40d3-bf66-ada355daa0ca 42 04/06/23 10:40:51.261
    STEP: Trying to relaunch the pod, now with labels. 04/06/23 10:40:51.266
    Apr  6 10:40:51.273: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4357" to be "not pending"
    Apr  6 10:40:51.277: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.723161ms
    Apr  6 10:40:53.284: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011491139s
    Apr  6 10:40:53.284: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-c11f7af1-db03-40d3-bf66-ada355daa0ca off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 10:40:53.289
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-c11f7af1-db03-40d3-bf66-ada355daa0ca 04/06/23 10:40:53.319
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:40:53.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4357" for this suite. 04/06/23 10:40:53.336
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:53.346
Apr  6 10:40:53.347: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename csistoragecapacity 04/06/23 10:40:53.348
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:53.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:53.37
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 04/06/23 10:40:53.378
STEP: getting /apis/storage.k8s.io 04/06/23 10:40:53.383
STEP: getting /apis/storage.k8s.io/v1 04/06/23 10:40:53.386
STEP: creating 04/06/23 10:40:53.389
STEP: watching 04/06/23 10:40:53.42
Apr  6 10:40:53.420: INFO: starting watch
STEP: getting 04/06/23 10:40:53.433
STEP: listing in namespace 04/06/23 10:40:53.439
STEP: listing across namespaces 04/06/23 10:40:53.444
STEP: patching 04/06/23 10:40:53.449
STEP: updating 04/06/23 10:40:53.456
Apr  6 10:40:53.464: INFO: waiting for watch events with expected annotations in namespace
Apr  6 10:40:53.464: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 04/06/23 10:40:53.464
STEP: deleting a collection 04/06/23 10:40:53.492
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Apr  6 10:40:53.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-2268" for this suite. 04/06/23 10:40:53.531
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":91,"skipped":1738,"failed":0}
------------------------------
â€¢ [0.190 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:53.346
    Apr  6 10:40:53.347: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename csistoragecapacity 04/06/23 10:40:53.348
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:53.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:53.37
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 04/06/23 10:40:53.378
    STEP: getting /apis/storage.k8s.io 04/06/23 10:40:53.383
    STEP: getting /apis/storage.k8s.io/v1 04/06/23 10:40:53.386
    STEP: creating 04/06/23 10:40:53.389
    STEP: watching 04/06/23 10:40:53.42
    Apr  6 10:40:53.420: INFO: starting watch
    STEP: getting 04/06/23 10:40:53.433
    STEP: listing in namespace 04/06/23 10:40:53.439
    STEP: listing across namespaces 04/06/23 10:40:53.444
    STEP: patching 04/06/23 10:40:53.449
    STEP: updating 04/06/23 10:40:53.456
    Apr  6 10:40:53.464: INFO: waiting for watch events with expected annotations in namespace
    Apr  6 10:40:53.464: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 04/06/23 10:40:53.464
    STEP: deleting a collection 04/06/23 10:40:53.492
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Apr  6 10:40:53.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-2268" for this suite. 04/06/23 10:40:53.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:53.54
Apr  6 10:40:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename conformance-tests 04/06/23 10:40:53.543
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:53.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:53.575
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 04/06/23 10:40:53.584
Apr  6 10:40:53.584: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Apr  6 10:40:53.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-819" for this suite. 04/06/23 10:40:53.623
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":92,"skipped":1752,"failed":0}
------------------------------
â€¢ [0.093 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:53.54
    Apr  6 10:40:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename conformance-tests 04/06/23 10:40:53.543
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:53.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:53.575
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 04/06/23 10:40:53.584
    Apr  6 10:40:53.584: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Apr  6 10:40:53.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-819" for this suite. 04/06/23 10:40:53.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:53.634
Apr  6 10:40:53.634: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename certificates 04/06/23 10:40:53.635
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:53.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:53.683
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 04/06/23 10:40:54.469
STEP: getting /apis/certificates.k8s.io 04/06/23 10:40:54.476
STEP: getting /apis/certificates.k8s.io/v1 04/06/23 10:40:54.479
STEP: creating 04/06/23 10:40:54.484
STEP: getting 04/06/23 10:40:54.51
STEP: listing 04/06/23 10:40:54.517
STEP: watching 04/06/23 10:40:54.526
Apr  6 10:40:54.526: INFO: starting watch
STEP: patching 04/06/23 10:40:54.529
STEP: updating 04/06/23 10:40:54.537
Apr  6 10:40:54.546: INFO: waiting for watch events with expected annotations
Apr  6 10:40:54.546: INFO: saw patched and updated annotations
STEP: getting /approval 04/06/23 10:40:54.546
STEP: patching /approval 04/06/23 10:40:54.552
STEP: updating /approval 04/06/23 10:40:54.562
STEP: getting /status 04/06/23 10:40:54.569
STEP: patching /status 04/06/23 10:40:54.573
STEP: updating /status 04/06/23 10:40:54.581
STEP: deleting 04/06/23 10:40:54.59
STEP: deleting a collection 04/06/23 10:40:54.628
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:40:54.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3462" for this suite. 04/06/23 10:40:54.655
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":93,"skipped":1779,"failed":0}
------------------------------
â€¢ [1.025 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:53.634
    Apr  6 10:40:53.634: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename certificates 04/06/23 10:40:53.635
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:53.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:53.683
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 04/06/23 10:40:54.469
    STEP: getting /apis/certificates.k8s.io 04/06/23 10:40:54.476
    STEP: getting /apis/certificates.k8s.io/v1 04/06/23 10:40:54.479
    STEP: creating 04/06/23 10:40:54.484
    STEP: getting 04/06/23 10:40:54.51
    STEP: listing 04/06/23 10:40:54.517
    STEP: watching 04/06/23 10:40:54.526
    Apr  6 10:40:54.526: INFO: starting watch
    STEP: patching 04/06/23 10:40:54.529
    STEP: updating 04/06/23 10:40:54.537
    Apr  6 10:40:54.546: INFO: waiting for watch events with expected annotations
    Apr  6 10:40:54.546: INFO: saw patched and updated annotations
    STEP: getting /approval 04/06/23 10:40:54.546
    STEP: patching /approval 04/06/23 10:40:54.552
    STEP: updating /approval 04/06/23 10:40:54.562
    STEP: getting /status 04/06/23 10:40:54.569
    STEP: patching /status 04/06/23 10:40:54.573
    STEP: updating /status 04/06/23 10:40:54.581
    STEP: deleting 04/06/23 10:40:54.59
    STEP: deleting a collection 04/06/23 10:40:54.628
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:40:54.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-3462" for this suite. 04/06/23 10:40:54.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:40:54.662
Apr  6 10:40:54.662: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:40:54.663
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:54.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:54.683
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-1033 04/06/23 10:40:54.689
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[] 04/06/23 10:40:54.709
Apr  6 10:40:54.735: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1033 04/06/23 10:40:54.735
Apr  6 10:40:54.749: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1033" to be "running and ready"
Apr  6 10:40:54.752: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490644ms
Apr  6 10:40:54.752: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:40:56.762: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013016596s
Apr  6 10:40:56.762: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr  6 10:40:56.762: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[pod1:[80]] 04/06/23 10:40:56.767
Apr  6 10:40:56.788: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 04/06/23 10:40:56.788
Apr  6 10:40:56.788: INFO: Creating new exec pod
Apr  6 10:40:56.801: INFO: Waiting up to 5m0s for pod "execpodd9bcg" in namespace "services-1033" to be "running"
Apr  6 10:40:56.812: INFO: Pod "execpodd9bcg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691187ms
Apr  6 10:40:58.836: INFO: Pod "execpodd9bcg": Phase="Running", Reason="", readiness=true. Elapsed: 2.029533285s
Apr  6 10:40:58.836: INFO: Pod "execpodd9bcg" satisfied condition "running"
Apr  6 10:40:59.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr  6 10:41:00.556: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr  6 10:41:00.557: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:41:00.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.69.67 80'
Apr  6 10:41:01.206: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.71.69.67 80\nConnection to 10.71.69.67 80 port [tcp/http] succeeded!\n"
Apr  6 10:41:01.206: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1033 04/06/23 10:41:01.206
Apr  6 10:41:01.223: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1033" to be "running and ready"
Apr  6 10:41:01.231: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.839037ms
Apr  6 10:41:01.231: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:41:03.238: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015493192s
Apr  6 10:41:03.238: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr  6 10:41:03.238: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[pod1:[80] pod2:[80]] 04/06/23 10:41:03.243
Apr  6 10:41:03.268: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 04/06/23 10:41:03.268
Apr  6 10:41:04.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr  6 10:41:04.927: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr  6 10:41:04.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:41:04.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.69.67 80'
Apr  6 10:41:05.489: INFO: stderr: "+ nc -v -t -w 2 10.71.69.67 80\n+ echo hostName\nConnection to 10.71.69.67 80 port [tcp/http] succeeded!\n"
Apr  6 10:41:05.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1033 04/06/23 10:41:05.489
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[pod2:[80]] 04/06/23 10:41:05.5
Apr  6 10:41:05.541: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 04/06/23 10:41:05.541
Apr  6 10:41:06.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Apr  6 10:41:07.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr  6 10:41:07.239: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:41:07.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.69.67 80'
Apr  6 10:41:07.857: INFO: stderr: "+ nc -v -t -w 2 10.71.69.67 80\nConnection to 10.71.69.67 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Apr  6 10:41:07.857: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1033 04/06/23 10:41:07.857
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[] 04/06/23 10:41:07.868
Apr  6 10:41:07.888: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:41:07.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1033" for this suite. 04/06/23 10:41:07.913
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":94,"skipped":1810,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.259 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:40:54.662
    Apr  6 10:40:54.662: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:40:54.663
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:40:54.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:40:54.683
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-1033 04/06/23 10:40:54.689
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[] 04/06/23 10:40:54.709
    Apr  6 10:40:54.735: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1033 04/06/23 10:40:54.735
    Apr  6 10:40:54.749: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1033" to be "running and ready"
    Apr  6 10:40:54.752: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490644ms
    Apr  6 10:40:54.752: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:40:56.762: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013016596s
    Apr  6 10:40:56.762: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr  6 10:40:56.762: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[pod1:[80]] 04/06/23 10:40:56.767
    Apr  6 10:40:56.788: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 04/06/23 10:40:56.788
    Apr  6 10:40:56.788: INFO: Creating new exec pod
    Apr  6 10:40:56.801: INFO: Waiting up to 5m0s for pod "execpodd9bcg" in namespace "services-1033" to be "running"
    Apr  6 10:40:56.812: INFO: Pod "execpodd9bcg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691187ms
    Apr  6 10:40:58.836: INFO: Pod "execpodd9bcg": Phase="Running", Reason="", readiness=true. Elapsed: 2.029533285s
    Apr  6 10:40:58.836: INFO: Pod "execpodd9bcg" satisfied condition "running"
    Apr  6 10:40:59.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Apr  6 10:41:00.556: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr  6 10:41:00.557: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:41:00.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.69.67 80'
    Apr  6 10:41:01.206: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.71.69.67 80\nConnection to 10.71.69.67 80 port [tcp/http] succeeded!\n"
    Apr  6 10:41:01.206: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-1033 04/06/23 10:41:01.206
    Apr  6 10:41:01.223: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1033" to be "running and ready"
    Apr  6 10:41:01.231: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.839037ms
    Apr  6 10:41:01.231: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:41:03.238: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015493192s
    Apr  6 10:41:03.238: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr  6 10:41:03.238: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[pod1:[80] pod2:[80]] 04/06/23 10:41:03.243
    Apr  6 10:41:03.268: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 04/06/23 10:41:03.268
    Apr  6 10:41:04.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Apr  6 10:41:04.927: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr  6 10:41:04.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:41:04.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.69.67 80'
    Apr  6 10:41:05.489: INFO: stderr: "+ nc -v -t -w 2 10.71.69.67 80\n+ echo hostName\nConnection to 10.71.69.67 80 port [tcp/http] succeeded!\n"
    Apr  6 10:41:05.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-1033 04/06/23 10:41:05.489
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[pod2:[80]] 04/06/23 10:41:05.5
    Apr  6 10:41:05.541: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 04/06/23 10:41:05.541
    Apr  6 10:41:06.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Apr  6 10:41:07.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr  6 10:41:07.239: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:41:07.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1033 exec execpodd9bcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.69.67 80'
    Apr  6 10:41:07.857: INFO: stderr: "+ nc -v -t -w 2 10.71.69.67 80\nConnection to 10.71.69.67 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Apr  6 10:41:07.857: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-1033 04/06/23 10:41:07.857
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1033 to expose endpoints map[] 04/06/23 10:41:07.868
    Apr  6 10:41:07.888: INFO: successfully validated that service endpoint-test2 in namespace services-1033 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:41:07.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1033" for this suite. 04/06/23 10:41:07.913
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:41:07.921
Apr  6 10:41:07.921: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-webhook 04/06/23 10:41:07.922
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:07.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:07.945
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/06/23 10:41:07.959
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/06/23 10:41:08.372
STEP: Deploying the custom resource conversion webhook pod 04/06/23 10:41:08.386
STEP: Wait for the deployment to be ready 04/06/23 10:41:08.404
Apr  6 10:41:08.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-59dfc5db8d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 04/06/23 10:41:10.433
STEP: Verifying the service has paired with the endpoint 04/06/23 10:41:10.446
Apr  6 10:41:11.446: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Apr  6 10:41:11.456: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Creating a v1 custom resource 04/06/23 10:41:14.311
STEP: Create a v2 custom resource 04/06/23 10:41:14.336
STEP: List CRs in v1 04/06/23 10:41:14.511
STEP: List CRs in v2 04/06/23 10:41:14.534
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:41:15.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7682" for this suite. 04/06/23 10:41:15.087
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":95,"skipped":1825,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.231 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:41:07.921
    Apr  6 10:41:07.921: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-webhook 04/06/23 10:41:07.922
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:07.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:07.945
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/06/23 10:41:07.959
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/06/23 10:41:08.372
    STEP: Deploying the custom resource conversion webhook pod 04/06/23 10:41:08.386
    STEP: Wait for the deployment to be ready 04/06/23 10:41:08.404
    Apr  6 10:41:08.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-59dfc5db8d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 10, 41, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 04/06/23 10:41:10.433
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:41:10.446
    Apr  6 10:41:11.446: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Apr  6 10:41:11.456: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Creating a v1 custom resource 04/06/23 10:41:14.311
    STEP: Create a v2 custom resource 04/06/23 10:41:14.336
    STEP: List CRs in v1 04/06/23 10:41:14.511
    STEP: List CRs in v2 04/06/23 10:41:14.534
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:41:15.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7682" for this suite. 04/06/23 10:41:15.087
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:41:15.156
Apr  6 10:41:15.156: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:41:15.157
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:15.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:15.206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:41:15.218
Apr  6 10:41:15.240: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48" in namespace "downward-api-555" to be "Succeeded or Failed"
Apr  6 10:41:15.246: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99647ms
Apr  6 10:41:17.260: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019860886s
Apr  6 10:41:19.259: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019548096s
STEP: Saw pod success 04/06/23 10:41:19.259
Apr  6 10:41:19.260: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48" satisfied condition "Succeeded or Failed"
Apr  6 10:41:19.268: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48 container client-container: <nil>
STEP: delete the pod 04/06/23 10:41:19.279
Apr  6 10:41:19.288: INFO: Waiting for pod downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48 to disappear
Apr  6 10:41:19.301: INFO: Pod downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 10:41:19.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-555" for this suite. 04/06/23 10:41:19.33
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":96,"skipped":1826,"failed":0}
------------------------------
â€¢ [4.187 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:41:15.156
    Apr  6 10:41:15.156: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:41:15.157
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:15.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:15.206
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:41:15.218
    Apr  6 10:41:15.240: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48" in namespace "downward-api-555" to be "Succeeded or Failed"
    Apr  6 10:41:15.246: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99647ms
    Apr  6 10:41:17.260: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019860886s
    Apr  6 10:41:19.259: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019548096s
    STEP: Saw pod success 04/06/23 10:41:19.259
    Apr  6 10:41:19.260: INFO: Pod "downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48" satisfied condition "Succeeded or Failed"
    Apr  6 10:41:19.268: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:41:19.279
    Apr  6 10:41:19.288: INFO: Waiting for pod downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48 to disappear
    Apr  6 10:41:19.301: INFO: Pod downwardapi-volume-36eb6cb3-ab4e-42c4-a932-d52cb58f9d48 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 10:41:19.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-555" for this suite. 04/06/23 10:41:19.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:41:19.35
Apr  6 10:41:19.350: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replication-controller 04/06/23 10:41:19.352
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:19.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:19.38
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 04/06/23 10:41:19.388
STEP: When the matched label of one of its pods change 04/06/23 10:41:19.394
Apr  6 10:41:19.409: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 04/06/23 10:41:19.425
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Apr  6 10:41:20.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9547" for this suite. 04/06/23 10:41:20.446
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":97,"skipped":1891,"failed":0}
------------------------------
â€¢ [1.101 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:41:19.35
    Apr  6 10:41:19.350: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replication-controller 04/06/23 10:41:19.352
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:19.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:19.38
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 04/06/23 10:41:19.388
    STEP: When the matched label of one of its pods change 04/06/23 10:41:19.394
    Apr  6 10:41:19.409: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/06/23 10:41:19.425
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Apr  6 10:41:20.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9547" for this suite. 04/06/23 10:41:20.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:41:20.454
Apr  6 10:41:20.454: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:41:20.455
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:20.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:20.475
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1243 04/06/23 10:41:20.483
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/06/23 10:41:20.498
STEP: creating service externalsvc in namespace services-1243 04/06/23 10:41:20.504
STEP: creating replication controller externalsvc in namespace services-1243 04/06/23 10:41:20.516
I0406 10:41:20.521631      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1243, replica count: 2
I0406 10:41:23.573855      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 04/06/23 10:41:23.582
Apr  6 10:41:23.604: INFO: Creating new exec pod
Apr  6 10:41:23.625: INFO: Waiting up to 5m0s for pod "execpodph67q" in namespace "services-1243" to be "running"
Apr  6 10:41:23.630: INFO: Pod "execpodph67q": Phase="Pending", Reason="", readiness=false. Elapsed: 5.130679ms
Apr  6 10:41:25.646: INFO: Pod "execpodph67q": Phase="Running", Reason="", readiness=true. Elapsed: 2.020837229s
Apr  6 10:41:25.646: INFO: Pod "execpodph67q" satisfied condition "running"
Apr  6 10:41:25.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1243 exec execpodph67q -- /bin/sh -x -c nslookup nodeport-service.services-1243.svc.cluster.local'
Apr  6 10:41:26.462: INFO: stderr: "+ nslookup nodeport-service.services-1243.svc.cluster.local\n"
Apr  6 10:41:26.462: INFO: stdout: "Server:\t\t10.64.0.10\nAddress:\t10.64.0.10#53\n\nnodeport-service.services-1243.svc.cluster.local\tcanonical name = externalsvc.services-1243.svc.cluster.local.\nName:\texternalsvc.services-1243.svc.cluster.local\nAddress: 10.69.144.129\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1243, will wait for the garbage collector to delete the pods 04/06/23 10:41:26.462
Apr  6 10:41:26.525: INFO: Deleting ReplicationController externalsvc took: 8.125905ms
Apr  6 10:41:26.626: INFO: Terminating ReplicationController externalsvc pods took: 101.021965ms
Apr  6 10:41:28.447: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:41:28.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1243" for this suite. 04/06/23 10:41:28.474
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":98,"skipped":1909,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.033 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:41:20.454
    Apr  6 10:41:20.454: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:41:20.455
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:20.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:20.475
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-1243 04/06/23 10:41:20.483
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/06/23 10:41:20.498
    STEP: creating service externalsvc in namespace services-1243 04/06/23 10:41:20.504
    STEP: creating replication controller externalsvc in namespace services-1243 04/06/23 10:41:20.516
    I0406 10:41:20.521631      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1243, replica count: 2
    I0406 10:41:23.573855      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 04/06/23 10:41:23.582
    Apr  6 10:41:23.604: INFO: Creating new exec pod
    Apr  6 10:41:23.625: INFO: Waiting up to 5m0s for pod "execpodph67q" in namespace "services-1243" to be "running"
    Apr  6 10:41:23.630: INFO: Pod "execpodph67q": Phase="Pending", Reason="", readiness=false. Elapsed: 5.130679ms
    Apr  6 10:41:25.646: INFO: Pod "execpodph67q": Phase="Running", Reason="", readiness=true. Elapsed: 2.020837229s
    Apr  6 10:41:25.646: INFO: Pod "execpodph67q" satisfied condition "running"
    Apr  6 10:41:25.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1243 exec execpodph67q -- /bin/sh -x -c nslookup nodeport-service.services-1243.svc.cluster.local'
    Apr  6 10:41:26.462: INFO: stderr: "+ nslookup nodeport-service.services-1243.svc.cluster.local\n"
    Apr  6 10:41:26.462: INFO: stdout: "Server:\t\t10.64.0.10\nAddress:\t10.64.0.10#53\n\nnodeport-service.services-1243.svc.cluster.local\tcanonical name = externalsvc.services-1243.svc.cluster.local.\nName:\texternalsvc.services-1243.svc.cluster.local\nAddress: 10.69.144.129\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1243, will wait for the garbage collector to delete the pods 04/06/23 10:41:26.462
    Apr  6 10:41:26.525: INFO: Deleting ReplicationController externalsvc took: 8.125905ms
    Apr  6 10:41:26.626: INFO: Terminating ReplicationController externalsvc pods took: 101.021965ms
    Apr  6 10:41:28.447: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:41:28.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1243" for this suite. 04/06/23 10:41:28.474
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:41:28.488
Apr  6 10:41:28.488: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 10:41:28.49
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:28.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:28.518
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 10:42:28.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1712" for this suite. 04/06/23 10:42:28.59
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":99,"skipped":1912,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.118 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:41:28.488
    Apr  6 10:41:28.488: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 10:41:28.49
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:41:28.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:41:28.518
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 10:42:28.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1712" for this suite. 04/06/23 10:42:28.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:28.608
Apr  6 10:42:28.608: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:42:28.61
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:28.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:28.645
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:42:28.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-379" for this suite. 04/06/23 10:42:28.683
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":100,"skipped":1945,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:28.608
    Apr  6 10:42:28.608: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:42:28.61
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:28.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:28.645
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:42:28.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-379" for this suite. 04/06/23 10:42:28.683
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:28.694
Apr  6 10:42:28.694: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 10:42:28.704
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:28.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:28.749
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 04/06/23 10:42:28.772
Apr  6 10:42:28.804: INFO: Waiting up to 5m0s for pod "pod-twt5w" in namespace "pods-9294" to be "running"
Apr  6 10:42:28.808: INFO: Pod "pod-twt5w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.898395ms
Apr  6 10:42:30.816: INFO: Pod "pod-twt5w": Phase="Running", Reason="", readiness=true. Elapsed: 2.0128175s
Apr  6 10:42:30.816: INFO: Pod "pod-twt5w" satisfied condition "running"
STEP: patching /status 04/06/23 10:42:30.816
Apr  6 10:42:30.826: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 10:42:30.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9294" for this suite. 04/06/23 10:42:30.833
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":101,"skipped":1969,"failed":0}
------------------------------
â€¢ [2.146 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:28.694
    Apr  6 10:42:28.694: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 10:42:28.704
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:28.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:28.749
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 04/06/23 10:42:28.772
    Apr  6 10:42:28.804: INFO: Waiting up to 5m0s for pod "pod-twt5w" in namespace "pods-9294" to be "running"
    Apr  6 10:42:28.808: INFO: Pod "pod-twt5w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.898395ms
    Apr  6 10:42:30.816: INFO: Pod "pod-twt5w": Phase="Running", Reason="", readiness=true. Elapsed: 2.0128175s
    Apr  6 10:42:30.816: INFO: Pod "pod-twt5w" satisfied condition "running"
    STEP: patching /status 04/06/23 10:42:30.816
    Apr  6 10:42:30.826: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 10:42:30.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9294" for this suite. 04/06/23 10:42:30.833
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:30.84
Apr  6 10:42:30.840: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename namespaces 04/06/23 10:42:30.841
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:30.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:30.862
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 04/06/23 10:42:30.867
STEP: patching the Namespace 04/06/23 10:42:30.892
STEP: get the Namespace and ensuring it has the label 04/06/23 10:42:30.897
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:42:30.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5004" for this suite. 04/06/23 10:42:30.907
STEP: Destroying namespace "nspatchtest-c2f1803a-75f6-4114-8099-10b968eaacc8-9698" for this suite. 04/06/23 10:42:30.912
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":102,"skipped":1972,"failed":0}
------------------------------
â€¢ [0.078 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:30.84
    Apr  6 10:42:30.840: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename namespaces 04/06/23 10:42:30.841
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:30.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:30.862
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 04/06/23 10:42:30.867
    STEP: patching the Namespace 04/06/23 10:42:30.892
    STEP: get the Namespace and ensuring it has the label 04/06/23 10:42:30.897
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:42:30.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5004" for this suite. 04/06/23 10:42:30.907
    STEP: Destroying namespace "nspatchtest-c2f1803a-75f6-4114-8099-10b968eaacc8-9698" for this suite. 04/06/23 10:42:30.912
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:30.918
Apr  6 10:42:30.919: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename runtimeclass 04/06/23 10:42:30.92
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:30.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:30.938
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Apr  6 10:42:30.962: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1518 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Apr  6 10:42:30.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1518" for this suite. 04/06/23 10:42:30.987
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":103,"skipped":1974,"failed":0}
------------------------------
â€¢ [0.076 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:30.918
    Apr  6 10:42:30.919: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename runtimeclass 04/06/23 10:42:30.92
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:30.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:30.938
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Apr  6 10:42:30.962: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1518 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Apr  6 10:42:30.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1518" for this suite. 04/06/23 10:42:30.987
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:30.995
Apr  6 10:42:30.995: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubelet-test 04/06/23 10:42:30.997
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:31.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:31.024
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Apr  6 10:42:35.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2735" for this suite. 04/06/23 10:42:35.072
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":104,"skipped":1977,"failed":0}
------------------------------
â€¢ [4.084 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:30.995
    Apr  6 10:42:30.995: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubelet-test 04/06/23 10:42:30.997
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:31.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:31.024
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Apr  6 10:42:35.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2735" for this suite. 04/06/23 10:42:35.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:35.096
Apr  6 10:42:35.096: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:42:35.101
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:35.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:35.136
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-02d2dbaf-8b5c-454d-b108-1f606d7bd5d8 04/06/23 10:42:35.155
STEP: Creating a pod to test consume configMaps 04/06/23 10:42:35.16
Apr  6 10:42:35.172: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473" in namespace "configmap-4392" to be "Succeeded or Failed"
Apr  6 10:42:35.179: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473": Phase="Pending", Reason="", readiness=false. Elapsed: 6.143756ms
Apr  6 10:42:37.186: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013280103s
Apr  6 10:42:39.186: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014041048s
STEP: Saw pod success 04/06/23 10:42:39.187
Apr  6 10:42:39.187: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473" satisfied condition "Succeeded or Failed"
Apr  6 10:42:39.192: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 10:42:39.225
Apr  6 10:42:39.236: INFO: Waiting for pod pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473 to disappear
Apr  6 10:42:39.241: INFO: Pod pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:42:39.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4392" for this suite. 04/06/23 10:42:39.26
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":105,"skipped":2007,"failed":0}
------------------------------
â€¢ [4.169 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:35.096
    Apr  6 10:42:35.096: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:42:35.101
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:35.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:35.136
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-02d2dbaf-8b5c-454d-b108-1f606d7bd5d8 04/06/23 10:42:35.155
    STEP: Creating a pod to test consume configMaps 04/06/23 10:42:35.16
    Apr  6 10:42:35.172: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473" in namespace "configmap-4392" to be "Succeeded or Failed"
    Apr  6 10:42:35.179: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473": Phase="Pending", Reason="", readiness=false. Elapsed: 6.143756ms
    Apr  6 10:42:37.186: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013280103s
    Apr  6 10:42:39.186: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014041048s
    STEP: Saw pod success 04/06/23 10:42:39.187
    Apr  6 10:42:39.187: INFO: Pod "pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473" satisfied condition "Succeeded or Failed"
    Apr  6 10:42:39.192: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 10:42:39.225
    Apr  6 10:42:39.236: INFO: Waiting for pod pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473 to disappear
    Apr  6 10:42:39.241: INFO: Pod pod-configmaps-dc5226c1-845f-4f3b-b925-33ab75c5c473 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:42:39.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4392" for this suite. 04/06/23 10:42:39.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:39.268
Apr  6 10:42:39.268: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:42:39.271
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:39.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:39.294
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-94b8fa7b-3ea6-43e0-9245-5f32aacd777c 04/06/23 10:42:39.299
STEP: Creating secret with name secret-projected-all-test-volume-b749a793-b144-490c-b06f-09bbe0e6e995 04/06/23 10:42:39.305
STEP: Creating a pod to test Check all projections for projected volume plugin 04/06/23 10:42:39.313
Apr  6 10:42:39.327: INFO: Waiting up to 5m0s for pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32" in namespace "projected-2583" to be "Succeeded or Failed"
Apr  6 10:42:39.331: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408803ms
Apr  6 10:42:41.338: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011041931s
Apr  6 10:42:43.344: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016349024s
STEP: Saw pod success 04/06/23 10:42:43.344
Apr  6 10:42:43.344: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32" satisfied condition "Succeeded or Failed"
Apr  6 10:42:43.354: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32 container projected-all-volume-test: <nil>
STEP: delete the pod 04/06/23 10:42:43.423
Apr  6 10:42:43.459: INFO: Waiting for pod projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32 to disappear
Apr  6 10:42:43.466: INFO: Pod projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Apr  6 10:42:43.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2583" for this suite. 04/06/23 10:42:43.476
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":106,"skipped":2012,"failed":0}
------------------------------
â€¢ [4.216 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:39.268
    Apr  6 10:42:39.268: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:42:39.271
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:39.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:39.294
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-94b8fa7b-3ea6-43e0-9245-5f32aacd777c 04/06/23 10:42:39.299
    STEP: Creating secret with name secret-projected-all-test-volume-b749a793-b144-490c-b06f-09bbe0e6e995 04/06/23 10:42:39.305
    STEP: Creating a pod to test Check all projections for projected volume plugin 04/06/23 10:42:39.313
    Apr  6 10:42:39.327: INFO: Waiting up to 5m0s for pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32" in namespace "projected-2583" to be "Succeeded or Failed"
    Apr  6 10:42:39.331: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408803ms
    Apr  6 10:42:41.338: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011041931s
    Apr  6 10:42:43.344: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016349024s
    STEP: Saw pod success 04/06/23 10:42:43.344
    Apr  6 10:42:43.344: INFO: Pod "projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32" satisfied condition "Succeeded or Failed"
    Apr  6 10:42:43.354: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32 container projected-all-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:42:43.423
    Apr  6 10:42:43.459: INFO: Waiting for pod projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32 to disappear
    Apr  6 10:42:43.466: INFO: Pod projected-volume-9fe21ba8-1f52-44a1-970b-e953859e3d32 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Apr  6 10:42:43.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2583" for this suite. 04/06/23 10:42:43.476
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:42:43.484
Apr  6 10:42:43.484: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 10:42:43.486
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:43.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:43.537
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2323 04/06/23 10:42:43.552
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-2323 04/06/23 10:42:43.574
Apr  6 10:42:43.603: INFO: Found 0 stateful pods, waiting for 1
Apr  6 10:42:53.620: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 04/06/23 10:42:53.632
STEP: updating a scale subresource 04/06/23 10:42:53.638
STEP: verifying the statefulset Spec.Replicas was modified 04/06/23 10:42:53.647
STEP: Patch a scale subresource 04/06/23 10:42:53.651
STEP: verifying the statefulset Spec.Replicas was modified 04/06/23 10:42:53.661
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 10:42:53.665: INFO: Deleting all statefulset in ns statefulset-2323
Apr  6 10:42:53.669: INFO: Scaling statefulset ss to 0
Apr  6 10:43:03.720: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 10:43:03.727: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 10:43:03.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2323" for this suite. 04/06/23 10:43:03.766
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":107,"skipped":2016,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.290 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:42:43.484
    Apr  6 10:42:43.484: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 10:42:43.486
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:42:43.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:42:43.537
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2323 04/06/23 10:42:43.552
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-2323 04/06/23 10:42:43.574
    Apr  6 10:42:43.603: INFO: Found 0 stateful pods, waiting for 1
    Apr  6 10:42:53.620: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 04/06/23 10:42:53.632
    STEP: updating a scale subresource 04/06/23 10:42:53.638
    STEP: verifying the statefulset Spec.Replicas was modified 04/06/23 10:42:53.647
    STEP: Patch a scale subresource 04/06/23 10:42:53.651
    STEP: verifying the statefulset Spec.Replicas was modified 04/06/23 10:42:53.661
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 10:42:53.665: INFO: Deleting all statefulset in ns statefulset-2323
    Apr  6 10:42:53.669: INFO: Scaling statefulset ss to 0
    Apr  6 10:43:03.720: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 10:43:03.727: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 10:43:03.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2323" for this suite. 04/06/23 10:43:03.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:43:03.775
Apr  6 10:43:03.775: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:43:03.776
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:43:03.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:43:03.815
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-8789 04/06/23 10:43:03.827
Apr  6 10:43:03.842: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8789" to be "running and ready"
Apr  6 10:43:03.849: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.587256ms
Apr  6 10:43:03.850: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:43:05.857: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015673597s
Apr  6 10:43:05.858: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Apr  6 10:43:05.858: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Apr  6 10:43:05.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr  6 10:43:06.559: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr  6 10:43:06.559: INFO: stdout: "iptables"
Apr  6 10:43:06.559: INFO: proxyMode: iptables
Apr  6 10:43:06.568: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr  6 10:43:06.575: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8789 04/06/23 10:43:06.575
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8789 04/06/23 10:43:06.595
I0406 10:43:06.601843      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8789, replica count: 3
I0406 10:43:09.656031      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 10:43:09.707: INFO: Creating new exec pod
Apr  6 10:43:09.725: INFO: Waiting up to 5m0s for pod "execpod-affinitytbb9b" in namespace "services-8789" to be "running"
Apr  6 10:43:09.736: INFO: Pod "execpod-affinitytbb9b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.725134ms
Apr  6 10:43:11.743: INFO: Pod "execpod-affinitytbb9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017939086s
Apr  6 10:43:11.743: INFO: Pod "execpod-affinitytbb9b" satisfied condition "running"
Apr  6 10:43:12.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Apr  6 10:43:13.399: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr  6 10:43:13.399: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:43:13.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.67.83.122 80'
Apr  6 10:43:14.121: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.67.83.122 80\nConnection to 10.67.83.122 80 port [tcp/http] succeeded!\n"
Apr  6 10:43:14.121: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:43:14.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 31641'
Apr  6 10:43:14.696: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.236 31641\nConnection to 10.250.2.236 31641 port [tcp/*] succeeded!\n"
Apr  6 10:43:14.696: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:43:14.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.220 31641'
Apr  6 10:43:15.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.220 31641\nConnection to 10.250.3.220 31641 port [tcp/*] succeeded!\n"
Apr  6 10:43:15.438: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:43:15.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:31641/ ; done'
Apr  6 10:43:16.189: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
Apr  6 10:43:16.189: INFO: stdout: "\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv"
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
Apr  6 10:43:16.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
Apr  6 10:43:16.827: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
Apr  6 10:43:16.827: INFO: stdout: "affinity-nodeport-timeout-td4jv"
Apr  6 10:43:36.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
Apr  6 10:43:37.435: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
Apr  6 10:43:37.435: INFO: stdout: "affinity-nodeport-timeout-td4jv"
Apr  6 10:43:57.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
Apr  6 10:43:58.112: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
Apr  6 10:43:58.112: INFO: stdout: "affinity-nodeport-timeout-td4jv"
Apr  6 10:44:18.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
Apr  6 10:44:18.775: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
Apr  6 10:44:18.775: INFO: stdout: "affinity-nodeport-timeout-lxqtz"
Apr  6 10:44:18.775: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8789, will wait for the garbage collector to delete the pods 04/06/23 10:44:18.791
Apr  6 10:44:18.867: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.195227ms
Apr  6 10:44:18.968: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.861454ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:44:21.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8789" for this suite. 04/06/23 10:44:21.633
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":108,"skipped":2041,"failed":0}
------------------------------
â€¢ [SLOW TEST] [77.866 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:43:03.775
    Apr  6 10:43:03.775: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:43:03.776
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:43:03.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:43:03.815
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-8789 04/06/23 10:43:03.827
    Apr  6 10:43:03.842: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8789" to be "running and ready"
    Apr  6 10:43:03.849: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.587256ms
    Apr  6 10:43:03.850: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:43:05.857: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.015673597s
    Apr  6 10:43:05.858: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Apr  6 10:43:05.858: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Apr  6 10:43:05.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Apr  6 10:43:06.559: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Apr  6 10:43:06.559: INFO: stdout: "iptables"
    Apr  6 10:43:06.559: INFO: proxyMode: iptables
    Apr  6 10:43:06.568: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Apr  6 10:43:06.575: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-8789 04/06/23 10:43:06.575
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-8789 04/06/23 10:43:06.595
    I0406 10:43:06.601843      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8789, replica count: 3
    I0406 10:43:09.656031      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 10:43:09.707: INFO: Creating new exec pod
    Apr  6 10:43:09.725: INFO: Waiting up to 5m0s for pod "execpod-affinitytbb9b" in namespace "services-8789" to be "running"
    Apr  6 10:43:09.736: INFO: Pod "execpod-affinitytbb9b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.725134ms
    Apr  6 10:43:11.743: INFO: Pod "execpod-affinitytbb9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017939086s
    Apr  6 10:43:11.743: INFO: Pod "execpod-affinitytbb9b" satisfied condition "running"
    Apr  6 10:43:12.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Apr  6 10:43:13.399: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Apr  6 10:43:13.399: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:43:13.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.67.83.122 80'
    Apr  6 10:43:14.121: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.67.83.122 80\nConnection to 10.67.83.122 80 port [tcp/http] succeeded!\n"
    Apr  6 10:43:14.121: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:43:14.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 31641'
    Apr  6 10:43:14.696: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.236 31641\nConnection to 10.250.2.236 31641 port [tcp/*] succeeded!\n"
    Apr  6 10:43:14.696: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:43:14.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.220 31641'
    Apr  6 10:43:15.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.220 31641\nConnection to 10.250.3.220 31641 port [tcp/*] succeeded!\n"
    Apr  6 10:43:15.438: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:43:15.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:31641/ ; done'
    Apr  6 10:43:16.189: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
    Apr  6 10:43:16.189: INFO: stdout: "\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv\naffinity-nodeport-timeout-td4jv"
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.189: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.190: INFO: Received response from host: affinity-nodeport-timeout-td4jv
    Apr  6 10:43:16.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
    Apr  6 10:43:16.827: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
    Apr  6 10:43:16.827: INFO: stdout: "affinity-nodeport-timeout-td4jv"
    Apr  6 10:43:36.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
    Apr  6 10:43:37.435: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
    Apr  6 10:43:37.435: INFO: stdout: "affinity-nodeport-timeout-td4jv"
    Apr  6 10:43:57.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
    Apr  6 10:43:58.112: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
    Apr  6 10:43:58.112: INFO: stdout: "affinity-nodeport-timeout-td4jv"
    Apr  6 10:44:18.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8789 exec execpod-affinitytbb9b -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.2.236:31641/'
    Apr  6 10:44:18.775: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.2.236:31641/\n"
    Apr  6 10:44:18.775: INFO: stdout: "affinity-nodeport-timeout-lxqtz"
    Apr  6 10:44:18.775: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8789, will wait for the garbage collector to delete the pods 04/06/23 10:44:18.791
    Apr  6 10:44:18.867: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.195227ms
    Apr  6 10:44:18.968: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.861454ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:44:21.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8789" for this suite. 04/06/23 10:44:21.633
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:21.641
Apr  6 10:44:21.642: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:44:21.643
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:21.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:21.67
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Apr  6 10:44:21.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-5869 version'
Apr  6 10:44:21.825: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Apr  6 10:44:21.825: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:39:54Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:02Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:44:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5869" for this suite. 04/06/23 10:44:21.841
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":109,"skipped":2042,"failed":0}
------------------------------
â€¢ [0.208 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:21.641
    Apr  6 10:44:21.642: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:44:21.643
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:21.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:21.67
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Apr  6 10:44:21.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-5869 version'
    Apr  6 10:44:21.825: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Apr  6 10:44:21.825: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:39:54Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:02Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:44:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5869" for this suite. 04/06/23 10:44:21.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:21.85
Apr  6 10:44:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename init-container 04/06/23 10:44:21.851
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:21.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:21.88
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 04/06/23 10:44:21.887
Apr  6 10:44:21.887: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Apr  6 10:44:26.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5619" for this suite. 04/06/23 10:44:26.011
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":110,"skipped":2060,"failed":0}
------------------------------
â€¢ [4.168 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:21.85
    Apr  6 10:44:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename init-container 04/06/23 10:44:21.851
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:21.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:21.88
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 04/06/23 10:44:21.887
    Apr  6 10:44:21.887: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Apr  6 10:44:26.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5619" for this suite. 04/06/23 10:44:26.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:26.019
Apr  6 10:44:26.019: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:44:26.021
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:26.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:26.045
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 04/06/23 10:44:26.067
Apr  6 10:44:26.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9008 api-versions'
Apr  6 10:44:26.184: INFO: stderr: ""
Apr  6 10:44:26.184: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:44:26.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9008" for this suite. 04/06/23 10:44:26.193
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":111,"skipped":2077,"failed":0}
------------------------------
â€¢ [0.180 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:26.019
    Apr  6 10:44:26.019: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:44:26.021
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:26.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:26.045
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 04/06/23 10:44:26.067
    Apr  6 10:44:26.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9008 api-versions'
    Apr  6 10:44:26.184: INFO: stderr: ""
    Apr  6 10:44:26.184: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:44:26.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9008" for this suite. 04/06/23 10:44:26.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:26.201
Apr  6 10:44:26.201: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:44:26.202
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:26.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:26.223
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 04/06/23 10:44:26.23
Apr  6 10:44:26.230: INFO: namespace kubectl-6708
Apr  6 10:44:26.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 create -f -'
Apr  6 10:44:27.344: INFO: stderr: ""
Apr  6 10:44:27.344: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/06/23 10:44:27.344
Apr  6 10:44:28.351: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:44:28.351: INFO: Found 0 / 1
Apr  6 10:44:29.352: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:44:29.352: INFO: Found 0 / 1
Apr  6 10:44:30.352: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:44:30.352: INFO: Found 1 / 1
Apr  6 10:44:30.352: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  6 10:44:30.356: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:44:30.356: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 10:44:30.356: INFO: wait on agnhost-primary startup in kubectl-6708 
Apr  6 10:44:30.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 logs agnhost-primary-kzf6n agnhost-primary'
Apr  6 10:44:30.582: INFO: stderr: ""
Apr  6 10:44:30.582: INFO: stdout: "Paused\n"
STEP: exposing RC 04/06/23 10:44:30.582
Apr  6 10:44:30.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr  6 10:44:30.774: INFO: stderr: ""
Apr  6 10:44:30.782: INFO: stdout: "service/rm2 exposed\n"
Apr  6 10:44:30.788: INFO: Service rm2 in namespace kubectl-6708 found.
STEP: exposing service 04/06/23 10:44:32.798
Apr  6 10:44:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr  6 10:44:33.106: INFO: stderr: ""
Apr  6 10:44:33.106: INFO: stdout: "service/rm3 exposed\n"
Apr  6 10:44:33.113: INFO: Service rm3 in namespace kubectl-6708 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:44:35.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6708" for this suite. 04/06/23 10:44:35.146
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":112,"skipped":2089,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.954 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:26.201
    Apr  6 10:44:26.201: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:44:26.202
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:26.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:26.223
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 04/06/23 10:44:26.23
    Apr  6 10:44:26.230: INFO: namespace kubectl-6708
    Apr  6 10:44:26.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 create -f -'
    Apr  6 10:44:27.344: INFO: stderr: ""
    Apr  6 10:44:27.344: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/06/23 10:44:27.344
    Apr  6 10:44:28.351: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:44:28.351: INFO: Found 0 / 1
    Apr  6 10:44:29.352: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:44:29.352: INFO: Found 0 / 1
    Apr  6 10:44:30.352: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:44:30.352: INFO: Found 1 / 1
    Apr  6 10:44:30.352: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr  6 10:44:30.356: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:44:30.356: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr  6 10:44:30.356: INFO: wait on agnhost-primary startup in kubectl-6708 
    Apr  6 10:44:30.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 logs agnhost-primary-kzf6n agnhost-primary'
    Apr  6 10:44:30.582: INFO: stderr: ""
    Apr  6 10:44:30.582: INFO: stdout: "Paused\n"
    STEP: exposing RC 04/06/23 10:44:30.582
    Apr  6 10:44:30.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Apr  6 10:44:30.774: INFO: stderr: ""
    Apr  6 10:44:30.782: INFO: stdout: "service/rm2 exposed\n"
    Apr  6 10:44:30.788: INFO: Service rm2 in namespace kubectl-6708 found.
    STEP: exposing service 04/06/23 10:44:32.798
    Apr  6 10:44:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-6708 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Apr  6 10:44:33.106: INFO: stderr: ""
    Apr  6 10:44:33.106: INFO: stdout: "service/rm3 exposed\n"
    Apr  6 10:44:33.113: INFO: Service rm3 in namespace kubectl-6708 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:44:35.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6708" for this suite. 04/06/23 10:44:35.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:35.157
Apr  6 10:44:35.157: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename events 04/06/23 10:44:35.158
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:35.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:35.2
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 04/06/23 10:44:35.207
STEP: get a list of Events with a label in the current namespace 04/06/23 10:44:35.226
STEP: delete a list of events 04/06/23 10:44:35.233
Apr  6 10:44:35.233: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/06/23 10:44:35.251
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Apr  6 10:44:35.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6286" for this suite. 04/06/23 10:44:35.264
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":113,"skipped":2139,"failed":0}
------------------------------
â€¢ [0.113 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:35.157
    Apr  6 10:44:35.157: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename events 04/06/23 10:44:35.158
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:35.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:35.2
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 04/06/23 10:44:35.207
    STEP: get a list of Events with a label in the current namespace 04/06/23 10:44:35.226
    STEP: delete a list of events 04/06/23 10:44:35.233
    Apr  6 10:44:35.233: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/06/23 10:44:35.251
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Apr  6 10:44:35.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6286" for this suite. 04/06/23 10:44:35.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:35.271
Apr  6 10:44:35.272: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:44:35.273
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:35.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:35.311
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 04/06/23 10:44:35.318
Apr  6 10:44:35.319: INFO: Creating e2e-svc-a-pxjr7
Apr  6 10:44:35.334: INFO: Creating e2e-svc-b-g7ch8
Apr  6 10:44:35.350: INFO: Creating e2e-svc-c-5rhjw
STEP: deleting service collection 04/06/23 10:44:35.367
Apr  6 10:44:35.404: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:44:35.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4387" for this suite. 04/06/23 10:44:35.415
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":114,"skipped":2150,"failed":0}
------------------------------
â€¢ [0.158 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:35.271
    Apr  6 10:44:35.272: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:44:35.273
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:35.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:35.311
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 04/06/23 10:44:35.318
    Apr  6 10:44:35.319: INFO: Creating e2e-svc-a-pxjr7
    Apr  6 10:44:35.334: INFO: Creating e2e-svc-b-g7ch8
    Apr  6 10:44:35.350: INFO: Creating e2e-svc-c-5rhjw
    STEP: deleting service collection 04/06/23 10:44:35.367
    Apr  6 10:44:35.404: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:44:35.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4387" for this suite. 04/06/23 10:44:35.415
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:35.43
Apr  6 10:44:35.431: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename proxy 04/06/23 10:44:35.432
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:35.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:35.457
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Apr  6 10:44:35.464: INFO: Creating pod...
Apr  6 10:44:35.480: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4734" to be "running"
Apr  6 10:44:35.485: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438174ms
Apr  6 10:44:37.496: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016244835s
Apr  6 10:44:37.496: INFO: Pod "agnhost" satisfied condition "running"
Apr  6 10:44:37.496: INFO: Creating service...
Apr  6 10:44:37.513: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=DELETE
Apr  6 10:44:37.642: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr  6 10:44:37.642: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=OPTIONS
Apr  6 10:44:37.685: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr  6 10:44:37.685: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=PATCH
Apr  6 10:44:37.694: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr  6 10:44:37.694: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=POST
Apr  6 10:44:37.704: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr  6 10:44:37.704: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=PUT
Apr  6 10:44:37.711: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr  6 10:44:37.711: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=DELETE
Apr  6 10:44:37.730: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr  6 10:44:37.730: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=OPTIONS
Apr  6 10:44:37.742: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr  6 10:44:37.744: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=PATCH
Apr  6 10:44:37.754: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr  6 10:44:37.755: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=POST
Apr  6 10:44:37.764: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr  6 10:44:37.764: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=PUT
Apr  6 10:44:37.772: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr  6 10:44:37.772: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=GET
Apr  6 10:44:37.775: INFO: http.Client request:GET StatusCode:301
Apr  6 10:44:37.776: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=GET
Apr  6 10:44:37.783: INFO: http.Client request:GET StatusCode:301
Apr  6 10:44:37.783: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=HEAD
Apr  6 10:44:37.788: INFO: http.Client request:HEAD StatusCode:301
Apr  6 10:44:37.788: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=HEAD
Apr  6 10:44:37.794: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Apr  6 10:44:37.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4734" for this suite. 04/06/23 10:44:37.811
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":115,"skipped":2159,"failed":0}
------------------------------
â€¢ [2.387 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:35.43
    Apr  6 10:44:35.431: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename proxy 04/06/23 10:44:35.432
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:35.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:35.457
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Apr  6 10:44:35.464: INFO: Creating pod...
    Apr  6 10:44:35.480: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4734" to be "running"
    Apr  6 10:44:35.485: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438174ms
    Apr  6 10:44:37.496: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016244835s
    Apr  6 10:44:37.496: INFO: Pod "agnhost" satisfied condition "running"
    Apr  6 10:44:37.496: INFO: Creating service...
    Apr  6 10:44:37.513: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=DELETE
    Apr  6 10:44:37.642: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr  6 10:44:37.642: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=OPTIONS
    Apr  6 10:44:37.685: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr  6 10:44:37.685: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=PATCH
    Apr  6 10:44:37.694: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr  6 10:44:37.694: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=POST
    Apr  6 10:44:37.704: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr  6 10:44:37.704: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=PUT
    Apr  6 10:44:37.711: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr  6 10:44:37.711: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=DELETE
    Apr  6 10:44:37.730: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr  6 10:44:37.730: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Apr  6 10:44:37.742: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr  6 10:44:37.744: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=PATCH
    Apr  6 10:44:37.754: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr  6 10:44:37.755: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=POST
    Apr  6 10:44:37.764: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr  6 10:44:37.764: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=PUT
    Apr  6 10:44:37.772: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr  6 10:44:37.772: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=GET
    Apr  6 10:44:37.775: INFO: http.Client request:GET StatusCode:301
    Apr  6 10:44:37.776: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=GET
    Apr  6 10:44:37.783: INFO: http.Client request:GET StatusCode:301
    Apr  6 10:44:37.783: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/pods/agnhost/proxy?method=HEAD
    Apr  6 10:44:37.788: INFO: http.Client request:HEAD StatusCode:301
    Apr  6 10:44:37.788: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-4734/services/e2e-proxy-test-service/proxy?method=HEAD
    Apr  6 10:44:37.794: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Apr  6 10:44:37.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4734" for this suite. 04/06/23 10:44:37.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:37.817
Apr  6 10:44:37.818: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:44:37.819
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:37.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:37.85
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-1296e451-1fce-4c91-8310-29231cd90ae6 04/06/23 10:44:37.857
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:44:37.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8922" for this suite. 04/06/23 10:44:37.869
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":116,"skipped":2167,"failed":0}
------------------------------
â€¢ [0.065 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:37.817
    Apr  6 10:44:37.818: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:44:37.819
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:37.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:37.85
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-1296e451-1fce-4c91-8310-29231cd90ae6 04/06/23 10:44:37.857
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:44:37.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8922" for this suite. 04/06/23 10:44:37.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:37.883
Apr  6 10:44:37.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:44:37.889
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:37.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:37.92
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Apr  6 10:44:37.929: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:44:38.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6581" for this suite. 04/06/23 10:44:38.485
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":117,"skipped":2177,"failed":0}
------------------------------
â€¢ [0.608 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:37.883
    Apr  6 10:44:37.883: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:44:37.889
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:37.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:37.92
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Apr  6 10:44:37.929: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:44:38.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6581" for this suite. 04/06/23 10:44:38.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:38.491
Apr  6 10:44:38.491: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:44:38.492
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:38.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:38.524
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:44:38.53
Apr  6 10:44:38.542: INFO: Waiting up to 5m0s for pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b" in namespace "projected-2343" to be "Succeeded or Failed"
Apr  6 10:44:38.554: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.001824ms
Apr  6 10:44:40.568: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025400508s
Apr  6 10:44:42.563: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020317515s
STEP: Saw pod success 04/06/23 10:44:42.563
Apr  6 10:44:42.563: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b" satisfied condition "Succeeded or Failed"
Apr  6 10:44:42.567: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b container client-container: <nil>
STEP: delete the pod 04/06/23 10:44:42.597
Apr  6 10:44:42.607: INFO: Waiting for pod downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b to disappear
Apr  6 10:44:42.611: INFO: Pod downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 10:44:42.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2343" for this suite. 04/06/23 10:44:42.626
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":118,"skipped":2182,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:38.491
    Apr  6 10:44:38.491: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:44:38.492
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:38.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:38.524
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:44:38.53
    Apr  6 10:44:38.542: INFO: Waiting up to 5m0s for pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b" in namespace "projected-2343" to be "Succeeded or Failed"
    Apr  6 10:44:38.554: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.001824ms
    Apr  6 10:44:40.568: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025400508s
    Apr  6 10:44:42.563: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020317515s
    STEP: Saw pod success 04/06/23 10:44:42.563
    Apr  6 10:44:42.563: INFO: Pod "downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b" satisfied condition "Succeeded or Failed"
    Apr  6 10:44:42.567: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b container client-container: <nil>
    STEP: delete the pod 04/06/23 10:44:42.597
    Apr  6 10:44:42.607: INFO: Waiting for pod downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b to disappear
    Apr  6 10:44:42.611: INFO: Pod downwardapi-volume-687cfae2-eb6c-4ca6-ba82-9e1a2f8c4d3b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 10:44:42.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2343" for this suite. 04/06/23 10:44:42.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:44:42.644
Apr  6 10:44:42.645: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-preemption 04/06/23 10:44:42.646
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:42.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:42.682
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Apr  6 10:44:42.706: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 10:45:42.821: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:45:42.826
Apr  6 10:45:42.826: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-preemption-path 04/06/23 10:45:42.828
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:45:42.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:45:42.859
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 04/06/23 10:45:42.866
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/06/23 10:45:42.866
Apr  6 10:45:42.880: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7434" to be "running"
Apr  6 10:45:42.885: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.225375ms
Apr  6 10:45:44.893: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013259183s
Apr  6 10:45:44.893: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/06/23 10:45:44.905
Apr  6 10:45:44.912: INFO: found a healthy node: shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Apr  6 10:46:01.037: INFO: pods created so far: [1 1 1]
Apr  6 10:46:01.037: INFO: length of pods created so far: 3
Apr  6 10:46:05.052: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Apr  6 10:46:12.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7434" for this suite. 04/06/23 10:46:12.064
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:46:12.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2030" for this suite. 04/06/23 10:46:12.13
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":119,"skipped":2220,"failed":0}
------------------------------
â€¢ [SLOW TEST] [89.631 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:44:42.644
    Apr  6 10:44:42.645: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-preemption 04/06/23 10:44:42.646
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:44:42.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:44:42.682
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Apr  6 10:44:42.706: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr  6 10:45:42.821: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:45:42.826
    Apr  6 10:45:42.826: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-preemption-path 04/06/23 10:45:42.828
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:45:42.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:45:42.859
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 04/06/23 10:45:42.866
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/06/23 10:45:42.866
    Apr  6 10:45:42.880: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7434" to be "running"
    Apr  6 10:45:42.885: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.225375ms
    Apr  6 10:45:44.893: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013259183s
    Apr  6 10:45:44.893: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/06/23 10:45:44.905
    Apr  6 10:45:44.912: INFO: found a healthy node: shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Apr  6 10:46:01.037: INFO: pods created so far: [1 1 1]
    Apr  6 10:46:01.037: INFO: length of pods created so far: 3
    Apr  6 10:46:05.052: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Apr  6 10:46:12.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7434" for this suite. 04/06/23 10:46:12.064
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:46:12.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2030" for this suite. 04/06/23 10:46:12.13
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:12.276
Apr  6 10:46:12.276: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:46:12.277
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:12.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:12.309
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:46:12.322
Apr  6 10:46:12.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2" in namespace "downward-api-6844" to be "Succeeded or Failed"
Apr  6 10:46:12.349: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124794ms
Apr  6 10:46:14.358: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016130414s
Apr  6 10:46:16.356: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014911223s
STEP: Saw pod success 04/06/23 10:46:16.357
Apr  6 10:46:16.357: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2" satisfied condition "Succeeded or Failed"
Apr  6 10:46:16.362: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2 container client-container: <nil>
STEP: delete the pod 04/06/23 10:46:16.374
Apr  6 10:46:16.382: INFO: Waiting for pod downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2 to disappear
Apr  6 10:46:16.388: INFO: Pod downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 10:46:16.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6844" for this suite. 04/06/23 10:46:16.4
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":120,"skipped":2231,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:12.276
    Apr  6 10:46:12.276: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:46:12.277
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:12.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:12.309
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:46:12.322
    Apr  6 10:46:12.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2" in namespace "downward-api-6844" to be "Succeeded or Failed"
    Apr  6 10:46:12.349: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124794ms
    Apr  6 10:46:14.358: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016130414s
    Apr  6 10:46:16.356: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014911223s
    STEP: Saw pod success 04/06/23 10:46:16.357
    Apr  6 10:46:16.357: INFO: Pod "downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2" satisfied condition "Succeeded or Failed"
    Apr  6 10:46:16.362: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:46:16.374
    Apr  6 10:46:16.382: INFO: Waiting for pod downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2 to disappear
    Apr  6 10:46:16.388: INFO: Pod downwardapi-volume-b583e50f-54f5-493b-aa84-8155247406f2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 10:46:16.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6844" for this suite. 04/06/23 10:46:16.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:16.41
Apr  6 10:46:16.410: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename runtimeclass 04/06/23 10:46:16.412
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:16.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:16.451
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Apr  6 10:46:16.479: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2166 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Apr  6 10:46:16.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2166" for this suite. 04/06/23 10:46:16.514
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":121,"skipped":2245,"failed":0}
------------------------------
â€¢ [0.116 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:16.41
    Apr  6 10:46:16.410: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename runtimeclass 04/06/23 10:46:16.412
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:16.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:16.451
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Apr  6 10:46:16.479: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2166 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Apr  6 10:46:16.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2166" for this suite. 04/06/23 10:46:16.514
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:16.526
Apr  6 10:46:16.527: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 10:46:16.528
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:16.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:16.549
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 04/06/23 10:46:16.559
Apr  6 10:46:16.572: INFO: Waiting up to 5m0s for pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3" in namespace "pods-4029" to be "running and ready"
Apr  6 10:46:16.577: INFO: Pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01312ms
Apr  6 10:46:16.585: INFO: The phase of Pod pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:46:18.602: INFO: Pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3": Phase="Running", Reason="", readiness=true. Elapsed: 2.030372218s
Apr  6 10:46:18.603: INFO: The phase of Pod pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3 is Running (Ready = true)
Apr  6 10:46:18.603: INFO: Pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3" satisfied condition "running and ready"
Apr  6 10:46:18.615: INFO: Pod pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3 has hostIP: 10.250.0.68
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 10:46:18.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4029" for this suite. 04/06/23 10:46:18.624
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":122,"skipped":2246,"failed":0}
------------------------------
â€¢ [2.113 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:16.526
    Apr  6 10:46:16.527: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 10:46:16.528
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:16.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:16.549
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 04/06/23 10:46:16.559
    Apr  6 10:46:16.572: INFO: Waiting up to 5m0s for pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3" in namespace "pods-4029" to be "running and ready"
    Apr  6 10:46:16.577: INFO: Pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01312ms
    Apr  6 10:46:16.585: INFO: The phase of Pod pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:46:18.602: INFO: Pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3": Phase="Running", Reason="", readiness=true. Elapsed: 2.030372218s
    Apr  6 10:46:18.603: INFO: The phase of Pod pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3 is Running (Ready = true)
    Apr  6 10:46:18.603: INFO: Pod "pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3" satisfied condition "running and ready"
    Apr  6 10:46:18.615: INFO: Pod pod-hostip-8d961e80-6a19-44d6-9ce0-aa104c720bf3 has hostIP: 10.250.0.68
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 10:46:18.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4029" for this suite. 04/06/23 10:46:18.624
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:18.64
Apr  6 10:46:18.640: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:46:18.641
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:18.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:18.666
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Apr  6 10:46:18.681: INFO: Got root ca configmap in namespace "svcaccounts-2552"
Apr  6 10:46:18.690: INFO: Deleted root ca configmap in namespace "svcaccounts-2552"
STEP: waiting for a new root ca configmap created 04/06/23 10:46:19.191
Apr  6 10:46:19.195: INFO: Recreated root ca configmap in namespace "svcaccounts-2552"
Apr  6 10:46:19.199: INFO: Updated root ca configmap in namespace "svcaccounts-2552"
STEP: waiting for the root ca configmap reconciled 04/06/23 10:46:19.7
Apr  6 10:46:19.705: INFO: Reconciled root ca configmap in namespace "svcaccounts-2552"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Apr  6 10:46:19.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2552" for this suite. 04/06/23 10:46:19.72
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":123,"skipped":2247,"failed":0}
------------------------------
â€¢ [1.090 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:18.64
    Apr  6 10:46:18.640: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename svcaccounts 04/06/23 10:46:18.641
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:18.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:18.666
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Apr  6 10:46:18.681: INFO: Got root ca configmap in namespace "svcaccounts-2552"
    Apr  6 10:46:18.690: INFO: Deleted root ca configmap in namespace "svcaccounts-2552"
    STEP: waiting for a new root ca configmap created 04/06/23 10:46:19.191
    Apr  6 10:46:19.195: INFO: Recreated root ca configmap in namespace "svcaccounts-2552"
    Apr  6 10:46:19.199: INFO: Updated root ca configmap in namespace "svcaccounts-2552"
    STEP: waiting for the root ca configmap reconciled 04/06/23 10:46:19.7
    Apr  6 10:46:19.705: INFO: Reconciled root ca configmap in namespace "svcaccounts-2552"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Apr  6 10:46:19.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2552" for this suite. 04/06/23 10:46:19.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:19.731
Apr  6 10:46:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:46:19.734
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:19.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:19.768
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:46:19.775
Apr  6 10:46:19.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195" in namespace "projected-8523" to be "Succeeded or Failed"
Apr  6 10:46:19.804: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539724ms
Apr  6 10:46:21.825: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024775822s
Apr  6 10:46:23.812: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012001389s
STEP: Saw pod success 04/06/23 10:46:23.812
Apr  6 10:46:23.812: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195" satisfied condition "Succeeded or Failed"
Apr  6 10:46:23.820: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195 container client-container: <nil>
STEP: delete the pod 04/06/23 10:46:23.838
Apr  6 10:46:23.852: INFO: Waiting for pod downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195 to disappear
Apr  6 10:46:23.856: INFO: Pod downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 10:46:23.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8523" for this suite. 04/06/23 10:46:23.869
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":124,"skipped":2255,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:19.731
    Apr  6 10:46:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:46:19.734
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:19.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:19.768
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:46:19.775
    Apr  6 10:46:19.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195" in namespace "projected-8523" to be "Succeeded or Failed"
    Apr  6 10:46:19.804: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539724ms
    Apr  6 10:46:21.825: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024775822s
    Apr  6 10:46:23.812: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012001389s
    STEP: Saw pod success 04/06/23 10:46:23.812
    Apr  6 10:46:23.812: INFO: Pod "downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195" satisfied condition "Succeeded or Failed"
    Apr  6 10:46:23.820: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:46:23.838
    Apr  6 10:46:23.852: INFO: Waiting for pod downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195 to disappear
    Apr  6 10:46:23.856: INFO: Pod downwardapi-volume-a97aa45b-afe0-41a3-9ee9-74f015a81195 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 10:46:23.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8523" for this suite. 04/06/23 10:46:23.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:23.877
Apr  6 10:46:23.877: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-runtime 04/06/23 10:46:23.878
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:23.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:23.905
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 04/06/23 10:46:23.915
STEP: wait for the container to reach Succeeded 04/06/23 10:46:23.93
STEP: get the container status 04/06/23 10:46:27.974
STEP: the container should be terminated 04/06/23 10:46:27.979
STEP: the termination message should be set 04/06/23 10:46:27.979
Apr  6 10:46:27.979: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 04/06/23 10:46:27.979
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Apr  6 10:46:28.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-255" for this suite. 04/06/23 10:46:28.017
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":125,"skipped":2262,"failed":0}
------------------------------
â€¢ [4.147 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:23.877
    Apr  6 10:46:23.877: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-runtime 04/06/23 10:46:23.878
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:23.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:23.905
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 04/06/23 10:46:23.915
    STEP: wait for the container to reach Succeeded 04/06/23 10:46:23.93
    STEP: get the container status 04/06/23 10:46:27.974
    STEP: the container should be terminated 04/06/23 10:46:27.979
    STEP: the termination message should be set 04/06/23 10:46:27.979
    Apr  6 10:46:27.979: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 04/06/23 10:46:27.979
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Apr  6 10:46:28.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-255" for this suite. 04/06/23 10:46:28.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:28.025
Apr  6 10:46:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename runtimeclass 04/06/23 10:46:28.026
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:28.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:28.057
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Apr  6 10:46:28.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-802" for this suite. 04/06/23 10:46:28.089
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":126,"skipped":2294,"failed":0}
------------------------------
â€¢ [0.074 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:28.025
    Apr  6 10:46:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename runtimeclass 04/06/23 10:46:28.026
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:28.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:28.057
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Apr  6 10:46:28.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-802" for this suite. 04/06/23 10:46:28.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:46:28.1
Apr  6 10:46:28.100: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename job 04/06/23 10:46:28.102
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:28.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:28.129
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 04/06/23 10:46:28.136
STEP: Ensuring active pods == parallelism 04/06/23 10:46:28.144
STEP: delete a job 04/06/23 10:46:30.152
STEP: deleting Job.batch foo in namespace job-9752, will wait for the garbage collector to delete the pods 04/06/23 10:46:30.153
Apr  6 10:46:30.219: INFO: Deleting Job.batch foo took: 8.079456ms
Apr  6 10:46:30.319: INFO: Terminating Job.batch foo pods took: 100.683745ms
STEP: Ensuring job was deleted 04/06/23 10:47:02.72
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Apr  6 10:47:02.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9752" for this suite. 04/06/23 10:47:02.766
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":127,"skipped":2302,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.677 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:46:28.1
    Apr  6 10:46:28.100: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename job 04/06/23 10:46:28.102
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:46:28.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:46:28.129
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 04/06/23 10:46:28.136
    STEP: Ensuring active pods == parallelism 04/06/23 10:46:28.144
    STEP: delete a job 04/06/23 10:46:30.152
    STEP: deleting Job.batch foo in namespace job-9752, will wait for the garbage collector to delete the pods 04/06/23 10:46:30.153
    Apr  6 10:46:30.219: INFO: Deleting Job.batch foo took: 8.079456ms
    Apr  6 10:46:30.319: INFO: Terminating Job.batch foo pods took: 100.683745ms
    STEP: Ensuring job was deleted 04/06/23 10:47:02.72
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Apr  6 10:47:02.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9752" for this suite. 04/06/23 10:47:02.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:02.781
Apr  6 10:47:02.786: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 10:47:02.819
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:02.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:02.856
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-940753e7-505e-4c5a-bde2-fbe8f08a52bb 04/06/23 10:47:02.864
STEP: Creating a pod to test consume secrets 04/06/23 10:47:02.871
Apr  6 10:47:02.889: INFO: Waiting up to 5m0s for pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779" in namespace "secrets-1650" to be "Succeeded or Failed"
Apr  6 10:47:02.899: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779": Phase="Pending", Reason="", readiness=false. Elapsed: 9.699254ms
Apr  6 10:47:04.912: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023539886s
Apr  6 10:47:06.906: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017255226s
STEP: Saw pod success 04/06/23 10:47:06.906
Apr  6 10:47:06.906: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779" satisfied condition "Succeeded or Failed"
Apr  6 10:47:06.911: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779 container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 10:47:06.969
Apr  6 10:47:06.979: INFO: Waiting for pod pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779 to disappear
Apr  6 10:47:06.983: INFO: Pod pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 10:47:06.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1650" for this suite. 04/06/23 10:47:06.994
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":128,"skipped":2332,"failed":0}
------------------------------
â€¢ [4.227 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:02.781
    Apr  6 10:47:02.786: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 10:47:02.819
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:02.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:02.856
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-940753e7-505e-4c5a-bde2-fbe8f08a52bb 04/06/23 10:47:02.864
    STEP: Creating a pod to test consume secrets 04/06/23 10:47:02.871
    Apr  6 10:47:02.889: INFO: Waiting up to 5m0s for pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779" in namespace "secrets-1650" to be "Succeeded or Failed"
    Apr  6 10:47:02.899: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779": Phase="Pending", Reason="", readiness=false. Elapsed: 9.699254ms
    Apr  6 10:47:04.912: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023539886s
    Apr  6 10:47:06.906: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017255226s
    STEP: Saw pod success 04/06/23 10:47:06.906
    Apr  6 10:47:06.906: INFO: Pod "pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779" satisfied condition "Succeeded or Failed"
    Apr  6 10:47:06.911: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779 container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:47:06.969
    Apr  6 10:47:06.979: INFO: Waiting for pod pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779 to disappear
    Apr  6 10:47:06.983: INFO: Pod pod-secrets-8c1b24b9-f228-4e41-ac1d-e470075e9779 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 10:47:06.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1650" for this suite. 04/06/23 10:47:06.994
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:07.008
Apr  6 10:47:07.008: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename lease-test 04/06/23 10:47:07.009
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:07.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:07.044
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Apr  6 10:47:07.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2483" for this suite. 04/06/23 10:47:07.134
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":129,"skipped":2332,"failed":0}
------------------------------
â€¢ [0.132 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:07.008
    Apr  6 10:47:07.008: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename lease-test 04/06/23 10:47:07.009
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:07.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:07.044
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Apr  6 10:47:07.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-2483" for this suite. 04/06/23 10:47:07.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:07.146
Apr  6 10:47:07.146: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename ingress 04/06/23 10:47:07.147
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:07.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:07.165
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 04/06/23 10:47:07.17
STEP: getting /apis/networking.k8s.io 04/06/23 10:47:07.176
STEP: getting /apis/networking.k8s.iov1 04/06/23 10:47:07.179
STEP: creating 04/06/23 10:47:07.181
STEP: getting 04/06/23 10:47:07.199
STEP: listing 04/06/23 10:47:07.203
STEP: watching 04/06/23 10:47:07.208
Apr  6 10:47:07.208: INFO: starting watch
STEP: cluster-wide listing 04/06/23 10:47:07.211
STEP: cluster-wide watching 04/06/23 10:47:07.215
Apr  6 10:47:07.215: INFO: starting watch
STEP: patching 04/06/23 10:47:07.218
STEP: updating 04/06/23 10:47:07.225
Apr  6 10:47:07.239: INFO: waiting for watch events with expected annotations
Apr  6 10:47:07.239: INFO: saw patched and updated annotations
STEP: patching /status 04/06/23 10:47:07.239
STEP: updating /status 04/06/23 10:47:07.244
STEP: get /status 04/06/23 10:47:07.255
STEP: deleting 04/06/23 10:47:07.26
STEP: deleting a collection 04/06/23 10:47:07.274
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Apr  6 10:47:07.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9860" for this suite. 04/06/23 10:47:07.299
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":130,"skipped":2369,"failed":0}
------------------------------
â€¢ [0.161 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:07.146
    Apr  6 10:47:07.146: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename ingress 04/06/23 10:47:07.147
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:07.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:07.165
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 04/06/23 10:47:07.17
    STEP: getting /apis/networking.k8s.io 04/06/23 10:47:07.176
    STEP: getting /apis/networking.k8s.iov1 04/06/23 10:47:07.179
    STEP: creating 04/06/23 10:47:07.181
    STEP: getting 04/06/23 10:47:07.199
    STEP: listing 04/06/23 10:47:07.203
    STEP: watching 04/06/23 10:47:07.208
    Apr  6 10:47:07.208: INFO: starting watch
    STEP: cluster-wide listing 04/06/23 10:47:07.211
    STEP: cluster-wide watching 04/06/23 10:47:07.215
    Apr  6 10:47:07.215: INFO: starting watch
    STEP: patching 04/06/23 10:47:07.218
    STEP: updating 04/06/23 10:47:07.225
    Apr  6 10:47:07.239: INFO: waiting for watch events with expected annotations
    Apr  6 10:47:07.239: INFO: saw patched and updated annotations
    STEP: patching /status 04/06/23 10:47:07.239
    STEP: updating /status 04/06/23 10:47:07.244
    STEP: get /status 04/06/23 10:47:07.255
    STEP: deleting 04/06/23 10:47:07.26
    STEP: deleting a collection 04/06/23 10:47:07.274
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Apr  6 10:47:07.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-9860" for this suite. 04/06/23 10:47:07.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:07.32
Apr  6 10:47:07.320: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:47:07.321
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:07.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:07.345
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 04/06/23 10:47:07.36
Apr  6 10:47:07.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7577 create -f -'
Apr  6 10:47:08.540: INFO: stderr: ""
Apr  6 10:47:08.540: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/06/23 10:47:08.54
Apr  6 10:47:09.548: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:47:09.548: INFO: Found 0 / 1
Apr  6 10:47:10.550: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:47:10.550: INFO: Found 1 / 1
Apr  6 10:47:10.550: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 04/06/23 10:47:10.55
Apr  6 10:47:10.554: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:47:10.554: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 10:47:10.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7577 patch pod agnhost-primary-kh787 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr  6 10:47:10.711: INFO: stderr: ""
Apr  6 10:47:10.711: INFO: stdout: "pod/agnhost-primary-kh787 patched\n"
STEP: checking annotations 04/06/23 10:47:10.711
Apr  6 10:47:10.717: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:47:10.717: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:47:10.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7577" for this suite. 04/06/23 10:47:10.748
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":131,"skipped":2417,"failed":0}
------------------------------
â€¢ [3.437 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:07.32
    Apr  6 10:47:07.320: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:47:07.321
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:07.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:07.345
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 04/06/23 10:47:07.36
    Apr  6 10:47:07.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7577 create -f -'
    Apr  6 10:47:08.540: INFO: stderr: ""
    Apr  6 10:47:08.540: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/06/23 10:47:08.54
    Apr  6 10:47:09.548: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:47:09.548: INFO: Found 0 / 1
    Apr  6 10:47:10.550: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:47:10.550: INFO: Found 1 / 1
    Apr  6 10:47:10.550: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 04/06/23 10:47:10.55
    Apr  6 10:47:10.554: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:47:10.554: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr  6 10:47:10.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-7577 patch pod agnhost-primary-kh787 -p {"metadata":{"annotations":{"x":"y"}}}'
    Apr  6 10:47:10.711: INFO: stderr: ""
    Apr  6 10:47:10.711: INFO: stdout: "pod/agnhost-primary-kh787 patched\n"
    STEP: checking annotations 04/06/23 10:47:10.711
    Apr  6 10:47:10.717: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:47:10.717: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:47:10.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7577" for this suite. 04/06/23 10:47:10.748
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:10.757
Apr  6 10:47:10.757: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:47:10.758
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:10.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:10.782
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 04/06/23 10:47:10.789
Apr  6 10:47:10.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 create -f -'
Apr  6 10:47:11.075: INFO: stderr: ""
Apr  6 10:47:11.075: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 10:47:11.075
Apr  6 10:47:11.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 10:47:11.293: INFO: stderr: ""
Apr  6 10:47:11.293: INFO: stdout: "update-demo-nautilus-lwprz update-demo-nautilus-s42kr "
Apr  6 10:47:11.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-lwprz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 10:47:11.466: INFO: stderr: ""
Apr  6 10:47:11.466: INFO: stdout: ""
Apr  6 10:47:11.466: INFO: update-demo-nautilus-lwprz is created but not running
Apr  6 10:47:16.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 10:47:16.612: INFO: stderr: ""
Apr  6 10:47:16.612: INFO: stdout: "update-demo-nautilus-lwprz update-demo-nautilus-s42kr "
Apr  6 10:47:16.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-lwprz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 10:47:16.732: INFO: stderr: ""
Apr  6 10:47:16.732: INFO: stdout: "true"
Apr  6 10:47:16.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-lwprz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 10:47:16.861: INFO: stderr: ""
Apr  6 10:47:16.863: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Apr  6 10:47:16.864: INFO: validating pod update-demo-nautilus-lwprz
Apr  6 10:47:16.991: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 10:47:16.991: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 10:47:16.992: INFO: update-demo-nautilus-lwprz is verified up and running
Apr  6 10:47:16.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 10:47:17.131: INFO: stderr: ""
Apr  6 10:47:17.131: INFO: stdout: "true"
Apr  6 10:47:17.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 10:47:17.241: INFO: stderr: ""
Apr  6 10:47:17.241: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Apr  6 10:47:17.241: INFO: validating pod update-demo-nautilus-s42kr
Apr  6 10:47:17.352: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 10:47:17.352: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 10:47:17.352: INFO: update-demo-nautilus-s42kr is verified up and running
STEP: scaling down the replication controller 04/06/23 10:47:17.352
Apr  6 10:47:17.354: INFO: scanned /root for discovery docs: <nil>
Apr  6 10:47:17.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr  6 10:47:18.549: INFO: stderr: ""
Apr  6 10:47:18.549: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 10:47:18.549
Apr  6 10:47:18.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 10:47:18.712: INFO: stderr: ""
Apr  6 10:47:18.713: INFO: stdout: "update-demo-nautilus-lwprz update-demo-nautilus-s42kr "
STEP: Replicas for name=update-demo: expected=1 actual=2 04/06/23 10:47:18.713
Apr  6 10:47:23.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 10:47:23.904: INFO: stderr: ""
Apr  6 10:47:23.904: INFO: stdout: "update-demo-nautilus-s42kr "
Apr  6 10:47:23.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 10:47:24.063: INFO: stderr: ""
Apr  6 10:47:24.063: INFO: stdout: "true"
Apr  6 10:47:24.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 10:47:24.210: INFO: stderr: ""
Apr  6 10:47:24.210: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Apr  6 10:47:24.210: INFO: validating pod update-demo-nautilus-s42kr
Apr  6 10:47:24.229: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 10:47:24.229: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 10:47:24.229: INFO: update-demo-nautilus-s42kr is verified up and running
STEP: scaling up the replication controller 04/06/23 10:47:24.229
Apr  6 10:47:24.235: INFO: scanned /root for discovery docs: <nil>
Apr  6 10:47:24.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr  6 10:47:25.522: INFO: stderr: ""
Apr  6 10:47:25.522: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 10:47:25.522
Apr  6 10:47:25.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 10:47:25.692: INFO: stderr: ""
Apr  6 10:47:25.692: INFO: stdout: "update-demo-nautilus-bfsc6 update-demo-nautilus-s42kr "
Apr  6 10:47:25.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-bfsc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 10:47:25.930: INFO: stderr: ""
Apr  6 10:47:25.930: INFO: stdout: "true"
Apr  6 10:47:25.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-bfsc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 10:47:26.124: INFO: stderr: ""
Apr  6 10:47:26.124: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Apr  6 10:47:26.124: INFO: validating pod update-demo-nautilus-bfsc6
Apr  6 10:47:26.230: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 10:47:26.230: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 10:47:26.230: INFO: update-demo-nautilus-bfsc6 is verified up and running
Apr  6 10:47:26.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 10:47:26.393: INFO: stderr: ""
Apr  6 10:47:26.393: INFO: stdout: "true"
Apr  6 10:47:26.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 10:47:26.547: INFO: stderr: ""
Apr  6 10:47:26.547: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Apr  6 10:47:26.547: INFO: validating pod update-demo-nautilus-s42kr
Apr  6 10:47:26.558: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 10:47:26.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 10:47:26.558: INFO: update-demo-nautilus-s42kr is verified up and running
STEP: using delete to clean up resources 04/06/23 10:47:26.558
Apr  6 10:47:26.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 delete --grace-period=0 --force -f -'
Apr  6 10:47:26.798: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:47:26.798: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  6 10:47:26.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get rc,svc -l name=update-demo --no-headers'
Apr  6 10:47:26.974: INFO: stderr: "No resources found in kubectl-478 namespace.\n"
Apr  6 10:47:26.974: INFO: stdout: ""
Apr  6 10:47:26.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 10:47:27.135: INFO: stderr: ""
Apr  6 10:47:27.135: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:47:27.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-478" for this suite. 04/06/23 10:47:27.147
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":132,"skipped":2419,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.399 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:10.757
    Apr  6 10:47:10.757: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:47:10.758
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:10.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:10.782
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 04/06/23 10:47:10.789
    Apr  6 10:47:10.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 create -f -'
    Apr  6 10:47:11.075: INFO: stderr: ""
    Apr  6 10:47:11.075: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 10:47:11.075
    Apr  6 10:47:11.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr  6 10:47:11.293: INFO: stderr: ""
    Apr  6 10:47:11.293: INFO: stdout: "update-demo-nautilus-lwprz update-demo-nautilus-s42kr "
    Apr  6 10:47:11.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-lwprz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 10:47:11.466: INFO: stderr: ""
    Apr  6 10:47:11.466: INFO: stdout: ""
    Apr  6 10:47:11.466: INFO: update-demo-nautilus-lwprz is created but not running
    Apr  6 10:47:16.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr  6 10:47:16.612: INFO: stderr: ""
    Apr  6 10:47:16.612: INFO: stdout: "update-demo-nautilus-lwprz update-demo-nautilus-s42kr "
    Apr  6 10:47:16.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-lwprz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 10:47:16.732: INFO: stderr: ""
    Apr  6 10:47:16.732: INFO: stdout: "true"
    Apr  6 10:47:16.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-lwprz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr  6 10:47:16.861: INFO: stderr: ""
    Apr  6 10:47:16.863: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Apr  6 10:47:16.864: INFO: validating pod update-demo-nautilus-lwprz
    Apr  6 10:47:16.991: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr  6 10:47:16.991: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr  6 10:47:16.992: INFO: update-demo-nautilus-lwprz is verified up and running
    Apr  6 10:47:16.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 10:47:17.131: INFO: stderr: ""
    Apr  6 10:47:17.131: INFO: stdout: "true"
    Apr  6 10:47:17.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr  6 10:47:17.241: INFO: stderr: ""
    Apr  6 10:47:17.241: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Apr  6 10:47:17.241: INFO: validating pod update-demo-nautilus-s42kr
    Apr  6 10:47:17.352: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr  6 10:47:17.352: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr  6 10:47:17.352: INFO: update-demo-nautilus-s42kr is verified up and running
    STEP: scaling down the replication controller 04/06/23 10:47:17.352
    Apr  6 10:47:17.354: INFO: scanned /root for discovery docs: <nil>
    Apr  6 10:47:17.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Apr  6 10:47:18.549: INFO: stderr: ""
    Apr  6 10:47:18.549: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 10:47:18.549
    Apr  6 10:47:18.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr  6 10:47:18.712: INFO: stderr: ""
    Apr  6 10:47:18.713: INFO: stdout: "update-demo-nautilus-lwprz update-demo-nautilus-s42kr "
    STEP: Replicas for name=update-demo: expected=1 actual=2 04/06/23 10:47:18.713
    Apr  6 10:47:23.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr  6 10:47:23.904: INFO: stderr: ""
    Apr  6 10:47:23.904: INFO: stdout: "update-demo-nautilus-s42kr "
    Apr  6 10:47:23.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 10:47:24.063: INFO: stderr: ""
    Apr  6 10:47:24.063: INFO: stdout: "true"
    Apr  6 10:47:24.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr  6 10:47:24.210: INFO: stderr: ""
    Apr  6 10:47:24.210: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Apr  6 10:47:24.210: INFO: validating pod update-demo-nautilus-s42kr
    Apr  6 10:47:24.229: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr  6 10:47:24.229: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr  6 10:47:24.229: INFO: update-demo-nautilus-s42kr is verified up and running
    STEP: scaling up the replication controller 04/06/23 10:47:24.229
    Apr  6 10:47:24.235: INFO: scanned /root for discovery docs: <nil>
    Apr  6 10:47:24.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Apr  6 10:47:25.522: INFO: stderr: ""
    Apr  6 10:47:25.522: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 10:47:25.522
    Apr  6 10:47:25.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr  6 10:47:25.692: INFO: stderr: ""
    Apr  6 10:47:25.692: INFO: stdout: "update-demo-nautilus-bfsc6 update-demo-nautilus-s42kr "
    Apr  6 10:47:25.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-bfsc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 10:47:25.930: INFO: stderr: ""
    Apr  6 10:47:25.930: INFO: stdout: "true"
    Apr  6 10:47:25.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-bfsc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr  6 10:47:26.124: INFO: stderr: ""
    Apr  6 10:47:26.124: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Apr  6 10:47:26.124: INFO: validating pod update-demo-nautilus-bfsc6
    Apr  6 10:47:26.230: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr  6 10:47:26.230: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr  6 10:47:26.230: INFO: update-demo-nautilus-bfsc6 is verified up and running
    Apr  6 10:47:26.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 10:47:26.393: INFO: stderr: ""
    Apr  6 10:47:26.393: INFO: stdout: "true"
    Apr  6 10:47:26.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods update-demo-nautilus-s42kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr  6 10:47:26.547: INFO: stderr: ""
    Apr  6 10:47:26.547: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Apr  6 10:47:26.547: INFO: validating pod update-demo-nautilus-s42kr
    Apr  6 10:47:26.558: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr  6 10:47:26.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr  6 10:47:26.558: INFO: update-demo-nautilus-s42kr is verified up and running
    STEP: using delete to clean up resources 04/06/23 10:47:26.558
    Apr  6 10:47:26.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 delete --grace-period=0 --force -f -'
    Apr  6 10:47:26.798: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:47:26.798: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr  6 10:47:26.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get rc,svc -l name=update-demo --no-headers'
    Apr  6 10:47:26.974: INFO: stderr: "No resources found in kubectl-478 namespace.\n"
    Apr  6 10:47:26.974: INFO: stdout: ""
    Apr  6 10:47:26.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-478 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr  6 10:47:27.135: INFO: stderr: ""
    Apr  6 10:47:27.135: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:47:27.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-478" for this suite. 04/06/23 10:47:27.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:27.163
Apr  6 10:47:27.163: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:47:27.164
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:27.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:27.199
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:47:27.22
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:47:28.038
STEP: Deploying the webhook pod 04/06/23 10:47:28.048
STEP: Wait for the deployment to be ready 04/06/23 10:47:28.06
Apr  6 10:47:28.069: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:47:30.102
STEP: Verifying the service has paired with the endpoint 04/06/23 10:47:30.112
Apr  6 10:47:31.113: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Apr  6 10:47:31.119: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2960-crds.webhook.example.com via the AdmissionRegistration API 04/06/23 10:47:31.641
STEP: Creating a custom resource that should be mutated by the webhook 04/06/23 10:47:31.775
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:47:34.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1109" for this suite. 04/06/23 10:47:34.625
STEP: Destroying namespace "webhook-1109-markers" for this suite. 04/06/23 10:47:34.632
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":133,"skipped":2451,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.535 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:27.163
    Apr  6 10:47:27.163: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:47:27.164
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:27.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:27.199
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:47:27.22
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:47:28.038
    STEP: Deploying the webhook pod 04/06/23 10:47:28.048
    STEP: Wait for the deployment to be ready 04/06/23 10:47:28.06
    Apr  6 10:47:28.069: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:47:30.102
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:47:30.112
    Apr  6 10:47:31.113: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Apr  6 10:47:31.119: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2960-crds.webhook.example.com via the AdmissionRegistration API 04/06/23 10:47:31.641
    STEP: Creating a custom resource that should be mutated by the webhook 04/06/23 10:47:31.775
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:47:34.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1109" for this suite. 04/06/23 10:47:34.625
    STEP: Destroying namespace "webhook-1109-markers" for this suite. 04/06/23 10:47:34.632
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:34.698
Apr  6 10:47:34.699: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replication-controller 04/06/23 10:47:34.703
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:34.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:34.732
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 04/06/23 10:47:34.738
Apr  6 10:47:34.748: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3579" to be "running and ready"
Apr  6 10:47:34.753: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.328051ms
Apr  6 10:47:34.753: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:47:36.761: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013533968s
Apr  6 10:47:36.761: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Apr  6 10:47:36.762: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 04/06/23 10:47:36.767
STEP: Then the orphan pod is adopted 04/06/23 10:47:36.785
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Apr  6 10:47:36.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3579" for this suite. 04/06/23 10:47:36.828
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":134,"skipped":2457,"failed":0}
------------------------------
â€¢ [2.137 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:34.698
    Apr  6 10:47:34.699: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replication-controller 04/06/23 10:47:34.703
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:34.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:34.732
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 04/06/23 10:47:34.738
    Apr  6 10:47:34.748: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3579" to be "running and ready"
    Apr  6 10:47:34.753: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.328051ms
    Apr  6 10:47:34.753: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:47:36.761: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013533968s
    Apr  6 10:47:36.761: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Apr  6 10:47:36.762: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 04/06/23 10:47:36.767
    STEP: Then the orphan pod is adopted 04/06/23 10:47:36.785
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Apr  6 10:47:36.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3579" for this suite. 04/06/23 10:47:36.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:36.84
Apr  6 10:47:36.840: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-webhook 04/06/23 10:47:36.841
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:36.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:36.873
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/06/23 10:47:36.883
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/06/23 10:47:37.495
STEP: Deploying the custom resource conversion webhook pod 04/06/23 10:47:37.512
STEP: Wait for the deployment to be ready 04/06/23 10:47:37.531
Apr  6 10:47:37.543: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:47:39.578
STEP: Verifying the service has paired with the endpoint 04/06/23 10:47:39.595
Apr  6 10:47:40.596: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Apr  6 10:47:40.603: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Creating a v1 custom resource 04/06/23 10:47:42.897
STEP: v2 custom resource should be converted 04/06/23 10:47:42.907
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:47:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9350" for this suite. 04/06/23 10:47:43.458
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":135,"skipped":2481,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.695 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:36.84
    Apr  6 10:47:36.840: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-webhook 04/06/23 10:47:36.841
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:36.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:36.873
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/06/23 10:47:36.883
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/06/23 10:47:37.495
    STEP: Deploying the custom resource conversion webhook pod 04/06/23 10:47:37.512
    STEP: Wait for the deployment to be ready 04/06/23 10:47:37.531
    Apr  6 10:47:37.543: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:47:39.578
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:47:39.595
    Apr  6 10:47:40.596: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Apr  6 10:47:40.603: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Creating a v1 custom resource 04/06/23 10:47:42.897
    STEP: v2 custom resource should be converted 04/06/23 10:47:42.907
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:47:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-9350" for this suite. 04/06/23 10:47:43.458
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:43.536
Apr  6 10:47:43.536: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:47:43.537
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:43.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:43.567
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 04/06/23 10:47:43.584
Apr  6 10:47:43.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-659 cluster-info'
Apr  6 10:47:43.767: INFO: stderr: ""
Apr  6 10:47:43.767: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:47:43.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-659" for this suite. 04/06/23 10:47:43.784
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":136,"skipped":2513,"failed":0}
------------------------------
â€¢ [0.257 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:43.536
    Apr  6 10:47:43.536: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:47:43.537
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:43.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:43.567
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 04/06/23 10:47:43.584
    Apr  6 10:47:43.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-659 cluster-info'
    Apr  6 10:47:43.767: INFO: stderr: ""
    Apr  6 10:47:43.767: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:47:43.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-659" for this suite. 04/06/23 10:47:43.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:43.794
Apr  6 10:47:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 10:47:43.795
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:43.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:43.842
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-244/configmap-test-bbb36a01-2502-4af0-bd34-510a35f92a16 04/06/23 10:47:43.85
STEP: Creating a pod to test consume configMaps 04/06/23 10:47:43.862
Apr  6 10:47:43.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15" in namespace "configmap-244" to be "Succeeded or Failed"
Apr  6 10:47:43.882: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15": Phase="Pending", Reason="", readiness=false. Elapsed: 6.277676ms
Apr  6 10:47:45.891: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014457772s
Apr  6 10:47:47.889: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013113035s
STEP: Saw pod success 04/06/23 10:47:47.889
Apr  6 10:47:47.889: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15" satisfied condition "Succeeded or Failed"
Apr  6 10:47:47.895: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15 container env-test: <nil>
STEP: delete the pod 04/06/23 10:47:47.918
Apr  6 10:47:47.927: INFO: Waiting for pod pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15 to disappear
Apr  6 10:47:47.930: INFO: Pod pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 10:47:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-244" for this suite. 04/06/23 10:47:47.939
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":137,"skipped":2526,"failed":0}
------------------------------
â€¢ [4.151 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:43.794
    Apr  6 10:47:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 10:47:43.795
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:43.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:43.842
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-244/configmap-test-bbb36a01-2502-4af0-bd34-510a35f92a16 04/06/23 10:47:43.85
    STEP: Creating a pod to test consume configMaps 04/06/23 10:47:43.862
    Apr  6 10:47:43.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15" in namespace "configmap-244" to be "Succeeded or Failed"
    Apr  6 10:47:43.882: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15": Phase="Pending", Reason="", readiness=false. Elapsed: 6.277676ms
    Apr  6 10:47:45.891: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014457772s
    Apr  6 10:47:47.889: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013113035s
    STEP: Saw pod success 04/06/23 10:47:47.889
    Apr  6 10:47:47.889: INFO: Pod "pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15" satisfied condition "Succeeded or Failed"
    Apr  6 10:47:47.895: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15 container env-test: <nil>
    STEP: delete the pod 04/06/23 10:47:47.918
    Apr  6 10:47:47.927: INFO: Waiting for pod pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15 to disappear
    Apr  6 10:47:47.930: INFO: Pod pod-configmaps-60b2a4ca-e6df-4f19-ae04-471092d62c15 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 10:47:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-244" for this suite. 04/06/23 10:47:47.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:47.946
Apr  6 10:47:47.946: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 10:47:47.948
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:47.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:47.971
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-309b4fd4-eb14-432e-a1f1-a5196f9bf280 04/06/23 10:47:47.976
STEP: Creating a pod to test consume secrets 04/06/23 10:47:47.988
Apr  6 10:47:48.002: INFO: Waiting up to 5m0s for pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e" in namespace "secrets-5176" to be "Succeeded or Failed"
Apr  6 10:47:48.014: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.897889ms
Apr  6 10:47:50.021: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e": Phase="Running", Reason="", readiness=false. Elapsed: 2.019232965s
Apr  6 10:47:52.023: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021048859s
STEP: Saw pod success 04/06/23 10:47:52.023
Apr  6 10:47:52.023: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e" satisfied condition "Succeeded or Failed"
Apr  6 10:47:52.031: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 10:47:52.09
Apr  6 10:47:52.102: INFO: Waiting for pod pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e to disappear
Apr  6 10:47:52.106: INFO: Pod pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 10:47:52.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5176" for this suite. 04/06/23 10:47:52.119
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":138,"skipped":2537,"failed":0}
------------------------------
â€¢ [4.180 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:47.946
    Apr  6 10:47:47.946: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 10:47:47.948
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:47.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:47.971
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-309b4fd4-eb14-432e-a1f1-a5196f9bf280 04/06/23 10:47:47.976
    STEP: Creating a pod to test consume secrets 04/06/23 10:47:47.988
    Apr  6 10:47:48.002: INFO: Waiting up to 5m0s for pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e" in namespace "secrets-5176" to be "Succeeded or Failed"
    Apr  6 10:47:48.014: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.897889ms
    Apr  6 10:47:50.021: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e": Phase="Running", Reason="", readiness=false. Elapsed: 2.019232965s
    Apr  6 10:47:52.023: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021048859s
    STEP: Saw pod success 04/06/23 10:47:52.023
    Apr  6 10:47:52.023: INFO: Pod "pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e" satisfied condition "Succeeded or Failed"
    Apr  6 10:47:52.031: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:47:52.09
    Apr  6 10:47:52.102: INFO: Waiting for pod pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e to disappear
    Apr  6 10:47:52.106: INFO: Pod pod-secrets-3a101cf1-3028-4db4-9acc-e2e60054ce4e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 10:47:52.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5176" for this suite. 04/06/23 10:47:52.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:47:52.127
Apr  6 10:47:52.128: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename watch 04/06/23 10:47:52.129
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:52.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:52.164
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 04/06/23 10:47:52.17
STEP: creating a watch on configmaps with label B 04/06/23 10:47:52.173
STEP: creating a watch on configmaps with label A or B 04/06/23 10:47:52.175
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/06/23 10:47:52.178
Apr  6 10:47:52.183: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17761 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 10:47:52.183: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17761 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/06/23 10:47:52.183
Apr  6 10:47:52.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17762 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 10:47:52.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17762 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/06/23 10:47:52.195
Apr  6 10:47:52.205: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17763 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 10:47:52.205: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17763 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/06/23 10:47:52.205
Apr  6 10:47:52.212: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17764 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 10:47:52.212: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17764 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/06/23 10:47:52.212
Apr  6 10:47:52.217: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17765 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 10:47:52.218: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17765 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/06/23 10:48:02.218
Apr  6 10:48:02.228: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17815 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 10:48:02.228: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17815 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Apr  6 10:48:12.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7930" for this suite. 04/06/23 10:48:12.244
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":139,"skipped":2548,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.124 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:47:52.127
    Apr  6 10:47:52.128: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename watch 04/06/23 10:47:52.129
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:47:52.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:47:52.164
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 04/06/23 10:47:52.17
    STEP: creating a watch on configmaps with label B 04/06/23 10:47:52.173
    STEP: creating a watch on configmaps with label A or B 04/06/23 10:47:52.175
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/06/23 10:47:52.178
    Apr  6 10:47:52.183: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17761 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 10:47:52.183: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17761 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/06/23 10:47:52.183
    Apr  6 10:47:52.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17762 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 10:47:52.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17762 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/06/23 10:47:52.195
    Apr  6 10:47:52.205: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17763 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 10:47:52.205: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17763 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/06/23 10:47:52.205
    Apr  6 10:47:52.212: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17764 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 10:47:52.212: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7930  a3ee3748-a3bf-403d-b8ea-58d955ce9f97 17764 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/06/23 10:47:52.212
    Apr  6 10:47:52.217: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17765 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 10:47:52.218: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17765 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/06/23 10:48:02.218
    Apr  6 10:48:02.228: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17815 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 10:48:02.228: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7930  fb58e4ac-72d5-4803-956b-69323f199a66 17815 0 2023-04-06 10:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-06 10:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Apr  6 10:48:12.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7930" for this suite. 04/06/23 10:48:12.244
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:48:12.252
Apr  6 10:48:12.252: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:48:12.254
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:12.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:12.279
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 04/06/23 10:48:12.286
Apr  6 10:48:12.304: INFO: Waiting up to 5m0s for pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e" in namespace "downward-api-2578" to be "Succeeded or Failed"
Apr  6 10:48:12.310: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.436426ms
Apr  6 10:48:14.318: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013949315s
Apr  6 10:48:16.317: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012719162s
STEP: Saw pod success 04/06/23 10:48:16.317
Apr  6 10:48:16.318: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e" satisfied condition "Succeeded or Failed"
Apr  6 10:48:16.323: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e container dapi-container: <nil>
STEP: delete the pod 04/06/23 10:48:16.335
Apr  6 10:48:16.345: INFO: Waiting for pod downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e to disappear
Apr  6 10:48:16.350: INFO: Pod downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Apr  6 10:48:16.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2578" for this suite. 04/06/23 10:48:16.36
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":140,"skipped":2551,"failed":0}
------------------------------
â€¢ [4.114 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:48:12.252
    Apr  6 10:48:12.252: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:48:12.254
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:12.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:12.279
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 04/06/23 10:48:12.286
    Apr  6 10:48:12.304: INFO: Waiting up to 5m0s for pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e" in namespace "downward-api-2578" to be "Succeeded or Failed"
    Apr  6 10:48:12.310: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.436426ms
    Apr  6 10:48:14.318: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013949315s
    Apr  6 10:48:16.317: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012719162s
    STEP: Saw pod success 04/06/23 10:48:16.317
    Apr  6 10:48:16.318: INFO: Pod "downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e" satisfied condition "Succeeded or Failed"
    Apr  6 10:48:16.323: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e container dapi-container: <nil>
    STEP: delete the pod 04/06/23 10:48:16.335
    Apr  6 10:48:16.345: INFO: Waiting for pod downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e to disappear
    Apr  6 10:48:16.350: INFO: Pod downward-api-a088d4d7-e201-4180-919b-9ddab5fc1e2e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Apr  6 10:48:16.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2578" for this suite. 04/06/23 10:48:16.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:48:16.369
Apr  6 10:48:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:48:16.371
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:16.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:16.408
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 04/06/23 10:48:16.418
Apr  6 10:48:16.446: INFO: Waiting up to 5m0s for pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f" in namespace "downward-api-1199" to be "Succeeded or Failed"
Apr  6 10:48:16.461: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.956095ms
Apr  6 10:48:18.468: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017851595s
Apr  6 10:48:20.468: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018137524s
STEP: Saw pod success 04/06/23 10:48:20.468
Apr  6 10:48:20.469: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f" satisfied condition "Succeeded or Failed"
Apr  6 10:48:20.473: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f container dapi-container: <nil>
STEP: delete the pod 04/06/23 10:48:20.488
Apr  6 10:48:20.497: INFO: Waiting for pod downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f to disappear
Apr  6 10:48:20.501: INFO: Pod downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Apr  6 10:48:20.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1199" for this suite. 04/06/23 10:48:20.513
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":141,"skipped":2578,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:48:16.369
    Apr  6 10:48:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:48:16.371
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:16.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:16.408
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 04/06/23 10:48:16.418
    Apr  6 10:48:16.446: INFO: Waiting up to 5m0s for pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f" in namespace "downward-api-1199" to be "Succeeded or Failed"
    Apr  6 10:48:16.461: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.956095ms
    Apr  6 10:48:18.468: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017851595s
    Apr  6 10:48:20.468: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018137524s
    STEP: Saw pod success 04/06/23 10:48:20.468
    Apr  6 10:48:20.469: INFO: Pod "downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f" satisfied condition "Succeeded or Failed"
    Apr  6 10:48:20.473: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f container dapi-container: <nil>
    STEP: delete the pod 04/06/23 10:48:20.488
    Apr  6 10:48:20.497: INFO: Waiting for pod downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f to disappear
    Apr  6 10:48:20.501: INFO: Pod downward-api-340905b0-debb-4dd3-8f5b-ea7228bed27f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Apr  6 10:48:20.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1199" for this suite. 04/06/23 10:48:20.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:48:20.529
Apr  6 10:48:20.529: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 10:48:20.53
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:20.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:20.548
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 04/06/23 10:48:20.553
Apr  6 10:48:20.563: INFO: Waiting up to 5m0s for pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c" in namespace "downward-api-1970" to be "Succeeded or Failed"
Apr  6 10:48:20.567: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879194ms
Apr  6 10:48:22.580: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016993948s
Apr  6 10:48:24.578: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015338444s
STEP: Saw pod success 04/06/23 10:48:24.578
Apr  6 10:48:24.578: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c" satisfied condition "Succeeded or Failed"
Apr  6 10:48:24.584: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c container dapi-container: <nil>
STEP: delete the pod 04/06/23 10:48:24.599
Apr  6 10:48:24.620: INFO: Waiting for pod downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c to disappear
Apr  6 10:48:24.625: INFO: Pod downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Apr  6 10:48:24.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1970" for this suite. 04/06/23 10:48:24.636
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":142,"skipped":2584,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:48:20.529
    Apr  6 10:48:20.529: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 10:48:20.53
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:20.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:20.548
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 04/06/23 10:48:20.553
    Apr  6 10:48:20.563: INFO: Waiting up to 5m0s for pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c" in namespace "downward-api-1970" to be "Succeeded or Failed"
    Apr  6 10:48:20.567: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879194ms
    Apr  6 10:48:22.580: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016993948s
    Apr  6 10:48:24.578: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015338444s
    STEP: Saw pod success 04/06/23 10:48:24.578
    Apr  6 10:48:24.578: INFO: Pod "downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c" satisfied condition "Succeeded or Failed"
    Apr  6 10:48:24.584: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c container dapi-container: <nil>
    STEP: delete the pod 04/06/23 10:48:24.599
    Apr  6 10:48:24.620: INFO: Waiting for pod downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c to disappear
    Apr  6 10:48:24.625: INFO: Pod downward-api-c3b22445-8e3c-4595-b89c-1203302f1b4c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Apr  6 10:48:24.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1970" for this suite. 04/06/23 10:48:24.636
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:48:24.643
Apr  6 10:48:24.643: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replication-controller 04/06/23 10:48:24.644
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:24.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:24.67
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Apr  6 10:48:24.678: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/06/23 10:48:24.69
STEP: Checking rc "condition-test" has the desired failure condition set 04/06/23 10:48:24.697
STEP: Scaling down rc "condition-test" to satisfy pod quota 04/06/23 10:48:25.711
Apr  6 10:48:25.756: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 04/06/23 10:48:25.756
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Apr  6 10:48:25.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8592" for this suite. 04/06/23 10:48:25.79
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":143,"skipped":2584,"failed":0}
------------------------------
â€¢ [1.154 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:48:24.643
    Apr  6 10:48:24.643: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replication-controller 04/06/23 10:48:24.644
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:24.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:24.67
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Apr  6 10:48:24.678: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/06/23 10:48:24.69
    STEP: Checking rc "condition-test" has the desired failure condition set 04/06/23 10:48:24.697
    STEP: Scaling down rc "condition-test" to satisfy pod quota 04/06/23 10:48:25.711
    Apr  6 10:48:25.756: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 04/06/23 10:48:25.756
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Apr  6 10:48:25.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8592" for this suite. 04/06/23 10:48:25.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:48:25.799
Apr  6 10:48:25.799: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 10:48:25.8
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:25.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:25.83
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7128.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 04/06/23 10:48:25.837
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7128.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 04/06/23 10:48:25.837
STEP: creating a pod to probe /etc/hosts 04/06/23 10:48:25.837
STEP: submitting the pod to kubernetes 04/06/23 10:48:25.838
Apr  6 10:48:25.854: INFO: Waiting up to 15m0s for pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831" in namespace "dns-7128" to be "running"
Apr  6 10:48:25.859: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330335ms
Apr  6 10:48:27.865: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010246883s
Apr  6 10:48:29.872: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831": Phase="Running", Reason="", readiness=true. Elapsed: 4.01729077s
Apr  6 10:48:29.880: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831" satisfied condition "running"
STEP: retrieving the pod 04/06/23 10:48:29.88
STEP: looking for the results for each expected name from probers 04/06/23 10:48:29.886
Apr  6 10:48:30.090: INFO: DNS probes using dns-7128/dns-test-427c96d8-378f-4743-b342-3ef7b9adc831 succeeded

STEP: deleting the pod 04/06/23 10:48:30.09
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 10:48:30.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7128" for this suite. 04/06/23 10:48:30.109
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":144,"skipped":2615,"failed":0}
------------------------------
â€¢ [4.316 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:48:25.799
    Apr  6 10:48:25.799: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 10:48:25.8
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:25.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:25.83
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7128.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     04/06/23 10:48:25.837
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7128.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     04/06/23 10:48:25.837
    STEP: creating a pod to probe /etc/hosts 04/06/23 10:48:25.837
    STEP: submitting the pod to kubernetes 04/06/23 10:48:25.838
    Apr  6 10:48:25.854: INFO: Waiting up to 15m0s for pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831" in namespace "dns-7128" to be "running"
    Apr  6 10:48:25.859: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330335ms
    Apr  6 10:48:27.865: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010246883s
    Apr  6 10:48:29.872: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831": Phase="Running", Reason="", readiness=true. Elapsed: 4.01729077s
    Apr  6 10:48:29.880: INFO: Pod "dns-test-427c96d8-378f-4743-b342-3ef7b9adc831" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 10:48:29.88
    STEP: looking for the results for each expected name from probers 04/06/23 10:48:29.886
    Apr  6 10:48:30.090: INFO: DNS probes using dns-7128/dns-test-427c96d8-378f-4743-b342-3ef7b9adc831 succeeded

    STEP: deleting the pod 04/06/23 10:48:30.09
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 10:48:30.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7128" for this suite. 04/06/23 10:48:30.109
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:48:30.12
Apr  6 10:48:30.120: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename endpointslice 04/06/23 10:48:30.121
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:30.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:30.146
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 04/06/23 10:48:35.228
STEP: referencing matching pods with named port 04/06/23 10:48:40.24
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/06/23 10:48:45.253
STEP: recreating EndpointSlices after they've been deleted 04/06/23 10:48:50.266
Apr  6 10:48:50.291: INFO: EndpointSlice for Service endpointslice-8307/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Apr  6 10:49:00.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8307" for this suite. 04/06/23 10:49:00.338
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":145,"skipped":2615,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.232 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:48:30.12
    Apr  6 10:48:30.120: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename endpointslice 04/06/23 10:48:30.121
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:48:30.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:48:30.146
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 04/06/23 10:48:35.228
    STEP: referencing matching pods with named port 04/06/23 10:48:40.24
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/06/23 10:48:45.253
    STEP: recreating EndpointSlices after they've been deleted 04/06/23 10:48:50.266
    Apr  6 10:48:50.291: INFO: EndpointSlice for Service endpointslice-8307/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Apr  6 10:49:00.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8307" for this suite. 04/06/23 10:49:00.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:49:00.379
Apr  6 10:49:00.381: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir-wrapper 04/06/23 10:49:00.386
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:00.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:00.412
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Apr  6 10:49:00.441: INFO: Waiting up to 5m0s for pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8" in namespace "emptydir-wrapper-3925" to be "running and ready"
Apr  6 10:49:00.444: INFO: Pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.770792ms
Apr  6 10:49:00.445: INFO: The phase of Pod pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:49:02.452: INFO: Pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011113696s
Apr  6 10:49:02.452: INFO: The phase of Pod pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8 is Running (Ready = true)
Apr  6 10:49:02.452: INFO: Pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8" satisfied condition "running and ready"
STEP: Cleaning up the secret 04/06/23 10:49:02.468
STEP: Cleaning up the configmap 04/06/23 10:49:02.476
STEP: Cleaning up the pod 04/06/23 10:49:02.483
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Apr  6 10:49:02.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3925" for this suite. 04/06/23 10:49:02.516
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":146,"skipped":2628,"failed":0}
------------------------------
â€¢ [2.145 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:49:00.379
    Apr  6 10:49:00.381: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir-wrapper 04/06/23 10:49:00.386
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:00.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:00.412
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Apr  6 10:49:00.441: INFO: Waiting up to 5m0s for pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8" in namespace "emptydir-wrapper-3925" to be "running and ready"
    Apr  6 10:49:00.444: INFO: Pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.770792ms
    Apr  6 10:49:00.445: INFO: The phase of Pod pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:49:02.452: INFO: Pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011113696s
    Apr  6 10:49:02.452: INFO: The phase of Pod pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8 is Running (Ready = true)
    Apr  6 10:49:02.452: INFO: Pod "pod-secrets-7bcf998c-891e-42d6-a71d-64469eeaaba8" satisfied condition "running and ready"
    STEP: Cleaning up the secret 04/06/23 10:49:02.468
    STEP: Cleaning up the configmap 04/06/23 10:49:02.476
    STEP: Cleaning up the pod 04/06/23 10:49:02.483
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Apr  6 10:49:02.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3925" for this suite. 04/06/23 10:49:02.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:49:02.524
Apr  6 10:49:02.524: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename job 04/06/23 10:49:02.525
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:02.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:02.554
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 04/06/23 10:49:02.57
STEP: Patching the Job 04/06/23 10:49:02.578
STEP: Watching for Job to be patched 04/06/23 10:49:02.6
Apr  6 10:49:02.604: INFO: Event ADDED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr  6 10:49:02.604: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr  6 10:49:02.604: INFO: Event MODIFIED found for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 04/06/23 10:49:02.604
STEP: Watching for Job to be updated 04/06/23 10:49:02.618
Apr  6 10:49:02.624: INFO: Event MODIFIED found for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr  6 10:49:02.624: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 04/06/23 10:49:02.624
Apr  6 10:49:02.629: INFO: Job: e2e-265cd as labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd]
STEP: Waiting for job to complete 04/06/23 10:49:02.629
STEP: Delete a job collection with a labelselector 04/06/23 10:49:10.637
STEP: Watching for Job to be deleted 04/06/23 10:49:10.644
Apr  6 10:49:10.648: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr  6 10:49:10.655: INFO: Event DELETED found for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 04/06/23 10:49:10.655
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Apr  6 10:49:10.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5507" for this suite. 04/06/23 10:49:10.68
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":147,"skipped":2643,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.161 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:49:02.524
    Apr  6 10:49:02.524: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename job 04/06/23 10:49:02.525
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:02.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:02.554
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 04/06/23 10:49:02.57
    STEP: Patching the Job 04/06/23 10:49:02.578
    STEP: Watching for Job to be patched 04/06/23 10:49:02.6
    Apr  6 10:49:02.604: INFO: Event ADDED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr  6 10:49:02.604: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr  6 10:49:02.604: INFO: Event MODIFIED found for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 04/06/23 10:49:02.604
    STEP: Watching for Job to be updated 04/06/23 10:49:02.618
    Apr  6 10:49:02.624: INFO: Event MODIFIED found for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr  6 10:49:02.624: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 04/06/23 10:49:02.624
    Apr  6 10:49:02.629: INFO: Job: e2e-265cd as labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd]
    STEP: Waiting for job to complete 04/06/23 10:49:02.629
    STEP: Delete a job collection with a labelselector 04/06/23 10:49:10.637
    STEP: Watching for Job to be deleted 04/06/23 10:49:10.644
    Apr  6 10:49:10.648: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr  6 10:49:10.655: INFO: Event MODIFIED observed for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr  6 10:49:10.655: INFO: Event DELETED found for Job e2e-265cd in namespace job-5507 with labels: map[e2e-265cd:patched e2e-job-label:e2e-265cd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 04/06/23 10:49:10.655
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Apr  6 10:49:10.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5507" for this suite. 04/06/23 10:49:10.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:49:10.689
Apr  6 10:49:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:49:10.691
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:10.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:10.716
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:49:10.723
Apr  6 10:49:10.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49" in namespace "projected-2325" to be "Succeeded or Failed"
Apr  6 10:49:10.738: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.317991ms
Apr  6 10:49:12.745: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010916669s
Apr  6 10:49:14.744: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009541208s
STEP: Saw pod success 04/06/23 10:49:14.744
Apr  6 10:49:14.745: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49" satisfied condition "Succeeded or Failed"
Apr  6 10:49:14.750: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49 container client-container: <nil>
STEP: delete the pod 04/06/23 10:49:14.761
Apr  6 10:49:14.771: INFO: Waiting for pod downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49 to disappear
Apr  6 10:49:14.775: INFO: Pod downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 10:49:14.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2325" for this suite. 04/06/23 10:49:14.783
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":148,"skipped":2655,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:49:10.689
    Apr  6 10:49:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:49:10.691
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:10.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:10.716
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:49:10.723
    Apr  6 10:49:10.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49" in namespace "projected-2325" to be "Succeeded or Failed"
    Apr  6 10:49:10.738: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.317991ms
    Apr  6 10:49:12.745: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010916669s
    Apr  6 10:49:14.744: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009541208s
    STEP: Saw pod success 04/06/23 10:49:14.744
    Apr  6 10:49:14.745: INFO: Pod "downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49" satisfied condition "Succeeded or Failed"
    Apr  6 10:49:14.750: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:49:14.761
    Apr  6 10:49:14.771: INFO: Waiting for pod downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49 to disappear
    Apr  6 10:49:14.775: INFO: Pod downwardapi-volume-3f01c227-53b7-45e9-8114-e53f0edebe49 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 10:49:14.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2325" for this suite. 04/06/23 10:49:14.783
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:49:14.789
Apr  6 10:49:14.789: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:49:14.79
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:14.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:14.817
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-2727 04/06/23 10:49:14.824
STEP: creating service affinity-clusterip-transition in namespace services-2727 04/06/23 10:49:14.824
STEP: creating replication controller affinity-clusterip-transition in namespace services-2727 04/06/23 10:49:14.839
I0406 10:49:14.850168      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2727, replica count: 3
I0406 10:49:17.902171      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 10:49:17.918: INFO: Creating new exec pod
Apr  6 10:49:17.931: INFO: Waiting up to 5m0s for pod "execpod-affinitys2qnp" in namespace "services-2727" to be "running"
Apr  6 10:49:17.938: INFO: Pod "execpod-affinitys2qnp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89076ms
Apr  6 10:49:19.946: INFO: Pod "execpod-affinitys2qnp": Phase="Running", Reason="", readiness=true. Elapsed: 2.014812586s
Apr  6 10:49:19.946: INFO: Pod "execpod-affinitys2qnp" satisfied condition "running"
Apr  6 10:49:20.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Apr  6 10:49:21.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr  6 10:49:21.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:49:21.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.65.114.172 80'
Apr  6 10:49:22.067: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.65.114.172 80\nConnection to 10.65.114.172 80 port [tcp/http] succeeded!\n"
Apr  6 10:49:22.067: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:49:22.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.65.114.172:80/ ; done'
Apr  6 10:49:22.998: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n"
Apr  6 10:49:22.998: INFO: stdout: "\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-9npzm"
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
Apr  6 10:49:23.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.65.114.172:80/ ; done'
Apr  6 10:49:23.957: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n"
Apr  6 10:49:23.957: INFO: stdout: "\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf"
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
Apr  6 10:49:23.957: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2727, will wait for the garbage collector to delete the pods 04/06/23 10:49:23.967
Apr  6 10:49:24.028: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.058782ms
Apr  6 10:49:24.135: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 106.945095ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:49:26.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2727" for this suite. 04/06/23 10:49:26.671
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":149,"skipped":2658,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.894 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:49:14.789
    Apr  6 10:49:14.789: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:49:14.79
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:14.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:14.817
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-2727 04/06/23 10:49:14.824
    STEP: creating service affinity-clusterip-transition in namespace services-2727 04/06/23 10:49:14.824
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2727 04/06/23 10:49:14.839
    I0406 10:49:14.850168      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2727, replica count: 3
    I0406 10:49:17.902171      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 10:49:17.918: INFO: Creating new exec pod
    Apr  6 10:49:17.931: INFO: Waiting up to 5m0s for pod "execpod-affinitys2qnp" in namespace "services-2727" to be "running"
    Apr  6 10:49:17.938: INFO: Pod "execpod-affinitys2qnp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89076ms
    Apr  6 10:49:19.946: INFO: Pod "execpod-affinitys2qnp": Phase="Running", Reason="", readiness=true. Elapsed: 2.014812586s
    Apr  6 10:49:19.946: INFO: Pod "execpod-affinitys2qnp" satisfied condition "running"
    Apr  6 10:49:20.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Apr  6 10:49:21.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Apr  6 10:49:21.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:49:21.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.65.114.172 80'
    Apr  6 10:49:22.067: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.65.114.172 80\nConnection to 10.65.114.172 80 port [tcp/http] succeeded!\n"
    Apr  6 10:49:22.067: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:49:22.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.65.114.172:80/ ; done'
    Apr  6 10:49:22.998: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n"
    Apr  6 10:49:22.998: INFO: stdout: "\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-gjhsh\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-9npzm\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-9npzm"
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gjhsh
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:22.998: INFO: Received response from host: affinity-clusterip-transition-9npzm
    Apr  6 10:49:23.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2727 exec execpod-affinitys2qnp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.65.114.172:80/ ; done'
    Apr  6 10:49:23.957: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.65.114.172:80/\n"
    Apr  6 10:49:23.957: INFO: stdout: "\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf\naffinity-clusterip-transition-gfxnf"
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Received response from host: affinity-clusterip-transition-gfxnf
    Apr  6 10:49:23.957: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2727, will wait for the garbage collector to delete the pods 04/06/23 10:49:23.967
    Apr  6 10:49:24.028: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.058782ms
    Apr  6 10:49:24.135: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 106.945095ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:49:26.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2727" for this suite. 04/06/23 10:49:26.671
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:49:26.687
Apr  6 10:49:26.687: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 10:49:26.688
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:26.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:26.734
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Apr  6 10:49:26.754: INFO: Waiting up to 5m0s for pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7" in namespace "container-probe-7598" to be "running and ready"
Apr  6 10:49:26.758: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.711078ms
Apr  6 10:49:26.762: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:49:28.773: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 2.019333386s
Apr  6 10:49:28.773: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:30.769: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 4.015237693s
Apr  6 10:49:30.769: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:32.770: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 6.016548427s
Apr  6 10:49:32.770: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:34.770: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 8.015949677s
Apr  6 10:49:34.770: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:36.771: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 10.017434013s
Apr  6 10:49:36.771: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:38.772: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 12.01799809s
Apr  6 10:49:38.772: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:40.771: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 14.017214515s
Apr  6 10:49:40.771: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:42.781: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 16.026857266s
Apr  6 10:49:42.781: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:44.772: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 18.017862729s
Apr  6 10:49:44.772: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:46.770: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 20.016496268s
Apr  6 10:49:46.770: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
Apr  6 10:49:48.773: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=true. Elapsed: 22.019148577s
Apr  6 10:49:48.773: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = true)
Apr  6 10:49:48.773: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7" satisfied condition "running and ready"
Apr  6 10:49:48.780: INFO: Container started at 2023-04-06 10:49:28 +0000 UTC, pod became ready at 2023-04-06 10:49:47 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 10:49:48.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7598" for this suite. 04/06/23 10:49:48.793
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":150,"skipped":2680,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.114 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:49:26.687
    Apr  6 10:49:26.687: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 10:49:26.688
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:26.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:26.734
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Apr  6 10:49:26.754: INFO: Waiting up to 5m0s for pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7" in namespace "container-probe-7598" to be "running and ready"
    Apr  6 10:49:26.758: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.711078ms
    Apr  6 10:49:26.762: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:49:28.773: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 2.019333386s
    Apr  6 10:49:28.773: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:30.769: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 4.015237693s
    Apr  6 10:49:30.769: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:32.770: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 6.016548427s
    Apr  6 10:49:32.770: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:34.770: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 8.015949677s
    Apr  6 10:49:34.770: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:36.771: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 10.017434013s
    Apr  6 10:49:36.771: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:38.772: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 12.01799809s
    Apr  6 10:49:38.772: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:40.771: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 14.017214515s
    Apr  6 10:49:40.771: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:42.781: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 16.026857266s
    Apr  6 10:49:42.781: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:44.772: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 18.017862729s
    Apr  6 10:49:44.772: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:46.770: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=false. Elapsed: 20.016496268s
    Apr  6 10:49:46.770: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = false)
    Apr  6 10:49:48.773: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7": Phase="Running", Reason="", readiness=true. Elapsed: 22.019148577s
    Apr  6 10:49:48.773: INFO: The phase of Pod test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7 is Running (Ready = true)
    Apr  6 10:49:48.773: INFO: Pod "test-webserver-067b66b8-1ba1-4fe3-8afa-2e0fc9ea7ef7" satisfied condition "running and ready"
    Apr  6 10:49:48.780: INFO: Container started at 2023-04-06 10:49:28 +0000 UTC, pod became ready at 2023-04-06 10:49:47 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 10:49:48.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7598" for this suite. 04/06/23 10:49:48.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:49:48.802
Apr  6 10:49:48.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-runtime 04/06/23 10:49:48.803
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:48.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:48.831
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/06/23 10:49:48.851
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/06/23 10:50:07.005
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/06/23 10:50:07.01
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/06/23 10:50:07.02
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/06/23 10:50:07.02
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/06/23 10:50:07.058
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/06/23 10:50:10.092
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/06/23 10:50:12.12
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/06/23 10:50:12.131
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/06/23 10:50:12.131
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/06/23 10:50:12.156
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/06/23 10:50:13.17
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/06/23 10:50:16.2
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/06/23 10:50:16.216
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/06/23 10:50:16.217
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Apr  6 10:50:16.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7233" for this suite. 04/06/23 10:50:16.26
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":151,"skipped":2686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.466 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:49:48.802
    Apr  6 10:49:48.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-runtime 04/06/23 10:49:48.803
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:49:48.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:49:48.831
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/06/23 10:49:48.851
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/06/23 10:50:07.005
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/06/23 10:50:07.01
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/06/23 10:50:07.02
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/06/23 10:50:07.02
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/06/23 10:50:07.058
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/06/23 10:50:10.092
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/06/23 10:50:12.12
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/06/23 10:50:12.131
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/06/23 10:50:12.131
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/06/23 10:50:12.156
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/06/23 10:50:13.17
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/06/23 10:50:16.2
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/06/23 10:50:16.216
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/06/23 10:50:16.217
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Apr  6 10:50:16.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7233" for this suite. 04/06/23 10:50:16.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:50:16.269
Apr  6 10:50:16.269: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:50:16.27
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:16.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:16.293
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Apr  6 10:50:16.300: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/06/23 10:50:21.445
Apr  6 10:50:21.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 create -f -'
Apr  6 10:50:22.327: INFO: stderr: ""
Apr  6 10:50:22.327: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr  6 10:50:22.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 delete e2e-test-crd-publish-openapi-7000-crds test-cr'
Apr  6 10:50:22.428: INFO: stderr: ""
Apr  6 10:50:22.428: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr  6 10:50:22.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 apply -f -'
Apr  6 10:50:22.794: INFO: stderr: ""
Apr  6 10:50:22.795: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr  6 10:50:22.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 delete e2e-test-crd-publish-openapi-7000-crds test-cr'
Apr  6 10:50:23.065: INFO: stderr: ""
Apr  6 10:50:23.065: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/06/23 10:50:23.065
Apr  6 10:50:23.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 explain e2e-test-crd-publish-openapi-7000-crds'
Apr  6 10:50:23.684: INFO: stderr: ""
Apr  6 10:50:23.684: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7000-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:50:29.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7222" for this suite. 04/06/23 10:50:29.043
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":152,"skipped":2699,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.781 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:50:16.269
    Apr  6 10:50:16.269: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:50:16.27
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:16.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:16.293
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Apr  6 10:50:16.300: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/06/23 10:50:21.445
    Apr  6 10:50:21.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 create -f -'
    Apr  6 10:50:22.327: INFO: stderr: ""
    Apr  6 10:50:22.327: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr  6 10:50:22.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 delete e2e-test-crd-publish-openapi-7000-crds test-cr'
    Apr  6 10:50:22.428: INFO: stderr: ""
    Apr  6 10:50:22.428: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Apr  6 10:50:22.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 apply -f -'
    Apr  6 10:50:22.794: INFO: stderr: ""
    Apr  6 10:50:22.795: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr  6 10:50:22.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 --namespace=crd-publish-openapi-7222 delete e2e-test-crd-publish-openapi-7000-crds test-cr'
    Apr  6 10:50:23.065: INFO: stderr: ""
    Apr  6 10:50:23.065: INFO: stdout: "e2e-test-crd-publish-openapi-7000-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/06/23 10:50:23.065
    Apr  6 10:50:23.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-7222 explain e2e-test-crd-publish-openapi-7000-crds'
    Apr  6 10:50:23.684: INFO: stderr: ""
    Apr  6 10:50:23.684: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7000-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:50:29.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7222" for this suite. 04/06/23 10:50:29.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:50:29.056
Apr  6 10:50:29.056: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename disruption 04/06/23 10:50:29.057
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:29.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:29.079
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 04/06/23 10:50:29.089
STEP: Waiting for all pods to be running 04/06/23 10:50:31.134
Apr  6 10:50:31.138: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Apr  6 10:50:33.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8229" for this suite. 04/06/23 10:50:33.158
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":153,"skipped":2711,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:50:29.056
    Apr  6 10:50:29.056: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename disruption 04/06/23 10:50:29.057
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:29.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:29.079
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 04/06/23 10:50:29.089
    STEP: Waiting for all pods to be running 04/06/23 10:50:31.134
    Apr  6 10:50:31.138: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Apr  6 10:50:33.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8229" for this suite. 04/06/23 10:50:33.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:50:33.165
Apr  6 10:50:33.165: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename daemonsets 04/06/23 10:50:33.166
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:33.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:33.208
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 04/06/23 10:50:33.281
STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 10:50:33.286
Apr  6 10:50:33.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:50:33.301: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 10:50:34.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 10:50:34.333: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 10:50:35.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr  6 10:50:35.330: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 10:50:36.316: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr  6 10:50:36.316: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 10:50:37.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr  6 10:50:37.315: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 10:50:38.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr  6 10:50:38.315: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 10:50:39.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 10:50:39.319: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
Apr  6 10:50:40.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 10:50:40.320: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets 04/06/23 10:50:40.324
STEP: DeleteCollection of the DaemonSets 04/06/23 10:50:40.329
STEP: Verify that ReplicaSets have been deleted 04/06/23 10:50:40.337
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Apr  6 10:50:40.387: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18931"},"items":null}

Apr  6 10:50:40.398: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18931"},"items":[{"metadata":{"name":"daemon-set-7lcsk","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"b3cb68ce-dbe2-45ed-8dd5-0f84aeca16ca","resourceVersion":"18930","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7bfc72570e03f9d10330cc6f754a0feca43edf50ba897c4f74b00430ebd2300a","cni.projectcalico.org/podIP":"10.96.3.9/32","cni.projectcalico.org/podIPs":"10.96.3.9/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-84jj2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-84jj2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.2.236","podIP":"10.96.3.9","podIPs":[{"ip":"10.96.3.9"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://fdd3dab569f1194d4384cddd0e181ae6bf825e3b10f54a14850e99af2609abc0","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lw6rl","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"e2651fc3-48ca-412c-8c90-8d60f895c7e4","resourceVersion":"18927","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"267ab3c895be634ca1a597e4f98393b52847a71f088c148fd04058b24e070830","cni.projectcalico.org/podIP":"10.96.2.83/32","cni.projectcalico.org/podIPs":"10.96.2.83/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2wx98","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2wx98","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.0.68","podIP":"10.96.2.83","podIPs":[{"ip":"10.96.2.83"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://17d74d13b9cf17d1d248ced1e300fafb9c4755eb11d731cb61a771e6bae2c9f2","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-sjqtz","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"1378c1cf-77cc-43ae-9877-665ba0e0ac9f","resourceVersion":"18929","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"069e1d5bdb07a304ce0071fc3ed1850b106df3460b89c7cab1d2b328fd20a891","cni.projectcalico.org/podIP":"10.96.4.8/32","cni.projectcalico.org/podIPs":"10.96.4.8/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.4.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lh9bq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lh9bq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.3.220","podIP":"10.96.4.8","podIPs":[{"ip":"10.96.4.8"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://92b5ff04070f2f87cfecda9648b139e491ff643a21a4aa5aa61e93165ad4bfae","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wkxjs","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"de46df3f-ae56-4a8c-9f37-fb691b578a88","resourceVersion":"18931","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"54aa3cb6eacff83f441c0bdb38363bd13921f786375603454a21f45b35c08f02","cni.projectcalico.org/podIP":"10.96.0.15/32","cni.projectcalico.org/podIPs":"10.96.0.15/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bd8l8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bd8l8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.2.89","podIP":"10.96.0.15","podIPs":[{"ip":"10.96.0.15"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://092bf44fe4b1400df17bfb5ca1e2ba654b6fdccc19e05f9c39b9852b72a199e6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z5n5q","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"78460393-ac6a-48c8-a4d8-05b500104550","resourceVersion":"18928","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9f0db66ac20c72b575ba86a113fb276c7fae08a1715499c3dcccdeae43e40c12","cni.projectcalico.org/podIP":"10.96.1.77/32","cni.projectcalico.org/podIPs":"10.96.1.77/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hwmq5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hwmq5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.1.148","podIP":"10.96.1.77","podIPs":[{"ip":"10.96.1.77"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b9a4c44db6045a5b808eceede238d7ef45f62bd6cd3afa753791a50f358d61eb","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:50:40.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2527" for this suite. 04/06/23 10:50:40.463
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":154,"skipped":2728,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.305 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:50:33.165
    Apr  6 10:50:33.165: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename daemonsets 04/06/23 10:50:33.166
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:33.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:33.208
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 04/06/23 10:50:33.281
    STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 10:50:33.286
    Apr  6 10:50:33.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:50:33.301: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 10:50:34.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 10:50:34.333: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 10:50:35.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr  6 10:50:35.330: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 10:50:36.316: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr  6 10:50:36.316: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 10:50:37.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr  6 10:50:37.315: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 10:50:38.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr  6 10:50:38.315: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 10:50:39.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 10:50:39.319: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
    Apr  6 10:50:40.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 10:50:40.320: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: listing all DeamonSets 04/06/23 10:50:40.324
    STEP: DeleteCollection of the DaemonSets 04/06/23 10:50:40.329
    STEP: Verify that ReplicaSets have been deleted 04/06/23 10:50:40.337
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Apr  6 10:50:40.387: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18931"},"items":null}

    Apr  6 10:50:40.398: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18931"},"items":[{"metadata":{"name":"daemon-set-7lcsk","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"b3cb68ce-dbe2-45ed-8dd5-0f84aeca16ca","resourceVersion":"18930","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7bfc72570e03f9d10330cc6f754a0feca43edf50ba897c4f74b00430ebd2300a","cni.projectcalico.org/podIP":"10.96.3.9/32","cni.projectcalico.org/podIPs":"10.96.3.9/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-84jj2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-84jj2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.2.236","podIP":"10.96.3.9","podIPs":[{"ip":"10.96.3.9"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://fdd3dab569f1194d4384cddd0e181ae6bf825e3b10f54a14850e99af2609abc0","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lw6rl","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"e2651fc3-48ca-412c-8c90-8d60f895c7e4","resourceVersion":"18927","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"267ab3c895be634ca1a597e4f98393b52847a71f088c148fd04058b24e070830","cni.projectcalico.org/podIP":"10.96.2.83/32","cni.projectcalico.org/podIPs":"10.96.2.83/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2wx98","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2wx98","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.0.68","podIP":"10.96.2.83","podIPs":[{"ip":"10.96.2.83"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://17d74d13b9cf17d1d248ced1e300fafb9c4755eb11d731cb61a771e6bae2c9f2","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-sjqtz","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"1378c1cf-77cc-43ae-9877-665ba0e0ac9f","resourceVersion":"18929","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"069e1d5bdb07a304ce0071fc3ed1850b106df3460b89c7cab1d2b328fd20a891","cni.projectcalico.org/podIP":"10.96.4.8/32","cni.projectcalico.org/podIPs":"10.96.4.8/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.4.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lh9bq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lh9bq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.3.220","podIP":"10.96.4.8","podIPs":[{"ip":"10.96.4.8"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://92b5ff04070f2f87cfecda9648b139e491ff643a21a4aa5aa61e93165ad4bfae","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wkxjs","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"de46df3f-ae56-4a8c-9f37-fb691b578a88","resourceVersion":"18931","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"54aa3cb6eacff83f441c0bdb38363bd13921f786375603454a21f45b35c08f02","cni.projectcalico.org/podIP":"10.96.0.15/32","cni.projectcalico.org/podIPs":"10.96.0.15/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bd8l8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bd8l8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.2.89","podIP":"10.96.0.15","podIPs":[{"ip":"10.96.0.15"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://092bf44fe4b1400df17bfb5ca1e2ba654b6fdccc19e05f9c39b9852b72a199e6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z5n5q","generateName":"daemon-set-","namespace":"daemonsets-2527","uid":"78460393-ac6a-48c8-a4d8-05b500104550","resourceVersion":"18928","creationTimestamp":"2023-04-06T10:50:33Z","deletionTimestamp":"2023-04-06T10:51:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9f0db66ac20c72b575ba86a113fb276c7fae08a1715499c3dcccdeae43e40c12","cni.projectcalico.org/podIP":"10.96.1.77/32","cni.projectcalico.org/podIPs":"10.96.1.77/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b47cee28-7a38-40dd-8fb3-72b4a66fe792","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b47cee28-7a38-40dd-8fb3-72b4a66fe792\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-06T10:50:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hwmq5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hwmq5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-06T10:50:33Z"}],"hostIP":"10.250.1.148","podIP":"10.96.1.77","podIPs":[{"ip":"10.96.1.77"}],"startTime":"2023-04-06T10:50:33Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-06T10:50:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b9a4c44db6045a5b808eceede238d7ef45f62bd6cd3afa753791a50f358d61eb","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:50:40.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2527" for this suite. 04/06/23 10:50:40.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:50:40.472
Apr  6 10:50:40.472: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pod-network-test 04/06/23 10:50:40.473
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:40.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:40.521
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-1777 04/06/23 10:50:40.526
STEP: creating a selector 04/06/23 10:50:40.526
STEP: Creating the service pods in kubernetes 04/06/23 10:50:40.527
Apr  6 10:50:40.527: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 10:50:40.622: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1777" to be "running and ready"
Apr  6 10:50:40.633: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.483008ms
Apr  6 10:50:40.633: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:50:42.651: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028336942s
Apr  6 10:50:42.651: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:50:44.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017044187s
Apr  6 10:50:44.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:50:46.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016448271s
Apr  6 10:50:46.647: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:50:48.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018112662s
Apr  6 10:50:48.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:50:50.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016403821s
Apr  6 10:50:50.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:50:52.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017839266s
Apr  6 10:50:52.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:50:54.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020983733s
Apr  6 10:50:54.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:50:56.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016448265s
Apr  6 10:50:56.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:50:58.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020391493s
Apr  6 10:50:58.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:51:00.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016908894s
Apr  6 10:51:00.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:51:02.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016703495s
Apr  6 10:51:02.639: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr  6 10:51:02.639: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr  6 10:51:02.643: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1777" to be "running and ready"
Apr  6 10:51:02.647: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.152792ms
Apr  6 10:51:02.648: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr  6 10:51:02.648: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr  6 10:51:02.652: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1777" to be "running and ready"
Apr  6 10:51:02.658: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.958946ms
Apr  6 10:51:02.658: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr  6 10:51:02.658: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr  6 10:51:02.664: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1777" to be "running and ready"
Apr  6 10:51:02.667: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.620002ms
Apr  6 10:51:02.667: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr  6 10:51:02.667: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr  6 10:51:02.672: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1777" to be "running and ready"
Apr  6 10:51:02.678: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.255966ms
Apr  6 10:51:02.678: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr  6 10:51:02.678: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 04/06/23 10:51:02.687
Apr  6 10:51:02.709: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1777" to be "running"
Apr  6 10:51:02.717: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.318332ms
Apr  6 10:51:04.724: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014393128s
Apr  6 10:51:04.724: INFO: Pod "test-container-pod" satisfied condition "running"
Apr  6 10:51:04.727: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1777" to be "running"
Apr  6 10:51:04.735: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.935769ms
Apr  6 10:51:04.735: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr  6 10:51:04.739: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Apr  6 10:51:04.739: INFO: Going to poll 10.96.3.10 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:51:04.743: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.3.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:51:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:51:04.744: INFO: ExecWithOptions: Clientset creation
Apr  6 10:51:04.745: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.3.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:51:06.283: INFO: Found all 1 expected endpoints: [netserver-0]
Apr  6 10:51:06.283: INFO: Going to poll 10.96.2.84 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:51:06.296: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.2.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:51:06.296: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:51:06.297: INFO: ExecWithOptions: Clientset creation
Apr  6 10:51:06.297: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.2.84+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:51:07.807: INFO: Found all 1 expected endpoints: [netserver-1]
Apr  6 10:51:07.807: INFO: Going to poll 10.96.0.16 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:51:07.813: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.0.16 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:51:07.813: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:51:07.814: INFO: ExecWithOptions: Clientset creation
Apr  6 10:51:07.814: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.0.16+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:51:09.308: INFO: Found all 1 expected endpoints: [netserver-2]
Apr  6 10:51:09.308: INFO: Going to poll 10.96.1.78 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:51:09.313: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.1.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:51:09.313: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:51:09.318: INFO: ExecWithOptions: Clientset creation
Apr  6 10:51:09.318: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.1.78+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:51:10.670: INFO: Found all 1 expected endpoints: [netserver-3]
Apr  6 10:51:10.671: INFO: Going to poll 10.96.4.9 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Apr  6 10:51:10.675: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.4.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:51:10.675: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:51:10.676: INFO: ExecWithOptions: Clientset creation
Apr  6 10:51:10.676: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.4.9+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 10:51:12.188: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Apr  6 10:51:12.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1777" for this suite. 04/06/23 10:51:12.201
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":155,"skipped":2742,"failed":0}
------------------------------
â€¢ [SLOW TEST] [31.735 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:50:40.472
    Apr  6 10:50:40.472: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pod-network-test 04/06/23 10:50:40.473
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:50:40.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:50:40.521
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-1777 04/06/23 10:50:40.526
    STEP: creating a selector 04/06/23 10:50:40.526
    STEP: Creating the service pods in kubernetes 04/06/23 10:50:40.527
    Apr  6 10:50:40.527: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr  6 10:50:40.622: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1777" to be "running and ready"
    Apr  6 10:50:40.633: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.483008ms
    Apr  6 10:50:40.633: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:50:42.651: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028336942s
    Apr  6 10:50:42.651: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:50:44.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017044187s
    Apr  6 10:50:44.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:50:46.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016448271s
    Apr  6 10:50:46.647: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:50:48.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018112662s
    Apr  6 10:50:48.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:50:50.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016403821s
    Apr  6 10:50:50.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:50:52.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017839266s
    Apr  6 10:50:52.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:50:54.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020983733s
    Apr  6 10:50:54.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:50:56.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016448265s
    Apr  6 10:50:56.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:50:58.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020391493s
    Apr  6 10:50:58.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:51:00.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016908894s
    Apr  6 10:51:00.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:51:02.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016703495s
    Apr  6 10:51:02.639: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr  6 10:51:02.639: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr  6 10:51:02.643: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1777" to be "running and ready"
    Apr  6 10:51:02.647: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.152792ms
    Apr  6 10:51:02.648: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr  6 10:51:02.648: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr  6 10:51:02.652: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1777" to be "running and ready"
    Apr  6 10:51:02.658: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.958946ms
    Apr  6 10:51:02.658: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr  6 10:51:02.658: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr  6 10:51:02.664: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1777" to be "running and ready"
    Apr  6 10:51:02.667: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.620002ms
    Apr  6 10:51:02.667: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr  6 10:51:02.667: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr  6 10:51:02.672: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1777" to be "running and ready"
    Apr  6 10:51:02.678: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.255966ms
    Apr  6 10:51:02.678: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr  6 10:51:02.678: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 04/06/23 10:51:02.687
    Apr  6 10:51:02.709: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1777" to be "running"
    Apr  6 10:51:02.717: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.318332ms
    Apr  6 10:51:04.724: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014393128s
    Apr  6 10:51:04.724: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr  6 10:51:04.727: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1777" to be "running"
    Apr  6 10:51:04.735: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.935769ms
    Apr  6 10:51:04.735: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr  6 10:51:04.739: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Apr  6 10:51:04.739: INFO: Going to poll 10.96.3.10 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:51:04.743: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.3.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:51:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:51:04.744: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:51:04.745: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.3.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:51:06.283: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr  6 10:51:06.283: INFO: Going to poll 10.96.2.84 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:51:06.296: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.2.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:51:06.296: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:51:06.297: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:51:06.297: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.2.84+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:51:07.807: INFO: Found all 1 expected endpoints: [netserver-1]
    Apr  6 10:51:07.807: INFO: Going to poll 10.96.0.16 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:51:07.813: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.0.16 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:51:07.813: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:51:07.814: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:51:07.814: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.0.16+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:51:09.308: INFO: Found all 1 expected endpoints: [netserver-2]
    Apr  6 10:51:09.308: INFO: Going to poll 10.96.1.78 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:51:09.313: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.1.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:51:09.313: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:51:09.318: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:51:09.318: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.1.78+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:51:10.670: INFO: Found all 1 expected endpoints: [netserver-3]
    Apr  6 10:51:10.671: INFO: Going to poll 10.96.4.9 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Apr  6 10:51:10.675: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.96.4.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1777 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:51:10.675: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:51:10.676: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:51:10.676: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-1777/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.96.4.9+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 10:51:12.188: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Apr  6 10:51:12.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1777" for this suite. 04/06/23 10:51:12.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:51:12.218
Apr  6 10:51:12.218: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:51:12.219
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:51:12.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:51:12.248
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:51:12.271
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:51:12.84
STEP: Deploying the webhook pod 04/06/23 10:51:12.851
STEP: Wait for the deployment to be ready 04/06/23 10:51:12.905
Apr  6 10:51:12.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:51:14.955
STEP: Verifying the service has paired with the endpoint 04/06/23 10:51:14.976
Apr  6 10:51:15.987: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 04/06/23 10:51:16.081
STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:51:16.167
STEP: Deleting the collection of validation webhooks 04/06/23 10:51:16.233
STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:51:16.264
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:51:16.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2641" for this suite. 04/06/23 10:51:16.288
STEP: Destroying namespace "webhook-2641-markers" for this suite. 04/06/23 10:51:16.293
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":156,"skipped":2815,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:51:12.218
    Apr  6 10:51:12.218: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:51:12.219
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:51:12.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:51:12.248
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:51:12.271
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:51:12.84
    STEP: Deploying the webhook pod 04/06/23 10:51:12.851
    STEP: Wait for the deployment to be ready 04/06/23 10:51:12.905
    Apr  6 10:51:12.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:51:14.955
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:51:14.976
    Apr  6 10:51:15.987: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 04/06/23 10:51:16.081
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:51:16.167
    STEP: Deleting the collection of validation webhooks 04/06/23 10:51:16.233
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/06/23 10:51:16.264
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:51:16.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2641" for this suite. 04/06/23 10:51:16.288
    STEP: Destroying namespace "webhook-2641-markers" for this suite. 04/06/23 10:51:16.293
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:51:16.33
Apr  6 10:51:16.330: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:51:16.331
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:51:16.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:51:16.352
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:51:16.367
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:51:17.045
STEP: Deploying the webhook pod 04/06/23 10:51:17.067
STEP: Wait for the deployment to be ready 04/06/23 10:51:17.077
Apr  6 10:51:17.085: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:51:19.103
STEP: Verifying the service has paired with the endpoint 04/06/23 10:51:19.118
Apr  6 10:51:20.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/06/23 10:51:20.123
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/06/23 10:51:20.242
STEP: Creating a dummy validating-webhook-configuration object 04/06/23 10:51:20.382
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/06/23 10:51:20.482
STEP: Creating a dummy mutating-webhook-configuration object 04/06/23 10:51:20.492
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/06/23 10:51:20.562
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:51:20.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4940" for this suite. 04/06/23 10:51:20.607
STEP: Destroying namespace "webhook-4940-markers" for this suite. 04/06/23 10:51:20.619
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":157,"skipped":2824,"failed":0}
------------------------------
â€¢ [4.333 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:51:16.33
    Apr  6 10:51:16.330: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:51:16.331
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:51:16.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:51:16.352
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:51:16.367
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:51:17.045
    STEP: Deploying the webhook pod 04/06/23 10:51:17.067
    STEP: Wait for the deployment to be ready 04/06/23 10:51:17.077
    Apr  6 10:51:17.085: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:51:19.103
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:51:19.118
    Apr  6 10:51:20.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/06/23 10:51:20.123
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/06/23 10:51:20.242
    STEP: Creating a dummy validating-webhook-configuration object 04/06/23 10:51:20.382
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/06/23 10:51:20.482
    STEP: Creating a dummy mutating-webhook-configuration object 04/06/23 10:51:20.492
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/06/23 10:51:20.562
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:51:20.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4940" for this suite. 04/06/23 10:51:20.607
    STEP: Destroying namespace "webhook-4940-markers" for this suite. 04/06/23 10:51:20.619
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:51:20.665
Apr  6 10:51:20.666: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 10:51:20.667
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:51:20.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:51:20.705
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d in namespace container-probe-7944 04/06/23 10:51:20.712
Apr  6 10:51:20.721: INFO: Waiting up to 5m0s for pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d" in namespace "container-probe-7944" to be "not pending"
Apr  6 10:51:20.724: INFO: Pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174839ms
Apr  6 10:51:22.729: INFO: Pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008096778s
Apr  6 10:51:22.729: INFO: Pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d" satisfied condition "not pending"
Apr  6 10:51:22.729: INFO: Started pod busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d in namespace container-probe-7944
STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 10:51:22.729
Apr  6 10:51:22.733: INFO: Initial restart count of pod busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d is 0
STEP: deleting the pod 04/06/23 10:55:23.69
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 10:55:23.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7944" for this suite. 04/06/23 10:55:23.72
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":158,"skipped":2824,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.061 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:51:20.665
    Apr  6 10:51:20.666: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 10:51:20.667
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:51:20.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:51:20.705
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d in namespace container-probe-7944 04/06/23 10:51:20.712
    Apr  6 10:51:20.721: INFO: Waiting up to 5m0s for pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d" in namespace "container-probe-7944" to be "not pending"
    Apr  6 10:51:20.724: INFO: Pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174839ms
    Apr  6 10:51:22.729: INFO: Pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008096778s
    Apr  6 10:51:22.729: INFO: Pod "busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d" satisfied condition "not pending"
    Apr  6 10:51:22.729: INFO: Started pod busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d in namespace container-probe-7944
    STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 10:51:22.729
    Apr  6 10:51:22.733: INFO: Initial restart count of pod busybox-5c132bc2-55d6-4c31-8c32-a25b5b67355d is 0
    STEP: deleting the pod 04/06/23 10:55:23.69
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 10:55:23.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7944" for this suite. 04/06/23 10:55:23.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:55:23.727
Apr  6 10:55:23.727: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename endpointslice 04/06/23 10:55:23.729
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:23.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:23.758
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Apr  6 10:55:25.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9765" for this suite. 04/06/23 10:55:25.874
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":159,"skipped":2829,"failed":0}
------------------------------
â€¢ [2.166 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:55:23.727
    Apr  6 10:55:23.727: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename endpointslice 04/06/23 10:55:23.729
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:23.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:23.758
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Apr  6 10:55:25.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9765" for this suite. 04/06/23 10:55:25.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:55:25.897
Apr  6 10:55:25.897: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pod-network-test 04/06/23 10:55:25.898
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:25.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:25.929
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6771 04/06/23 10:55:25.94
STEP: creating a selector 04/06/23 10:55:25.942
STEP: Creating the service pods in kubernetes 04/06/23 10:55:25.942
Apr  6 10:55:25.942: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 10:55:26.054: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6771" to be "running and ready"
Apr  6 10:55:26.060: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.550507ms
Apr  6 10:55:26.060: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:55:28.066: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011181592s
Apr  6 10:55:28.066: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:55:30.066: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011865536s
Apr  6 10:55:30.066: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:55:32.067: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012665616s
Apr  6 10:55:32.067: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:55:34.067: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012144545s
Apr  6 10:55:34.067: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:55:36.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014790309s
Apr  6 10:55:36.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 10:55:38.066: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011886298s
Apr  6 10:55:38.066: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr  6 10:55:38.066: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr  6 10:55:38.070: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6771" to be "running and ready"
Apr  6 10:55:38.080: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 10.49671ms
Apr  6 10:55:38.081: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Apr  6 10:55:40.087: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.016949399s
Apr  6 10:55:40.087: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Apr  6 10:55:42.087: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.017339944s
Apr  6 10:55:42.087: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Apr  6 10:55:44.088: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.017646942s
Apr  6 10:55:44.088: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Apr  6 10:55:46.090: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.019758227s
Apr  6 10:55:46.090: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Apr  6 10:55:48.105: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.035551922s
Apr  6 10:55:48.105: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr  6 10:55:48.105: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr  6 10:55:48.120: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6771" to be "running and ready"
Apr  6 10:55:48.132: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 12.293447ms
Apr  6 10:55:48.132: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr  6 10:55:48.132: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr  6 10:55:48.136: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6771" to be "running and ready"
Apr  6 10:55:48.140: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.792085ms
Apr  6 10:55:48.140: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr  6 10:55:48.140: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr  6 10:55:48.147: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6771" to be "running and ready"
Apr  6 10:55:48.151: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.382158ms
Apr  6 10:55:48.151: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr  6 10:55:48.151: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 04/06/23 10:55:48.155
Apr  6 10:55:48.164: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6771" to be "running"
Apr  6 10:55:48.176: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.645391ms
Apr  6 10:55:50.186: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018008251s
Apr  6 10:55:50.190: INFO: Pod "test-container-pod" satisfied condition "running"
Apr  6 10:55:50.196: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Apr  6 10:55:50.196: INFO: Breadth first check of 10.96.3.11 on host 10.250.2.236...
Apr  6 10:55:50.201: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.3.11&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:55:50.201: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:55:50.202: INFO: ExecWithOptions: Clientset creation
Apr  6 10:55:50.202: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.3.11%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 10:55:50.645: INFO: Waiting for responses: map[]
Apr  6 10:55:50.645: INFO: reached 10.96.3.11 after 0/1 tries
Apr  6 10:55:50.645: INFO: Breadth first check of 10.96.2.85 on host 10.250.0.68...
Apr  6 10:55:50.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.2.85&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:55:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:55:50.652: INFO: ExecWithOptions: Clientset creation
Apr  6 10:55:50.652: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.2.85%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 10:55:51.132: INFO: Waiting for responses: map[]
Apr  6 10:55:51.132: INFO: reached 10.96.2.85 after 0/1 tries
Apr  6 10:55:51.132: INFO: Breadth first check of 10.96.0.17 on host 10.250.2.89...
Apr  6 10:55:51.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.0.17&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:55:51.138: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:55:51.138: INFO: ExecWithOptions: Clientset creation
Apr  6 10:55:51.138: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.0.17%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 10:55:51.706: INFO: Waiting for responses: map[]
Apr  6 10:55:51.706: INFO: reached 10.96.0.17 after 0/1 tries
Apr  6 10:55:51.706: INFO: Breadth first check of 10.96.1.83 on host 10.250.1.148...
Apr  6 10:55:51.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.1.83&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:55:51.718: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:55:51.719: INFO: ExecWithOptions: Clientset creation
Apr  6 10:55:51.719: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.1.83%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 10:55:52.213: INFO: Waiting for responses: map[]
Apr  6 10:55:52.213: INFO: reached 10.96.1.83 after 0/1 tries
Apr  6 10:55:52.213: INFO: Breadth first check of 10.96.4.10 on host 10.250.3.220...
Apr  6 10:55:52.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.4.10&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 10:55:52.218: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 10:55:52.218: INFO: ExecWithOptions: Clientset creation
Apr  6 10:55:52.218: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.4.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 10:55:52.695: INFO: Waiting for responses: map[]
Apr  6 10:55:52.696: INFO: reached 10.96.4.10 after 0/1 tries
Apr  6 10:55:52.696: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Apr  6 10:55:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6771" for this suite. 04/06/23 10:55:52.715
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":160,"skipped":2942,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.823 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:55:25.897
    Apr  6 10:55:25.897: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pod-network-test 04/06/23 10:55:25.898
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:25.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:25.929
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6771 04/06/23 10:55:25.94
    STEP: creating a selector 04/06/23 10:55:25.942
    STEP: Creating the service pods in kubernetes 04/06/23 10:55:25.942
    Apr  6 10:55:25.942: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr  6 10:55:26.054: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6771" to be "running and ready"
    Apr  6 10:55:26.060: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.550507ms
    Apr  6 10:55:26.060: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:55:28.066: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011181592s
    Apr  6 10:55:28.066: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:55:30.066: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011865536s
    Apr  6 10:55:30.066: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:55:32.067: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012665616s
    Apr  6 10:55:32.067: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:55:34.067: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012144545s
    Apr  6 10:55:34.067: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:55:36.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014790309s
    Apr  6 10:55:36.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 10:55:38.066: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011886298s
    Apr  6 10:55:38.066: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr  6 10:55:38.066: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr  6 10:55:38.070: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6771" to be "running and ready"
    Apr  6 10:55:38.080: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 10.49671ms
    Apr  6 10:55:38.081: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Apr  6 10:55:40.087: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.016949399s
    Apr  6 10:55:40.087: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Apr  6 10:55:42.087: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.017339944s
    Apr  6 10:55:42.087: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Apr  6 10:55:44.088: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.017646942s
    Apr  6 10:55:44.088: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Apr  6 10:55:46.090: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.019758227s
    Apr  6 10:55:46.090: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Apr  6 10:55:48.105: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.035551922s
    Apr  6 10:55:48.105: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr  6 10:55:48.105: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr  6 10:55:48.120: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6771" to be "running and ready"
    Apr  6 10:55:48.132: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 12.293447ms
    Apr  6 10:55:48.132: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr  6 10:55:48.132: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr  6 10:55:48.136: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6771" to be "running and ready"
    Apr  6 10:55:48.140: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.792085ms
    Apr  6 10:55:48.140: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr  6 10:55:48.140: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr  6 10:55:48.147: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6771" to be "running and ready"
    Apr  6 10:55:48.151: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.382158ms
    Apr  6 10:55:48.151: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr  6 10:55:48.151: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 04/06/23 10:55:48.155
    Apr  6 10:55:48.164: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6771" to be "running"
    Apr  6 10:55:48.176: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.645391ms
    Apr  6 10:55:50.186: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018008251s
    Apr  6 10:55:50.190: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr  6 10:55:50.196: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Apr  6 10:55:50.196: INFO: Breadth first check of 10.96.3.11 on host 10.250.2.236...
    Apr  6 10:55:50.201: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.3.11&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:55:50.201: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:55:50.202: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:55:50.202: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.3.11%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 10:55:50.645: INFO: Waiting for responses: map[]
    Apr  6 10:55:50.645: INFO: reached 10.96.3.11 after 0/1 tries
    Apr  6 10:55:50.645: INFO: Breadth first check of 10.96.2.85 on host 10.250.0.68...
    Apr  6 10:55:50.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.2.85&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:55:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:55:50.652: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:55:50.652: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.2.85%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 10:55:51.132: INFO: Waiting for responses: map[]
    Apr  6 10:55:51.132: INFO: reached 10.96.2.85 after 0/1 tries
    Apr  6 10:55:51.132: INFO: Breadth first check of 10.96.0.17 on host 10.250.2.89...
    Apr  6 10:55:51.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.0.17&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:55:51.138: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:55:51.138: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:55:51.138: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.0.17%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 10:55:51.706: INFO: Waiting for responses: map[]
    Apr  6 10:55:51.706: INFO: reached 10.96.0.17 after 0/1 tries
    Apr  6 10:55:51.706: INFO: Breadth first check of 10.96.1.83 on host 10.250.1.148...
    Apr  6 10:55:51.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.1.83&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:55:51.718: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:55:51.719: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:55:51.719: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.1.83%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 10:55:52.213: INFO: Waiting for responses: map[]
    Apr  6 10:55:52.213: INFO: reached 10.96.1.83 after 0/1 tries
    Apr  6 10:55:52.213: INFO: Breadth first check of 10.96.4.10 on host 10.250.3.220...
    Apr  6 10:55:52.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.84:9080/dial?request=hostname&protocol=udp&host=10.96.4.10&port=8081&tries=1'] Namespace:pod-network-test-6771 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 10:55:52.218: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 10:55:52.218: INFO: ExecWithOptions: Clientset creation
    Apr  6 10:55:52.218: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-6771/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.84%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.96.4.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 10:55:52.695: INFO: Waiting for responses: map[]
    Apr  6 10:55:52.696: INFO: reached 10.96.4.10 after 0/1 tries
    Apr  6 10:55:52.696: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Apr  6 10:55:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6771" for this suite. 04/06/23 10:55:52.715
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:55:52.721
Apr  6 10:55:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replicaset 04/06/23 10:55:52.722
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:52.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:52.743
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 04/06/23 10:55:52.753
STEP: Verify that the required pods have come up. 04/06/23 10:55:52.758
Apr  6 10:55:52.763: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 10:55:57.769: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/06/23 10:55:57.772
STEP: Getting /status 04/06/23 10:55:57.772
Apr  6 10:55:57.778: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 04/06/23 10:55:57.778
Apr  6 10:55:57.791: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 04/06/23 10:55:57.791
Apr  6 10:55:57.797: INFO: Observed &ReplicaSet event: ADDED
Apr  6 10:55:57.797: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 10:55:57.797: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 10:55:57.798: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 10:55:57.798: INFO: Found replicaset test-rs in namespace replicaset-7126 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 10:55:57.798: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 04/06/23 10:55:57.798
Apr  6 10:55:57.798: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr  6 10:55:57.804: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 04/06/23 10:55:57.804
Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: ADDED
Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 10:55:57.807: INFO: Observed replicaset test-rs in namespace replicaset-7126 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
Apr  6 10:55:57.807: INFO: Found replicaset test-rs in namespace replicaset-7126 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Apr  6 10:55:57.807: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Apr  6 10:55:57.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7126" for this suite. 04/06/23 10:55:57.815
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":161,"skipped":2943,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.101 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:55:52.721
    Apr  6 10:55:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replicaset 04/06/23 10:55:52.722
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:52.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:52.743
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 04/06/23 10:55:52.753
    STEP: Verify that the required pods have come up. 04/06/23 10:55:52.758
    Apr  6 10:55:52.763: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr  6 10:55:57.769: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/06/23 10:55:57.772
    STEP: Getting /status 04/06/23 10:55:57.772
    Apr  6 10:55:57.778: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 04/06/23 10:55:57.778
    Apr  6 10:55:57.791: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 04/06/23 10:55:57.791
    Apr  6 10:55:57.797: INFO: Observed &ReplicaSet event: ADDED
    Apr  6 10:55:57.797: INFO: Observed &ReplicaSet event: MODIFIED
    Apr  6 10:55:57.797: INFO: Observed &ReplicaSet event: MODIFIED
    Apr  6 10:55:57.798: INFO: Observed &ReplicaSet event: MODIFIED
    Apr  6 10:55:57.798: INFO: Found replicaset test-rs in namespace replicaset-7126 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr  6 10:55:57.798: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 04/06/23 10:55:57.798
    Apr  6 10:55:57.798: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr  6 10:55:57.804: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 04/06/23 10:55:57.804
    Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: ADDED
    Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
    Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
    Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
    Apr  6 10:55:57.807: INFO: Observed replicaset test-rs in namespace replicaset-7126 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr  6 10:55:57.807: INFO: Observed &ReplicaSet event: MODIFIED
    Apr  6 10:55:57.807: INFO: Found replicaset test-rs in namespace replicaset-7126 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Apr  6 10:55:57.807: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Apr  6 10:55:57.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7126" for this suite. 04/06/23 10:55:57.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:55:57.823
Apr  6 10:55:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:55:57.824
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:57.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:57.843
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5805f726-5973-464b-982d-8638d2431c54 04/06/23 10:55:57.869
STEP: Creating the pod 04/06/23 10:55:57.879
Apr  6 10:55:57.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441" in namespace "projected-1112" to be "running and ready"
Apr  6 10:55:57.895: INFO: Pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09947ms
Apr  6 10:55:57.895: INFO: The phase of Pod pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:55:59.902: INFO: Pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441": Phase="Running", Reason="", readiness=true. Elapsed: 2.013151951s
Apr  6 10:55:59.902: INFO: The phase of Pod pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441 is Running (Ready = true)
Apr  6 10:55:59.902: INFO: Pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-5805f726-5973-464b-982d-8638d2431c54 04/06/23 10:55:59.932
STEP: waiting to observe update in volume 04/06/23 10:55:59.937
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 10:56:02.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1112" for this suite. 04/06/23 10:56:02.091
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":162,"skipped":2969,"failed":0}
------------------------------
â€¢ [4.274 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:55:57.823
    Apr  6 10:55:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:55:57.824
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:55:57.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:55:57.843
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-5805f726-5973-464b-982d-8638d2431c54 04/06/23 10:55:57.869
    STEP: Creating the pod 04/06/23 10:55:57.879
    Apr  6 10:55:57.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441" in namespace "projected-1112" to be "running and ready"
    Apr  6 10:55:57.895: INFO: Pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09947ms
    Apr  6 10:55:57.895: INFO: The phase of Pod pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:55:59.902: INFO: Pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441": Phase="Running", Reason="", readiness=true. Elapsed: 2.013151951s
    Apr  6 10:55:59.902: INFO: The phase of Pod pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441 is Running (Ready = true)
    Apr  6 10:55:59.902: INFO: Pod "pod-projected-configmaps-e2e6b44f-7c50-4198-8e54-8e0ba03fb441" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-5805f726-5973-464b-982d-8638d2431c54 04/06/23 10:55:59.932
    STEP: waiting to observe update in volume 04/06/23 10:55:59.937
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 10:56:02.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1112" for this suite. 04/06/23 10:56:02.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:02.1
Apr  6 10:56:02.100: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:56:02.101
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:02.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:02.126
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 04/06/23 10:56:02.132
Apr  6 10:56:02.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 create -f -'
Apr  6 10:56:02.955: INFO: stderr: ""
Apr  6 10:56:02.955: INFO: stdout: "pod/pause created\n"
Apr  6 10:56:02.955: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr  6 10:56:02.955: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-378" to be "running and ready"
Apr  6 10:56:02.960: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078026ms
Apr  6 10:56:02.960: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk' to be 'Running' but was 'Pending'
Apr  6 10:56:04.969: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.013367687s
Apr  6 10:56:04.969: INFO: Pod "pause" satisfied condition "running and ready"
Apr  6 10:56:04.969: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 04/06/23 10:56:04.969
Apr  6 10:56:04.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 label pods pause testing-label=testing-label-value'
Apr  6 10:56:05.261: INFO: stderr: ""
Apr  6 10:56:05.261: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 04/06/23 10:56:05.261
Apr  6 10:56:05.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get pod pause -L testing-label'
Apr  6 10:56:05.473: INFO: stderr: ""
Apr  6 10:56:05.477: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 04/06/23 10:56:05.477
Apr  6 10:56:05.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 label pods pause testing-label-'
Apr  6 10:56:05.718: INFO: stderr: ""
Apr  6 10:56:05.718: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 04/06/23 10:56:05.718
Apr  6 10:56:05.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get pod pause -L testing-label'
Apr  6 10:56:05.856: INFO: stderr: ""
Apr  6 10:56:05.856: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 04/06/23 10:56:05.856
Apr  6 10:56:05.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 delete --grace-period=0 --force -f -'
Apr  6 10:56:06.089: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 10:56:06.089: INFO: stdout: "pod \"pause\" force deleted\n"
Apr  6 10:56:06.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get rc,svc -l name=pause --no-headers'
Apr  6 10:56:06.318: INFO: stderr: "No resources found in kubectl-378 namespace.\n"
Apr  6 10:56:06.318: INFO: stdout: ""
Apr  6 10:56:06.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 10:56:06.463: INFO: stderr: ""
Apr  6 10:56:06.463: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:56:06.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-378" for this suite. 04/06/23 10:56:06.472
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":163,"skipped":3027,"failed":0}
------------------------------
â€¢ [4.379 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:02.1
    Apr  6 10:56:02.100: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:56:02.101
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:02.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:02.126
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 04/06/23 10:56:02.132
    Apr  6 10:56:02.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 create -f -'
    Apr  6 10:56:02.955: INFO: stderr: ""
    Apr  6 10:56:02.955: INFO: stdout: "pod/pause created\n"
    Apr  6 10:56:02.955: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Apr  6 10:56:02.955: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-378" to be "running and ready"
    Apr  6 10:56:02.960: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078026ms
    Apr  6 10:56:02.960: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk' to be 'Running' but was 'Pending'
    Apr  6 10:56:04.969: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.013367687s
    Apr  6 10:56:04.969: INFO: Pod "pause" satisfied condition "running and ready"
    Apr  6 10:56:04.969: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 04/06/23 10:56:04.969
    Apr  6 10:56:04.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 label pods pause testing-label=testing-label-value'
    Apr  6 10:56:05.261: INFO: stderr: ""
    Apr  6 10:56:05.261: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 04/06/23 10:56:05.261
    Apr  6 10:56:05.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get pod pause -L testing-label'
    Apr  6 10:56:05.473: INFO: stderr: ""
    Apr  6 10:56:05.477: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 04/06/23 10:56:05.477
    Apr  6 10:56:05.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 label pods pause testing-label-'
    Apr  6 10:56:05.718: INFO: stderr: ""
    Apr  6 10:56:05.718: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 04/06/23 10:56:05.718
    Apr  6 10:56:05.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get pod pause -L testing-label'
    Apr  6 10:56:05.856: INFO: stderr: ""
    Apr  6 10:56:05.856: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 04/06/23 10:56:05.856
    Apr  6 10:56:05.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 delete --grace-period=0 --force -f -'
    Apr  6 10:56:06.089: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 10:56:06.089: INFO: stdout: "pod \"pause\" force deleted\n"
    Apr  6 10:56:06.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get rc,svc -l name=pause --no-headers'
    Apr  6 10:56:06.318: INFO: stderr: "No resources found in kubectl-378 namespace.\n"
    Apr  6 10:56:06.318: INFO: stdout: ""
    Apr  6 10:56:06.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-378 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr  6 10:56:06.463: INFO: stderr: ""
    Apr  6 10:56:06.463: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:56:06.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-378" for this suite. 04/06/23 10:56:06.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:06.48
Apr  6 10:56:06.480: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 10:56:06.481
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:06.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:06.504
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Apr  6 10:56:06.511: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: creating the pod 04/06/23 10:56:06.512
STEP: submitting the pod to kubernetes 04/06/23 10:56:06.512
Apr  6 10:56:06.522: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e" in namespace "pods-3831" to be "running and ready"
Apr  6 10:56:06.526: INFO: Pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.438301ms
Apr  6 10:56:06.526: INFO: The phase of Pod pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:56:08.532: INFO: Pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010202727s
Apr  6 10:56:08.532: INFO: The phase of Pod pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e is Running (Ready = true)
Apr  6 10:56:08.532: INFO: Pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 10:56:08.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3831" for this suite. 04/06/23 10:56:08.621
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":164,"skipped":3032,"failed":0}
------------------------------
â€¢ [2.148 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:06.48
    Apr  6 10:56:06.480: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 10:56:06.481
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:06.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:06.504
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Apr  6 10:56:06.511: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: creating the pod 04/06/23 10:56:06.512
    STEP: submitting the pod to kubernetes 04/06/23 10:56:06.512
    Apr  6 10:56:06.522: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e" in namespace "pods-3831" to be "running and ready"
    Apr  6 10:56:06.526: INFO: Pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.438301ms
    Apr  6 10:56:06.526: INFO: The phase of Pod pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:56:08.532: INFO: Pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010202727s
    Apr  6 10:56:08.532: INFO: The phase of Pod pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e is Running (Ready = true)
    Apr  6 10:56:08.532: INFO: Pod "pod-logs-websocket-5cdf498f-a4fb-412f-b877-9e6000be5d0e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 10:56:08.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3831" for this suite. 04/06/23 10:56:08.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:08.629
Apr  6 10:56:08.629: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:56:08.63
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:08.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:08.661
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 04/06/23 10:56:08.667
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/06/23 10:56:08.67
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/06/23 10:56:08.67
STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/06/23 10:56:08.67
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/06/23 10:56:08.672
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/06/23 10:56:08.672
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/06/23 10:56:08.675
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:56:08.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-424" for this suite. 04/06/23 10:56:08.681
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":165,"skipped":3050,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:08.629
    Apr  6 10:56:08.629: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 10:56:08.63
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:08.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:08.661
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 04/06/23 10:56:08.667
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/06/23 10:56:08.67
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/06/23 10:56:08.67
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/06/23 10:56:08.67
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/06/23 10:56:08.672
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/06/23 10:56:08.672
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/06/23 10:56:08.675
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:56:08.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-424" for this suite. 04/06/23 10:56:08.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:08.708
Apr  6 10:56:08.708: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:56:08.709
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:08.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:08.737
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:56:08.744
Apr  6 10:56:08.755: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36" in namespace "projected-5975" to be "Succeeded or Failed"
Apr  6 10:56:08.765: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36": Phase="Pending", Reason="", readiness=false. Elapsed: 9.36722ms
Apr  6 10:56:10.771: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015154343s
Apr  6 10:56:12.771: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015414428s
STEP: Saw pod success 04/06/23 10:56:12.771
Apr  6 10:56:12.771: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36" satisfied condition "Succeeded or Failed"
Apr  6 10:56:12.778: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36 container client-container: <nil>
STEP: delete the pod 04/06/23 10:56:12.794
Apr  6 10:56:12.811: INFO: Waiting for pod downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36 to disappear
Apr  6 10:56:12.819: INFO: Pod downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 10:56:12.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5975" for this suite. 04/06/23 10:56:12.832
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":166,"skipped":3083,"failed":0}
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:08.708
    Apr  6 10:56:08.708: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:56:08.709
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:08.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:08.737
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:56:08.744
    Apr  6 10:56:08.755: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36" in namespace "projected-5975" to be "Succeeded or Failed"
    Apr  6 10:56:08.765: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36": Phase="Pending", Reason="", readiness=false. Elapsed: 9.36722ms
    Apr  6 10:56:10.771: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015154343s
    Apr  6 10:56:12.771: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015414428s
    STEP: Saw pod success 04/06/23 10:56:12.771
    Apr  6 10:56:12.771: INFO: Pod "downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36" satisfied condition "Succeeded or Failed"
    Apr  6 10:56:12.778: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:56:12.794
    Apr  6 10:56:12.811: INFO: Waiting for pod downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36 to disappear
    Apr  6 10:56:12.819: INFO: Pod downwardapi-volume-b717afb4-0339-4040-8a7c-f60c71072e36 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 10:56:12.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5975" for this suite. 04/06/23 10:56:12.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:12.85
Apr  6 10:56:12.850: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:56:12.851
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:12.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:12.873
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 04/06/23 10:56:12.884
Apr  6 10:56:12.885: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: mark a version not serverd 04/06/23 10:56:22.592
STEP: check the unserved version gets removed 04/06/23 10:56:22.644
STEP: check the other version is not changed 04/06/23 10:56:27.021
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:56:34.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6828" for this suite. 04/06/23 10:56:34.325
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":167,"skipped":3103,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.481 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:12.85
    Apr  6 10:56:12.850: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 10:56:12.851
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:12.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:12.873
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 04/06/23 10:56:12.884
    Apr  6 10:56:12.885: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: mark a version not serverd 04/06/23 10:56:22.592
    STEP: check the unserved version gets removed 04/06/23 10:56:22.644
    STEP: check the other version is not changed 04/06/23 10:56:27.021
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:56:34.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6828" for this suite. 04/06/23 10:56:34.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:34.333
Apr  6 10:56:34.333: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replicaset 04/06/23 10:56:34.335
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:34.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:34.352
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 04/06/23 10:56:34.357
STEP: Verify that the required pods have come up 04/06/23 10:56:34.362
Apr  6 10:56:34.366: INFO: Pod name sample-pod: Found 0 pods out of 3
Apr  6 10:56:39.372: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 04/06/23 10:56:39.372
Apr  6 10:56:39.375: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 04/06/23 10:56:39.375
STEP: DeleteCollection of the ReplicaSets 04/06/23 10:56:39.381
STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/06/23 10:56:39.387
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Apr  6 10:56:39.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1107" for this suite. 04/06/23 10:56:39.401
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":168,"skipped":3122,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.087 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:34.333
    Apr  6 10:56:34.333: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replicaset 04/06/23 10:56:34.335
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:34.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:34.352
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 04/06/23 10:56:34.357
    STEP: Verify that the required pods have come up 04/06/23 10:56:34.362
    Apr  6 10:56:34.366: INFO: Pod name sample-pod: Found 0 pods out of 3
    Apr  6 10:56:39.372: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 04/06/23 10:56:39.372
    Apr  6 10:56:39.375: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 04/06/23 10:56:39.375
    STEP: DeleteCollection of the ReplicaSets 04/06/23 10:56:39.381
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/06/23 10:56:39.387
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Apr  6 10:56:39.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1107" for this suite. 04/06/23 10:56:39.401
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:39.426
Apr  6 10:56:39.426: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubelet-test 04/06/23 10:56:39.427
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:39.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:39.445
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Apr  6 10:56:39.467: INFO: Waiting up to 5m0s for pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921" in namespace "kubelet-test-9888" to be "running and ready"
Apr  6 10:56:39.472: INFO: Pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921": Phase="Pending", Reason="", readiness=false. Elapsed: 4.899199ms
Apr  6 10:56:39.472: INFO: The phase of Pod busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:56:41.485: INFO: Pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921": Phase="Running", Reason="", readiness=true. Elapsed: 2.017744325s
Apr  6 10:56:41.485: INFO: The phase of Pod busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921 is Running (Ready = true)
Apr  6 10:56:41.485: INFO: Pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Apr  6 10:56:41.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9888" for this suite. 04/06/23 10:56:41.524
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":169,"skipped":3125,"failed":0}
------------------------------
â€¢ [2.110 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:39.426
    Apr  6 10:56:39.426: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubelet-test 04/06/23 10:56:39.427
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:39.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:39.445
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Apr  6 10:56:39.467: INFO: Waiting up to 5m0s for pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921" in namespace "kubelet-test-9888" to be "running and ready"
    Apr  6 10:56:39.472: INFO: Pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921": Phase="Pending", Reason="", readiness=false. Elapsed: 4.899199ms
    Apr  6 10:56:39.472: INFO: The phase of Pod busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:56:41.485: INFO: Pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921": Phase="Running", Reason="", readiness=true. Elapsed: 2.017744325s
    Apr  6 10:56:41.485: INFO: The phase of Pod busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921 is Running (Ready = true)
    Apr  6 10:56:41.485: INFO: Pod "busybox-scheduling-be60c69c-8627-4c33-8c7d-8be162fd4921" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Apr  6 10:56:41.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9888" for this suite. 04/06/23 10:56:41.524
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:56:41.536
Apr  6 10:56:41.536: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-preemption 04/06/23 10:56:41.537
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:41.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:41.556
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Apr  6 10:56:41.579: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 10:57:41.666: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 04/06/23 10:57:41.673
Apr  6 10:57:41.715: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr  6 10:57:41.722: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr  6 10:57:41.745: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr  6 10:57:41.754: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr  6 10:57:41.773: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr  6 10:57:41.780: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Apr  6 10:57:41.799: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Apr  6 10:57:41.810: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Apr  6 10:57:41.832: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Apr  6 10:57:41.840: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/06/23 10:57:41.841
Apr  6 10:57:41.842: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:41.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.190462ms
Apr  6 10:57:43.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013235592s
Apr  6 10:57:45.858: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015012931s
Apr  6 10:57:47.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012752332s
Apr  6 10:57:49.859: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016048258s
Apr  6 10:57:51.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012765981s
Apr  6 10:57:53.865: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.021564415s
Apr  6 10:57:53.865: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr  6 10:57:53.865: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.268392ms
Apr  6 10:57:53.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.875: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.771649ms
Apr  6 10:57:53.875: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.875: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.880: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.655647ms
Apr  6 10:57:53.880: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.880: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.885: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.741724ms
Apr  6 10:57:53.885: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.885: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.888: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.529091ms
Apr  6 10:57:53.888: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.888: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.892: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.599806ms
Apr  6 10:57:53.892: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.892: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.907: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.620273ms
Apr  6 10:57:53.907: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.907: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.926: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.042366ms
Apr  6 10:57:53.926: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 10:57:53.926: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.934: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.244745ms
Apr  6 10:57:53.934: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/06/23 10:57:53.934
Apr  6 10:57:53.948: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4544" to be "running"
Apr  6 10:57:53.952: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121781ms
Apr  6 10:57:55.963: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015470448s
Apr  6 10:57:57.962: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014071809s
Apr  6 10:57:59.958: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009945893s
Apr  6 10:57:59.958: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Apr  6 10:57:59.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4544" for this suite. 04/06/23 10:58:00.005
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":170,"skipped":3126,"failed":0}
------------------------------
â€¢ [SLOW TEST] [78.562 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:56:41.536
    Apr  6 10:56:41.536: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-preemption 04/06/23 10:56:41.537
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:56:41.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:56:41.556
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Apr  6 10:56:41.579: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr  6 10:57:41.666: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 04/06/23 10:57:41.673
    Apr  6 10:57:41.715: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr  6 10:57:41.722: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr  6 10:57:41.745: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr  6 10:57:41.754: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Apr  6 10:57:41.773: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Apr  6 10:57:41.780: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Apr  6 10:57:41.799: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Apr  6 10:57:41.810: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Apr  6 10:57:41.832: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Apr  6 10:57:41.840: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/06/23 10:57:41.841
    Apr  6 10:57:41.842: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:41.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.190462ms
    Apr  6 10:57:43.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013235592s
    Apr  6 10:57:45.858: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015012931s
    Apr  6 10:57:47.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012752332s
    Apr  6 10:57:49.859: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016048258s
    Apr  6 10:57:51.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012765981s
    Apr  6 10:57:53.865: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.021564415s
    Apr  6 10:57:53.865: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr  6 10:57:53.865: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.268392ms
    Apr  6 10:57:53.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.875: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.771649ms
    Apr  6 10:57:53.875: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.875: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.880: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.655647ms
    Apr  6 10:57:53.880: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.880: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.885: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.741724ms
    Apr  6 10:57:53.885: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.885: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.888: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.529091ms
    Apr  6 10:57:53.888: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.888: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.892: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.599806ms
    Apr  6 10:57:53.892: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.892: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.907: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.620273ms
    Apr  6 10:57:53.907: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.907: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.926: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.042366ms
    Apr  6 10:57:53.926: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 10:57:53.926: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.934: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.244745ms
    Apr  6 10:57:53.934: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/06/23 10:57:53.934
    Apr  6 10:57:53.948: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4544" to be "running"
    Apr  6 10:57:53.952: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121781ms
    Apr  6 10:57:55.963: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015470448s
    Apr  6 10:57:57.962: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014071809s
    Apr  6 10:57:59.958: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009945893s
    Apr  6 10:57:59.958: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 10:57:59.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4544" for this suite. 04/06/23 10:58:00.005
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:58:00.099
Apr  6 10:58:00.099: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename security-context-test 04/06/23 10:58:00.1
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:00.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:00.116
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Apr  6 10:58:00.130: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561" in namespace "security-context-test-2581" to be "Succeeded or Failed"
Apr  6 10:58:00.138: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008769ms
Apr  6 10:58:02.145: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014191594s
Apr  6 10:58:04.148: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017136391s
Apr  6 10:58:04.148: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Apr  6 10:58:04.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2581" for this suite. 04/06/23 10:58:04.155
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":171,"skipped":3135,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:58:00.099
    Apr  6 10:58:00.099: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename security-context-test 04/06/23 10:58:00.1
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:00.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:00.116
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Apr  6 10:58:00.130: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561" in namespace "security-context-test-2581" to be "Succeeded or Failed"
    Apr  6 10:58:00.138: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008769ms
    Apr  6 10:58:02.145: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014191594s
    Apr  6 10:58:04.148: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017136391s
    Apr  6 10:58:04.148: INFO: Pod "busybox-readonly-false-ee7244a9-b85b-4a36-a75a-da5d841a6561" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Apr  6 10:58:04.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2581" for this suite. 04/06/23 10:58:04.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:58:04.164
Apr  6 10:58:04.164: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 10:58:04.165
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:04.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:04.196
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/06/23 10:58:04.201
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/06/23 10:58:04.201
STEP: creating a pod to probe DNS 04/06/23 10:58:04.201
STEP: submitting the pod to kubernetes 04/06/23 10:58:04.201
Apr  6 10:58:04.214: INFO: Waiting up to 15m0s for pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3" in namespace "dns-4137" to be "running"
Apr  6 10:58:04.219: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.97685ms
Apr  6 10:58:06.245: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031207489s
Apr  6 10:58:08.228: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3": Phase="Running", Reason="", readiness=true. Elapsed: 4.013518611s
Apr  6 10:58:08.228: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3" satisfied condition "running"
STEP: retrieving the pod 04/06/23 10:58:08.228
STEP: looking for the results for each expected name from probers 04/06/23 10:58:08.232
Apr  6 10:58:08.396: INFO: DNS probes using dns-4137/dns-test-5381d222-202b-4058-be81-4dd1a66abdc3 succeeded

STEP: deleting the pod 04/06/23 10:58:08.396
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 10:58:08.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4137" for this suite. 04/06/23 10:58:08.414
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":172,"skipped":3166,"failed":0}
------------------------------
â€¢ [4.256 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:58:04.164
    Apr  6 10:58:04.164: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 10:58:04.165
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:04.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:04.196
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/06/23 10:58:04.201
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/06/23 10:58:04.201
    STEP: creating a pod to probe DNS 04/06/23 10:58:04.201
    STEP: submitting the pod to kubernetes 04/06/23 10:58:04.201
    Apr  6 10:58:04.214: INFO: Waiting up to 15m0s for pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3" in namespace "dns-4137" to be "running"
    Apr  6 10:58:04.219: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.97685ms
    Apr  6 10:58:06.245: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031207489s
    Apr  6 10:58:08.228: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3": Phase="Running", Reason="", readiness=true. Elapsed: 4.013518611s
    Apr  6 10:58:08.228: INFO: Pod "dns-test-5381d222-202b-4058-be81-4dd1a66abdc3" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 10:58:08.228
    STEP: looking for the results for each expected name from probers 04/06/23 10:58:08.232
    Apr  6 10:58:08.396: INFO: DNS probes using dns-4137/dns-test-5381d222-202b-4058-be81-4dd1a66abdc3 succeeded

    STEP: deleting the pod 04/06/23 10:58:08.396
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 10:58:08.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4137" for this suite. 04/06/23 10:58:08.414
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:58:08.428
Apr  6 10:58:08.429: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:58:08.43
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:08.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:08.461
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-7af28cfe-2d2e-48e5-a76f-522169d24117 04/06/23 10:58:08.466
STEP: Creating a pod to test consume secrets 04/06/23 10:58:08.472
Apr  6 10:58:08.483: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69" in namespace "projected-3813" to be "Succeeded or Failed"
Apr  6 10:58:08.491: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.303525ms
Apr  6 10:58:10.499: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015099972s
Apr  6 10:58:12.503: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019205878s
STEP: Saw pod success 04/06/23 10:58:12.503
Apr  6 10:58:12.503: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69" satisfied condition "Succeeded or Failed"
Apr  6 10:58:12.507: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69 container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 10:58:12.519
Apr  6 10:58:12.529: INFO: Waiting for pod pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69 to disappear
Apr  6 10:58:12.532: INFO: Pod pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Apr  6 10:58:12.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3813" for this suite. 04/06/23 10:58:12.553
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":173,"skipped":3167,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:58:08.428
    Apr  6 10:58:08.429: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:58:08.43
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:08.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:08.461
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-7af28cfe-2d2e-48e5-a76f-522169d24117 04/06/23 10:58:08.466
    STEP: Creating a pod to test consume secrets 04/06/23 10:58:08.472
    Apr  6 10:58:08.483: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69" in namespace "projected-3813" to be "Succeeded or Failed"
    Apr  6 10:58:08.491: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69": Phase="Pending", Reason="", readiness=false. Elapsed: 7.303525ms
    Apr  6 10:58:10.499: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015099972s
    Apr  6 10:58:12.503: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019205878s
    STEP: Saw pod success 04/06/23 10:58:12.503
    Apr  6 10:58:12.503: INFO: Pod "pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69" satisfied condition "Succeeded or Failed"
    Apr  6 10:58:12.507: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69 container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 10:58:12.519
    Apr  6 10:58:12.529: INFO: Waiting for pod pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69 to disappear
    Apr  6 10:58:12.532: INFO: Pod pod-projected-secrets-7149af84-2225-4281-add9-76054ea45c69 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Apr  6 10:58:12.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3813" for this suite. 04/06/23 10:58:12.553
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:58:12.575
Apr  6 10:58:12.575: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:58:12.577
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:12.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:12.612
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-1986 04/06/23 10:58:12.623
Apr  6 10:58:12.634: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1986" to be "running and ready"
Apr  6 10:58:12.637: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156477ms
Apr  6 10:58:12.637: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Apr  6 10:58:14.642: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008186316s
Apr  6 10:58:14.642: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Apr  6 10:58:14.642: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Apr  6 10:58:14.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr  6 10:58:15.238: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr  6 10:58:15.238: INFO: stdout: "iptables"
Apr  6 10:58:15.238: INFO: proxyMode: iptables
Apr  6 10:58:15.251: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr  6 10:58:15.255: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1986 04/06/23 10:58:15.255
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1986 04/06/23 10:58:15.265
I0406 10:58:15.271774      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1986, replica count: 3
I0406 10:58:18.323034      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 10:58:18.339: INFO: Creating new exec pod
Apr  6 10:58:18.346: INFO: Waiting up to 5m0s for pod "execpod-affinity9xkp2" in namespace "services-1986" to be "running"
Apr  6 10:58:18.351: INFO: Pod "execpod-affinity9xkp2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.760598ms
Apr  6 10:58:20.357: INFO: Pod "execpod-affinity9xkp2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010883842s
Apr  6 10:58:20.357: INFO: Pod "execpod-affinity9xkp2" satisfied condition "running"
Apr  6 10:58:21.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Apr  6 10:58:21.934: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr  6 10:58:21.934: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:58:21.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.70.70.59 80'
Apr  6 10:58:22.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.70.70.59 80\nConnection to 10.70.70.59 80 port [tcp/http] succeeded!\n"
Apr  6 10:58:22.567: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:58:22.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.70.70.59:80/ ; done'
Apr  6 10:58:23.394: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n"
Apr  6 10:58:23.394: INFO: stdout: "\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6"
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
Apr  6 10:58:23.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.70.70.59:80/'
Apr  6 10:58:24.045: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n"
Apr  6 10:58:24.045: INFO: stdout: "affinity-clusterip-timeout-wptl6"
Apr  6 10:58:44.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.70.70.59:80/'
Apr  6 10:58:44.610: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n"
Apr  6 10:58:44.610: INFO: stdout: "affinity-clusterip-timeout-n56px"
Apr  6 10:58:44.610: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1986, will wait for the garbage collector to delete the pods 04/06/23 10:58:44.62
Apr  6 10:58:44.682: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.622019ms
Apr  6 10:58:44.782: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.138723ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:58:47.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1986" for this suite. 04/06/23 10:58:47.307
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":174,"skipped":3170,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.738 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:58:12.575
    Apr  6 10:58:12.575: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:58:12.577
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:12.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:12.612
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-1986 04/06/23 10:58:12.623
    Apr  6 10:58:12.634: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1986" to be "running and ready"
    Apr  6 10:58:12.637: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156477ms
    Apr  6 10:58:12.637: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 10:58:14.642: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008186316s
    Apr  6 10:58:14.642: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Apr  6 10:58:14.642: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Apr  6 10:58:14.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Apr  6 10:58:15.238: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Apr  6 10:58:15.238: INFO: stdout: "iptables"
    Apr  6 10:58:15.238: INFO: proxyMode: iptables
    Apr  6 10:58:15.251: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Apr  6 10:58:15.255: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-1986 04/06/23 10:58:15.255
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-1986 04/06/23 10:58:15.265
    I0406 10:58:15.271774      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1986, replica count: 3
    I0406 10:58:18.323034      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 10:58:18.339: INFO: Creating new exec pod
    Apr  6 10:58:18.346: INFO: Waiting up to 5m0s for pod "execpod-affinity9xkp2" in namespace "services-1986" to be "running"
    Apr  6 10:58:18.351: INFO: Pod "execpod-affinity9xkp2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.760598ms
    Apr  6 10:58:20.357: INFO: Pod "execpod-affinity9xkp2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010883842s
    Apr  6 10:58:20.357: INFO: Pod "execpod-affinity9xkp2" satisfied condition "running"
    Apr  6 10:58:21.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Apr  6 10:58:21.934: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Apr  6 10:58:21.934: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:58:21.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.70.70.59 80'
    Apr  6 10:58:22.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.70.70.59 80\nConnection to 10.70.70.59 80 port [tcp/http] succeeded!\n"
    Apr  6 10:58:22.567: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:58:22.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.70.70.59:80/ ; done'
    Apr  6 10:58:23.394: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n"
    Apr  6 10:58:23.394: INFO: stdout: "\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6\naffinity-clusterip-timeout-wptl6"
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Received response from host: affinity-clusterip-timeout-wptl6
    Apr  6 10:58:23.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.70.70.59:80/'
    Apr  6 10:58:24.045: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n"
    Apr  6 10:58:24.045: INFO: stdout: "affinity-clusterip-timeout-wptl6"
    Apr  6 10:58:44.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1986 exec execpod-affinity9xkp2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.70.70.59:80/'
    Apr  6 10:58:44.610: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.70.70.59:80/\n"
    Apr  6 10:58:44.610: INFO: stdout: "affinity-clusterip-timeout-n56px"
    Apr  6 10:58:44.610: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1986, will wait for the garbage collector to delete the pods 04/06/23 10:58:44.62
    Apr  6 10:58:44.682: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.622019ms
    Apr  6 10:58:44.782: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.138723ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:58:47.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1986" for this suite. 04/06/23 10:58:47.307
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:58:47.318
Apr  6 10:58:47.318: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 10:58:47.319
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:47.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:47.339
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 04/06/23 10:58:47.345
STEP: Counting existing ResourceQuota 04/06/23 10:58:52.35
STEP: Creating a ResourceQuota 04/06/23 10:58:57.356
STEP: Ensuring resource quota status is calculated 04/06/23 10:58:57.361
STEP: Creating a Secret 04/06/23 10:58:59.369
STEP: Ensuring resource quota status captures secret creation 04/06/23 10:58:59.387
STEP: Deleting a secret 04/06/23 10:59:01.404
STEP: Ensuring resource quota status released usage 04/06/23 10:59:01.409
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 10:59:03.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7715" for this suite. 04/06/23 10:59:03.431
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":175,"skipped":3206,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:58:47.318
    Apr  6 10:58:47.318: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 10:58:47.319
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:58:47.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:58:47.339
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 04/06/23 10:58:47.345
    STEP: Counting existing ResourceQuota 04/06/23 10:58:52.35
    STEP: Creating a ResourceQuota 04/06/23 10:58:57.356
    STEP: Ensuring resource quota status is calculated 04/06/23 10:58:57.361
    STEP: Creating a Secret 04/06/23 10:58:59.369
    STEP: Ensuring resource quota status captures secret creation 04/06/23 10:58:59.387
    STEP: Deleting a secret 04/06/23 10:59:01.404
    STEP: Ensuring resource quota status released usage 04/06/23 10:59:01.409
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 10:59:03.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7715" for this suite. 04/06/23 10:59:03.431
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:03.439
Apr  6 10:59:03.439: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 10:59:03.441
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:03.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:03.461
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Apr  6 10:59:03.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 create -f -'
Apr  6 10:59:04.473: INFO: stderr: ""
Apr  6 10:59:04.473: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr  6 10:59:04.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 create -f -'
Apr  6 10:59:05.395: INFO: stderr: ""
Apr  6 10:59:05.396: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/06/23 10:59:05.396
Apr  6 10:59:06.406: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:59:06.406: INFO: Found 1 / 1
Apr  6 10:59:06.406: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  6 10:59:06.410: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 10:59:06.410: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 10:59:06.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe pod agnhost-primary-4w5mp'
Apr  6 10:59:06.546: INFO: stderr: ""
Apr  6 10:59:06.546: INFO: stdout: "Name:             agnhost-primary-4w5mp\nNamespace:        kubectl-9550\nPriority:         0\nService Account:  default\nNode:             shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk/10.250.1.148\nStart Time:       Thu, 06 Apr 2023 10:59:04 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 0de3fee17327dceb060a6adfdc3a9a199cb783ae9d992c853b6c2ae99740cbbd\n                  cni.projectcalico.org/podIP: 10.96.1.91/32\n                  cni.projectcalico.org/podIPs: 10.96.1.91/32\nStatus:           Running\nIP:               10.96.1.91\nIPs:\n  IP:           10.96.1.91\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://17880e4ad091aefc4107cf729a27b58bbc565878c91e5562b34cc846a2271403\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 06 Apr 2023 10:59:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2hn8g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2hn8g:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9550/agnhost-primary-4w5mp to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr  6 10:59:06.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe rc agnhost-primary'
Apr  6 10:59:06.736: INFO: stderr: ""
Apr  6 10:59:06.736: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9550\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-4w5mp\n"
Apr  6 10:59:06.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe service agnhost-primary'
Apr  6 10:59:06.901: INFO: stderr: ""
Apr  6 10:59:06.901: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9550\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.64.37.2\nIPs:               10.64.37.2\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.96.1.91:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr  6 10:59:06.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st'
Apr  6 10:59:07.152: INFO: stderr: ""
Apr  6 10:59:07.152: INFO: stdout: "Name:               shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.3\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=eu01-2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=false\n                    node.kubernetes.io/instance-type=c1.3\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=eu01-2\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=eu01-2\n                    worker.garden.sapcloud.io/group=pool-5srtzxdh8p\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.25.8\n                    worker.gardener.cloud/pool=pool-5srtzxdh8p\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 7ce780d45bd5d6756955c909b9ec5d92a8389d6892f3be01db042b4ca01b4ef5\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"937c7ebd-b17a-4c5f-9117-f960fada6117\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"false\",\"n...\n                    projectcalico.org/IPv4Address: 10.250.2.236/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.96.3.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 06 Apr 2023 10:13:17 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 06 Apr 2023 10:59:02 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  KernelDeadlock                False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentKubeletRestart        False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  CorruptDockerOverlay2         False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  NetworkUnavailable            False   Thu, 06 Apr 2023 10:13:59 +0000   Thu, 06 Apr 2023 10:13:59 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:17 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:17 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:17 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:48 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.2.236\n  Hostname:    shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      17885708Ki\n  hugepages-2Mi:          0\n  memory:                 8144780Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3920m\n  ephemeral-storage:      17399216729\n  hugepages-2Mi:          0\n  memory:                 6993804Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 b9126cb14acf401db1c0ef579fff1611\n  System UUID:                937c7ebd-b17a-4c5f-9117-f960fada6117\n  Boot ID:                    fadaa452-524c-4e54-b088-27017a3b669b\n  Kernel Version:             5.15.86-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 3374.2.3 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.25.8\n  Kube-Proxy Version:         v1.25.8\nPodCIDR:                      10.96.3.0/24\nPodCIDRs:                     10.96.3.0/24\nProviderID:                   openstack:///937c7ebd-b17a-4c5f-9117-f960fada6117\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 apiserver-proxy-5t8ck                                      40m (1%)      0 (0%)      40Mi (0%)        1114Mi (16%)   45m\n  kube-system                 calico-node-9wkq6                                          250m (6%)     0 (0%)      100Mi (1%)       2800Mi (40%)   45m\n  kube-system                 csi-driver-node-llx9z                                      37m (0%)      0 (0%)      106Mi (1%)       3272Mi (47%)   45m\n  kube-system                 kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz                   20m (0%)      0 (0%)      64Mi (0%)        0 (0%)         45m\n  kube-system                 node-exporter-9qcsn                                        50m (1%)      200m (5%)   50Mi (0%)        200Mi (2%)     45m\n  kube-system                 node-problem-detector-mfcn2                                20m (0%)      0 (0%)      20Mi (0%)        500Mi (7%)     45m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  sonobuoy                    sonobuoy-e2e-job-fa29383266594ee9                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    417m (10%)  200m (5%)\n  memory                 380Mi (5%)  7886Mi (115%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type     Reason                    Age                From                   Message\n  ----     ------                    ----               ----                   -------\n  Normal   Starting                  45m                kube-proxy             \n  Normal   Starting                  45m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity       45m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory   45m (x2 over 45m)  kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure     45m (x2 over 45m)  kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      45m (x2 over 45m)  kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeHasSufficientPID\n  Warning  CheckLimitsForResolvConf  45m                kubelet                open /etc/resolv-for-kubelet.conf: no such file or directory\n  Normal   NodeAllocatableEnforced   45m                kubelet                Updated Node Allocatable limit across pods\n  Normal   Synced                    45m                cloud-node-controller  Node synced successfully\n  Normal   RegisteredNode            45m                node-controller        Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st event: Registered Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st in Controller\n  Normal   NodeReady                 45m                kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeReady\n"
Apr  6 10:59:07.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe namespace kubectl-9550'
Apr  6 10:59:07.356: INFO: stderr: ""
Apr  6 10:59:07.356: INFO: stdout: "Name:         kubectl-9550\nLabels:       e2e-framework=kubectl\n              e2e-run=29d2c07a-ee25-4bc9-9950-c07bb92b6fe9\n              kubernetes.io/metadata.name=kubectl-9550\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 10:59:07.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9550" for this suite. 04/06/23 10:59:07.376
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":176,"skipped":3208,"failed":0}
------------------------------
â€¢ [3.945 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:03.439
    Apr  6 10:59:03.439: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 10:59:03.441
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:03.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:03.461
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Apr  6 10:59:03.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 create -f -'
    Apr  6 10:59:04.473: INFO: stderr: ""
    Apr  6 10:59:04.473: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Apr  6 10:59:04.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 create -f -'
    Apr  6 10:59:05.395: INFO: stderr: ""
    Apr  6 10:59:05.396: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/06/23 10:59:05.396
    Apr  6 10:59:06.406: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:59:06.406: INFO: Found 1 / 1
    Apr  6 10:59:06.406: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr  6 10:59:06.410: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr  6 10:59:06.410: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr  6 10:59:06.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe pod agnhost-primary-4w5mp'
    Apr  6 10:59:06.546: INFO: stderr: ""
    Apr  6 10:59:06.546: INFO: stdout: "Name:             agnhost-primary-4w5mp\nNamespace:        kubectl-9550\nPriority:         0\nService Account:  default\nNode:             shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk/10.250.1.148\nStart Time:       Thu, 06 Apr 2023 10:59:04 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 0de3fee17327dceb060a6adfdc3a9a199cb783ae9d992c853b6c2ae99740cbbd\n                  cni.projectcalico.org/podIP: 10.96.1.91/32\n                  cni.projectcalico.org/podIPs: 10.96.1.91/32\nStatus:           Running\nIP:               10.96.1.91\nIPs:\n  IP:           10.96.1.91\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://17880e4ad091aefc4107cf729a27b58bbc565878c91e5562b34cc846a2271403\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 06 Apr 2023 10:59:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2hn8g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2hn8g:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9550/agnhost-primary-4w5mp to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Apr  6 10:59:06.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe rc agnhost-primary'
    Apr  6 10:59:06.736: INFO: stderr: ""
    Apr  6 10:59:06.736: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9550\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-4w5mp\n"
    Apr  6 10:59:06.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe service agnhost-primary'
    Apr  6 10:59:06.901: INFO: stderr: ""
    Apr  6 10:59:06.901: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9550\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.64.37.2\nIPs:               10.64.37.2\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.96.1.91:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Apr  6 10:59:06.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st'
    Apr  6 10:59:07.152: INFO: stderr: ""
    Apr  6 10:59:07.152: INFO: stdout: "Name:               shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.3\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=eu01-2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\n                    kubernetes.io/os=linux\n                    networking.gardener.cloud/node-local-dns-enabled=false\n                    node.kubernetes.io/instance-type=c1.3\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=eu01-2\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=eu01-2\n                    worker.garden.sapcloud.io/group=pool-5srtzxdh8p\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/kubernetes-version=1.25.8\n                    worker.gardener.cloud/pool=pool-5srtzxdh8p\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 7ce780d45bd5d6756955c909b9ec5d92a8389d6892f3be01db042b4ca01b4ef5\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"937c7ebd-b17a-4c5f-9117-f960fada6117\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"kubernetes.io/arch\":\"amd64\",\"networking.gardener.cloud/node-local-dns-enabled\":\"false\",\"n...\n                    projectcalico.org/IPv4Address: 10.250.2.236/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.96.3.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 06 Apr 2023 10:13:17 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 06 Apr 2023 10:59:02 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  KernelDeadlock                False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentKubeletRestart        False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  CorruptDockerOverlay2         False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Thu, 06 Apr 2023 10:54:20 +0000   Thu, 06 Apr 2023 10:14:13 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  NetworkUnavailable            False   Thu, 06 Apr 2023 10:13:59 +0000   Thu, 06 Apr 2023 10:13:59 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:17 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:17 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:17 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Thu, 06 Apr 2023 10:59:01 +0000   Thu, 06 Apr 2023 10:13:48 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.2.236\n  Hostname:    shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      17885708Ki\n  hugepages-2Mi:          0\n  memory:                 8144780Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3920m\n  ephemeral-storage:      17399216729\n  hugepages-2Mi:          0\n  memory:                 6993804Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 b9126cb14acf401db1c0ef579fff1611\n  System UUID:                937c7ebd-b17a-4c5f-9117-f960fada6117\n  Boot ID:                    fadaa452-524c-4e54-b088-27017a3b669b\n  Kernel Version:             5.15.86-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 3374.2.3 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.25.8\n  Kube-Proxy Version:         v1.25.8\nPodCIDR:                      10.96.3.0/24\nPodCIDRs:                     10.96.3.0/24\nProviderID:                   openstack:///937c7ebd-b17a-4c5f-9117-f960fada6117\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 apiserver-proxy-5t8ck                                      40m (1%)      0 (0%)      40Mi (0%)        1114Mi (16%)   45m\n  kube-system                 calico-node-9wkq6                                          250m (6%)     0 (0%)      100Mi (1%)       2800Mi (40%)   45m\n  kube-system                 csi-driver-node-llx9z                                      37m (0%)      0 (0%)      106Mi (1%)       3272Mi (47%)   45m\n  kube-system                 kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz                   20m (0%)      0 (0%)      64Mi (0%)        0 (0%)         45m\n  kube-system                 node-exporter-9qcsn                                        50m (1%)      200m (5%)   50Mi (0%)        200Mi (2%)     45m\n  kube-system                 node-problem-detector-mfcn2                                20m (0%)      0 (0%)      20Mi (0%)        500Mi (7%)     45m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  sonobuoy                    sonobuoy-e2e-job-fa29383266594ee9                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    417m (10%)  200m (5%)\n  memory                 380Mi (5%)  7886Mi (115%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type     Reason                    Age                From                   Message\n  ----     ------                    ----               ----                   -------\n  Normal   Starting                  45m                kube-proxy             \n  Normal   Starting                  45m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity       45m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory   45m (x2 over 45m)  kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure     45m (x2 over 45m)  kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID      45m (x2 over 45m)  kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeHasSufficientPID\n  Warning  CheckLimitsForResolvConf  45m                kubelet                open /etc/resolv-for-kubelet.conf: no such file or directory\n  Normal   NodeAllocatableEnforced   45m                kubelet                Updated Node Allocatable limit across pods\n  Normal   Synced                    45m                cloud-node-controller  Node synced successfully\n  Normal   RegisteredNode            45m                node-controller        Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st event: Registered Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st in Controller\n  Normal   NodeReady                 45m                kubelet                Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st status is now: NodeReady\n"
    Apr  6 10:59:07.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9550 describe namespace kubectl-9550'
    Apr  6 10:59:07.356: INFO: stderr: ""
    Apr  6 10:59:07.356: INFO: stdout: "Name:         kubectl-9550\nLabels:       e2e-framework=kubectl\n              e2e-run=29d2c07a-ee25-4bc9-9950-c07bb92b6fe9\n              kubernetes.io/metadata.name=kubectl-9550\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 10:59:07.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9550" for this suite. 04/06/23 10:59:07.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:07.385
Apr  6 10:59:07.385: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:59:07.386
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:07.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:07.415
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:59:07.434
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:59:08.073
STEP: Deploying the webhook pod 04/06/23 10:59:08.083
STEP: Wait for the deployment to be ready 04/06/23 10:59:08.093
Apr  6 10:59:08.102: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:59:10.127
STEP: Verifying the service has paired with the endpoint 04/06/23 10:59:10.144
Apr  6 10:59:11.144: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 04/06/23 10:59:11.154
STEP: create a pod that should be denied by the webhook 04/06/23 10:59:11.247
STEP: create a pod that causes the webhook to hang 04/06/23 10:59:11.332
STEP: create a configmap that should be denied by the webhook 04/06/23 10:59:21.345
STEP: create a configmap that should be admitted by the webhook 04/06/23 10:59:21.526
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/06/23 10:59:21.67
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/06/23 10:59:21.737
STEP: create a namespace that bypass the webhook 04/06/23 10:59:21.793
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/06/23 10:59:21.799
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:59:21.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4083" for this suite. 04/06/23 10:59:21.856
STEP: Destroying namespace "webhook-4083-markers" for this suite. 04/06/23 10:59:21.861
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":177,"skipped":3221,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.560 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:07.385
    Apr  6 10:59:07.385: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:59:07.386
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:07.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:07.415
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:59:07.434
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:59:08.073
    STEP: Deploying the webhook pod 04/06/23 10:59:08.083
    STEP: Wait for the deployment to be ready 04/06/23 10:59:08.093
    Apr  6 10:59:08.102: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:59:10.127
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:59:10.144
    Apr  6 10:59:11.144: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 04/06/23 10:59:11.154
    STEP: create a pod that should be denied by the webhook 04/06/23 10:59:11.247
    STEP: create a pod that causes the webhook to hang 04/06/23 10:59:11.332
    STEP: create a configmap that should be denied by the webhook 04/06/23 10:59:21.345
    STEP: create a configmap that should be admitted by the webhook 04/06/23 10:59:21.526
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/06/23 10:59:21.67
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/06/23 10:59:21.737
    STEP: create a namespace that bypass the webhook 04/06/23 10:59:21.793
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/06/23 10:59:21.799
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:59:21.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4083" for this suite. 04/06/23 10:59:21.856
    STEP: Destroying namespace "webhook-4083-markers" for this suite. 04/06/23 10:59:21.861
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:21.951
Apr  6 10:59:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 10:59:21.952
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:21.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:21.983
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 10:59:22.021
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:59:22.513
STEP: Deploying the webhook pod 04/06/23 10:59:22.52
STEP: Wait for the deployment to be ready 04/06/23 10:59:22.532
Apr  6 10:59:22.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 10:59:24.557
STEP: Verifying the service has paired with the endpoint 04/06/23 10:59:24.567
Apr  6 10:59:25.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 04/06/23 10:59:25.573
STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/06/23 10:59:25.728
STEP: Creating a configMap that should not be mutated 04/06/23 10:59:25.751
STEP: Patching a mutating webhook configuration's rules to include the create operation 04/06/23 10:59:25.764
STEP: Creating a configMap that should be mutated 04/06/23 10:59:25.775
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 10:59:25.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6905" for this suite. 04/06/23 10:59:25.939
STEP: Destroying namespace "webhook-6905-markers" for this suite. 04/06/23 10:59:25.947
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":178,"skipped":3271,"failed":0}
------------------------------
â€¢ [4.047 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:21.951
    Apr  6 10:59:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 10:59:21.952
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:21.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:21.983
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 10:59:22.021
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 10:59:22.513
    STEP: Deploying the webhook pod 04/06/23 10:59:22.52
    STEP: Wait for the deployment to be ready 04/06/23 10:59:22.532
    Apr  6 10:59:22.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 10:59:24.557
    STEP: Verifying the service has paired with the endpoint 04/06/23 10:59:24.567
    Apr  6 10:59:25.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 04/06/23 10:59:25.573
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/06/23 10:59:25.728
    STEP: Creating a configMap that should not be mutated 04/06/23 10:59:25.751
    STEP: Patching a mutating webhook configuration's rules to include the create operation 04/06/23 10:59:25.764
    STEP: Creating a configMap that should be mutated 04/06/23 10:59:25.775
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 10:59:25.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6905" for this suite. 04/06/23 10:59:25.939
    STEP: Destroying namespace "webhook-6905-markers" for this suite. 04/06/23 10:59:25.947
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:26.009
Apr  6 10:59:26.009: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 10:59:26.01
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:26.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:26.038
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 04/06/23 10:59:26.046
Apr  6 10:59:26.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806" in namespace "projected-3246" to be "Succeeded or Failed"
Apr  6 10:59:26.064: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035763ms
Apr  6 10:59:28.070: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009618523s
Apr  6 10:59:30.090: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029710665s
STEP: Saw pod success 04/06/23 10:59:30.09
Apr  6 10:59:30.090: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806" satisfied condition "Succeeded or Failed"
Apr  6 10:59:30.098: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806 container client-container: <nil>
STEP: delete the pod 04/06/23 10:59:30.112
Apr  6 10:59:30.121: INFO: Waiting for pod downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806 to disappear
Apr  6 10:59:30.124: INFO: Pod downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 10:59:30.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3246" for this suite. 04/06/23 10:59:30.138
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":179,"skipped":3281,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:26.009
    Apr  6 10:59:26.009: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 10:59:26.01
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:26.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:26.038
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 04/06/23 10:59:26.046
    Apr  6 10:59:26.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806" in namespace "projected-3246" to be "Succeeded or Failed"
    Apr  6 10:59:26.064: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035763ms
    Apr  6 10:59:28.070: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009618523s
    Apr  6 10:59:30.090: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029710665s
    STEP: Saw pod success 04/06/23 10:59:30.09
    Apr  6 10:59:30.090: INFO: Pod "downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806" satisfied condition "Succeeded or Failed"
    Apr  6 10:59:30.098: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806 container client-container: <nil>
    STEP: delete the pod 04/06/23 10:59:30.112
    Apr  6 10:59:30.121: INFO: Waiting for pod downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806 to disappear
    Apr  6 10:59:30.124: INFO: Pod downwardapi-volume-9ddaee11-ec3b-43de-b026-43b3ca83f806 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 10:59:30.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3246" for this suite. 04/06/23 10:59:30.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:30.146
Apr  6 10:59:30.146: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:59:30.147
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:30.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:30.166
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-1137 04/06/23 10:59:30.171
STEP: creating service affinity-nodeport-transition in namespace services-1137 04/06/23 10:59:30.171
STEP: creating replication controller affinity-nodeport-transition in namespace services-1137 04/06/23 10:59:30.184
I0406 10:59:30.192486      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1137, replica count: 3
I0406 10:59:33.243946      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 10:59:33.259: INFO: Creating new exec pod
Apr  6 10:59:33.267: INFO: Waiting up to 5m0s for pod "execpod-affinity9dkrm" in namespace "services-1137" to be "running"
Apr  6 10:59:33.272: INFO: Pod "execpod-affinity9dkrm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090233ms
Apr  6 10:59:35.276: INFO: Pod "execpod-affinity9dkrm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008093113s
Apr  6 10:59:35.276: INFO: Pod "execpod-affinity9dkrm" satisfied condition "running"
Apr  6 10:59:36.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Apr  6 10:59:36.836: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr  6 10:59:36.836: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:59:36.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.70.125.91 80'
Apr  6 10:59:37.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.70.125.91 80\nConnection to 10.70.125.91 80 port [tcp/http] succeeded!\n"
Apr  6 10:59:37.359: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:59:37.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.89 30393'
Apr  6 10:59:37.989: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.89 30393\nConnection to 10.250.2.89 30393 port [tcp/*] succeeded!\n"
Apr  6 10:59:37.989: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:59:37.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 30393'
Apr  6 10:59:38.525: INFO: stderr: "+ nc -v -t -w 2 10.250.2.236 30393\n+ echo hostName\nConnection to 10.250.2.236 30393 port [tcp/*] succeeded!\n"
Apr  6 10:59:38.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 10:59:38.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:30393/ ; done'
Apr  6 10:59:39.347: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n"
Apr  6 10:59:39.347: INFO: stdout: "\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps"
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:39.348: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:39.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:30393/ ; done'
Apr  6 10:59:40.209: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n"
Apr  6 10:59:40.209: INFO: stdout: "\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps"
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
Apr  6 10:59:40.210: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1137, will wait for the garbage collector to delete the pods 04/06/23 10:59:40.218
Apr  6 10:59:40.280: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.690776ms
Apr  6 10:59:40.386: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 105.405729ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:59:42.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1137" for this suite. 04/06/23 10:59:42.85
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":180,"skipped":3295,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.712 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:30.146
    Apr  6 10:59:30.146: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:59:30.147
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:30.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:30.166
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-1137 04/06/23 10:59:30.171
    STEP: creating service affinity-nodeport-transition in namespace services-1137 04/06/23 10:59:30.171
    STEP: creating replication controller affinity-nodeport-transition in namespace services-1137 04/06/23 10:59:30.184
    I0406 10:59:30.192486      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1137, replica count: 3
    I0406 10:59:33.243946      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 10:59:33.259: INFO: Creating new exec pod
    Apr  6 10:59:33.267: INFO: Waiting up to 5m0s for pod "execpod-affinity9dkrm" in namespace "services-1137" to be "running"
    Apr  6 10:59:33.272: INFO: Pod "execpod-affinity9dkrm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090233ms
    Apr  6 10:59:35.276: INFO: Pod "execpod-affinity9dkrm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008093113s
    Apr  6 10:59:35.276: INFO: Pod "execpod-affinity9dkrm" satisfied condition "running"
    Apr  6 10:59:36.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Apr  6 10:59:36.836: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Apr  6 10:59:36.836: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:59:36.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.70.125.91 80'
    Apr  6 10:59:37.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.70.125.91 80\nConnection to 10.70.125.91 80 port [tcp/http] succeeded!\n"
    Apr  6 10:59:37.359: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:59:37.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.89 30393'
    Apr  6 10:59:37.989: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.89 30393\nConnection to 10.250.2.89 30393 port [tcp/*] succeeded!\n"
    Apr  6 10:59:37.989: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:59:37.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 30393'
    Apr  6 10:59:38.525: INFO: stderr: "+ nc -v -t -w 2 10.250.2.236 30393\n+ echo hostName\nConnection to 10.250.2.236 30393 port [tcp/*] succeeded!\n"
    Apr  6 10:59:38.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 10:59:38.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:30393/ ; done'
    Apr  6 10:59:39.347: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n"
    Apr  6 10:59:39.347: INFO: stdout: "\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-mjv65\naffinity-nodeport-transition-qlfzn\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps"
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-mjv65
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-qlfzn
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:39.347: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:39.348: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:39.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1137 exec execpod-affinity9dkrm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.2.236:30393/ ; done'
    Apr  6 10:59:40.209: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.2.236:30393/\n"
    Apr  6 10:59:40.209: INFO: stdout: "\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps\naffinity-nodeport-transition-nvtps"
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Received response from host: affinity-nodeport-transition-nvtps
    Apr  6 10:59:40.210: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1137, will wait for the garbage collector to delete the pods 04/06/23 10:59:40.218
    Apr  6 10:59:40.280: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.690776ms
    Apr  6 10:59:40.386: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 105.405729ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:59:42.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1137" for this suite. 04/06/23 10:59:42.85
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:42.86
Apr  6 10:59:42.860: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 10:59:42.862
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:42.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:42.916
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-051c7e26-9fcd-47ba-926d-2b6d070141ae 04/06/23 10:59:42.922
STEP: Creating a pod to test consume secrets 04/06/23 10:59:42.929
Apr  6 10:59:42.938: INFO: Waiting up to 5m0s for pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1" in namespace "secrets-1754" to be "Succeeded or Failed"
Apr  6 10:59:42.942: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.002084ms
Apr  6 10:59:44.946: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007790989s
Apr  6 10:59:46.946: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007851349s
Apr  6 10:59:48.947: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008227672s
STEP: Saw pod success 04/06/23 10:59:48.947
Apr  6 10:59:48.947: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1" satisfied condition "Succeeded or Failed"
Apr  6 10:59:48.951: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1 container secret-env-test: <nil>
STEP: delete the pod 04/06/23 10:59:48.964
Apr  6 10:59:48.974: INFO: Waiting for pod pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1 to disappear
Apr  6 10:59:48.978: INFO: Pod pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Apr  6 10:59:48.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1754" for this suite. 04/06/23 10:59:48.986
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":181,"skipped":3308,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.131 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:42.86
    Apr  6 10:59:42.860: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 10:59:42.862
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:42.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:42.916
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-051c7e26-9fcd-47ba-926d-2b6d070141ae 04/06/23 10:59:42.922
    STEP: Creating a pod to test consume secrets 04/06/23 10:59:42.929
    Apr  6 10:59:42.938: INFO: Waiting up to 5m0s for pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1" in namespace "secrets-1754" to be "Succeeded or Failed"
    Apr  6 10:59:42.942: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.002084ms
    Apr  6 10:59:44.946: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007790989s
    Apr  6 10:59:46.946: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007851349s
    Apr  6 10:59:48.947: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008227672s
    STEP: Saw pod success 04/06/23 10:59:48.947
    Apr  6 10:59:48.947: INFO: Pod "pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1" satisfied condition "Succeeded or Failed"
    Apr  6 10:59:48.951: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1 container secret-env-test: <nil>
    STEP: delete the pod 04/06/23 10:59:48.964
    Apr  6 10:59:48.974: INFO: Waiting for pod pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1 to disappear
    Apr  6 10:59:48.978: INFO: Pod pod-secrets-190c4e9d-734f-406e-843a-fa274ddea3c1 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 10:59:48.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1754" for this suite. 04/06/23 10:59:48.986
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:48.991
Apr  6 10:59:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 10:59:48.992
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:49.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:49.017
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1185 04/06/23 10:59:49.028
STEP: changing the ExternalName service to type=ClusterIP 04/06/23 10:59:49.034
STEP: creating replication controller externalname-service in namespace services-1185 04/06/23 10:59:49.049
I0406 10:59:49.059625      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1185, replica count: 2
I0406 10:59:52.110420      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 10:59:52.115: INFO: Creating new exec pod
Apr  6 10:59:52.126: INFO: Waiting up to 5m0s for pod "execpodrdvtz" in namespace "services-1185" to be "running"
Apr  6 10:59:52.132: INFO: Pod "execpodrdvtz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.31847ms
Apr  6 10:59:54.138: INFO: Pod "execpodrdvtz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011921079s
Apr  6 10:59:54.138: INFO: Pod "execpodrdvtz" satisfied condition "running"
Apr  6 10:59:55.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1185 exec execpodrdvtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr  6 10:59:55.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 10:59:55.758: INFO: stdout: "externalname-service-2jv5g"
Apr  6 10:59:55.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1185 exec execpodrdvtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.9.10 80'
Apr  6 10:59:56.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.71.9.10 80\nConnection to 10.71.9.10 80 port [tcp/http] succeeded!\n"
Apr  6 10:59:56.549: INFO: stdout: "externalname-service-rbfmr"
Apr  6 10:59:56.549: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 10:59:56.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1185" for this suite. 04/06/23 10:59:56.576
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":182,"skipped":3310,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.590 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:48.991
    Apr  6 10:59:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 10:59:48.992
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:49.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:49.017
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1185 04/06/23 10:59:49.028
    STEP: changing the ExternalName service to type=ClusterIP 04/06/23 10:59:49.034
    STEP: creating replication controller externalname-service in namespace services-1185 04/06/23 10:59:49.049
    I0406 10:59:49.059625      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1185, replica count: 2
    I0406 10:59:52.110420      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 10:59:52.115: INFO: Creating new exec pod
    Apr  6 10:59:52.126: INFO: Waiting up to 5m0s for pod "execpodrdvtz" in namespace "services-1185" to be "running"
    Apr  6 10:59:52.132: INFO: Pod "execpodrdvtz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.31847ms
    Apr  6 10:59:54.138: INFO: Pod "execpodrdvtz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011921079s
    Apr  6 10:59:54.138: INFO: Pod "execpodrdvtz" satisfied condition "running"
    Apr  6 10:59:55.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1185 exec execpodrdvtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Apr  6 10:59:55.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr  6 10:59:55.758: INFO: stdout: "externalname-service-2jv5g"
    Apr  6 10:59:55.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-1185 exec execpodrdvtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.71.9.10 80'
    Apr  6 10:59:56.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.71.9.10 80\nConnection to 10.71.9.10 80 port [tcp/http] succeeded!\n"
    Apr  6 10:59:56.549: INFO: stdout: "externalname-service-rbfmr"
    Apr  6 10:59:56.549: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 10:59:56.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1185" for this suite. 04/06/23 10:59:56.576
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 10:59:56.581
Apr  6 10:59:56.581: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename cronjob 04/06/23 10:59:56.582
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:56.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:56.631
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 04/06/23 10:59:56.637
STEP: Ensuring a job is scheduled 04/06/23 10:59:56.644
STEP: Ensuring exactly one is scheduled 04/06/23 11:00:00.649
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/06/23 11:00:00.653
STEP: Ensuring the job is replaced with a new one 04/06/23 11:00:00.658
STEP: Removing cronjob 04/06/23 11:01:00.663
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Apr  6 11:01:00.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6427" for this suite. 04/06/23 11:01:00.677
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":183,"skipped":3313,"failed":0}
------------------------------
â€¢ [SLOW TEST] [64.101 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 10:59:56.581
    Apr  6 10:59:56.581: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename cronjob 04/06/23 10:59:56.582
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 10:59:56.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 10:59:56.631
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 04/06/23 10:59:56.637
    STEP: Ensuring a job is scheduled 04/06/23 10:59:56.644
    STEP: Ensuring exactly one is scheduled 04/06/23 11:00:00.649
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/06/23 11:00:00.653
    STEP: Ensuring the job is replaced with a new one 04/06/23 11:00:00.658
    STEP: Removing cronjob 04/06/23 11:01:00.663
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Apr  6 11:01:00.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6427" for this suite. 04/06/23 11:01:00.677
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:01:00.683
Apr  6 11:01:00.683: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 11:01:00.685
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:00.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:00.704
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-1736b870-2d3d-466f-93d2-5600fa526b30 04/06/23 11:01:00.708
STEP: Creating a pod to test consume secrets 04/06/23 11:01:00.713
Apr  6 11:01:00.722: INFO: Waiting up to 5m0s for pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0" in namespace "secrets-2482" to be "Succeeded or Failed"
Apr  6 11:01:00.725: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27933ms
Apr  6 11:01:02.741: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019364044s
Apr  6 11:01:04.730: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008402459s
STEP: Saw pod success 04/06/23 11:01:04.73
Apr  6 11:01:04.730: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0" satisfied condition "Succeeded or Failed"
Apr  6 11:01:04.736: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0 container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 11:01:04.791
Apr  6 11:01:04.801: INFO: Waiting for pod pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0 to disappear
Apr  6 11:01:04.804: INFO: Pod pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 11:01:04.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2482" for this suite. 04/06/23 11:01:04.813
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":184,"skipped":3315,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:01:00.683
    Apr  6 11:01:00.683: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 11:01:00.685
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:00.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:00.704
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-1736b870-2d3d-466f-93d2-5600fa526b30 04/06/23 11:01:00.708
    STEP: Creating a pod to test consume secrets 04/06/23 11:01:00.713
    Apr  6 11:01:00.722: INFO: Waiting up to 5m0s for pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0" in namespace "secrets-2482" to be "Succeeded or Failed"
    Apr  6 11:01:00.725: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27933ms
    Apr  6 11:01:02.741: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019364044s
    Apr  6 11:01:04.730: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008402459s
    STEP: Saw pod success 04/06/23 11:01:04.73
    Apr  6 11:01:04.730: INFO: Pod "pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0" satisfied condition "Succeeded or Failed"
    Apr  6 11:01:04.736: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0 container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:01:04.791
    Apr  6 11:01:04.801: INFO: Waiting for pod pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0 to disappear
    Apr  6 11:01:04.804: INFO: Pod pod-secrets-074185f4-e737-4ad2-acd9-556c8d2919b0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 11:01:04.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2482" for this suite. 04/06/23 11:01:04.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:01:04.828
Apr  6 11:01:04.828: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 11:01:04.834
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:04.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:04.871
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 04/06/23 11:01:04.876
Apr  6 11:01:04.888: INFO: created test-pod-1
Apr  6 11:01:04.894: INFO: created test-pod-2
Apr  6 11:01:04.901: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 04/06/23 11:01:04.901
Apr  6 11:01:04.902: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1702' to be running and ready
Apr  6 11:01:04.915: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr  6 11:01:04.915: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr  6 11:01:04.915: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr  6 11:01:04.915: INFO: 0 / 3 pods in namespace 'pods-1702' are running and ready (0 seconds elapsed)
Apr  6 11:01:04.915: INFO: expected 0 pod replicas in namespace 'pods-1702', 0 are Running and Ready.
Apr  6 11:01:04.915: INFO: POD         NODE                                                           PHASE    GRACE  CONDITIONS
Apr  6 11:01:04.915: INFO: test-pod-1  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
Apr  6 11:01:04.915: INFO: test-pod-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
Apr  6 11:01:04.915: INFO: test-pod-3  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
Apr  6 11:01:04.915: INFO: 
Apr  6 11:01:06.939: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr  6 11:01:06.939: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr  6 11:01:06.939: INFO: 1 / 3 pods in namespace 'pods-1702' are running and ready (2 seconds elapsed)
Apr  6 11:01:06.939: INFO: expected 0 pod replicas in namespace 'pods-1702', 0 are Running and Ready.
Apr  6 11:01:06.939: INFO: POD         NODE                                                           PHASE    GRACE  CONDITIONS
Apr  6 11:01:06.939: INFO: test-pod-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
Apr  6 11:01:06.939: INFO: test-pod-3  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
Apr  6 11:01:06.939: INFO: 
Apr  6 11:01:08.930: INFO: 3 / 3 pods in namespace 'pods-1702' are running and ready (4 seconds elapsed)
Apr  6 11:01:08.930: INFO: expected 0 pod replicas in namespace 'pods-1702', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 04/06/23 11:01:08.943
Apr  6 11:01:08.970: INFO: Pod quantity 3 is different from expected quantity 0
Apr  6 11:01:09.975: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 11:01:10.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1702" for this suite. 04/06/23 11:01:10.987
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":185,"skipped":3343,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.165 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:01:04.828
    Apr  6 11:01:04.828: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 11:01:04.834
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:04.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:04.871
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 04/06/23 11:01:04.876
    Apr  6 11:01:04.888: INFO: created test-pod-1
    Apr  6 11:01:04.894: INFO: created test-pod-2
    Apr  6 11:01:04.901: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 04/06/23 11:01:04.901
    Apr  6 11:01:04.902: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1702' to be running and ready
    Apr  6 11:01:04.915: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr  6 11:01:04.915: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr  6 11:01:04.915: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr  6 11:01:04.915: INFO: 0 / 3 pods in namespace 'pods-1702' are running and ready (0 seconds elapsed)
    Apr  6 11:01:04.915: INFO: expected 0 pod replicas in namespace 'pods-1702', 0 are Running and Ready.
    Apr  6 11:01:04.915: INFO: POD         NODE                                                           PHASE    GRACE  CONDITIONS
    Apr  6 11:01:04.915: INFO: test-pod-1  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
    Apr  6 11:01:04.915: INFO: test-pod-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
    Apr  6 11:01:04.915: INFO: test-pod-3  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
    Apr  6 11:01:04.915: INFO: 
    Apr  6 11:01:06.939: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr  6 11:01:06.939: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr  6 11:01:06.939: INFO: 1 / 3 pods in namespace 'pods-1702' are running and ready (2 seconds elapsed)
    Apr  6 11:01:06.939: INFO: expected 0 pod replicas in namespace 'pods-1702', 0 are Running and Ready.
    Apr  6 11:01:06.939: INFO: POD         NODE                                                           PHASE    GRACE  CONDITIONS
    Apr  6 11:01:06.939: INFO: test-pod-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
    Apr  6 11:01:06.939: INFO: test-pod-3  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:01:04 +0000 UTC  }]
    Apr  6 11:01:06.939: INFO: 
    Apr  6 11:01:08.930: INFO: 3 / 3 pods in namespace 'pods-1702' are running and ready (4 seconds elapsed)
    Apr  6 11:01:08.930: INFO: expected 0 pod replicas in namespace 'pods-1702', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 04/06/23 11:01:08.943
    Apr  6 11:01:08.970: INFO: Pod quantity 3 is different from expected quantity 0
    Apr  6 11:01:09.975: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 11:01:10.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1702" for this suite. 04/06/23 11:01:10.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:01:10.995
Apr  6 11:01:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:01:10.997
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:11.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:11.015
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/06/23 11:01:11.021
Apr  6 11:01:11.030: INFO: Waiting up to 5m0s for pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840" in namespace "emptydir-5853" to be "Succeeded or Failed"
Apr  6 11:01:11.035: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458144ms
Apr  6 11:01:13.051: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020831981s
Apr  6 11:01:15.051: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020948356s
STEP: Saw pod success 04/06/23 11:01:15.051
Apr  6 11:01:15.051: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840" satisfied condition "Succeeded or Failed"
Apr  6 11:01:15.055: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840 container test-container: <nil>
STEP: delete the pod 04/06/23 11:01:15.074
Apr  6 11:01:15.087: INFO: Waiting for pod pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840 to disappear
Apr  6 11:01:15.101: INFO: Pod pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:01:15.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5853" for this suite. 04/06/23 11:01:15.12
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":186,"skipped":3353,"failed":0}
------------------------------
â€¢ [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:01:10.995
    Apr  6 11:01:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:01:10.997
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:11.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:11.015
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/06/23 11:01:11.021
    Apr  6 11:01:11.030: INFO: Waiting up to 5m0s for pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840" in namespace "emptydir-5853" to be "Succeeded or Failed"
    Apr  6 11:01:11.035: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458144ms
    Apr  6 11:01:13.051: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020831981s
    Apr  6 11:01:15.051: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020948356s
    STEP: Saw pod success 04/06/23 11:01:15.051
    Apr  6 11:01:15.051: INFO: Pod "pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840" satisfied condition "Succeeded or Failed"
    Apr  6 11:01:15.055: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:01:15.074
    Apr  6 11:01:15.087: INFO: Waiting for pod pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840 to disappear
    Apr  6 11:01:15.101: INFO: Pod pod-e4b9b675-7957-4aa9-81d0-dfeaf97f1840 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:01:15.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5853" for this suite. 04/06/23 11:01:15.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:01:15.13
Apr  6 11:01:15.130: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 11:01:15.132
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:15.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:15.167
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 04/06/23 11:01:15.175
Apr  6 11:01:15.188: INFO: Waiting up to 5m0s for pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b" in namespace "downward-api-5443" to be "Succeeded or Failed"
Apr  6 11:01:15.194: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094303ms
Apr  6 11:01:17.200: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011566007s
Apr  6 11:01:19.219: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030859802s
Apr  6 11:01:21.203: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015063678s
STEP: Saw pod success 04/06/23 11:01:21.203
Apr  6 11:01:21.204: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b" satisfied condition "Succeeded or Failed"
Apr  6 11:01:21.208: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b container dapi-container: <nil>
STEP: delete the pod 04/06/23 11:01:21.233
Apr  6 11:01:21.243: INFO: Waiting for pod downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b to disappear
Apr  6 11:01:21.246: INFO: Pod downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Apr  6 11:01:21.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5443" for this suite. 04/06/23 11:01:21.256
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":187,"skipped":3368,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.130 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:01:15.13
    Apr  6 11:01:15.130: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 11:01:15.132
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:15.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:15.167
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 04/06/23 11:01:15.175
    Apr  6 11:01:15.188: INFO: Waiting up to 5m0s for pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b" in namespace "downward-api-5443" to be "Succeeded or Failed"
    Apr  6 11:01:15.194: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094303ms
    Apr  6 11:01:17.200: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011566007s
    Apr  6 11:01:19.219: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030859802s
    Apr  6 11:01:21.203: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015063678s
    STEP: Saw pod success 04/06/23 11:01:21.203
    Apr  6 11:01:21.204: INFO: Pod "downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b" satisfied condition "Succeeded or Failed"
    Apr  6 11:01:21.208: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b container dapi-container: <nil>
    STEP: delete the pod 04/06/23 11:01:21.233
    Apr  6 11:01:21.243: INFO: Waiting for pod downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b to disappear
    Apr  6 11:01:21.246: INFO: Pod downward-api-97990758-22c4-478a-b95e-b1ff3d6ec34b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Apr  6 11:01:21.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5443" for this suite. 04/06/23 11:01:21.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:01:21.262
Apr  6 11:01:21.263: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:01:21.265
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:21.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:21.28
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 04/06/23 11:01:21.285
Apr  6 11:01:21.286: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: rename a version 04/06/23 11:01:31.458
STEP: check the new version name is served 04/06/23 11:01:31.476
STEP: check the old version name is removed 04/06/23 11:01:36.206
STEP: check the other version is not changed 04/06/23 11:01:38.206
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:01:46.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7288" for this suite. 04/06/23 11:01:46.147
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":188,"skipped":3379,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.898 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:01:21.262
    Apr  6 11:01:21.263: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:01:21.265
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:21.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:21.28
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 04/06/23 11:01:21.285
    Apr  6 11:01:21.286: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: rename a version 04/06/23 11:01:31.458
    STEP: check the new version name is served 04/06/23 11:01:31.476
    STEP: check the old version name is removed 04/06/23 11:01:36.206
    STEP: check the other version is not changed 04/06/23 11:01:38.206
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:01:46.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7288" for this suite. 04/06/23 11:01:46.147
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:01:46.16
Apr  6 11:01:46.161: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 11:01:46.162
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:46.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:46.179
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-a1c3a648-11b8-4d44-906f-7a02e4fc59c2 04/06/23 11:01:46.213
STEP: Creating the pod 04/06/23 11:01:46.218
Apr  6 11:01:46.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306" in namespace "configmap-5379" to be "running and ready"
Apr  6 11:01:46.239: INFO: Pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306": Phase="Pending", Reason="", readiness=false. Elapsed: 4.975804ms
Apr  6 11:01:46.239: INFO: The phase of Pod pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:01:48.245: INFO: Pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306": Phase="Running", Reason="", readiness=true. Elapsed: 2.01062181s
Apr  6 11:01:48.245: INFO: The phase of Pod pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306 is Running (Ready = true)
Apr  6 11:01:48.245: INFO: Pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-a1c3a648-11b8-4d44-906f-7a02e4fc59c2 04/06/23 11:01:48.281
STEP: waiting to observe update in volume 04/06/23 11:01:48.286
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 11:01:50.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5379" for this suite. 04/06/23 11:01:50.421
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":189,"skipped":3383,"failed":0}
------------------------------
â€¢ [4.286 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:01:46.16
    Apr  6 11:01:46.161: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 11:01:46.162
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:46.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:46.179
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-a1c3a648-11b8-4d44-906f-7a02e4fc59c2 04/06/23 11:01:46.213
    STEP: Creating the pod 04/06/23 11:01:46.218
    Apr  6 11:01:46.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306" in namespace "configmap-5379" to be "running and ready"
    Apr  6 11:01:46.239: INFO: Pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306": Phase="Pending", Reason="", readiness=false. Elapsed: 4.975804ms
    Apr  6 11:01:46.239: INFO: The phase of Pod pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:01:48.245: INFO: Pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306": Phase="Running", Reason="", readiness=true. Elapsed: 2.01062181s
    Apr  6 11:01:48.245: INFO: The phase of Pod pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306 is Running (Ready = true)
    Apr  6 11:01:48.245: INFO: Pod "pod-configmaps-288cbfba-5c92-41ab-b130-caa04fb46306" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-a1c3a648-11b8-4d44-906f-7a02e4fc59c2 04/06/23 11:01:48.281
    STEP: waiting to observe update in volume 04/06/23 11:01:48.286
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 11:01:50.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5379" for this suite. 04/06/23 11:01:50.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:01:50.448
Apr  6 11:01:50.448: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename svc-latency 04/06/23 11:01:50.449
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:50.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:50.467
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Apr  6 11:01:50.475: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1672 04/06/23 11:01:50.476
I0406 11:01:50.487543      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1672, replica count: 1
I0406 11:01:51.539443      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 11:01:52.539757      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 11:01:52.652: INFO: Created: latency-svc-bvbhs
Apr  6 11:01:52.654: INFO: Got endpoints: latency-svc-bvbhs [14.059891ms]
Apr  6 11:01:52.666: INFO: Created: latency-svc-dvcvm
Apr  6 11:01:52.679: INFO: Got endpoints: latency-svc-dvcvm [24.3208ms]
Apr  6 11:01:52.680: INFO: Created: latency-svc-dxdf9
Apr  6 11:01:52.680: INFO: Got endpoints: latency-svc-dxdf9 [25.503001ms]
Apr  6 11:01:52.680: INFO: Created: latency-svc-cdbpq
Apr  6 11:01:52.682: INFO: Got endpoints: latency-svc-cdbpq [27.195323ms]
Apr  6 11:01:52.696: INFO: Created: latency-svc-2ndk9
Apr  6 11:01:52.697: INFO: Got endpoints: latency-svc-2ndk9 [41.939006ms]
Apr  6 11:01:52.697: INFO: Created: latency-svc-nmbs9
Apr  6 11:01:52.698: INFO: Got endpoints: latency-svc-nmbs9 [42.984461ms]
Apr  6 11:01:52.698: INFO: Created: latency-svc-gpnln
Apr  6 11:01:52.703: INFO: Got endpoints: latency-svc-gpnln [47.305756ms]
Apr  6 11:01:52.704: INFO: Created: latency-svc-g5tq4
Apr  6 11:01:52.709: INFO: Got endpoints: latency-svc-g5tq4 [53.464495ms]
Apr  6 11:01:52.710: INFO: Created: latency-svc-vp5v8
Apr  6 11:01:52.723: INFO: Got endpoints: latency-svc-vp5v8 [67.233921ms]
Apr  6 11:01:52.727: INFO: Created: latency-svc-j7jsh
Apr  6 11:01:52.728: INFO: Got endpoints: latency-svc-j7jsh [71.700052ms]
Apr  6 11:01:52.735: INFO: Created: latency-svc-fxm67
Apr  6 11:01:52.736: INFO: Created: latency-svc-dwm6g
Apr  6 11:01:52.736: INFO: Got endpoints: latency-svc-dwm6g [79.335278ms]
Apr  6 11:01:52.736: INFO: Got endpoints: latency-svc-fxm67 [79.746745ms]
Apr  6 11:01:52.737: INFO: Created: latency-svc-l9hkn
Apr  6 11:01:52.750: INFO: Created: latency-svc-6d5qg
Apr  6 11:01:52.750: INFO: Got endpoints: latency-svc-l9hkn [93.560864ms]
Apr  6 11:01:52.750: INFO: Created: latency-svc-wsqgd
Apr  6 11:01:52.750: INFO: Got endpoints: latency-svc-6d5qg [93.423402ms]
Apr  6 11:01:52.755: INFO: Created: latency-svc-rb6v4
Apr  6 11:01:52.756: INFO: Got endpoints: latency-svc-wsqgd [100.246331ms]
Apr  6 11:01:52.761: INFO: Got endpoints: latency-svc-rb6v4 [105.011823ms]
Apr  6 11:01:52.762: INFO: Created: latency-svc-wgb7m
Apr  6 11:01:52.770: INFO: Got endpoints: latency-svc-wgb7m [90.474097ms]
Apr  6 11:01:52.772: INFO: Created: latency-svc-zpjk2
Apr  6 11:01:52.785: INFO: Got endpoints: latency-svc-zpjk2 [105.441568ms]
Apr  6 11:01:52.786: INFO: Created: latency-svc-m68k9
Apr  6 11:01:52.787: INFO: Got endpoints: latency-svc-m68k9 [104.275637ms]
Apr  6 11:01:52.796: INFO: Created: latency-svc-n22ct
Apr  6 11:01:52.799: INFO: Got endpoints: latency-svc-n22ct [101.325226ms]
Apr  6 11:01:52.800: INFO: Created: latency-svc-drt5x
Apr  6 11:01:52.806: INFO: Got endpoints: latency-svc-drt5x [107.638318ms]
Apr  6 11:01:52.811: INFO: Created: latency-svc-llpr2
Apr  6 11:01:52.811: INFO: Got endpoints: latency-svc-llpr2 [108.162183ms]
Apr  6 11:01:52.827: INFO: Created: latency-svc-p2kfv
Apr  6 11:01:52.830: INFO: Got endpoints: latency-svc-p2kfv [120.493743ms]
Apr  6 11:01:52.836: INFO: Created: latency-svc-n7rq5
Apr  6 11:01:52.837: INFO: Got endpoints: latency-svc-n7rq5 [110.744463ms]
Apr  6 11:01:52.841: INFO: Created: latency-svc-t8zg2
Apr  6 11:01:52.845: INFO: Got endpoints: latency-svc-t8zg2 [117.464606ms]
Apr  6 11:01:52.852: INFO: Created: latency-svc-vmvsc
Apr  6 11:01:52.868: INFO: Got endpoints: latency-svc-vmvsc [132.110293ms]
Apr  6 11:01:52.876: INFO: Created: latency-svc-2nkfb
Apr  6 11:01:52.876: INFO: Got endpoints: latency-svc-2nkfb [139.882326ms]
Apr  6 11:01:52.878: INFO: Created: latency-svc-5pdlj
Apr  6 11:01:52.882: INFO: Got endpoints: latency-svc-5pdlj [131.737834ms]
Apr  6 11:01:52.883: INFO: Created: latency-svc-cs6rp
Apr  6 11:01:52.887: INFO: Got endpoints: latency-svc-cs6rp [136.760802ms]
Apr  6 11:01:52.888: INFO: Created: latency-svc-gwtkf
Apr  6 11:01:52.905: INFO: Got endpoints: latency-svc-gwtkf [149.012484ms]
Apr  6 11:01:52.906: INFO: Created: latency-svc-9t427
Apr  6 11:01:52.927: INFO: Created: latency-svc-x7g9m
Apr  6 11:01:52.927: INFO: Got endpoints: latency-svc-9t427 [166.115849ms]
Apr  6 11:01:52.930: INFO: Got endpoints: latency-svc-x7g9m [160.051748ms]
Apr  6 11:01:52.931: INFO: Created: latency-svc-gt8jl
Apr  6 11:01:52.933: INFO: Got endpoints: latency-svc-gt8jl [147.960954ms]
Apr  6 11:01:52.937: INFO: Created: latency-svc-fpbbr
Apr  6 11:01:52.947: INFO: Created: latency-svc-hdbm4
Apr  6 11:01:52.947: INFO: Got endpoints: latency-svc-hdbm4 [148.21476ms]
Apr  6 11:01:52.948: INFO: Created: latency-svc-fqxgf
Apr  6 11:01:52.948: INFO: Got endpoints: latency-svc-fpbbr [161.069077ms]
Apr  6 11:01:52.960: INFO: Got endpoints: latency-svc-fqxgf [154.154534ms]
Apr  6 11:01:52.960: INFO: Created: latency-svc-jfbck
Apr  6 11:01:52.960: INFO: Created: latency-svc-bsml4
Apr  6 11:01:52.962: INFO: Created: latency-svc-bw88n
Apr  6 11:01:52.967: INFO: Created: latency-svc-8wfmf
Apr  6 11:01:52.979: INFO: Created: latency-svc-2xchj
Apr  6 11:01:52.979: INFO: Created: latency-svc-lvbm9
Apr  6 11:01:52.980: INFO: Created: latency-svc-5ntnb
Apr  6 11:01:52.988: INFO: Created: latency-svc-pbr7q
Apr  6 11:01:52.995: INFO: Created: latency-svc-gf24c
Apr  6 11:01:52.995: INFO: Created: latency-svc-mqgbt
Apr  6 11:01:53.005: INFO: Created: latency-svc-5kn7b
Apr  6 11:01:53.005: INFO: Created: latency-svc-lqvsg
Apr  6 11:01:53.007: INFO: Got endpoints: latency-svc-jfbck [196.182172ms]
Apr  6 11:01:53.014: INFO: Created: latency-svc-qzfgz
Apr  6 11:01:53.016: INFO: Created: latency-svc-p57ln
Apr  6 11:01:53.019: INFO: Created: latency-svc-r4mdl
Apr  6 11:01:53.028: INFO: Created: latency-svc-8xhn2
Apr  6 11:01:53.055: INFO: Got endpoints: latency-svc-bsml4 [225.377806ms]
Apr  6 11:01:53.066: INFO: Created: latency-svc-h6pvv
Apr  6 11:01:53.105: INFO: Got endpoints: latency-svc-bw88n [268.039122ms]
Apr  6 11:01:53.126: INFO: Created: latency-svc-6x46k
Apr  6 11:01:53.156: INFO: Got endpoints: latency-svc-8wfmf [311.282177ms]
Apr  6 11:01:53.192: INFO: Created: latency-svc-n4j56
Apr  6 11:01:53.205: INFO: Got endpoints: latency-svc-2xchj [336.901783ms]
Apr  6 11:01:53.218: INFO: Created: latency-svc-dktkr
Apr  6 11:01:53.257: INFO: Got endpoints: latency-svc-lvbm9 [380.727718ms]
Apr  6 11:01:53.271: INFO: Created: latency-svc-dpmjb
Apr  6 11:01:53.306: INFO: Got endpoints: latency-svc-5ntnb [424.057437ms]
Apr  6 11:01:53.317: INFO: Created: latency-svc-s44mp
Apr  6 11:01:53.355: INFO: Got endpoints: latency-svc-pbr7q [468.075839ms]
Apr  6 11:01:53.366: INFO: Created: latency-svc-clhm8
Apr  6 11:01:53.412: INFO: Got endpoints: latency-svc-gf24c [506.37186ms]
Apr  6 11:01:53.434: INFO: Created: latency-svc-tfxnc
Apr  6 11:01:53.456: INFO: Got endpoints: latency-svc-mqgbt [527.99367ms]
Apr  6 11:01:53.466: INFO: Created: latency-svc-2hskv
Apr  6 11:01:53.505: INFO: Got endpoints: latency-svc-5kn7b [575.415295ms]
Apr  6 11:01:53.519: INFO: Created: latency-svc-v2l9s
Apr  6 11:01:53.555: INFO: Got endpoints: latency-svc-lqvsg [621.625298ms]
Apr  6 11:01:53.567: INFO: Created: latency-svc-5tbft
Apr  6 11:01:53.610: INFO: Got endpoints: latency-svc-qzfgz [662.566086ms]
Apr  6 11:01:53.624: INFO: Created: latency-svc-h2bp4
Apr  6 11:01:53.656: INFO: Got endpoints: latency-svc-p57ln [708.621329ms]
Apr  6 11:01:53.668: INFO: Created: latency-svc-ch9nz
Apr  6 11:01:53.713: INFO: Got endpoints: latency-svc-r4mdl [752.77368ms]
Apr  6 11:01:53.730: INFO: Created: latency-svc-pls8p
Apr  6 11:01:53.755: INFO: Got endpoints: latency-svc-8xhn2 [747.856893ms]
Apr  6 11:01:53.767: INFO: Created: latency-svc-8kwhg
Apr  6 11:01:53.807: INFO: Got endpoints: latency-svc-h6pvv [751.606307ms]
Apr  6 11:01:53.835: INFO: Created: latency-svc-jj5kw
Apr  6 11:01:53.856: INFO: Got endpoints: latency-svc-6x46k [750.085234ms]
Apr  6 11:01:53.873: INFO: Created: latency-svc-kwgbs
Apr  6 11:01:53.916: INFO: Got endpoints: latency-svc-n4j56 [760.026578ms]
Apr  6 11:01:53.927: INFO: Created: latency-svc-r94gs
Apr  6 11:01:53.962: INFO: Got endpoints: latency-svc-dktkr [757.48166ms]
Apr  6 11:01:53.980: INFO: Created: latency-svc-2wwcr
Apr  6 11:01:54.004: INFO: Got endpoints: latency-svc-dpmjb [747.602355ms]
Apr  6 11:01:54.020: INFO: Created: latency-svc-zwcbx
Apr  6 11:01:54.062: INFO: Got endpoints: latency-svc-s44mp [755.539714ms]
Apr  6 11:01:54.073: INFO: Created: latency-svc-97nmh
Apr  6 11:01:54.111: INFO: Got endpoints: latency-svc-clhm8 [755.652534ms]
Apr  6 11:01:54.124: INFO: Created: latency-svc-4kr67
Apr  6 11:01:54.156: INFO: Got endpoints: latency-svc-tfxnc [742.783075ms]
Apr  6 11:01:54.167: INFO: Created: latency-svc-jcgbm
Apr  6 11:01:54.204: INFO: Got endpoints: latency-svc-2hskv [748.716716ms]
Apr  6 11:01:54.228: INFO: Created: latency-svc-n5ltv
Apr  6 11:01:54.255: INFO: Got endpoints: latency-svc-v2l9s [749.424026ms]
Apr  6 11:01:54.266: INFO: Created: latency-svc-pbs55
Apr  6 11:01:54.305: INFO: Got endpoints: latency-svc-5tbft [749.592373ms]
Apr  6 11:01:54.314: INFO: Created: latency-svc-5csqn
Apr  6 11:01:54.358: INFO: Got endpoints: latency-svc-h2bp4 [747.941988ms]
Apr  6 11:01:54.369: INFO: Created: latency-svc-nwg52
Apr  6 11:01:54.405: INFO: Got endpoints: latency-svc-ch9nz [748.665267ms]
Apr  6 11:01:54.416: INFO: Created: latency-svc-jvv5l
Apr  6 11:01:54.457: INFO: Got endpoints: latency-svc-pls8p [743.257324ms]
Apr  6 11:01:54.475: INFO: Created: latency-svc-zjhr8
Apr  6 11:01:54.511: INFO: Got endpoints: latency-svc-8kwhg [756.171805ms]
Apr  6 11:01:54.521: INFO: Created: latency-svc-krgtq
Apr  6 11:01:54.555: INFO: Got endpoints: latency-svc-jj5kw [747.753524ms]
Apr  6 11:01:54.565: INFO: Created: latency-svc-khb6m
Apr  6 11:01:54.609: INFO: Got endpoints: latency-svc-kwgbs [753.53884ms]
Apr  6 11:01:54.625: INFO: Created: latency-svc-6r5m8
Apr  6 11:01:54.668: INFO: Got endpoints: latency-svc-r94gs [751.749943ms]
Apr  6 11:01:54.678: INFO: Created: latency-svc-2j4h5
Apr  6 11:01:54.705: INFO: Got endpoints: latency-svc-2wwcr [743.019351ms]
Apr  6 11:01:54.721: INFO: Created: latency-svc-gbvvj
Apr  6 11:01:54.761: INFO: Got endpoints: latency-svc-zwcbx [756.513323ms]
Apr  6 11:01:54.781: INFO: Created: latency-svc-dxwtv
Apr  6 11:01:54.806: INFO: Got endpoints: latency-svc-97nmh [743.993131ms]
Apr  6 11:01:54.821: INFO: Created: latency-svc-r7zj4
Apr  6 11:01:54.855: INFO: Got endpoints: latency-svc-4kr67 [743.478179ms]
Apr  6 11:01:54.865: INFO: Created: latency-svc-pkz89
Apr  6 11:01:54.906: INFO: Got endpoints: latency-svc-jcgbm [750.124108ms]
Apr  6 11:01:54.916: INFO: Created: latency-svc-mx5cr
Apr  6 11:01:54.956: INFO: Got endpoints: latency-svc-n5ltv [751.537598ms]
Apr  6 11:01:54.967: INFO: Created: latency-svc-jrs8f
Apr  6 11:01:55.005: INFO: Got endpoints: latency-svc-pbs55 [750.111174ms]
Apr  6 11:01:55.022: INFO: Created: latency-svc-tnnxm
Apr  6 11:01:55.057: INFO: Got endpoints: latency-svc-5csqn [752.254288ms]
Apr  6 11:01:55.081: INFO: Created: latency-svc-cs26g
Apr  6 11:01:55.105: INFO: Got endpoints: latency-svc-nwg52 [746.949003ms]
Apr  6 11:01:55.115: INFO: Created: latency-svc-9qz6v
Apr  6 11:01:55.160: INFO: Got endpoints: latency-svc-jvv5l [754.778629ms]
Apr  6 11:01:55.172: INFO: Created: latency-svc-kcssn
Apr  6 11:01:55.217: INFO: Got endpoints: latency-svc-zjhr8 [760.696096ms]
Apr  6 11:01:55.229: INFO: Created: latency-svc-jzhf2
Apr  6 11:01:55.255: INFO: Got endpoints: latency-svc-krgtq [743.080249ms]
Apr  6 11:01:55.265: INFO: Created: latency-svc-tqwtp
Apr  6 11:01:55.311: INFO: Got endpoints: latency-svc-khb6m [755.578217ms]
Apr  6 11:01:55.321: INFO: Created: latency-svc-v2ndr
Apr  6 11:01:55.371: INFO: Got endpoints: latency-svc-6r5m8 [762.064584ms]
Apr  6 11:01:55.382: INFO: Created: latency-svc-8d7cj
Apr  6 11:01:55.408: INFO: Got endpoints: latency-svc-2j4h5 [739.733031ms]
Apr  6 11:01:55.418: INFO: Created: latency-svc-mzxhq
Apr  6 11:01:55.456: INFO: Got endpoints: latency-svc-gbvvj [751.04822ms]
Apr  6 11:01:55.465: INFO: Created: latency-svc-htddq
Apr  6 11:01:55.505: INFO: Got endpoints: latency-svc-dxwtv [744.121305ms]
Apr  6 11:01:55.517: INFO: Created: latency-svc-msjcb
Apr  6 11:01:55.554: INFO: Got endpoints: latency-svc-r7zj4 [747.612409ms]
Apr  6 11:01:55.562: INFO: Created: latency-svc-tp469
Apr  6 11:01:55.604: INFO: Got endpoints: latency-svc-pkz89 [749.74521ms]
Apr  6 11:01:55.616: INFO: Created: latency-svc-p2q4h
Apr  6 11:01:55.653: INFO: Got endpoints: latency-svc-mx5cr [747.673619ms]
Apr  6 11:01:55.671: INFO: Created: latency-svc-2s89z
Apr  6 11:01:55.706: INFO: Got endpoints: latency-svc-jrs8f [750.087583ms]
Apr  6 11:01:55.717: INFO: Created: latency-svc-rh8d2
Apr  6 11:01:55.757: INFO: Got endpoints: latency-svc-tnnxm [751.926775ms]
Apr  6 11:01:55.771: INFO: Created: latency-svc-hs4zw
Apr  6 11:01:55.806: INFO: Got endpoints: latency-svc-cs26g [744.750569ms]
Apr  6 11:01:55.818: INFO: Created: latency-svc-45b48
Apr  6 11:01:55.858: INFO: Got endpoints: latency-svc-9qz6v [753.093042ms]
Apr  6 11:01:55.868: INFO: Created: latency-svc-wfcmd
Apr  6 11:01:55.905: INFO: Got endpoints: latency-svc-kcssn [744.930089ms]
Apr  6 11:01:55.915: INFO: Created: latency-svc-tlhcq
Apr  6 11:01:55.956: INFO: Got endpoints: latency-svc-jzhf2 [738.870194ms]
Apr  6 11:01:55.973: INFO: Created: latency-svc-h52v4
Apr  6 11:01:56.008: INFO: Got endpoints: latency-svc-tqwtp [752.961329ms]
Apr  6 11:01:56.019: INFO: Created: latency-svc-nmbcz
Apr  6 11:01:56.055: INFO: Got endpoints: latency-svc-v2ndr [744.477257ms]
Apr  6 11:01:56.065: INFO: Created: latency-svc-7v7xf
Apr  6 11:01:56.107: INFO: Got endpoints: latency-svc-8d7cj [735.016361ms]
Apr  6 11:01:56.116: INFO: Created: latency-svc-xq76f
Apr  6 11:01:56.156: INFO: Got endpoints: latency-svc-mzxhq [748.356834ms]
Apr  6 11:01:56.175: INFO: Created: latency-svc-cjhnp
Apr  6 11:01:56.205: INFO: Got endpoints: latency-svc-htddq [748.547391ms]
Apr  6 11:01:56.217: INFO: Created: latency-svc-jwd7w
Apr  6 11:01:56.255: INFO: Got endpoints: latency-svc-msjcb [749.299189ms]
Apr  6 11:01:56.264: INFO: Created: latency-svc-ngbbl
Apr  6 11:01:56.305: INFO: Got endpoints: latency-svc-tp469 [750.800883ms]
Apr  6 11:01:56.316: INFO: Created: latency-svc-sdlgm
Apr  6 11:01:56.355: INFO: Got endpoints: latency-svc-p2q4h [750.645653ms]
Apr  6 11:01:56.365: INFO: Created: latency-svc-bhm72
Apr  6 11:01:56.406: INFO: Got endpoints: latency-svc-2s89z [752.264877ms]
Apr  6 11:01:56.428: INFO: Created: latency-svc-xzttq
Apr  6 11:01:56.456: INFO: Got endpoints: latency-svc-rh8d2 [749.786542ms]
Apr  6 11:01:56.481: INFO: Created: latency-svc-g5psz
Apr  6 11:01:56.515: INFO: Got endpoints: latency-svc-hs4zw [758.686949ms]
Apr  6 11:01:56.525: INFO: Created: latency-svc-wck99
Apr  6 11:01:56.555: INFO: Got endpoints: latency-svc-45b48 [749.41881ms]
Apr  6 11:01:56.564: INFO: Created: latency-svc-ljts6
Apr  6 11:01:56.611: INFO: Got endpoints: latency-svc-wfcmd [752.897567ms]
Apr  6 11:01:56.624: INFO: Created: latency-svc-6kzbr
Apr  6 11:01:56.656: INFO: Got endpoints: latency-svc-tlhcq [749.992917ms]
Apr  6 11:01:56.669: INFO: Created: latency-svc-2gqmh
Apr  6 11:01:56.704: INFO: Got endpoints: latency-svc-h52v4 [747.864164ms]
Apr  6 11:01:56.716: INFO: Created: latency-svc-8trmh
Apr  6 11:01:56.758: INFO: Got endpoints: latency-svc-nmbcz [750.50093ms]
Apr  6 11:01:56.784: INFO: Created: latency-svc-7bncw
Apr  6 11:01:56.807: INFO: Got endpoints: latency-svc-7v7xf [752.009501ms]
Apr  6 11:01:56.817: INFO: Created: latency-svc-2rjjc
Apr  6 11:01:56.857: INFO: Got endpoints: latency-svc-xq76f [750.061826ms]
Apr  6 11:01:56.881: INFO: Created: latency-svc-pvjwb
Apr  6 11:01:56.905: INFO: Got endpoints: latency-svc-cjhnp [748.765004ms]
Apr  6 11:01:56.914: INFO: Created: latency-svc-s26r6
Apr  6 11:01:56.955: INFO: Got endpoints: latency-svc-jwd7w [749.624332ms]
Apr  6 11:01:56.966: INFO: Created: latency-svc-mbdv4
Apr  6 11:01:57.012: INFO: Got endpoints: latency-svc-ngbbl [757.358982ms]
Apr  6 11:01:57.034: INFO: Created: latency-svc-mbvml
Apr  6 11:01:57.059: INFO: Got endpoints: latency-svc-sdlgm [754.027258ms]
Apr  6 11:01:57.069: INFO: Created: latency-svc-rtghb
Apr  6 11:01:57.108: INFO: Got endpoints: latency-svc-bhm72 [752.880863ms]
Apr  6 11:01:57.117: INFO: Created: latency-svc-qqmvp
Apr  6 11:01:57.157: INFO: Got endpoints: latency-svc-xzttq [751.087538ms]
Apr  6 11:01:57.187: INFO: Created: latency-svc-qgdch
Apr  6 11:01:57.206: INFO: Got endpoints: latency-svc-g5psz [749.157175ms]
Apr  6 11:01:57.226: INFO: Created: latency-svc-m9g8w
Apr  6 11:01:57.259: INFO: Got endpoints: latency-svc-wck99 [743.058753ms]
Apr  6 11:01:57.271: INFO: Created: latency-svc-4n9kr
Apr  6 11:01:57.306: INFO: Got endpoints: latency-svc-ljts6 [751.288997ms]
Apr  6 11:01:57.318: INFO: Created: latency-svc-mqht5
Apr  6 11:01:57.357: INFO: Got endpoints: latency-svc-6kzbr [745.919108ms]
Apr  6 11:01:57.380: INFO: Created: latency-svc-9pptb
Apr  6 11:01:57.408: INFO: Got endpoints: latency-svc-2gqmh [752.787644ms]
Apr  6 11:01:57.423: INFO: Created: latency-svc-cd2nr
Apr  6 11:01:57.457: INFO: Got endpoints: latency-svc-8trmh [752.321393ms]
Apr  6 11:01:57.467: INFO: Created: latency-svc-p8rht
Apr  6 11:01:57.515: INFO: Got endpoints: latency-svc-7bncw [756.595647ms]
Apr  6 11:01:57.534: INFO: Created: latency-svc-jqkz6
Apr  6 11:01:57.555: INFO: Got endpoints: latency-svc-2rjjc [747.31078ms]
Apr  6 11:01:57.565: INFO: Created: latency-svc-jjb6r
Apr  6 11:01:57.611: INFO: Got endpoints: latency-svc-pvjwb [753.908601ms]
Apr  6 11:01:57.621: INFO: Created: latency-svc-b2rvm
Apr  6 11:01:57.657: INFO: Got endpoints: latency-svc-s26r6 [752.011744ms]
Apr  6 11:01:57.667: INFO: Created: latency-svc-59mwc
Apr  6 11:01:57.705: INFO: Got endpoints: latency-svc-mbdv4 [750.462842ms]
Apr  6 11:01:57.715: INFO: Created: latency-svc-4wbpp
Apr  6 11:01:57.756: INFO: Got endpoints: latency-svc-mbvml [744.008545ms]
Apr  6 11:01:57.766: INFO: Created: latency-svc-sznld
Apr  6 11:01:57.805: INFO: Got endpoints: latency-svc-rtghb [745.975288ms]
Apr  6 11:01:57.814: INFO: Created: latency-svc-jqfrv
Apr  6 11:01:57.856: INFO: Got endpoints: latency-svc-qqmvp [748.118596ms]
Apr  6 11:01:57.865: INFO: Created: latency-svc-vrbbp
Apr  6 11:01:57.911: INFO: Got endpoints: latency-svc-qgdch [753.735668ms]
Apr  6 11:01:57.928: INFO: Created: latency-svc-xpw2f
Apr  6 11:01:57.956: INFO: Got endpoints: latency-svc-m9g8w [747.804367ms]
Apr  6 11:01:57.966: INFO: Created: latency-svc-snb84
Apr  6 11:01:58.013: INFO: Got endpoints: latency-svc-4n9kr [754.581441ms]
Apr  6 11:01:58.029: INFO: Created: latency-svc-bcvx4
Apr  6 11:01:58.056: INFO: Got endpoints: latency-svc-mqht5 [749.820674ms]
Apr  6 11:01:58.066: INFO: Created: latency-svc-dztxx
Apr  6 11:01:58.110: INFO: Got endpoints: latency-svc-9pptb [752.385464ms]
Apr  6 11:01:58.120: INFO: Created: latency-svc-zs7xb
Apr  6 11:01:58.162: INFO: Got endpoints: latency-svc-cd2nr [753.311042ms]
Apr  6 11:01:58.173: INFO: Created: latency-svc-s9p8v
Apr  6 11:01:58.205: INFO: Got endpoints: latency-svc-p8rht [748.228425ms]
Apr  6 11:01:58.218: INFO: Created: latency-svc-48jft
Apr  6 11:01:58.256: INFO: Got endpoints: latency-svc-jqkz6 [740.758117ms]
Apr  6 11:01:58.265: INFO: Created: latency-svc-zcspr
Apr  6 11:01:58.306: INFO: Got endpoints: latency-svc-jjb6r [750.968985ms]
Apr  6 11:01:58.319: INFO: Created: latency-svc-tghcn
Apr  6 11:01:58.356: INFO: Got endpoints: latency-svc-b2rvm [744.815001ms]
Apr  6 11:01:58.365: INFO: Created: latency-svc-8tk2m
Apr  6 11:01:58.406: INFO: Got endpoints: latency-svc-59mwc [748.301524ms]
Apr  6 11:01:58.416: INFO: Created: latency-svc-mnfg8
Apr  6 11:01:58.455: INFO: Got endpoints: latency-svc-4wbpp [749.436288ms]
Apr  6 11:01:58.464: INFO: Created: latency-svc-mp549
Apr  6 11:01:58.505: INFO: Got endpoints: latency-svc-sznld [749.100295ms]
Apr  6 11:01:58.516: INFO: Created: latency-svc-7n95w
Apr  6 11:01:58.558: INFO: Got endpoints: latency-svc-jqfrv [752.904561ms]
Apr  6 11:01:58.570: INFO: Created: latency-svc-ljq4w
Apr  6 11:01:58.606: INFO: Got endpoints: latency-svc-vrbbp [750.303292ms]
Apr  6 11:01:58.619: INFO: Created: latency-svc-4pfxl
Apr  6 11:01:58.655: INFO: Got endpoints: latency-svc-xpw2f [743.829155ms]
Apr  6 11:01:58.667: INFO: Created: latency-svc-wxl27
Apr  6 11:01:58.717: INFO: Got endpoints: latency-svc-snb84 [760.5307ms]
Apr  6 11:01:58.728: INFO: Created: latency-svc-29bq5
Apr  6 11:01:58.762: INFO: Got endpoints: latency-svc-bcvx4 [749.015668ms]
Apr  6 11:01:58.778: INFO: Created: latency-svc-rp2q8
Apr  6 11:01:58.805: INFO: Got endpoints: latency-svc-dztxx [748.854083ms]
Apr  6 11:01:58.818: INFO: Created: latency-svc-6d5ld
Apr  6 11:01:58.857: INFO: Got endpoints: latency-svc-zs7xb [747.357297ms]
Apr  6 11:01:58.871: INFO: Created: latency-svc-g22gs
Apr  6 11:01:58.906: INFO: Got endpoints: latency-svc-s9p8v [743.946411ms]
Apr  6 11:01:58.928: INFO: Created: latency-svc-9rm8v
Apr  6 11:01:58.955: INFO: Got endpoints: latency-svc-48jft [749.897861ms]
Apr  6 11:01:58.970: INFO: Created: latency-svc-vgjz8
Apr  6 11:01:59.012: INFO: Got endpoints: latency-svc-zcspr [755.734067ms]
Apr  6 11:01:59.021: INFO: Created: latency-svc-brm5j
Apr  6 11:01:59.055: INFO: Got endpoints: latency-svc-tghcn [749.632911ms]
Apr  6 11:01:59.069: INFO: Created: latency-svc-v5s4s
Apr  6 11:01:59.109: INFO: Got endpoints: latency-svc-8tk2m [753.269587ms]
Apr  6 11:01:59.120: INFO: Created: latency-svc-zwmv4
Apr  6 11:01:59.155: INFO: Got endpoints: latency-svc-mnfg8 [749.014315ms]
Apr  6 11:01:59.165: INFO: Created: latency-svc-hgs25
Apr  6 11:01:59.206: INFO: Got endpoints: latency-svc-mp549 [751.020133ms]
Apr  6 11:01:59.219: INFO: Created: latency-svc-nf5bp
Apr  6 11:01:59.255: INFO: Got endpoints: latency-svc-7n95w [750.033267ms]
Apr  6 11:01:59.266: INFO: Created: latency-svc-dhxws
Apr  6 11:01:59.305: INFO: Got endpoints: latency-svc-ljq4w [747.433727ms]
Apr  6 11:01:59.321: INFO: Created: latency-svc-wx94m
Apr  6 11:01:59.355: INFO: Got endpoints: latency-svc-4pfxl [748.048595ms]
Apr  6 11:01:59.367: INFO: Created: latency-svc-2qct7
Apr  6 11:01:59.407: INFO: Got endpoints: latency-svc-wxl27 [752.376207ms]
Apr  6 11:01:59.436: INFO: Created: latency-svc-xxghz
Apr  6 11:01:59.455: INFO: Got endpoints: latency-svc-29bq5 [738.299561ms]
Apr  6 11:01:59.467: INFO: Created: latency-svc-r2mmq
Apr  6 11:01:59.507: INFO: Got endpoints: latency-svc-rp2q8 [744.613298ms]
Apr  6 11:01:59.519: INFO: Created: latency-svc-dsgmb
Apr  6 11:01:59.555: INFO: Got endpoints: latency-svc-6d5ld [749.303287ms]
Apr  6 11:01:59.567: INFO: Created: latency-svc-qnk24
Apr  6 11:01:59.605: INFO: Got endpoints: latency-svc-g22gs [747.789496ms]
Apr  6 11:01:59.614: INFO: Created: latency-svc-ltvb4
Apr  6 11:01:59.655: INFO: Got endpoints: latency-svc-9rm8v [748.705733ms]
Apr  6 11:01:59.664: INFO: Created: latency-svc-rlnl7
Apr  6 11:01:59.711: INFO: Got endpoints: latency-svc-vgjz8 [755.762416ms]
Apr  6 11:01:59.725: INFO: Created: latency-svc-8rwx8
Apr  6 11:01:59.755: INFO: Got endpoints: latency-svc-brm5j [743.595625ms]
Apr  6 11:01:59.768: INFO: Created: latency-svc-rs65m
Apr  6 11:01:59.806: INFO: Got endpoints: latency-svc-v5s4s [750.53725ms]
Apr  6 11:01:59.820: INFO: Created: latency-svc-66dnb
Apr  6 11:01:59.855: INFO: Got endpoints: latency-svc-zwmv4 [746.191085ms]
Apr  6 11:01:59.865: INFO: Created: latency-svc-9rbxv
Apr  6 11:01:59.919: INFO: Got endpoints: latency-svc-hgs25 [763.870734ms]
Apr  6 11:01:59.937: INFO: Created: latency-svc-wvrxb
Apr  6 11:01:59.964: INFO: Got endpoints: latency-svc-nf5bp [757.70569ms]
Apr  6 11:01:59.978: INFO: Created: latency-svc-lcrfh
Apr  6 11:02:00.007: INFO: Got endpoints: latency-svc-dhxws [751.44663ms]
Apr  6 11:02:00.021: INFO: Created: latency-svc-gvqbw
Apr  6 11:02:00.056: INFO: Got endpoints: latency-svc-wx94m [750.673915ms]
Apr  6 11:02:00.068: INFO: Created: latency-svc-fnfjs
Apr  6 11:02:00.105: INFO: Got endpoints: latency-svc-2qct7 [750.483681ms]
Apr  6 11:02:00.114: INFO: Created: latency-svc-8z2wh
Apr  6 11:02:00.157: INFO: Got endpoints: latency-svc-xxghz [750.099175ms]
Apr  6 11:02:00.170: INFO: Created: latency-svc-mbtv6
Apr  6 11:02:00.206: INFO: Got endpoints: latency-svc-r2mmq [750.710974ms]
Apr  6 11:02:00.217: INFO: Created: latency-svc-kvtwv
Apr  6 11:02:00.254: INFO: Got endpoints: latency-svc-dsgmb [746.881063ms]
Apr  6 11:02:00.270: INFO: Created: latency-svc-4dj8f
Apr  6 11:02:00.304: INFO: Got endpoints: latency-svc-qnk24 [749.524059ms]
Apr  6 11:02:00.319: INFO: Created: latency-svc-8nwnz
Apr  6 11:02:00.358: INFO: Got endpoints: latency-svc-ltvb4 [752.723187ms]
Apr  6 11:02:00.368: INFO: Created: latency-svc-mx6qz
Apr  6 11:02:00.406: INFO: Got endpoints: latency-svc-rlnl7 [751.540152ms]
Apr  6 11:02:00.420: INFO: Created: latency-svc-wq7xq
Apr  6 11:02:00.455: INFO: Got endpoints: latency-svc-8rwx8 [743.855537ms]
Apr  6 11:02:00.470: INFO: Created: latency-svc-kdwfk
Apr  6 11:02:00.507: INFO: Got endpoints: latency-svc-rs65m [751.383815ms]
Apr  6 11:02:00.555: INFO: Got endpoints: latency-svc-66dnb [748.815723ms]
Apr  6 11:02:00.606: INFO: Got endpoints: latency-svc-9rbxv [751.051598ms]
Apr  6 11:02:00.659: INFO: Got endpoints: latency-svc-wvrxb [739.663241ms]
Apr  6 11:02:00.715: INFO: Got endpoints: latency-svc-lcrfh [751.615661ms]
Apr  6 11:02:00.755: INFO: Got endpoints: latency-svc-gvqbw [747.936098ms]
Apr  6 11:02:00.806: INFO: Got endpoints: latency-svc-fnfjs [749.830252ms]
Apr  6 11:02:00.856: INFO: Got endpoints: latency-svc-8z2wh [750.361206ms]
Apr  6 11:02:00.905: INFO: Got endpoints: latency-svc-mbtv6 [747.865407ms]
Apr  6 11:02:00.956: INFO: Got endpoints: latency-svc-kvtwv [749.436015ms]
Apr  6 11:02:01.007: INFO: Got endpoints: latency-svc-4dj8f [752.667602ms]
Apr  6 11:02:01.067: INFO: Got endpoints: latency-svc-8nwnz [762.587905ms]
Apr  6 11:02:01.107: INFO: Got endpoints: latency-svc-mx6qz [749.303956ms]
Apr  6 11:02:01.156: INFO: Got endpoints: latency-svc-wq7xq [749.030226ms]
Apr  6 11:02:01.204: INFO: Got endpoints: latency-svc-kdwfk [749.367562ms]
Apr  6 11:02:01.205: INFO: Latencies: [24.3208ms 25.503001ms 27.195323ms 41.939006ms 42.984461ms 47.305756ms 53.464495ms 67.233921ms 71.700052ms 79.335278ms 79.746745ms 90.474097ms 93.423402ms 93.560864ms 100.246331ms 101.325226ms 104.275637ms 105.011823ms 105.441568ms 107.638318ms 108.162183ms 110.744463ms 117.464606ms 120.493743ms 131.737834ms 132.110293ms 136.760802ms 139.882326ms 147.960954ms 148.21476ms 149.012484ms 154.154534ms 160.051748ms 161.069077ms 166.115849ms 196.182172ms 225.377806ms 268.039122ms 311.282177ms 336.901783ms 380.727718ms 424.057437ms 468.075839ms 506.37186ms 527.99367ms 575.415295ms 621.625298ms 662.566086ms 708.621329ms 735.016361ms 738.299561ms 738.870194ms 739.663241ms 739.733031ms 740.758117ms 742.783075ms 743.019351ms 743.058753ms 743.080249ms 743.257324ms 743.478179ms 743.595625ms 743.829155ms 743.855537ms 743.946411ms 743.993131ms 744.008545ms 744.121305ms 744.477257ms 744.613298ms 744.750569ms 744.815001ms 744.930089ms 745.919108ms 745.975288ms 746.191085ms 746.881063ms 746.949003ms 747.31078ms 747.357297ms 747.433727ms 747.602355ms 747.612409ms 747.673619ms 747.753524ms 747.789496ms 747.804367ms 747.856893ms 747.864164ms 747.865407ms 747.936098ms 747.941988ms 748.048595ms 748.118596ms 748.228425ms 748.301524ms 748.356834ms 748.547391ms 748.665267ms 748.705733ms 748.716716ms 748.765004ms 748.815723ms 748.854083ms 749.014315ms 749.015668ms 749.030226ms 749.100295ms 749.157175ms 749.299189ms 749.303287ms 749.303956ms 749.367562ms 749.41881ms 749.424026ms 749.436015ms 749.436288ms 749.524059ms 749.592373ms 749.624332ms 749.632911ms 749.74521ms 749.786542ms 749.820674ms 749.830252ms 749.897861ms 749.992917ms 750.033267ms 750.061826ms 750.085234ms 750.087583ms 750.099175ms 750.111174ms 750.124108ms 750.303292ms 750.361206ms 750.462842ms 750.483681ms 750.50093ms 750.53725ms 750.645653ms 750.673915ms 750.710974ms 750.800883ms 750.968985ms 751.020133ms 751.04822ms 751.051598ms 751.087538ms 751.288997ms 751.383815ms 751.44663ms 751.537598ms 751.540152ms 751.606307ms 751.615661ms 751.749943ms 751.926775ms 752.009501ms 752.011744ms 752.254288ms 752.264877ms 752.321393ms 752.376207ms 752.385464ms 752.667602ms 752.723187ms 752.77368ms 752.787644ms 752.880863ms 752.897567ms 752.904561ms 752.961329ms 753.093042ms 753.269587ms 753.311042ms 753.53884ms 753.735668ms 753.908601ms 754.027258ms 754.581441ms 754.778629ms 755.539714ms 755.578217ms 755.652534ms 755.734067ms 755.762416ms 756.171805ms 756.513323ms 756.595647ms 757.358982ms 757.48166ms 757.70569ms 758.686949ms 760.026578ms 760.5307ms 760.696096ms 762.064584ms 762.587905ms 763.870734ms]
Apr  6 11:02:01.205: INFO: 50 %ile: 748.716716ms
Apr  6 11:02:01.205: INFO: 90 %ile: 754.581441ms
Apr  6 11:02:01.205: INFO: 99 %ile: 762.587905ms
Apr  6 11:02:01.205: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Apr  6 11:02:01.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1672" for this suite. 04/06/23 11:02:01.216
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":190,"skipped":3414,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.775 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:01:50.448
    Apr  6 11:01:50.448: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename svc-latency 04/06/23 11:01:50.449
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:01:50.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:01:50.467
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Apr  6 11:01:50.475: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1672 04/06/23 11:01:50.476
    I0406 11:01:50.487543      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1672, replica count: 1
    I0406 11:01:51.539443      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0406 11:01:52.539757      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 11:01:52.652: INFO: Created: latency-svc-bvbhs
    Apr  6 11:01:52.654: INFO: Got endpoints: latency-svc-bvbhs [14.059891ms]
    Apr  6 11:01:52.666: INFO: Created: latency-svc-dvcvm
    Apr  6 11:01:52.679: INFO: Got endpoints: latency-svc-dvcvm [24.3208ms]
    Apr  6 11:01:52.680: INFO: Created: latency-svc-dxdf9
    Apr  6 11:01:52.680: INFO: Got endpoints: latency-svc-dxdf9 [25.503001ms]
    Apr  6 11:01:52.680: INFO: Created: latency-svc-cdbpq
    Apr  6 11:01:52.682: INFO: Got endpoints: latency-svc-cdbpq [27.195323ms]
    Apr  6 11:01:52.696: INFO: Created: latency-svc-2ndk9
    Apr  6 11:01:52.697: INFO: Got endpoints: latency-svc-2ndk9 [41.939006ms]
    Apr  6 11:01:52.697: INFO: Created: latency-svc-nmbs9
    Apr  6 11:01:52.698: INFO: Got endpoints: latency-svc-nmbs9 [42.984461ms]
    Apr  6 11:01:52.698: INFO: Created: latency-svc-gpnln
    Apr  6 11:01:52.703: INFO: Got endpoints: latency-svc-gpnln [47.305756ms]
    Apr  6 11:01:52.704: INFO: Created: latency-svc-g5tq4
    Apr  6 11:01:52.709: INFO: Got endpoints: latency-svc-g5tq4 [53.464495ms]
    Apr  6 11:01:52.710: INFO: Created: latency-svc-vp5v8
    Apr  6 11:01:52.723: INFO: Got endpoints: latency-svc-vp5v8 [67.233921ms]
    Apr  6 11:01:52.727: INFO: Created: latency-svc-j7jsh
    Apr  6 11:01:52.728: INFO: Got endpoints: latency-svc-j7jsh [71.700052ms]
    Apr  6 11:01:52.735: INFO: Created: latency-svc-fxm67
    Apr  6 11:01:52.736: INFO: Created: latency-svc-dwm6g
    Apr  6 11:01:52.736: INFO: Got endpoints: latency-svc-dwm6g [79.335278ms]
    Apr  6 11:01:52.736: INFO: Got endpoints: latency-svc-fxm67 [79.746745ms]
    Apr  6 11:01:52.737: INFO: Created: latency-svc-l9hkn
    Apr  6 11:01:52.750: INFO: Created: latency-svc-6d5qg
    Apr  6 11:01:52.750: INFO: Got endpoints: latency-svc-l9hkn [93.560864ms]
    Apr  6 11:01:52.750: INFO: Created: latency-svc-wsqgd
    Apr  6 11:01:52.750: INFO: Got endpoints: latency-svc-6d5qg [93.423402ms]
    Apr  6 11:01:52.755: INFO: Created: latency-svc-rb6v4
    Apr  6 11:01:52.756: INFO: Got endpoints: latency-svc-wsqgd [100.246331ms]
    Apr  6 11:01:52.761: INFO: Got endpoints: latency-svc-rb6v4 [105.011823ms]
    Apr  6 11:01:52.762: INFO: Created: latency-svc-wgb7m
    Apr  6 11:01:52.770: INFO: Got endpoints: latency-svc-wgb7m [90.474097ms]
    Apr  6 11:01:52.772: INFO: Created: latency-svc-zpjk2
    Apr  6 11:01:52.785: INFO: Got endpoints: latency-svc-zpjk2 [105.441568ms]
    Apr  6 11:01:52.786: INFO: Created: latency-svc-m68k9
    Apr  6 11:01:52.787: INFO: Got endpoints: latency-svc-m68k9 [104.275637ms]
    Apr  6 11:01:52.796: INFO: Created: latency-svc-n22ct
    Apr  6 11:01:52.799: INFO: Got endpoints: latency-svc-n22ct [101.325226ms]
    Apr  6 11:01:52.800: INFO: Created: latency-svc-drt5x
    Apr  6 11:01:52.806: INFO: Got endpoints: latency-svc-drt5x [107.638318ms]
    Apr  6 11:01:52.811: INFO: Created: latency-svc-llpr2
    Apr  6 11:01:52.811: INFO: Got endpoints: latency-svc-llpr2 [108.162183ms]
    Apr  6 11:01:52.827: INFO: Created: latency-svc-p2kfv
    Apr  6 11:01:52.830: INFO: Got endpoints: latency-svc-p2kfv [120.493743ms]
    Apr  6 11:01:52.836: INFO: Created: latency-svc-n7rq5
    Apr  6 11:01:52.837: INFO: Got endpoints: latency-svc-n7rq5 [110.744463ms]
    Apr  6 11:01:52.841: INFO: Created: latency-svc-t8zg2
    Apr  6 11:01:52.845: INFO: Got endpoints: latency-svc-t8zg2 [117.464606ms]
    Apr  6 11:01:52.852: INFO: Created: latency-svc-vmvsc
    Apr  6 11:01:52.868: INFO: Got endpoints: latency-svc-vmvsc [132.110293ms]
    Apr  6 11:01:52.876: INFO: Created: latency-svc-2nkfb
    Apr  6 11:01:52.876: INFO: Got endpoints: latency-svc-2nkfb [139.882326ms]
    Apr  6 11:01:52.878: INFO: Created: latency-svc-5pdlj
    Apr  6 11:01:52.882: INFO: Got endpoints: latency-svc-5pdlj [131.737834ms]
    Apr  6 11:01:52.883: INFO: Created: latency-svc-cs6rp
    Apr  6 11:01:52.887: INFO: Got endpoints: latency-svc-cs6rp [136.760802ms]
    Apr  6 11:01:52.888: INFO: Created: latency-svc-gwtkf
    Apr  6 11:01:52.905: INFO: Got endpoints: latency-svc-gwtkf [149.012484ms]
    Apr  6 11:01:52.906: INFO: Created: latency-svc-9t427
    Apr  6 11:01:52.927: INFO: Created: latency-svc-x7g9m
    Apr  6 11:01:52.927: INFO: Got endpoints: latency-svc-9t427 [166.115849ms]
    Apr  6 11:01:52.930: INFO: Got endpoints: latency-svc-x7g9m [160.051748ms]
    Apr  6 11:01:52.931: INFO: Created: latency-svc-gt8jl
    Apr  6 11:01:52.933: INFO: Got endpoints: latency-svc-gt8jl [147.960954ms]
    Apr  6 11:01:52.937: INFO: Created: latency-svc-fpbbr
    Apr  6 11:01:52.947: INFO: Created: latency-svc-hdbm4
    Apr  6 11:01:52.947: INFO: Got endpoints: latency-svc-hdbm4 [148.21476ms]
    Apr  6 11:01:52.948: INFO: Created: latency-svc-fqxgf
    Apr  6 11:01:52.948: INFO: Got endpoints: latency-svc-fpbbr [161.069077ms]
    Apr  6 11:01:52.960: INFO: Got endpoints: latency-svc-fqxgf [154.154534ms]
    Apr  6 11:01:52.960: INFO: Created: latency-svc-jfbck
    Apr  6 11:01:52.960: INFO: Created: latency-svc-bsml4
    Apr  6 11:01:52.962: INFO: Created: latency-svc-bw88n
    Apr  6 11:01:52.967: INFO: Created: latency-svc-8wfmf
    Apr  6 11:01:52.979: INFO: Created: latency-svc-2xchj
    Apr  6 11:01:52.979: INFO: Created: latency-svc-lvbm9
    Apr  6 11:01:52.980: INFO: Created: latency-svc-5ntnb
    Apr  6 11:01:52.988: INFO: Created: latency-svc-pbr7q
    Apr  6 11:01:52.995: INFO: Created: latency-svc-gf24c
    Apr  6 11:01:52.995: INFO: Created: latency-svc-mqgbt
    Apr  6 11:01:53.005: INFO: Created: latency-svc-5kn7b
    Apr  6 11:01:53.005: INFO: Created: latency-svc-lqvsg
    Apr  6 11:01:53.007: INFO: Got endpoints: latency-svc-jfbck [196.182172ms]
    Apr  6 11:01:53.014: INFO: Created: latency-svc-qzfgz
    Apr  6 11:01:53.016: INFO: Created: latency-svc-p57ln
    Apr  6 11:01:53.019: INFO: Created: latency-svc-r4mdl
    Apr  6 11:01:53.028: INFO: Created: latency-svc-8xhn2
    Apr  6 11:01:53.055: INFO: Got endpoints: latency-svc-bsml4 [225.377806ms]
    Apr  6 11:01:53.066: INFO: Created: latency-svc-h6pvv
    Apr  6 11:01:53.105: INFO: Got endpoints: latency-svc-bw88n [268.039122ms]
    Apr  6 11:01:53.126: INFO: Created: latency-svc-6x46k
    Apr  6 11:01:53.156: INFO: Got endpoints: latency-svc-8wfmf [311.282177ms]
    Apr  6 11:01:53.192: INFO: Created: latency-svc-n4j56
    Apr  6 11:01:53.205: INFO: Got endpoints: latency-svc-2xchj [336.901783ms]
    Apr  6 11:01:53.218: INFO: Created: latency-svc-dktkr
    Apr  6 11:01:53.257: INFO: Got endpoints: latency-svc-lvbm9 [380.727718ms]
    Apr  6 11:01:53.271: INFO: Created: latency-svc-dpmjb
    Apr  6 11:01:53.306: INFO: Got endpoints: latency-svc-5ntnb [424.057437ms]
    Apr  6 11:01:53.317: INFO: Created: latency-svc-s44mp
    Apr  6 11:01:53.355: INFO: Got endpoints: latency-svc-pbr7q [468.075839ms]
    Apr  6 11:01:53.366: INFO: Created: latency-svc-clhm8
    Apr  6 11:01:53.412: INFO: Got endpoints: latency-svc-gf24c [506.37186ms]
    Apr  6 11:01:53.434: INFO: Created: latency-svc-tfxnc
    Apr  6 11:01:53.456: INFO: Got endpoints: latency-svc-mqgbt [527.99367ms]
    Apr  6 11:01:53.466: INFO: Created: latency-svc-2hskv
    Apr  6 11:01:53.505: INFO: Got endpoints: latency-svc-5kn7b [575.415295ms]
    Apr  6 11:01:53.519: INFO: Created: latency-svc-v2l9s
    Apr  6 11:01:53.555: INFO: Got endpoints: latency-svc-lqvsg [621.625298ms]
    Apr  6 11:01:53.567: INFO: Created: latency-svc-5tbft
    Apr  6 11:01:53.610: INFO: Got endpoints: latency-svc-qzfgz [662.566086ms]
    Apr  6 11:01:53.624: INFO: Created: latency-svc-h2bp4
    Apr  6 11:01:53.656: INFO: Got endpoints: latency-svc-p57ln [708.621329ms]
    Apr  6 11:01:53.668: INFO: Created: latency-svc-ch9nz
    Apr  6 11:01:53.713: INFO: Got endpoints: latency-svc-r4mdl [752.77368ms]
    Apr  6 11:01:53.730: INFO: Created: latency-svc-pls8p
    Apr  6 11:01:53.755: INFO: Got endpoints: latency-svc-8xhn2 [747.856893ms]
    Apr  6 11:01:53.767: INFO: Created: latency-svc-8kwhg
    Apr  6 11:01:53.807: INFO: Got endpoints: latency-svc-h6pvv [751.606307ms]
    Apr  6 11:01:53.835: INFO: Created: latency-svc-jj5kw
    Apr  6 11:01:53.856: INFO: Got endpoints: latency-svc-6x46k [750.085234ms]
    Apr  6 11:01:53.873: INFO: Created: latency-svc-kwgbs
    Apr  6 11:01:53.916: INFO: Got endpoints: latency-svc-n4j56 [760.026578ms]
    Apr  6 11:01:53.927: INFO: Created: latency-svc-r94gs
    Apr  6 11:01:53.962: INFO: Got endpoints: latency-svc-dktkr [757.48166ms]
    Apr  6 11:01:53.980: INFO: Created: latency-svc-2wwcr
    Apr  6 11:01:54.004: INFO: Got endpoints: latency-svc-dpmjb [747.602355ms]
    Apr  6 11:01:54.020: INFO: Created: latency-svc-zwcbx
    Apr  6 11:01:54.062: INFO: Got endpoints: latency-svc-s44mp [755.539714ms]
    Apr  6 11:01:54.073: INFO: Created: latency-svc-97nmh
    Apr  6 11:01:54.111: INFO: Got endpoints: latency-svc-clhm8 [755.652534ms]
    Apr  6 11:01:54.124: INFO: Created: latency-svc-4kr67
    Apr  6 11:01:54.156: INFO: Got endpoints: latency-svc-tfxnc [742.783075ms]
    Apr  6 11:01:54.167: INFO: Created: latency-svc-jcgbm
    Apr  6 11:01:54.204: INFO: Got endpoints: latency-svc-2hskv [748.716716ms]
    Apr  6 11:01:54.228: INFO: Created: latency-svc-n5ltv
    Apr  6 11:01:54.255: INFO: Got endpoints: latency-svc-v2l9s [749.424026ms]
    Apr  6 11:01:54.266: INFO: Created: latency-svc-pbs55
    Apr  6 11:01:54.305: INFO: Got endpoints: latency-svc-5tbft [749.592373ms]
    Apr  6 11:01:54.314: INFO: Created: latency-svc-5csqn
    Apr  6 11:01:54.358: INFO: Got endpoints: latency-svc-h2bp4 [747.941988ms]
    Apr  6 11:01:54.369: INFO: Created: latency-svc-nwg52
    Apr  6 11:01:54.405: INFO: Got endpoints: latency-svc-ch9nz [748.665267ms]
    Apr  6 11:01:54.416: INFO: Created: latency-svc-jvv5l
    Apr  6 11:01:54.457: INFO: Got endpoints: latency-svc-pls8p [743.257324ms]
    Apr  6 11:01:54.475: INFO: Created: latency-svc-zjhr8
    Apr  6 11:01:54.511: INFO: Got endpoints: latency-svc-8kwhg [756.171805ms]
    Apr  6 11:01:54.521: INFO: Created: latency-svc-krgtq
    Apr  6 11:01:54.555: INFO: Got endpoints: latency-svc-jj5kw [747.753524ms]
    Apr  6 11:01:54.565: INFO: Created: latency-svc-khb6m
    Apr  6 11:01:54.609: INFO: Got endpoints: latency-svc-kwgbs [753.53884ms]
    Apr  6 11:01:54.625: INFO: Created: latency-svc-6r5m8
    Apr  6 11:01:54.668: INFO: Got endpoints: latency-svc-r94gs [751.749943ms]
    Apr  6 11:01:54.678: INFO: Created: latency-svc-2j4h5
    Apr  6 11:01:54.705: INFO: Got endpoints: latency-svc-2wwcr [743.019351ms]
    Apr  6 11:01:54.721: INFO: Created: latency-svc-gbvvj
    Apr  6 11:01:54.761: INFO: Got endpoints: latency-svc-zwcbx [756.513323ms]
    Apr  6 11:01:54.781: INFO: Created: latency-svc-dxwtv
    Apr  6 11:01:54.806: INFO: Got endpoints: latency-svc-97nmh [743.993131ms]
    Apr  6 11:01:54.821: INFO: Created: latency-svc-r7zj4
    Apr  6 11:01:54.855: INFO: Got endpoints: latency-svc-4kr67 [743.478179ms]
    Apr  6 11:01:54.865: INFO: Created: latency-svc-pkz89
    Apr  6 11:01:54.906: INFO: Got endpoints: latency-svc-jcgbm [750.124108ms]
    Apr  6 11:01:54.916: INFO: Created: latency-svc-mx5cr
    Apr  6 11:01:54.956: INFO: Got endpoints: latency-svc-n5ltv [751.537598ms]
    Apr  6 11:01:54.967: INFO: Created: latency-svc-jrs8f
    Apr  6 11:01:55.005: INFO: Got endpoints: latency-svc-pbs55 [750.111174ms]
    Apr  6 11:01:55.022: INFO: Created: latency-svc-tnnxm
    Apr  6 11:01:55.057: INFO: Got endpoints: latency-svc-5csqn [752.254288ms]
    Apr  6 11:01:55.081: INFO: Created: latency-svc-cs26g
    Apr  6 11:01:55.105: INFO: Got endpoints: latency-svc-nwg52 [746.949003ms]
    Apr  6 11:01:55.115: INFO: Created: latency-svc-9qz6v
    Apr  6 11:01:55.160: INFO: Got endpoints: latency-svc-jvv5l [754.778629ms]
    Apr  6 11:01:55.172: INFO: Created: latency-svc-kcssn
    Apr  6 11:01:55.217: INFO: Got endpoints: latency-svc-zjhr8 [760.696096ms]
    Apr  6 11:01:55.229: INFO: Created: latency-svc-jzhf2
    Apr  6 11:01:55.255: INFO: Got endpoints: latency-svc-krgtq [743.080249ms]
    Apr  6 11:01:55.265: INFO: Created: latency-svc-tqwtp
    Apr  6 11:01:55.311: INFO: Got endpoints: latency-svc-khb6m [755.578217ms]
    Apr  6 11:01:55.321: INFO: Created: latency-svc-v2ndr
    Apr  6 11:01:55.371: INFO: Got endpoints: latency-svc-6r5m8 [762.064584ms]
    Apr  6 11:01:55.382: INFO: Created: latency-svc-8d7cj
    Apr  6 11:01:55.408: INFO: Got endpoints: latency-svc-2j4h5 [739.733031ms]
    Apr  6 11:01:55.418: INFO: Created: latency-svc-mzxhq
    Apr  6 11:01:55.456: INFO: Got endpoints: latency-svc-gbvvj [751.04822ms]
    Apr  6 11:01:55.465: INFO: Created: latency-svc-htddq
    Apr  6 11:01:55.505: INFO: Got endpoints: latency-svc-dxwtv [744.121305ms]
    Apr  6 11:01:55.517: INFO: Created: latency-svc-msjcb
    Apr  6 11:01:55.554: INFO: Got endpoints: latency-svc-r7zj4 [747.612409ms]
    Apr  6 11:01:55.562: INFO: Created: latency-svc-tp469
    Apr  6 11:01:55.604: INFO: Got endpoints: latency-svc-pkz89 [749.74521ms]
    Apr  6 11:01:55.616: INFO: Created: latency-svc-p2q4h
    Apr  6 11:01:55.653: INFO: Got endpoints: latency-svc-mx5cr [747.673619ms]
    Apr  6 11:01:55.671: INFO: Created: latency-svc-2s89z
    Apr  6 11:01:55.706: INFO: Got endpoints: latency-svc-jrs8f [750.087583ms]
    Apr  6 11:01:55.717: INFO: Created: latency-svc-rh8d2
    Apr  6 11:01:55.757: INFO: Got endpoints: latency-svc-tnnxm [751.926775ms]
    Apr  6 11:01:55.771: INFO: Created: latency-svc-hs4zw
    Apr  6 11:01:55.806: INFO: Got endpoints: latency-svc-cs26g [744.750569ms]
    Apr  6 11:01:55.818: INFO: Created: latency-svc-45b48
    Apr  6 11:01:55.858: INFO: Got endpoints: latency-svc-9qz6v [753.093042ms]
    Apr  6 11:01:55.868: INFO: Created: latency-svc-wfcmd
    Apr  6 11:01:55.905: INFO: Got endpoints: latency-svc-kcssn [744.930089ms]
    Apr  6 11:01:55.915: INFO: Created: latency-svc-tlhcq
    Apr  6 11:01:55.956: INFO: Got endpoints: latency-svc-jzhf2 [738.870194ms]
    Apr  6 11:01:55.973: INFO: Created: latency-svc-h52v4
    Apr  6 11:01:56.008: INFO: Got endpoints: latency-svc-tqwtp [752.961329ms]
    Apr  6 11:01:56.019: INFO: Created: latency-svc-nmbcz
    Apr  6 11:01:56.055: INFO: Got endpoints: latency-svc-v2ndr [744.477257ms]
    Apr  6 11:01:56.065: INFO: Created: latency-svc-7v7xf
    Apr  6 11:01:56.107: INFO: Got endpoints: latency-svc-8d7cj [735.016361ms]
    Apr  6 11:01:56.116: INFO: Created: latency-svc-xq76f
    Apr  6 11:01:56.156: INFO: Got endpoints: latency-svc-mzxhq [748.356834ms]
    Apr  6 11:01:56.175: INFO: Created: latency-svc-cjhnp
    Apr  6 11:01:56.205: INFO: Got endpoints: latency-svc-htddq [748.547391ms]
    Apr  6 11:01:56.217: INFO: Created: latency-svc-jwd7w
    Apr  6 11:01:56.255: INFO: Got endpoints: latency-svc-msjcb [749.299189ms]
    Apr  6 11:01:56.264: INFO: Created: latency-svc-ngbbl
    Apr  6 11:01:56.305: INFO: Got endpoints: latency-svc-tp469 [750.800883ms]
    Apr  6 11:01:56.316: INFO: Created: latency-svc-sdlgm
    Apr  6 11:01:56.355: INFO: Got endpoints: latency-svc-p2q4h [750.645653ms]
    Apr  6 11:01:56.365: INFO: Created: latency-svc-bhm72
    Apr  6 11:01:56.406: INFO: Got endpoints: latency-svc-2s89z [752.264877ms]
    Apr  6 11:01:56.428: INFO: Created: latency-svc-xzttq
    Apr  6 11:01:56.456: INFO: Got endpoints: latency-svc-rh8d2 [749.786542ms]
    Apr  6 11:01:56.481: INFO: Created: latency-svc-g5psz
    Apr  6 11:01:56.515: INFO: Got endpoints: latency-svc-hs4zw [758.686949ms]
    Apr  6 11:01:56.525: INFO: Created: latency-svc-wck99
    Apr  6 11:01:56.555: INFO: Got endpoints: latency-svc-45b48 [749.41881ms]
    Apr  6 11:01:56.564: INFO: Created: latency-svc-ljts6
    Apr  6 11:01:56.611: INFO: Got endpoints: latency-svc-wfcmd [752.897567ms]
    Apr  6 11:01:56.624: INFO: Created: latency-svc-6kzbr
    Apr  6 11:01:56.656: INFO: Got endpoints: latency-svc-tlhcq [749.992917ms]
    Apr  6 11:01:56.669: INFO: Created: latency-svc-2gqmh
    Apr  6 11:01:56.704: INFO: Got endpoints: latency-svc-h52v4 [747.864164ms]
    Apr  6 11:01:56.716: INFO: Created: latency-svc-8trmh
    Apr  6 11:01:56.758: INFO: Got endpoints: latency-svc-nmbcz [750.50093ms]
    Apr  6 11:01:56.784: INFO: Created: latency-svc-7bncw
    Apr  6 11:01:56.807: INFO: Got endpoints: latency-svc-7v7xf [752.009501ms]
    Apr  6 11:01:56.817: INFO: Created: latency-svc-2rjjc
    Apr  6 11:01:56.857: INFO: Got endpoints: latency-svc-xq76f [750.061826ms]
    Apr  6 11:01:56.881: INFO: Created: latency-svc-pvjwb
    Apr  6 11:01:56.905: INFO: Got endpoints: latency-svc-cjhnp [748.765004ms]
    Apr  6 11:01:56.914: INFO: Created: latency-svc-s26r6
    Apr  6 11:01:56.955: INFO: Got endpoints: latency-svc-jwd7w [749.624332ms]
    Apr  6 11:01:56.966: INFO: Created: latency-svc-mbdv4
    Apr  6 11:01:57.012: INFO: Got endpoints: latency-svc-ngbbl [757.358982ms]
    Apr  6 11:01:57.034: INFO: Created: latency-svc-mbvml
    Apr  6 11:01:57.059: INFO: Got endpoints: latency-svc-sdlgm [754.027258ms]
    Apr  6 11:01:57.069: INFO: Created: latency-svc-rtghb
    Apr  6 11:01:57.108: INFO: Got endpoints: latency-svc-bhm72 [752.880863ms]
    Apr  6 11:01:57.117: INFO: Created: latency-svc-qqmvp
    Apr  6 11:01:57.157: INFO: Got endpoints: latency-svc-xzttq [751.087538ms]
    Apr  6 11:01:57.187: INFO: Created: latency-svc-qgdch
    Apr  6 11:01:57.206: INFO: Got endpoints: latency-svc-g5psz [749.157175ms]
    Apr  6 11:01:57.226: INFO: Created: latency-svc-m9g8w
    Apr  6 11:01:57.259: INFO: Got endpoints: latency-svc-wck99 [743.058753ms]
    Apr  6 11:01:57.271: INFO: Created: latency-svc-4n9kr
    Apr  6 11:01:57.306: INFO: Got endpoints: latency-svc-ljts6 [751.288997ms]
    Apr  6 11:01:57.318: INFO: Created: latency-svc-mqht5
    Apr  6 11:01:57.357: INFO: Got endpoints: latency-svc-6kzbr [745.919108ms]
    Apr  6 11:01:57.380: INFO: Created: latency-svc-9pptb
    Apr  6 11:01:57.408: INFO: Got endpoints: latency-svc-2gqmh [752.787644ms]
    Apr  6 11:01:57.423: INFO: Created: latency-svc-cd2nr
    Apr  6 11:01:57.457: INFO: Got endpoints: latency-svc-8trmh [752.321393ms]
    Apr  6 11:01:57.467: INFO: Created: latency-svc-p8rht
    Apr  6 11:01:57.515: INFO: Got endpoints: latency-svc-7bncw [756.595647ms]
    Apr  6 11:01:57.534: INFO: Created: latency-svc-jqkz6
    Apr  6 11:01:57.555: INFO: Got endpoints: latency-svc-2rjjc [747.31078ms]
    Apr  6 11:01:57.565: INFO: Created: latency-svc-jjb6r
    Apr  6 11:01:57.611: INFO: Got endpoints: latency-svc-pvjwb [753.908601ms]
    Apr  6 11:01:57.621: INFO: Created: latency-svc-b2rvm
    Apr  6 11:01:57.657: INFO: Got endpoints: latency-svc-s26r6 [752.011744ms]
    Apr  6 11:01:57.667: INFO: Created: latency-svc-59mwc
    Apr  6 11:01:57.705: INFO: Got endpoints: latency-svc-mbdv4 [750.462842ms]
    Apr  6 11:01:57.715: INFO: Created: latency-svc-4wbpp
    Apr  6 11:01:57.756: INFO: Got endpoints: latency-svc-mbvml [744.008545ms]
    Apr  6 11:01:57.766: INFO: Created: latency-svc-sznld
    Apr  6 11:01:57.805: INFO: Got endpoints: latency-svc-rtghb [745.975288ms]
    Apr  6 11:01:57.814: INFO: Created: latency-svc-jqfrv
    Apr  6 11:01:57.856: INFO: Got endpoints: latency-svc-qqmvp [748.118596ms]
    Apr  6 11:01:57.865: INFO: Created: latency-svc-vrbbp
    Apr  6 11:01:57.911: INFO: Got endpoints: latency-svc-qgdch [753.735668ms]
    Apr  6 11:01:57.928: INFO: Created: latency-svc-xpw2f
    Apr  6 11:01:57.956: INFO: Got endpoints: latency-svc-m9g8w [747.804367ms]
    Apr  6 11:01:57.966: INFO: Created: latency-svc-snb84
    Apr  6 11:01:58.013: INFO: Got endpoints: latency-svc-4n9kr [754.581441ms]
    Apr  6 11:01:58.029: INFO: Created: latency-svc-bcvx4
    Apr  6 11:01:58.056: INFO: Got endpoints: latency-svc-mqht5 [749.820674ms]
    Apr  6 11:01:58.066: INFO: Created: latency-svc-dztxx
    Apr  6 11:01:58.110: INFO: Got endpoints: latency-svc-9pptb [752.385464ms]
    Apr  6 11:01:58.120: INFO: Created: latency-svc-zs7xb
    Apr  6 11:01:58.162: INFO: Got endpoints: latency-svc-cd2nr [753.311042ms]
    Apr  6 11:01:58.173: INFO: Created: latency-svc-s9p8v
    Apr  6 11:01:58.205: INFO: Got endpoints: latency-svc-p8rht [748.228425ms]
    Apr  6 11:01:58.218: INFO: Created: latency-svc-48jft
    Apr  6 11:01:58.256: INFO: Got endpoints: latency-svc-jqkz6 [740.758117ms]
    Apr  6 11:01:58.265: INFO: Created: latency-svc-zcspr
    Apr  6 11:01:58.306: INFO: Got endpoints: latency-svc-jjb6r [750.968985ms]
    Apr  6 11:01:58.319: INFO: Created: latency-svc-tghcn
    Apr  6 11:01:58.356: INFO: Got endpoints: latency-svc-b2rvm [744.815001ms]
    Apr  6 11:01:58.365: INFO: Created: latency-svc-8tk2m
    Apr  6 11:01:58.406: INFO: Got endpoints: latency-svc-59mwc [748.301524ms]
    Apr  6 11:01:58.416: INFO: Created: latency-svc-mnfg8
    Apr  6 11:01:58.455: INFO: Got endpoints: latency-svc-4wbpp [749.436288ms]
    Apr  6 11:01:58.464: INFO: Created: latency-svc-mp549
    Apr  6 11:01:58.505: INFO: Got endpoints: latency-svc-sznld [749.100295ms]
    Apr  6 11:01:58.516: INFO: Created: latency-svc-7n95w
    Apr  6 11:01:58.558: INFO: Got endpoints: latency-svc-jqfrv [752.904561ms]
    Apr  6 11:01:58.570: INFO: Created: latency-svc-ljq4w
    Apr  6 11:01:58.606: INFO: Got endpoints: latency-svc-vrbbp [750.303292ms]
    Apr  6 11:01:58.619: INFO: Created: latency-svc-4pfxl
    Apr  6 11:01:58.655: INFO: Got endpoints: latency-svc-xpw2f [743.829155ms]
    Apr  6 11:01:58.667: INFO: Created: latency-svc-wxl27
    Apr  6 11:01:58.717: INFO: Got endpoints: latency-svc-snb84 [760.5307ms]
    Apr  6 11:01:58.728: INFO: Created: latency-svc-29bq5
    Apr  6 11:01:58.762: INFO: Got endpoints: latency-svc-bcvx4 [749.015668ms]
    Apr  6 11:01:58.778: INFO: Created: latency-svc-rp2q8
    Apr  6 11:01:58.805: INFO: Got endpoints: latency-svc-dztxx [748.854083ms]
    Apr  6 11:01:58.818: INFO: Created: latency-svc-6d5ld
    Apr  6 11:01:58.857: INFO: Got endpoints: latency-svc-zs7xb [747.357297ms]
    Apr  6 11:01:58.871: INFO: Created: latency-svc-g22gs
    Apr  6 11:01:58.906: INFO: Got endpoints: latency-svc-s9p8v [743.946411ms]
    Apr  6 11:01:58.928: INFO: Created: latency-svc-9rm8v
    Apr  6 11:01:58.955: INFO: Got endpoints: latency-svc-48jft [749.897861ms]
    Apr  6 11:01:58.970: INFO: Created: latency-svc-vgjz8
    Apr  6 11:01:59.012: INFO: Got endpoints: latency-svc-zcspr [755.734067ms]
    Apr  6 11:01:59.021: INFO: Created: latency-svc-brm5j
    Apr  6 11:01:59.055: INFO: Got endpoints: latency-svc-tghcn [749.632911ms]
    Apr  6 11:01:59.069: INFO: Created: latency-svc-v5s4s
    Apr  6 11:01:59.109: INFO: Got endpoints: latency-svc-8tk2m [753.269587ms]
    Apr  6 11:01:59.120: INFO: Created: latency-svc-zwmv4
    Apr  6 11:01:59.155: INFO: Got endpoints: latency-svc-mnfg8 [749.014315ms]
    Apr  6 11:01:59.165: INFO: Created: latency-svc-hgs25
    Apr  6 11:01:59.206: INFO: Got endpoints: latency-svc-mp549 [751.020133ms]
    Apr  6 11:01:59.219: INFO: Created: latency-svc-nf5bp
    Apr  6 11:01:59.255: INFO: Got endpoints: latency-svc-7n95w [750.033267ms]
    Apr  6 11:01:59.266: INFO: Created: latency-svc-dhxws
    Apr  6 11:01:59.305: INFO: Got endpoints: latency-svc-ljq4w [747.433727ms]
    Apr  6 11:01:59.321: INFO: Created: latency-svc-wx94m
    Apr  6 11:01:59.355: INFO: Got endpoints: latency-svc-4pfxl [748.048595ms]
    Apr  6 11:01:59.367: INFO: Created: latency-svc-2qct7
    Apr  6 11:01:59.407: INFO: Got endpoints: latency-svc-wxl27 [752.376207ms]
    Apr  6 11:01:59.436: INFO: Created: latency-svc-xxghz
    Apr  6 11:01:59.455: INFO: Got endpoints: latency-svc-29bq5 [738.299561ms]
    Apr  6 11:01:59.467: INFO: Created: latency-svc-r2mmq
    Apr  6 11:01:59.507: INFO: Got endpoints: latency-svc-rp2q8 [744.613298ms]
    Apr  6 11:01:59.519: INFO: Created: latency-svc-dsgmb
    Apr  6 11:01:59.555: INFO: Got endpoints: latency-svc-6d5ld [749.303287ms]
    Apr  6 11:01:59.567: INFO: Created: latency-svc-qnk24
    Apr  6 11:01:59.605: INFO: Got endpoints: latency-svc-g22gs [747.789496ms]
    Apr  6 11:01:59.614: INFO: Created: latency-svc-ltvb4
    Apr  6 11:01:59.655: INFO: Got endpoints: latency-svc-9rm8v [748.705733ms]
    Apr  6 11:01:59.664: INFO: Created: latency-svc-rlnl7
    Apr  6 11:01:59.711: INFO: Got endpoints: latency-svc-vgjz8 [755.762416ms]
    Apr  6 11:01:59.725: INFO: Created: latency-svc-8rwx8
    Apr  6 11:01:59.755: INFO: Got endpoints: latency-svc-brm5j [743.595625ms]
    Apr  6 11:01:59.768: INFO: Created: latency-svc-rs65m
    Apr  6 11:01:59.806: INFO: Got endpoints: latency-svc-v5s4s [750.53725ms]
    Apr  6 11:01:59.820: INFO: Created: latency-svc-66dnb
    Apr  6 11:01:59.855: INFO: Got endpoints: latency-svc-zwmv4 [746.191085ms]
    Apr  6 11:01:59.865: INFO: Created: latency-svc-9rbxv
    Apr  6 11:01:59.919: INFO: Got endpoints: latency-svc-hgs25 [763.870734ms]
    Apr  6 11:01:59.937: INFO: Created: latency-svc-wvrxb
    Apr  6 11:01:59.964: INFO: Got endpoints: latency-svc-nf5bp [757.70569ms]
    Apr  6 11:01:59.978: INFO: Created: latency-svc-lcrfh
    Apr  6 11:02:00.007: INFO: Got endpoints: latency-svc-dhxws [751.44663ms]
    Apr  6 11:02:00.021: INFO: Created: latency-svc-gvqbw
    Apr  6 11:02:00.056: INFO: Got endpoints: latency-svc-wx94m [750.673915ms]
    Apr  6 11:02:00.068: INFO: Created: latency-svc-fnfjs
    Apr  6 11:02:00.105: INFO: Got endpoints: latency-svc-2qct7 [750.483681ms]
    Apr  6 11:02:00.114: INFO: Created: latency-svc-8z2wh
    Apr  6 11:02:00.157: INFO: Got endpoints: latency-svc-xxghz [750.099175ms]
    Apr  6 11:02:00.170: INFO: Created: latency-svc-mbtv6
    Apr  6 11:02:00.206: INFO: Got endpoints: latency-svc-r2mmq [750.710974ms]
    Apr  6 11:02:00.217: INFO: Created: latency-svc-kvtwv
    Apr  6 11:02:00.254: INFO: Got endpoints: latency-svc-dsgmb [746.881063ms]
    Apr  6 11:02:00.270: INFO: Created: latency-svc-4dj8f
    Apr  6 11:02:00.304: INFO: Got endpoints: latency-svc-qnk24 [749.524059ms]
    Apr  6 11:02:00.319: INFO: Created: latency-svc-8nwnz
    Apr  6 11:02:00.358: INFO: Got endpoints: latency-svc-ltvb4 [752.723187ms]
    Apr  6 11:02:00.368: INFO: Created: latency-svc-mx6qz
    Apr  6 11:02:00.406: INFO: Got endpoints: latency-svc-rlnl7 [751.540152ms]
    Apr  6 11:02:00.420: INFO: Created: latency-svc-wq7xq
    Apr  6 11:02:00.455: INFO: Got endpoints: latency-svc-8rwx8 [743.855537ms]
    Apr  6 11:02:00.470: INFO: Created: latency-svc-kdwfk
    Apr  6 11:02:00.507: INFO: Got endpoints: latency-svc-rs65m [751.383815ms]
    Apr  6 11:02:00.555: INFO: Got endpoints: latency-svc-66dnb [748.815723ms]
    Apr  6 11:02:00.606: INFO: Got endpoints: latency-svc-9rbxv [751.051598ms]
    Apr  6 11:02:00.659: INFO: Got endpoints: latency-svc-wvrxb [739.663241ms]
    Apr  6 11:02:00.715: INFO: Got endpoints: latency-svc-lcrfh [751.615661ms]
    Apr  6 11:02:00.755: INFO: Got endpoints: latency-svc-gvqbw [747.936098ms]
    Apr  6 11:02:00.806: INFO: Got endpoints: latency-svc-fnfjs [749.830252ms]
    Apr  6 11:02:00.856: INFO: Got endpoints: latency-svc-8z2wh [750.361206ms]
    Apr  6 11:02:00.905: INFO: Got endpoints: latency-svc-mbtv6 [747.865407ms]
    Apr  6 11:02:00.956: INFO: Got endpoints: latency-svc-kvtwv [749.436015ms]
    Apr  6 11:02:01.007: INFO: Got endpoints: latency-svc-4dj8f [752.667602ms]
    Apr  6 11:02:01.067: INFO: Got endpoints: latency-svc-8nwnz [762.587905ms]
    Apr  6 11:02:01.107: INFO: Got endpoints: latency-svc-mx6qz [749.303956ms]
    Apr  6 11:02:01.156: INFO: Got endpoints: latency-svc-wq7xq [749.030226ms]
    Apr  6 11:02:01.204: INFO: Got endpoints: latency-svc-kdwfk [749.367562ms]
    Apr  6 11:02:01.205: INFO: Latencies: [24.3208ms 25.503001ms 27.195323ms 41.939006ms 42.984461ms 47.305756ms 53.464495ms 67.233921ms 71.700052ms 79.335278ms 79.746745ms 90.474097ms 93.423402ms 93.560864ms 100.246331ms 101.325226ms 104.275637ms 105.011823ms 105.441568ms 107.638318ms 108.162183ms 110.744463ms 117.464606ms 120.493743ms 131.737834ms 132.110293ms 136.760802ms 139.882326ms 147.960954ms 148.21476ms 149.012484ms 154.154534ms 160.051748ms 161.069077ms 166.115849ms 196.182172ms 225.377806ms 268.039122ms 311.282177ms 336.901783ms 380.727718ms 424.057437ms 468.075839ms 506.37186ms 527.99367ms 575.415295ms 621.625298ms 662.566086ms 708.621329ms 735.016361ms 738.299561ms 738.870194ms 739.663241ms 739.733031ms 740.758117ms 742.783075ms 743.019351ms 743.058753ms 743.080249ms 743.257324ms 743.478179ms 743.595625ms 743.829155ms 743.855537ms 743.946411ms 743.993131ms 744.008545ms 744.121305ms 744.477257ms 744.613298ms 744.750569ms 744.815001ms 744.930089ms 745.919108ms 745.975288ms 746.191085ms 746.881063ms 746.949003ms 747.31078ms 747.357297ms 747.433727ms 747.602355ms 747.612409ms 747.673619ms 747.753524ms 747.789496ms 747.804367ms 747.856893ms 747.864164ms 747.865407ms 747.936098ms 747.941988ms 748.048595ms 748.118596ms 748.228425ms 748.301524ms 748.356834ms 748.547391ms 748.665267ms 748.705733ms 748.716716ms 748.765004ms 748.815723ms 748.854083ms 749.014315ms 749.015668ms 749.030226ms 749.100295ms 749.157175ms 749.299189ms 749.303287ms 749.303956ms 749.367562ms 749.41881ms 749.424026ms 749.436015ms 749.436288ms 749.524059ms 749.592373ms 749.624332ms 749.632911ms 749.74521ms 749.786542ms 749.820674ms 749.830252ms 749.897861ms 749.992917ms 750.033267ms 750.061826ms 750.085234ms 750.087583ms 750.099175ms 750.111174ms 750.124108ms 750.303292ms 750.361206ms 750.462842ms 750.483681ms 750.50093ms 750.53725ms 750.645653ms 750.673915ms 750.710974ms 750.800883ms 750.968985ms 751.020133ms 751.04822ms 751.051598ms 751.087538ms 751.288997ms 751.383815ms 751.44663ms 751.537598ms 751.540152ms 751.606307ms 751.615661ms 751.749943ms 751.926775ms 752.009501ms 752.011744ms 752.254288ms 752.264877ms 752.321393ms 752.376207ms 752.385464ms 752.667602ms 752.723187ms 752.77368ms 752.787644ms 752.880863ms 752.897567ms 752.904561ms 752.961329ms 753.093042ms 753.269587ms 753.311042ms 753.53884ms 753.735668ms 753.908601ms 754.027258ms 754.581441ms 754.778629ms 755.539714ms 755.578217ms 755.652534ms 755.734067ms 755.762416ms 756.171805ms 756.513323ms 756.595647ms 757.358982ms 757.48166ms 757.70569ms 758.686949ms 760.026578ms 760.5307ms 760.696096ms 762.064584ms 762.587905ms 763.870734ms]
    Apr  6 11:02:01.205: INFO: 50 %ile: 748.716716ms
    Apr  6 11:02:01.205: INFO: 90 %ile: 754.581441ms
    Apr  6 11:02:01.205: INFO: 99 %ile: 762.587905ms
    Apr  6 11:02:01.205: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Apr  6 11:02:01.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-1672" for this suite. 04/06/23 11:02:01.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:02:01.23
Apr  6 11:02:01.230: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:02:01.231
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:01.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:01.266
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/06/23 11:02:01.271
Apr  6 11:02:01.284: INFO: Waiting up to 5m0s for pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0" in namespace "emptydir-3225" to be "Succeeded or Failed"
Apr  6 11:02:01.290: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.530261ms
Apr  6 11:02:03.296: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011533839s
Apr  6 11:02:05.297: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01271844s
Apr  6 11:02:07.298: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013375426s
STEP: Saw pod success 04/06/23 11:02:07.298
Apr  6 11:02:07.298: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0" satisfied condition "Succeeded or Failed"
Apr  6 11:02:07.310: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0 container test-container: <nil>
STEP: delete the pod 04/06/23 11:02:07.39
Apr  6 11:02:07.401: INFO: Waiting for pod pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0 to disappear
Apr  6 11:02:07.405: INFO: Pod pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:02:07.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3225" for this suite. 04/06/23 11:02:07.416
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":191,"skipped":3493,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.197 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:02:01.23
    Apr  6 11:02:01.230: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:02:01.231
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:01.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:01.266
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/06/23 11:02:01.271
    Apr  6 11:02:01.284: INFO: Waiting up to 5m0s for pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0" in namespace "emptydir-3225" to be "Succeeded or Failed"
    Apr  6 11:02:01.290: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.530261ms
    Apr  6 11:02:03.296: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011533839s
    Apr  6 11:02:05.297: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01271844s
    Apr  6 11:02:07.298: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013375426s
    STEP: Saw pod success 04/06/23 11:02:07.298
    Apr  6 11:02:07.298: INFO: Pod "pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0" satisfied condition "Succeeded or Failed"
    Apr  6 11:02:07.310: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:02:07.39
    Apr  6 11:02:07.401: INFO: Waiting for pod pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0 to disappear
    Apr  6 11:02:07.405: INFO: Pod pod-e4fe5a76-b99f-487a-aaaf-ee00cdea39a0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:02:07.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3225" for this suite. 04/06/23 11:02:07.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:02:07.443
Apr  6 11:02:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:02:07.447
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:07.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:07.469
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 04/06/23 11:02:07.474
STEP: Getting a ResourceQuota 04/06/23 11:02:07.478
STEP: Updating a ResourceQuota 04/06/23 11:02:07.495
STEP: Verifying a ResourceQuota was modified 04/06/23 11:02:07.509
STEP: Deleting a ResourceQuota 04/06/23 11:02:07.521
STEP: Verifying the deleted ResourceQuota 04/06/23 11:02:07.537
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:02:07.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3736" for this suite. 04/06/23 11:02:07.563
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":192,"skipped":3507,"failed":0}
------------------------------
â€¢ [0.129 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:02:07.443
    Apr  6 11:02:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:02:07.447
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:07.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:07.469
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 04/06/23 11:02:07.474
    STEP: Getting a ResourceQuota 04/06/23 11:02:07.478
    STEP: Updating a ResourceQuota 04/06/23 11:02:07.495
    STEP: Verifying a ResourceQuota was modified 04/06/23 11:02:07.509
    STEP: Deleting a ResourceQuota 04/06/23 11:02:07.521
    STEP: Verifying the deleted ResourceQuota 04/06/23 11:02:07.537
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:02:07.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3736" for this suite. 04/06/23 11:02:07.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:02:07.573
Apr  6 11:02:07.573: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename daemonsets 04/06/23 11:02:07.574
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:07.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:07.612
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Apr  6 11:02:07.675: INFO: Create a RollingUpdate DaemonSet
Apr  6 11:02:07.685: INFO: Check that daemon pods launch on every node of the cluster
Apr  6 11:02:07.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:02:07.704: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:02:08.727: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr  6 11:02:08.727: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:02:09.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr  6 11:02:09.731: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 11:02:10.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:02:10.716: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
Apr  6 11:02:10.716: INFO: Update the DaemonSet to trigger a rollout
Apr  6 11:02:10.744: INFO: Updating DaemonSet daemon-set
Apr  6 11:02:13.819: INFO: Roll back the DaemonSet before rollout is complete
Apr  6 11:02:13.846: INFO: Updating DaemonSet daemon-set
Apr  6 11:02:13.846: INFO: Make sure DaemonSet rollback is complete
Apr  6 11:02:15.869: INFO: Pod daemon-set-cgjpw is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:02:15.892
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8126, will wait for the garbage collector to delete the pods 04/06/23 11:02:15.892
Apr  6 11:02:15.958: INFO: Deleting DaemonSet.extensions daemon-set took: 5.567408ms
Apr  6 11:02:16.058: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.400168ms
Apr  6 11:02:18.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:02:18.964: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr  6 11:02:18.968: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24604"},"items":null}

Apr  6 11:02:18.974: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24604"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:02:19.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8126" for this suite. 04/06/23 11:02:19.024
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":193,"skipped":3523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.456 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:02:07.573
    Apr  6 11:02:07.573: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename daemonsets 04/06/23 11:02:07.574
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:07.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:07.612
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Apr  6 11:02:07.675: INFO: Create a RollingUpdate DaemonSet
    Apr  6 11:02:07.685: INFO: Check that daemon pods launch on every node of the cluster
    Apr  6 11:02:07.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:02:07.704: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:02:08.727: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr  6 11:02:08.727: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:02:09.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr  6 11:02:09.731: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 11:02:10.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:02:10.716: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    Apr  6 11:02:10.716: INFO: Update the DaemonSet to trigger a rollout
    Apr  6 11:02:10.744: INFO: Updating DaemonSet daemon-set
    Apr  6 11:02:13.819: INFO: Roll back the DaemonSet before rollout is complete
    Apr  6 11:02:13.846: INFO: Updating DaemonSet daemon-set
    Apr  6 11:02:13.846: INFO: Make sure DaemonSet rollback is complete
    Apr  6 11:02:15.869: INFO: Pod daemon-set-cgjpw is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:02:15.892
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8126, will wait for the garbage collector to delete the pods 04/06/23 11:02:15.892
    Apr  6 11:02:15.958: INFO: Deleting DaemonSet.extensions daemon-set took: 5.567408ms
    Apr  6 11:02:16.058: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.400168ms
    Apr  6 11:02:18.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:02:18.964: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr  6 11:02:18.968: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24604"},"items":null}

    Apr  6 11:02:18.974: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24604"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:02:19.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8126" for this suite. 04/06/23 11:02:19.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:02:19.032
Apr  6 11:02:19.032: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 11:02:19.033
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:19.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:19.07
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-2631 04/06/23 11:02:19.085
STEP: creating replication controller nodeport-test in namespace services-2631 04/06/23 11:02:19.107
I0406 11:02:19.117736      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2631, replica count: 2
I0406 11:02:22.171520      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 11:02:22.171: INFO: Creating new exec pod
Apr  6 11:02:22.180: INFO: Waiting up to 5m0s for pod "execpod5k4sb" in namespace "services-2631" to be "running"
Apr  6 11:02:22.185: INFO: Pod "execpod5k4sb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650023ms
Apr  6 11:02:24.193: INFO: Pod "execpod5k4sb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012298091s
Apr  6 11:02:24.193: INFO: Pod "execpod5k4sb" satisfied condition "running"
Apr  6 11:02:25.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Apr  6 11:02:25.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr  6 11:02:25.903: INFO: stdout: "nodeport-test-f9dc9"
Apr  6 11:02:25.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.69.171.229 80'
Apr  6 11:02:26.475: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.69.171.229 80\nConnection to 10.69.171.229 80 port [tcp/http] succeeded!\n"
Apr  6 11:02:26.475: INFO: stdout: "nodeport-test-f9dc9"
Apr  6 11:02:26.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.68 32284'
Apr  6 11:02:27.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.68 32284\nConnection to 10.250.0.68 32284 port [tcp/*] succeeded!\n"
Apr  6 11:02:27.103: INFO: stdout: "nodeport-test-xmtqz"
Apr  6 11:02:27.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.89 32284'
Apr  6 11:02:27.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.89 32284\nConnection to 10.250.2.89 32284 port [tcp/*] succeeded!\n"
Apr  6 11:02:27.748: INFO: stdout: "nodeport-test-f9dc9"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 11:02:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2631" for this suite. 04/06/23 11:02:27.76
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":194,"skipped":3559,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.734 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:02:19.032
    Apr  6 11:02:19.032: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 11:02:19.033
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:19.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:19.07
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-2631 04/06/23 11:02:19.085
    STEP: creating replication controller nodeport-test in namespace services-2631 04/06/23 11:02:19.107
    I0406 11:02:19.117736      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2631, replica count: 2
    I0406 11:02:22.171520      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 11:02:22.171: INFO: Creating new exec pod
    Apr  6 11:02:22.180: INFO: Waiting up to 5m0s for pod "execpod5k4sb" in namespace "services-2631" to be "running"
    Apr  6 11:02:22.185: INFO: Pod "execpod5k4sb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650023ms
    Apr  6 11:02:24.193: INFO: Pod "execpod5k4sb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012298091s
    Apr  6 11:02:24.193: INFO: Pod "execpod5k4sb" satisfied condition "running"
    Apr  6 11:02:25.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Apr  6 11:02:25.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Apr  6 11:02:25.903: INFO: stdout: "nodeport-test-f9dc9"
    Apr  6 11:02:25.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.69.171.229 80'
    Apr  6 11:02:26.475: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.69.171.229 80\nConnection to 10.69.171.229 80 port [tcp/http] succeeded!\n"
    Apr  6 11:02:26.475: INFO: stdout: "nodeport-test-f9dc9"
    Apr  6 11:02:26.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.68 32284'
    Apr  6 11:02:27.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.68 32284\nConnection to 10.250.0.68 32284 port [tcp/*] succeeded!\n"
    Apr  6 11:02:27.103: INFO: stdout: "nodeport-test-xmtqz"
    Apr  6 11:02:27.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2631 exec execpod5k4sb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.89 32284'
    Apr  6 11:02:27.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.89 32284\nConnection to 10.250.2.89 32284 port [tcp/*] succeeded!\n"
    Apr  6 11:02:27.748: INFO: stdout: "nodeport-test-f9dc9"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 11:02:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2631" for this suite. 04/06/23 11:02:27.76
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:02:27.771
Apr  6 11:02:27.771: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename cronjob 04/06/23 11:02:27.773
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:27.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:27.793
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 04/06/23 11:02:27.815
STEP: Ensuring a job is scheduled 04/06/23 11:02:27.821
STEP: Ensuring exactly one is scheduled 04/06/23 11:03:01.828
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/06/23 11:03:01.832
STEP: Ensuring no more jobs are scheduled 04/06/23 11:03:01.837
STEP: Removing cronjob 04/06/23 11:08:01.847
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Apr  6 11:08:01.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3589" for this suite. 04/06/23 11:08:01.859
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":195,"skipped":3632,"failed":0}
------------------------------
â€¢ [SLOW TEST] [334.097 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:02:27.771
    Apr  6 11:02:27.771: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename cronjob 04/06/23 11:02:27.773
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:02:27.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:02:27.793
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 04/06/23 11:02:27.815
    STEP: Ensuring a job is scheduled 04/06/23 11:02:27.821
    STEP: Ensuring exactly one is scheduled 04/06/23 11:03:01.828
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/06/23 11:03:01.832
    STEP: Ensuring no more jobs are scheduled 04/06/23 11:03:01.837
    STEP: Removing cronjob 04/06/23 11:08:01.847
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Apr  6 11:08:01.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3589" for this suite. 04/06/23 11:08:01.859
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:08:01.87
Apr  6 11:08:01.870: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 11:08:01.873
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:01.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:01.899
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-2227 04/06/23 11:08:01.904
STEP: creating service affinity-clusterip in namespace services-2227 04/06/23 11:08:01.904
STEP: creating replication controller affinity-clusterip in namespace services-2227 04/06/23 11:08:01.922
I0406 11:08:01.933968      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2227, replica count: 3
I0406 11:08:04.998000      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 11:08:05.023: INFO: Creating new exec pod
Apr  6 11:08:05.032: INFO: Waiting up to 5m0s for pod "execpod-affinityg8xwk" in namespace "services-2227" to be "running"
Apr  6 11:08:05.035: INFO: Pod "execpod-affinityg8xwk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.453801ms
Apr  6 11:08:07.041: INFO: Pod "execpod-affinityg8xwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.008754285s
Apr  6 11:08:07.041: INFO: Pod "execpod-affinityg8xwk" satisfied condition "running"
Apr  6 11:08:08.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2227 exec execpod-affinityg8xwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Apr  6 11:08:08.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr  6 11:08:08.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 11:08:08.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2227 exec execpod-affinityg8xwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.64.185.229 80'
Apr  6 11:08:09.457: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.64.185.229 80\nConnection to 10.64.185.229 80 port [tcp/http] succeeded!\n"
Apr  6 11:08:09.457: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 11:08:09.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2227 exec execpod-affinityg8xwk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.64.185.229:80/ ; done'
Apr  6 11:08:10.301: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n"
Apr  6 11:08:10.305: INFO: stdout: "\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh"
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
Apr  6 11:08:10.305: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2227, will wait for the garbage collector to delete the pods 04/06/23 11:08:10.315
Apr  6 11:08:10.380: INFO: Deleting ReplicationController affinity-clusterip took: 11.372636ms
Apr  6 11:08:10.482: INFO: Terminating ReplicationController affinity-clusterip pods took: 102.134207ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 11:08:12.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2227" for this suite. 04/06/23 11:08:12.906
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":196,"skipped":3634,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.044 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:08:01.87
    Apr  6 11:08:01.870: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 11:08:01.873
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:01.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:01.899
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-2227 04/06/23 11:08:01.904
    STEP: creating service affinity-clusterip in namespace services-2227 04/06/23 11:08:01.904
    STEP: creating replication controller affinity-clusterip in namespace services-2227 04/06/23 11:08:01.922
    I0406 11:08:01.933968      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2227, replica count: 3
    I0406 11:08:04.998000      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 11:08:05.023: INFO: Creating new exec pod
    Apr  6 11:08:05.032: INFO: Waiting up to 5m0s for pod "execpod-affinityg8xwk" in namespace "services-2227" to be "running"
    Apr  6 11:08:05.035: INFO: Pod "execpod-affinityg8xwk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.453801ms
    Apr  6 11:08:07.041: INFO: Pod "execpod-affinityg8xwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.008754285s
    Apr  6 11:08:07.041: INFO: Pod "execpod-affinityg8xwk" satisfied condition "running"
    Apr  6 11:08:08.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2227 exec execpod-affinityg8xwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Apr  6 11:08:08.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Apr  6 11:08:08.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 11:08:08.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2227 exec execpod-affinityg8xwk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.64.185.229 80'
    Apr  6 11:08:09.457: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.64.185.229 80\nConnection to 10.64.185.229 80 port [tcp/http] succeeded!\n"
    Apr  6 11:08:09.457: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 11:08:09.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-2227 exec execpod-affinityg8xwk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.64.185.229:80/ ; done'
    Apr  6 11:08:10.301: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.64.185.229:80/\n"
    Apr  6 11:08:10.305: INFO: stdout: "\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh\naffinity-clusterip-crtxh"
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Received response from host: affinity-clusterip-crtxh
    Apr  6 11:08:10.305: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-2227, will wait for the garbage collector to delete the pods 04/06/23 11:08:10.315
    Apr  6 11:08:10.380: INFO: Deleting ReplicationController affinity-clusterip took: 11.372636ms
    Apr  6 11:08:10.482: INFO: Terminating ReplicationController affinity-clusterip pods took: 102.134207ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 11:08:12.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2227" for this suite. 04/06/23 11:08:12.906
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:08:12.917
Apr  6 11:08:12.917: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:08:12.918
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:12.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:12.936
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-9df08ab0-913f-41d7-b295-31fbb6505c17 04/06/23 11:08:12.941
STEP: Creating a pod to test consume configMaps 04/06/23 11:08:12.946
Apr  6 11:08:12.956: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e" in namespace "projected-6564" to be "Succeeded or Failed"
Apr  6 11:08:12.960: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420423ms
Apr  6 11:08:14.973: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017753579s
Apr  6 11:08:16.965: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00973292s
STEP: Saw pod success 04/06/23 11:08:16.965
Apr  6 11:08:16.966: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e" satisfied condition "Succeeded or Failed"
Apr  6 11:08:16.971: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e container projected-configmap-volume-test: <nil>
STEP: delete the pod 04/06/23 11:08:17.039
Apr  6 11:08:17.072: INFO: Waiting for pod pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e to disappear
Apr  6 11:08:17.081: INFO: Pod pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 11:08:17.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6564" for this suite. 04/06/23 11:08:17.089
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":197,"skipped":3680,"failed":0}
------------------------------
â€¢ [4.178 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:08:12.917
    Apr  6 11:08:12.917: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:08:12.918
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:12.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:12.936
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-9df08ab0-913f-41d7-b295-31fbb6505c17 04/06/23 11:08:12.941
    STEP: Creating a pod to test consume configMaps 04/06/23 11:08:12.946
    Apr  6 11:08:12.956: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e" in namespace "projected-6564" to be "Succeeded or Failed"
    Apr  6 11:08:12.960: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420423ms
    Apr  6 11:08:14.973: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017753579s
    Apr  6 11:08:16.965: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00973292s
    STEP: Saw pod success 04/06/23 11:08:16.965
    Apr  6 11:08:16.966: INFO: Pod "pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e" satisfied condition "Succeeded or Failed"
    Apr  6 11:08:16.971: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e container projected-configmap-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:08:17.039
    Apr  6 11:08:17.072: INFO: Waiting for pod pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e to disappear
    Apr  6 11:08:17.081: INFO: Pod pod-projected-configmaps-64a35c3b-0af8-4cbc-802d-ebd65fa0120e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 11:08:17.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6564" for this suite. 04/06/23 11:08:17.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:08:17.096
Apr  6 11:08:17.096: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename security-context-test 04/06/23 11:08:17.097
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:17.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:17.122
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Apr  6 11:08:17.141: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce" in namespace "security-context-test-4320" to be "Succeeded or Failed"
Apr  6 11:08:17.150: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.664582ms
Apr  6 11:08:19.155: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010692978s
Apr  6 11:08:21.167: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022552119s
Apr  6 11:08:23.155: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010293798s
Apr  6 11:08:23.155: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Apr  6 11:08:23.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4320" for this suite. 04/06/23 11:08:23.172
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":198,"skipped":3702,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.082 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:08:17.096
    Apr  6 11:08:17.096: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename security-context-test 04/06/23 11:08:17.097
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:17.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:17.122
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Apr  6 11:08:17.141: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce" in namespace "security-context-test-4320" to be "Succeeded or Failed"
    Apr  6 11:08:17.150: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.664582ms
    Apr  6 11:08:19.155: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010692978s
    Apr  6 11:08:21.167: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022552119s
    Apr  6 11:08:23.155: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010293798s
    Apr  6 11:08:23.155: INFO: Pod "alpine-nnp-false-d43a1a00-aed9-413c-bb94-62698764c5ce" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Apr  6 11:08:23.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4320" for this suite. 04/06/23 11:08:23.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:08:23.178
Apr  6 11:08:23.178: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 11:08:23.179
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:23.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:23.202
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 04/06/23 11:08:23.207
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
 04/06/23 11:08:23.213
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
 04/06/23 11:08:23.213
STEP: creating a pod to probe DNS 04/06/23 11:08:23.213
STEP: submitting the pod to kubernetes 04/06/23 11:08:23.214
Apr  6 11:08:23.224: INFO: Waiting up to 15m0s for pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a" in namespace "dns-3869" to be "running"
Apr  6 11:08:23.227: INFO: Pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709926ms
Apr  6 11:08:25.234: INFO: Pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009951223s
Apr  6 11:08:25.234: INFO: Pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a" satisfied condition "running"
STEP: retrieving the pod 04/06/23 11:08:25.234
STEP: looking for the results for each expected name from probers 04/06/23 11:08:25.238
Apr  6 11:08:25.408: INFO: DNS probes using dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a succeeded

STEP: deleting the pod 04/06/23 11:08:25.408
STEP: changing the externalName to bar.example.com 04/06/23 11:08:25.418
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
 04/06/23 11:08:25.429
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
 04/06/23 11:08:25.43
STEP: creating a second pod to probe DNS 04/06/23 11:08:25.43
STEP: submitting the pod to kubernetes 04/06/23 11:08:25.43
Apr  6 11:08:25.441: INFO: Waiting up to 15m0s for pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83" in namespace "dns-3869" to be "running"
Apr  6 11:08:25.447: INFO: Pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83": Phase="Pending", Reason="", readiness=false. Elapsed: 5.807996ms
Apr  6 11:08:27.454: INFO: Pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83": Phase="Running", Reason="", readiness=true. Elapsed: 2.012630661s
Apr  6 11:08:27.454: INFO: Pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83" satisfied condition "running"
STEP: retrieving the pod 04/06/23 11:08:27.454
STEP: looking for the results for each expected name from probers 04/06/23 11:08:27.458
Apr  6 11:08:27.559: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:27.603: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:27.603: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

Apr  6 11:08:32.625: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:32.670: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:32.670: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

Apr  6 11:08:37.614: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:37.658: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:37.658: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

Apr  6 11:08:42.613: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:42.656: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:42.656: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

Apr  6 11:08:47.611: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:47.657: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:47.657: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

Apr  6 11:08:52.613: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:52.645: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 11:08:52.645: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

Apr  6 11:08:57.662: INFO: DNS probes using dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 succeeded

STEP: deleting the pod 04/06/23 11:08:57.662
STEP: changing the service to type=ClusterIP 04/06/23 11:08:57.672
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
 04/06/23 11:08:57.693
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
 04/06/23 11:08:57.693
STEP: creating a third pod to probe DNS 04/06/23 11:08:57.693
STEP: submitting the pod to kubernetes 04/06/23 11:08:57.706
Apr  6 11:08:57.718: INFO: Waiting up to 15m0s for pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9" in namespace "dns-3869" to be "running"
Apr  6 11:08:57.724: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453017ms
Apr  6 11:08:59.732: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013988343s
Apr  6 11:09:01.743: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9": Phase="Running", Reason="", readiness=true. Elapsed: 4.024534508s
Apr  6 11:09:01.743: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9" satisfied condition "running"
STEP: retrieving the pod 04/06/23 11:09:01.743
STEP: looking for the results for each expected name from probers 04/06/23 11:09:01.762
Apr  6 11:09:01.924: INFO: DNS probes using dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9 succeeded

STEP: deleting the pod 04/06/23 11:09:01.924
STEP: deleting the test externalName service 04/06/23 11:09:01.94
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 11:09:01.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3869" for this suite. 04/06/23 11:09:01.964
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":199,"skipped":3709,"failed":0}
------------------------------
â€¢ [SLOW TEST] [38.790 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:08:23.178
    Apr  6 11:08:23.178: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 11:08:23.179
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:08:23.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:08:23.202
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 04/06/23 11:08:23.207
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
     04/06/23 11:08:23.213
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
     04/06/23 11:08:23.213
    STEP: creating a pod to probe DNS 04/06/23 11:08:23.213
    STEP: submitting the pod to kubernetes 04/06/23 11:08:23.214
    Apr  6 11:08:23.224: INFO: Waiting up to 15m0s for pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a" in namespace "dns-3869" to be "running"
    Apr  6 11:08:23.227: INFO: Pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709926ms
    Apr  6 11:08:25.234: INFO: Pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009951223s
    Apr  6 11:08:25.234: INFO: Pod "dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 11:08:25.234
    STEP: looking for the results for each expected name from probers 04/06/23 11:08:25.238
    Apr  6 11:08:25.408: INFO: DNS probes using dns-test-63652ec1-fded-4ef7-8e1b-95c6698f7c0a succeeded

    STEP: deleting the pod 04/06/23 11:08:25.408
    STEP: changing the externalName to bar.example.com 04/06/23 11:08:25.418
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
     04/06/23 11:08:25.429
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
     04/06/23 11:08:25.43
    STEP: creating a second pod to probe DNS 04/06/23 11:08:25.43
    STEP: submitting the pod to kubernetes 04/06/23 11:08:25.43
    Apr  6 11:08:25.441: INFO: Waiting up to 15m0s for pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83" in namespace "dns-3869" to be "running"
    Apr  6 11:08:25.447: INFO: Pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83": Phase="Pending", Reason="", readiness=false. Elapsed: 5.807996ms
    Apr  6 11:08:27.454: INFO: Pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83": Phase="Running", Reason="", readiness=true. Elapsed: 2.012630661s
    Apr  6 11:08:27.454: INFO: Pod "dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 11:08:27.454
    STEP: looking for the results for each expected name from probers 04/06/23 11:08:27.458
    Apr  6 11:08:27.559: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:27.603: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:27.603: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

    Apr  6 11:08:32.625: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:32.670: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:32.670: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

    Apr  6 11:08:37.614: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:37.658: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:37.658: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

    Apr  6 11:08:42.613: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:42.656: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:42.656: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

    Apr  6 11:08:47.611: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:47.657: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:47.657: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

    Apr  6 11:08:52.613: INFO: File wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:52.645: INFO: File jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local from pod  dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr  6 11:08:52.645: INFO: Lookups using dns-3869/dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 failed for: [wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local]

    Apr  6 11:08:57.662: INFO: DNS probes using dns-test-9ea3ff97-066e-49c3-a1c0-b3d9f416cd83 succeeded

    STEP: deleting the pod 04/06/23 11:08:57.662
    STEP: changing the service to type=ClusterIP 04/06/23 11:08:57.672
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
     04/06/23 11:08:57.693
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3869.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3869.svc.cluster.local; sleep 1; done
     04/06/23 11:08:57.693
    STEP: creating a third pod to probe DNS 04/06/23 11:08:57.693
    STEP: submitting the pod to kubernetes 04/06/23 11:08:57.706
    Apr  6 11:08:57.718: INFO: Waiting up to 15m0s for pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9" in namespace "dns-3869" to be "running"
    Apr  6 11:08:57.724: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453017ms
    Apr  6 11:08:59.732: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013988343s
    Apr  6 11:09:01.743: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9": Phase="Running", Reason="", readiness=true. Elapsed: 4.024534508s
    Apr  6 11:09:01.743: INFO: Pod "dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 11:09:01.743
    STEP: looking for the results for each expected name from probers 04/06/23 11:09:01.762
    Apr  6 11:09:01.924: INFO: DNS probes using dns-test-2be3ce63-6200-44cb-8a27-ee6616715db9 succeeded

    STEP: deleting the pod 04/06/23 11:09:01.924
    STEP: deleting the test externalName service 04/06/23 11:09:01.94
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 11:09:01.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3869" for this suite. 04/06/23 11:09:01.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:09:01.976
Apr  6 11:09:01.976: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 11:09:01.978
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:02.023
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Apr  6 11:09:02.042: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr  6 11:09:02.053: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 11:09:07.062: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/06/23 11:09:07.062
Apr  6 11:09:07.062: INFO: Creating deployment "test-rolling-update-deployment"
Apr  6 11:09:07.067: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr  6 11:09:07.074: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr  6 11:09:09.089: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr  6 11:09:09.094: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 11:09:09.113: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4505  a95e77ad-6034-483c-a741-f04abb133cce 26396 1 2023-04-06 11:09:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035fec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-06 11:09:07 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-04-06 11:09:08 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 11:09:09.118: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4505  9622895e-78d2-4192-ae25-115db2069850 26389 1 2023-04-06 11:09:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a95e77ad-6034-483c-a741-f04abb133cce 0xc0035ff147 0xc0035ff148}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a95e77ad-6034-483c-a741-f04abb133cce\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ff1f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:09:09.118: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr  6 11:09:09.118: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4505  69ccb310-d8b6-428b-b0ff-e02fd66f7d27 26395 2 2023-04-06 11:09:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a95e77ad-6034-483c-a741-f04abb133cce 0xc0035ff017 0xc0035ff018}] [] [{e2e.test Update apps/v1 2023-04-06 11:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a95e77ad-6034-483c-a741-f04abb133cce\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035ff0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:09:09.125: INFO: Pod "test-rolling-update-deployment-78f575d8ff-9wp87" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-9wp87 test-rolling-update-deployment-78f575d8ff- deployment-4505  0177b59c-3836-4265-94ee-910f146e933c 26388 0 2023-04-06 11:09:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:84e7893b6cfa39b84c097f3813ccd1e9f30d4015ff546dd455c441a88f35ea90 cni.projectcalico.org/podIP:10.96.2.117/32 cni.projectcalico.org/podIPs:10.96.2.117/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 9622895e-78d2-4192-ae25-115db2069850 0xc0035ff657 0xc0035ff658}] [] [{Go-http-client Update v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9622895e-78d2-4192-ae25-115db2069850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwm2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwm2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.117,StartTime:2023-04-06 11:09:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:09:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://fb221f5e483968061b0c59e2f18c55b87fc1fbd169077da95003ec344a75e847,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 11:09:09.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4505" for this suite. 04/06/23 11:09:09.142
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":200,"skipped":3744,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.171 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:09:01.976
    Apr  6 11:09:01.976: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 11:09:01.978
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:02.023
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Apr  6 11:09:02.042: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Apr  6 11:09:02.053: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr  6 11:09:07.062: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/06/23 11:09:07.062
    Apr  6 11:09:07.062: INFO: Creating deployment "test-rolling-update-deployment"
    Apr  6 11:09:07.067: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Apr  6 11:09:07.074: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Apr  6 11:09:09.089: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Apr  6 11:09:09.094: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 11:09:09.113: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4505  a95e77ad-6034-483c-a741-f04abb133cce 26396 1 2023-04-06 11:09:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035fec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-06 11:09:07 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-04-06 11:09:08 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr  6 11:09:09.118: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4505  9622895e-78d2-4192-ae25-115db2069850 26389 1 2023-04-06 11:09:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a95e77ad-6034-483c-a741-f04abb133cce 0xc0035ff147 0xc0035ff148}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a95e77ad-6034-483c-a741-f04abb133cce\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ff1f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:09:09.118: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Apr  6 11:09:09.118: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4505  69ccb310-d8b6-428b-b0ff-e02fd66f7d27 26395 2 2023-04-06 11:09:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a95e77ad-6034-483c-a741-f04abb133cce 0xc0035ff017 0xc0035ff018}] [] [{e2e.test Update apps/v1 2023-04-06 11:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a95e77ad-6034-483c-a741-f04abb133cce\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035ff0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:09:09.125: INFO: Pod "test-rolling-update-deployment-78f575d8ff-9wp87" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-9wp87 test-rolling-update-deployment-78f575d8ff- deployment-4505  0177b59c-3836-4265-94ee-910f146e933c 26388 0 2023-04-06 11:09:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:84e7893b6cfa39b84c097f3813ccd1e9f30d4015ff546dd455c441a88f35ea90 cni.projectcalico.org/podIP:10.96.2.117/32 cni.projectcalico.org/podIPs:10.96.2.117/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 9622895e-78d2-4192-ae25-115db2069850 0xc0035ff657 0xc0035ff658}] [] [{Go-http-client Update v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-06 11:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9622895e-78d2-4192-ae25-115db2069850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:09:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwm2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwm2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.117,StartTime:2023-04-06 11:09:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:09:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://fb221f5e483968061b0c59e2f18c55b87fc1fbd169077da95003ec344a75e847,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 11:09:09.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4505" for this suite. 04/06/23 11:09:09.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:09:09.15
Apr  6 11:09:09.150: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:09:09.154
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:09.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:09.174
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/06/23 11:09:09.179
Apr  6 11:09:09.193: INFO: Waiting up to 5m0s for pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1" in namespace "emptydir-7803" to be "Succeeded or Failed"
Apr  6 11:09:09.196: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512652ms
Apr  6 11:09:11.203: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009794117s
Apr  6 11:09:13.208: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015104903s
STEP: Saw pod success 04/06/23 11:09:13.213
Apr  6 11:09:13.213: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1" satisfied condition "Succeeded or Failed"
Apr  6 11:09:13.220: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1 container test-container: <nil>
STEP: delete the pod 04/06/23 11:09:13.279
Apr  6 11:09:13.291: INFO: Waiting for pod pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1 to disappear
Apr  6 11:09:13.297: INFO: Pod pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:09:13.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7803" for this suite. 04/06/23 11:09:13.307
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":201,"skipped":3779,"failed":0}
------------------------------
â€¢ [4.171 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:09:09.15
    Apr  6 11:09:09.150: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:09:09.154
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:09.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:09.174
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/06/23 11:09:09.179
    Apr  6 11:09:09.193: INFO: Waiting up to 5m0s for pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1" in namespace "emptydir-7803" to be "Succeeded or Failed"
    Apr  6 11:09:09.196: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512652ms
    Apr  6 11:09:11.203: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009794117s
    Apr  6 11:09:13.208: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015104903s
    STEP: Saw pod success 04/06/23 11:09:13.213
    Apr  6 11:09:13.213: INFO: Pod "pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1" satisfied condition "Succeeded or Failed"
    Apr  6 11:09:13.220: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:09:13.279
    Apr  6 11:09:13.291: INFO: Waiting for pod pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1 to disappear
    Apr  6 11:09:13.297: INFO: Pod pod-ca23e491-57f6-4b23-92f6-f0d9d15bf9f1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:09:13.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7803" for this suite. 04/06/23 11:09:13.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:09:13.323
Apr  6 11:09:13.323: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:09:13.324
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:13.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:13.357
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 04/06/23 11:09:13.362
STEP: Creating a ResourceQuota 04/06/23 11:09:18.367
STEP: Ensuring resource quota status is calculated 04/06/23 11:09:18.373
STEP: Creating a ReplicationController 04/06/23 11:09:20.378
STEP: Ensuring resource quota status captures replication controller creation 04/06/23 11:09:20.389
STEP: Deleting a ReplicationController 04/06/23 11:09:22.395
STEP: Ensuring resource quota status released usage 04/06/23 11:09:22.404
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:09:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3512" for this suite. 04/06/23 11:09:24.427
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":202,"skipped":3802,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.118 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:09:13.323
    Apr  6 11:09:13.323: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:09:13.324
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:13.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:13.357
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 04/06/23 11:09:13.362
    STEP: Creating a ResourceQuota 04/06/23 11:09:18.367
    STEP: Ensuring resource quota status is calculated 04/06/23 11:09:18.373
    STEP: Creating a ReplicationController 04/06/23 11:09:20.378
    STEP: Ensuring resource quota status captures replication controller creation 04/06/23 11:09:20.389
    STEP: Deleting a ReplicationController 04/06/23 11:09:22.395
    STEP: Ensuring resource quota status released usage 04/06/23 11:09:22.404
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:09:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3512" for this suite. 04/06/23 11:09:24.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:09:24.443
Apr  6 11:09:24.443: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:09:24.444
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:24.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:24.473
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Apr  6 11:09:24.479: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/06/23 11:09:27.737
Apr  6 11:09:27.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 create -f -'
Apr  6 11:09:28.671: INFO: stderr: ""
Apr  6 11:09:28.673: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr  6 11:09:28.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 delete e2e-test-crd-publish-openapi-7011-crds test-cr'
Apr  6 11:09:28.937: INFO: stderr: ""
Apr  6 11:09:28.937: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr  6 11:09:28.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 apply -f -'
Apr  6 11:09:29.326: INFO: stderr: ""
Apr  6 11:09:29.327: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr  6 11:09:29.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 delete e2e-test-crd-publish-openapi-7011-crds test-cr'
Apr  6 11:09:29.467: INFO: stderr: ""
Apr  6 11:09:29.467: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 04/06/23 11:09:29.467
Apr  6 11:09:29.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 explain e2e-test-crd-publish-openapi-7011-crds'
Apr  6 11:09:30.303: INFO: stderr: ""
Apr  6 11:09:30.303: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7011-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:09:33.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6898" for this suite. 04/06/23 11:09:33.835
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":203,"skipped":3854,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.400 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:09:24.443
    Apr  6 11:09:24.443: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:09:24.444
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:24.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:24.473
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Apr  6 11:09:24.479: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/06/23 11:09:27.737
    Apr  6 11:09:27.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 create -f -'
    Apr  6 11:09:28.671: INFO: stderr: ""
    Apr  6 11:09:28.673: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr  6 11:09:28.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 delete e2e-test-crd-publish-openapi-7011-crds test-cr'
    Apr  6 11:09:28.937: INFO: stderr: ""
    Apr  6 11:09:28.937: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Apr  6 11:09:28.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 apply -f -'
    Apr  6 11:09:29.326: INFO: stderr: ""
    Apr  6 11:09:29.327: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr  6 11:09:29.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 --namespace=crd-publish-openapi-6898 delete e2e-test-crd-publish-openapi-7011-crds test-cr'
    Apr  6 11:09:29.467: INFO: stderr: ""
    Apr  6 11:09:29.467: INFO: stdout: "e2e-test-crd-publish-openapi-7011-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 04/06/23 11:09:29.467
    Apr  6 11:09:29.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-6898 explain e2e-test-crd-publish-openapi-7011-crds'
    Apr  6 11:09:30.303: INFO: stderr: ""
    Apr  6 11:09:30.303: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7011-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:09:33.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6898" for this suite. 04/06/23 11:09:33.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:09:33.86
Apr  6 11:09:33.860: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 11:09:33.861
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:33.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:33.886
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Apr  6 11:09:33.904: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr  6 11:09:38.914: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/06/23 11:09:38.914
Apr  6 11:09:38.914: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/06/23 11:09:38.947
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 11:09:40.981: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6501  7ead0fbc-e3d3-47bb-84a1-7e3fc31999fa 26614 1 2023-04-06 11:09:38 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-06 11:09:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00382ce18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-06 11:09:38 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-04-06 11:09:40 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 11:09:40.995: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6501  bdd68ca3-cfc8-4ed4-bc31-3e7135e37b6f 26607 1 2023-04-06 11:09:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 7ead0fbc-e3d3-47bb-84a1-7e3fc31999fa 0xc00382d2b7 0xc00382d2b8}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:09:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ead0fbc-e3d3-47bb-84a1-7e3fc31999fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00382d378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:09:41.000: INFO: Pod "test-cleanup-deployment-69cb9c5497-2pzqb" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-2pzqb test-cleanup-deployment-69cb9c5497- deployment-6501  7bad7f4e-814f-4e22-bb10-fd2a7319d09d 26606 0 2023-04-06 11:09:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:a1579a805550fb791b36ff7ce2ce330df70ef5723f9b2802f719dcb963e06533 cni.projectcalico.org/podIP:10.96.1.110/32 cni.projectcalico.org/podIPs:10.96.1.110/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 bdd68ca3-cfc8-4ed4-bc31-3e7135e37b6f 0xc00382d817 0xc00382d818}] [] [{kube-controller-manager Update v1 2023-04-06 11:09:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bdd68ca3-cfc8-4ed4-bc31-3e7135e37b6f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:09:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:09:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rqqnl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rqqnl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.110,StartTime:2023-04-06 11:09:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:09:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://0f2a251ae2c8670db1df996d1f515d3744497b5c71ff68e330930be5aefcba9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 11:09:41.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6501" for this suite. 04/06/23 11:09:41.016
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":204,"skipped":3943,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.163 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:09:33.86
    Apr  6 11:09:33.860: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 11:09:33.861
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:33.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:33.886
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Apr  6 11:09:33.904: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Apr  6 11:09:38.914: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/06/23 11:09:38.914
    Apr  6 11:09:38.914: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/06/23 11:09:38.947
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 11:09:40.981: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6501  7ead0fbc-e3d3-47bb-84a1-7e3fc31999fa 26614 1 2023-04-06 11:09:38 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-06 11:09:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00382ce18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-06 11:09:38 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-04-06 11:09:40 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr  6 11:09:40.995: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6501  bdd68ca3-cfc8-4ed4-bc31-3e7135e37b6f 26607 1 2023-04-06 11:09:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 7ead0fbc-e3d3-47bb-84a1-7e3fc31999fa 0xc00382d2b7 0xc00382d2b8}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:09:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ead0fbc-e3d3-47bb-84a1-7e3fc31999fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:09:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00382d378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:09:41.000: INFO: Pod "test-cleanup-deployment-69cb9c5497-2pzqb" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-2pzqb test-cleanup-deployment-69cb9c5497- deployment-6501  7bad7f4e-814f-4e22-bb10-fd2a7319d09d 26606 0 2023-04-06 11:09:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:a1579a805550fb791b36ff7ce2ce330df70ef5723f9b2802f719dcb963e06533 cni.projectcalico.org/podIP:10.96.1.110/32 cni.projectcalico.org/podIPs:10.96.1.110/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 bdd68ca3-cfc8-4ed4-bc31-3e7135e37b6f 0xc00382d817 0xc00382d818}] [] [{kube-controller-manager Update v1 2023-04-06 11:09:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bdd68ca3-cfc8-4ed4-bc31-3e7135e37b6f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:09:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:09:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rqqnl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rqqnl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:09:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.110,StartTime:2023-04-06 11:09:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:09:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://0f2a251ae2c8670db1df996d1f515d3744497b5c71ff68e330930be5aefcba9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 11:09:41.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6501" for this suite. 04/06/23 11:09:41.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:09:41.025
Apr  6 11:09:41.025: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 11:09:41.027
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:41.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:41.047
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df in namespace container-probe-5731 04/06/23 11:09:41.052
Apr  6 11:09:41.076: INFO: Waiting up to 5m0s for pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df" in namespace "container-probe-5731" to be "not pending"
Apr  6 11:09:41.080: INFO: Pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154732ms
Apr  6 11:09:43.086: INFO: Pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df": Phase="Running", Reason="", readiness=true. Elapsed: 2.00970837s
Apr  6 11:09:43.086: INFO: Pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df" satisfied condition "not pending"
Apr  6 11:09:43.086: INFO: Started pod test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df in namespace container-probe-5731
STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 11:09:43.086
Apr  6 11:09:43.091: INFO: Initial restart count of pod test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df is 0
STEP: deleting the pod 04/06/23 11:13:44.236
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 11:13:44.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5731" for this suite. 04/06/23 11:13:44.274
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":205,"skipped":3961,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.257 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:09:41.025
    Apr  6 11:09:41.025: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 11:09:41.027
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:09:41.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:09:41.047
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df in namespace container-probe-5731 04/06/23 11:09:41.052
    Apr  6 11:09:41.076: INFO: Waiting up to 5m0s for pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df" in namespace "container-probe-5731" to be "not pending"
    Apr  6 11:09:41.080: INFO: Pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154732ms
    Apr  6 11:09:43.086: INFO: Pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df": Phase="Running", Reason="", readiness=true. Elapsed: 2.00970837s
    Apr  6 11:09:43.086: INFO: Pod "test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df" satisfied condition "not pending"
    Apr  6 11:09:43.086: INFO: Started pod test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df in namespace container-probe-5731
    STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 11:09:43.086
    Apr  6 11:09:43.091: INFO: Initial restart count of pod test-webserver-2e08e922-0f88-4ebf-909e-1e63b9d561df is 0
    STEP: deleting the pod 04/06/23 11:13:44.236
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 11:13:44.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5731" for this suite. 04/06/23 11:13:44.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:13:44.298
Apr  6 11:13:44.299: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 11:13:44.301
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:13:44.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:13:44.338
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-35f97e89-661b-4cdb-a13b-23af951fa81b 04/06/23 11:13:44.343
STEP: Creating a pod to test consume configMaps 04/06/23 11:13:44.352
Apr  6 11:13:44.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5" in namespace "configmap-3820" to be "Succeeded or Failed"
Apr  6 11:13:44.374: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685693ms
Apr  6 11:13:46.381: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010253714s
Apr  6 11:13:48.379: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008600895s
STEP: Saw pod success 04/06/23 11:13:48.379
Apr  6 11:13:48.379: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5" satisfied condition "Succeeded or Failed"
Apr  6 11:13:48.383: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:13:48.439
Apr  6 11:13:48.462: INFO: Waiting for pod pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5 to disappear
Apr  6 11:13:48.466: INFO: Pod pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 11:13:48.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3820" for this suite. 04/06/23 11:13:48.482
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":206,"skipped":3968,"failed":0}
------------------------------
â€¢ [4.191 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:13:44.298
    Apr  6 11:13:44.299: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 11:13:44.301
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:13:44.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:13:44.338
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-35f97e89-661b-4cdb-a13b-23af951fa81b 04/06/23 11:13:44.343
    STEP: Creating a pod to test consume configMaps 04/06/23 11:13:44.352
    Apr  6 11:13:44.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5" in namespace "configmap-3820" to be "Succeeded or Failed"
    Apr  6 11:13:44.374: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685693ms
    Apr  6 11:13:46.381: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010253714s
    Apr  6 11:13:48.379: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008600895s
    STEP: Saw pod success 04/06/23 11:13:48.379
    Apr  6 11:13:48.379: INFO: Pod "pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5" satisfied condition "Succeeded or Failed"
    Apr  6 11:13:48.383: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:13:48.439
    Apr  6 11:13:48.462: INFO: Waiting for pod pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5 to disappear
    Apr  6 11:13:48.466: INFO: Pod pod-configmaps-26ccaee1-8e74-4c4d-b26f-84ecb40aeab5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 11:13:48.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3820" for this suite. 04/06/23 11:13:48.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:13:48.491
Apr  6 11:13:48.491: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 11:13:48.492
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:13:48.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:13:48.524
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731 in namespace container-probe-2352 04/06/23 11:13:48.531
Apr  6 11:13:48.545: INFO: Waiting up to 5m0s for pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731" in namespace "container-probe-2352" to be "not pending"
Apr  6 11:13:48.550: INFO: Pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63063ms
Apr  6 11:13:50.556: INFO: Pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731": Phase="Running", Reason="", readiness=true. Elapsed: 2.010927781s
Apr  6 11:13:50.556: INFO: Pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731" satisfied condition "not pending"
Apr  6 11:13:50.556: INFO: Started pod liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731 in namespace container-probe-2352
STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 11:13:50.556
Apr  6 11:13:50.561: INFO: Initial restart count of pod liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731 is 0
STEP: deleting the pod 04/06/23 11:17:51.763
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 11:17:51.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2352" for this suite. 04/06/23 11:17:51.795
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":207,"skipped":4031,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.314 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:13:48.491
    Apr  6 11:13:48.491: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 11:13:48.492
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:13:48.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:13:48.524
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731 in namespace container-probe-2352 04/06/23 11:13:48.531
    Apr  6 11:13:48.545: INFO: Waiting up to 5m0s for pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731" in namespace "container-probe-2352" to be "not pending"
    Apr  6 11:13:48.550: INFO: Pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63063ms
    Apr  6 11:13:50.556: INFO: Pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731": Phase="Running", Reason="", readiness=true. Elapsed: 2.010927781s
    Apr  6 11:13:50.556: INFO: Pod "liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731" satisfied condition "not pending"
    Apr  6 11:13:50.556: INFO: Started pod liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731 in namespace container-probe-2352
    STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 11:13:50.556
    Apr  6 11:13:50.561: INFO: Initial restart count of pod liveness-7f2a0638-61a4-4c72-ad5c-e7d1917ef731 is 0
    STEP: deleting the pod 04/06/23 11:17:51.763
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 11:17:51.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2352" for this suite. 04/06/23 11:17:51.795
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:17:51.816
Apr  6 11:17:51.817: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 11:17:51.819
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:17:51.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:17:51.843
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 04/06/23 11:17:51.85
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6113;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6113;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +notcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_tcp@PTR;sleep 1; done
 04/06/23 11:17:51.877
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6113;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6113;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +notcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_tcp@PTR;sleep 1; done
 04/06/23 11:17:51.878
STEP: creating a pod to probe DNS 04/06/23 11:17:51.878
STEP: submitting the pod to kubernetes 04/06/23 11:17:51.878
Apr  6 11:17:51.910: INFO: Waiting up to 15m0s for pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095" in namespace "dns-6113" to be "running"
Apr  6 11:17:51.919: INFO: Pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095": Phase="Pending", Reason="", readiness=false. Elapsed: 8.94049ms
Apr  6 11:17:53.927: INFO: Pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095": Phase="Running", Reason="", readiness=true. Elapsed: 2.016491874s
Apr  6 11:17:53.927: INFO: Pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095" satisfied condition "running"
STEP: retrieving the pod 04/06/23 11:17:53.927
STEP: looking for the results for each expected name from probers 04/06/23 11:17:53.933
Apr  6 11:17:54.037: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.080: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.102: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.110: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.170: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.177: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.188: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.197: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.204: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.211: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:54.254: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

Apr  6 11:17:59.279: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.312: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.321: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.331: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.348: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.365: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.431: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.440: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.452: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.462: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.470: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.478: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:17:59.531: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

Apr  6 11:18:04.267: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.311: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.328: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.337: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.344: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.420: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.428: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.445: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.456: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.465: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.474: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:04.534: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

Apr  6 11:18:09.264: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.309: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.336: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.343: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.404: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.420: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.427: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.435: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.442: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.450: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:09.633: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

Apr  6 11:18:14.305: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.353: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.379: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.400: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.476: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.486: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.495: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.503: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.516: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.525: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:14.613: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

Apr  6 11:18:19.264: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.308: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.316: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.324: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.385: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.394: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.483: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.492: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.503: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.515: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.542: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.551: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
Apr  6 11:18:19.607: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

Apr  6 11:18:24.631: INFO: DNS probes using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 succeeded

STEP: deleting the pod 04/06/23 11:18:24.631
STEP: deleting the test service 04/06/23 11:18:24.648
STEP: deleting the test headless service 04/06/23 11:18:24.669
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 11:18:24.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6113" for this suite. 04/06/23 11:18:24.696
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":208,"skipped":4031,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.890 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:17:51.816
    Apr  6 11:17:51.817: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 11:17:51.819
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:17:51.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:17:51.843
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 04/06/23 11:17:51.85
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6113;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6113;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +notcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_tcp@PTR;sleep 1; done
     04/06/23 11:17:51.877
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6113;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6113;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6113.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6113.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6113.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6113.svc;check="$$(dig +notcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_udp@PTR;check="$$(dig +tcp +noall +answer +search 191.158.64.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.64.158.191_tcp@PTR;sleep 1; done
     04/06/23 11:17:51.878
    STEP: creating a pod to probe DNS 04/06/23 11:17:51.878
    STEP: submitting the pod to kubernetes 04/06/23 11:17:51.878
    Apr  6 11:17:51.910: INFO: Waiting up to 15m0s for pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095" in namespace "dns-6113" to be "running"
    Apr  6 11:17:51.919: INFO: Pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095": Phase="Pending", Reason="", readiness=false. Elapsed: 8.94049ms
    Apr  6 11:17:53.927: INFO: Pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095": Phase="Running", Reason="", readiness=true. Elapsed: 2.016491874s
    Apr  6 11:17:53.927: INFO: Pod "dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 11:17:53.927
    STEP: looking for the results for each expected name from probers 04/06/23 11:17:53.933
    Apr  6 11:17:54.037: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.080: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.102: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.110: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.170: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.177: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.188: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.197: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.204: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.211: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:54.254: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

    Apr  6 11:17:59.279: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.312: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.321: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.331: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.348: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.365: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.431: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.440: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.452: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.462: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.470: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.478: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:17:59.531: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

    Apr  6 11:18:04.267: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.311: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.328: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.337: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.344: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.420: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.428: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.445: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.456: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.465: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.474: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:04.534: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

    Apr  6 11:18:09.264: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.309: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.336: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.343: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.404: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.420: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.427: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.435: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.442: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.450: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:09.633: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

    Apr  6 11:18:14.305: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.353: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.379: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.400: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.476: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.486: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.495: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.503: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.516: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.525: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:14.613: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

    Apr  6 11:18:19.264: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.308: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.316: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.324: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.385: INFO: Unable to read wheezy_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.394: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.483: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.492: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.503: INFO: Unable to read jessie_udp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.515: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113 from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.542: INFO: Unable to read jessie_udp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.551: INFO: Unable to read jessie_tcp@dns-test-service.dns-6113.svc from pod dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095: the server could not find the requested resource (get pods dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095)
    Apr  6 11:18:19.607: INFO: Lookups using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6113 wheezy_tcp@dns-test-service.dns-6113 wheezy_udp@dns-test-service.dns-6113.svc wheezy_tcp@dns-test-service.dns-6113.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6113 jessie_tcp@dns-test-service.dns-6113 jessie_udp@dns-test-service.dns-6113.svc jessie_tcp@dns-test-service.dns-6113.svc]

    Apr  6 11:18:24.631: INFO: DNS probes using dns-6113/dns-test-709716ec-7f35-4c82-b8e7-ee0970a74095 succeeded

    STEP: deleting the pod 04/06/23 11:18:24.631
    STEP: deleting the test service 04/06/23 11:18:24.648
    STEP: deleting the test headless service 04/06/23 11:18:24.669
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 11:18:24.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6113" for this suite. 04/06/23 11:18:24.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:18:24.707
Apr  6 11:18:24.707: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename podtemplate 04/06/23 11:18:24.708
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:24.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:24.758
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 04/06/23 11:18:24.768
STEP: Replace a pod template 04/06/23 11:18:24.773
Apr  6 11:18:24.794: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Apr  6 11:18:24.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7570" for this suite. 04/06/23 11:18:24.801
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":209,"skipped":4039,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:18:24.707
    Apr  6 11:18:24.707: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename podtemplate 04/06/23 11:18:24.708
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:24.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:24.758
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 04/06/23 11:18:24.768
    STEP: Replace a pod template 04/06/23 11:18:24.773
    Apr  6 11:18:24.794: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Apr  6 11:18:24.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7570" for this suite. 04/06/23 11:18:24.801
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:18:24.809
Apr  6 11:18:24.809: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:18:24.81
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:24.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:24.831
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 04/06/23 11:18:24.835
Apr  6 11:18:24.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e" in namespace "projected-4108" to be "Succeeded or Failed"
Apr  6 11:18:24.854: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801139ms
Apr  6 11:18:26.862: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01573063s
Apr  6 11:18:28.863: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016289942s
STEP: Saw pod success 04/06/23 11:18:28.863
Apr  6 11:18:28.863: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e" satisfied condition "Succeeded or Failed"
Apr  6 11:18:28.868: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e container client-container: <nil>
STEP: delete the pod 04/06/23 11:18:28.887
Apr  6 11:18:28.901: INFO: Waiting for pod downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e to disappear
Apr  6 11:18:28.906: INFO: Pod downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 11:18:28.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4108" for this suite. 04/06/23 11:18:28.916
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":210,"skipped":4041,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:18:24.809
    Apr  6 11:18:24.809: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:18:24.81
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:24.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:24.831
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 04/06/23 11:18:24.835
    Apr  6 11:18:24.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e" in namespace "projected-4108" to be "Succeeded or Failed"
    Apr  6 11:18:24.854: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801139ms
    Apr  6 11:18:26.862: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01573063s
    Apr  6 11:18:28.863: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016289942s
    STEP: Saw pod success 04/06/23 11:18:28.863
    Apr  6 11:18:28.863: INFO: Pod "downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e" satisfied condition "Succeeded or Failed"
    Apr  6 11:18:28.868: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e container client-container: <nil>
    STEP: delete the pod 04/06/23 11:18:28.887
    Apr  6 11:18:28.901: INFO: Waiting for pod downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e to disappear
    Apr  6 11:18:28.906: INFO: Pod downwardapi-volume-2b5f6540-9dc8-444f-a5a6-fb10ff8ed24e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 11:18:28.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4108" for this suite. 04/06/23 11:18:28.916
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:18:28.934
Apr  6 11:18:28.935: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename subpath 04/06/23 11:18:28.936
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:28.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:28.962
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/06/23 11:18:28.969
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-6tr2 04/06/23 11:18:28.981
STEP: Creating a pod to test atomic-volume-subpath 04/06/23 11:18:28.981
Apr  6 11:18:28.996: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6tr2" in namespace "subpath-6261" to be "Succeeded or Failed"
Apr  6 11:18:29.000: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917258ms
Apr  6 11:18:31.009: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013010074s
Apr  6 11:18:33.006: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010064385s
Apr  6 11:18:35.017: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 6.021568263s
Apr  6 11:18:37.010: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 8.014920911s
Apr  6 11:18:39.008: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 10.012806718s
Apr  6 11:18:41.007: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 12.0118896s
Apr  6 11:18:43.007: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 14.01126878s
Apr  6 11:18:45.010: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 16.01479848s
Apr  6 11:18:47.014: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 18.018042304s
Apr  6 11:18:49.007: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 20.011642533s
Apr  6 11:18:51.019: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=false. Elapsed: 22.022982391s
Apr  6 11:18:53.018: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022593317s
STEP: Saw pod success 04/06/23 11:18:53.019
Apr  6 11:18:53.020: INFO: Pod "pod-subpath-test-configmap-6tr2" satisfied condition "Succeeded or Failed"
Apr  6 11:18:53.027: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-subpath-test-configmap-6tr2 container test-container-subpath-configmap-6tr2: <nil>
STEP: delete the pod 04/06/23 11:18:53.054
Apr  6 11:18:53.062: INFO: Waiting for pod pod-subpath-test-configmap-6tr2 to disappear
Apr  6 11:18:53.066: INFO: Pod pod-subpath-test-configmap-6tr2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6tr2 04/06/23 11:18:53.066
Apr  6 11:18:53.066: INFO: Deleting pod "pod-subpath-test-configmap-6tr2" in namespace "subpath-6261"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Apr  6 11:18:53.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6261" for this suite. 04/06/23 11:18:53.081
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":211,"skipped":4041,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.157 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:18:28.934
    Apr  6 11:18:28.935: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename subpath 04/06/23 11:18:28.936
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:28.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:28.962
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/06/23 11:18:28.969
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-6tr2 04/06/23 11:18:28.981
    STEP: Creating a pod to test atomic-volume-subpath 04/06/23 11:18:28.981
    Apr  6 11:18:28.996: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6tr2" in namespace "subpath-6261" to be "Succeeded or Failed"
    Apr  6 11:18:29.000: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917258ms
    Apr  6 11:18:31.009: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013010074s
    Apr  6 11:18:33.006: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010064385s
    Apr  6 11:18:35.017: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 6.021568263s
    Apr  6 11:18:37.010: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 8.014920911s
    Apr  6 11:18:39.008: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 10.012806718s
    Apr  6 11:18:41.007: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 12.0118896s
    Apr  6 11:18:43.007: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 14.01126878s
    Apr  6 11:18:45.010: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 16.01479848s
    Apr  6 11:18:47.014: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 18.018042304s
    Apr  6 11:18:49.007: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=true. Elapsed: 20.011642533s
    Apr  6 11:18:51.019: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Running", Reason="", readiness=false. Elapsed: 22.022982391s
    Apr  6 11:18:53.018: INFO: Pod "pod-subpath-test-configmap-6tr2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022593317s
    STEP: Saw pod success 04/06/23 11:18:53.019
    Apr  6 11:18:53.020: INFO: Pod "pod-subpath-test-configmap-6tr2" satisfied condition "Succeeded or Failed"
    Apr  6 11:18:53.027: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-subpath-test-configmap-6tr2 container test-container-subpath-configmap-6tr2: <nil>
    STEP: delete the pod 04/06/23 11:18:53.054
    Apr  6 11:18:53.062: INFO: Waiting for pod pod-subpath-test-configmap-6tr2 to disappear
    Apr  6 11:18:53.066: INFO: Pod pod-subpath-test-configmap-6tr2 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6tr2 04/06/23 11:18:53.066
    Apr  6 11:18:53.066: INFO: Deleting pod "pod-subpath-test-configmap-6tr2" in namespace "subpath-6261"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Apr  6 11:18:53.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6261" for this suite. 04/06/23 11:18:53.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:18:53.093
Apr  6 11:18:53.093: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename daemonsets 04/06/23 11:18:53.094
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:53.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:53.124
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 04/06/23 11:18:53.182
STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:18:53.189
Apr  6 11:18:53.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:18:53.203: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:18:54.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:18:54.229: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:18:55.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:18:55.221: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
Apr  6 11:18:56.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:18:56.232: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 04/06/23 11:18:56.24
Apr  6 11:18:56.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:18:56.279: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
Apr  6 11:18:57.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:18:57.304: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
Apr  6 11:18:58.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:18:58.312: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
Apr  6 11:18:59.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:18:59.296: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
Apr  6 11:19:00.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:19:00.299: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:19:00.303
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4770, will wait for the garbage collector to delete the pods 04/06/23 11:19:00.303
Apr  6 11:19:00.365: INFO: Deleting DaemonSet.extensions daemon-set took: 5.888221ms
Apr  6 11:19:00.471: INFO: Terminating DaemonSet.extensions daemon-set pods took: 106.941511ms
Apr  6 11:19:03.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:19:03.380: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr  6 11:19:03.385: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28922"},"items":null}

Apr  6 11:19:03.389: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28922"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:19:03.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4770" for this suite. 04/06/23 11:19:03.442
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":212,"skipped":4066,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.359 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:18:53.093
    Apr  6 11:18:53.093: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename daemonsets 04/06/23 11:18:53.094
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:18:53.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:18:53.124
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 04/06/23 11:18:53.182
    STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:18:53.189
    Apr  6 11:18:53.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:18:53.203: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:18:54.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:18:54.229: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:18:55.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:18:55.221: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
    Apr  6 11:18:56.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:18:56.232: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 04/06/23 11:18:56.24
    Apr  6 11:18:56.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:18:56.279: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
    Apr  6 11:18:57.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:18:57.304: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
    Apr  6 11:18:58.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:18:58.312: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
    Apr  6 11:18:59.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:18:59.296: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
    Apr  6 11:19:00.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:19:00.299: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:19:00.303
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4770, will wait for the garbage collector to delete the pods 04/06/23 11:19:00.303
    Apr  6 11:19:00.365: INFO: Deleting DaemonSet.extensions daemon-set took: 5.888221ms
    Apr  6 11:19:00.471: INFO: Terminating DaemonSet.extensions daemon-set pods took: 106.941511ms
    Apr  6 11:19:03.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:19:03.380: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr  6 11:19:03.385: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28922"},"items":null}

    Apr  6 11:19:03.389: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28922"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:19:03.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4770" for this suite. 04/06/23 11:19:03.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:19:03.453
Apr  6 11:19:03.453: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:19:03.454
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:19:03.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:19:03.473
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 04/06/23 11:19:03.48
Apr  6 11:19:03.499: INFO: Waiting up to 5m0s for pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0" in namespace "emptydir-7087" to be "Succeeded or Failed"
Apr  6 11:19:03.504: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.998054ms
Apr  6 11:19:05.511: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012200393s
Apr  6 11:19:07.515: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015687071s
STEP: Saw pod success 04/06/23 11:19:07.515
Apr  6 11:19:07.515: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0" satisfied condition "Succeeded or Failed"
Apr  6 11:19:07.526: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0 container test-container: <nil>
STEP: delete the pod 04/06/23 11:19:07.542
Apr  6 11:19:07.553: INFO: Waiting for pod pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0 to disappear
Apr  6 11:19:07.558: INFO: Pod pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:19:07.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7087" for this suite. 04/06/23 11:19:07.573
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":213,"skipped":4088,"failed":0}
------------------------------
â€¢ [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:19:03.453
    Apr  6 11:19:03.453: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:19:03.454
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:19:03.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:19:03.473
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/06/23 11:19:03.48
    Apr  6 11:19:03.499: INFO: Waiting up to 5m0s for pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0" in namespace "emptydir-7087" to be "Succeeded or Failed"
    Apr  6 11:19:03.504: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.998054ms
    Apr  6 11:19:05.511: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012200393s
    Apr  6 11:19:07.515: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015687071s
    STEP: Saw pod success 04/06/23 11:19:07.515
    Apr  6 11:19:07.515: INFO: Pod "pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0" satisfied condition "Succeeded or Failed"
    Apr  6 11:19:07.526: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:19:07.542
    Apr  6 11:19:07.553: INFO: Waiting for pod pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0 to disappear
    Apr  6 11:19:07.558: INFO: Pod pod-b6c408fb-1eec-4882-82a0-2b3b511c01c0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:19:07.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7087" for this suite. 04/06/23 11:19:07.573
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:19:07.583
Apr  6 11:19:07.583: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-preemption 04/06/23 11:19:07.584
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:19:07.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:19:07.645
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Apr  6 11:19:07.683: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 11:20:07.794: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 04/06/23 11:20:07.799
Apr  6 11:20:07.835: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr  6 11:20:07.848: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr  6 11:20:07.892: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr  6 11:20:07.906: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr  6 11:20:07.940: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr  6 11:20:07.949: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Apr  6 11:20:07.983: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Apr  6 11:20:08.000: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Apr  6 11:20:08.064: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Apr  6 11:20:08.073: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/06/23 11:20:08.073
Apr  6 11:20:08.074: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:08.078: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.991755ms
Apr  6 11:20:10.091: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.017612111s
Apr  6 11:20:10.091: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr  6 11:20:10.098: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.110: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.906202ms
Apr  6 11:20:10.110: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.110: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.115: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.391278ms
Apr  6 11:20:10.115: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.115: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.120: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.016494ms
Apr  6 11:20:10.120: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.120: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.124: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.107206ms
Apr  6 11:20:10.124: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.124: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.128: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.892289ms
Apr  6 11:20:10.128: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.128: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.132: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.406721ms
Apr  6 11:20:10.132: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.132: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.137: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.192653ms
Apr  6 11:20:10.137: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.137: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.141: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.60212ms
Apr  6 11:20:10.141: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Apr  6 11:20:10.141: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
Apr  6 11:20:10.146: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.295684ms
Apr  6 11:20:10.146: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 04/06/23 11:20:10.146
Apr  6 11:20:10.168: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Apr  6 11:20:10.179: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.475485ms
Apr  6 11:20:12.186: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017998665s
Apr  6 11:20:14.187: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018979286s
Apr  6 11:20:16.194: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.025787182s
Apr  6 11:20:16.194: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:20:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4538" for this suite. 04/06/23 11:20:16.315
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":214,"skipped":4090,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.891 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:19:07.583
    Apr  6 11:19:07.583: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-preemption 04/06/23 11:19:07.584
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:19:07.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:19:07.645
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Apr  6 11:19:07.683: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr  6 11:20:07.794: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 04/06/23 11:20:07.799
    Apr  6 11:20:07.835: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr  6 11:20:07.848: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr  6 11:20:07.892: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr  6 11:20:07.906: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Apr  6 11:20:07.940: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Apr  6 11:20:07.949: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Apr  6 11:20:07.983: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Apr  6 11:20:08.000: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Apr  6 11:20:08.064: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Apr  6 11:20:08.073: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/06/23 11:20:08.073
    Apr  6 11:20:08.074: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:08.078: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.991755ms
    Apr  6 11:20:10.091: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.017612111s
    Apr  6 11:20:10.091: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr  6 11:20:10.098: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.110: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.906202ms
    Apr  6 11:20:10.110: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.110: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.115: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.391278ms
    Apr  6 11:20:10.115: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.115: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.120: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.016494ms
    Apr  6 11:20:10.120: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.120: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.124: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.107206ms
    Apr  6 11:20:10.124: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.124: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.128: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.892289ms
    Apr  6 11:20:10.128: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.128: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.132: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.406721ms
    Apr  6 11:20:10.132: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.132: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.137: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.192653ms
    Apr  6 11:20:10.137: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.137: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.141: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.60212ms
    Apr  6 11:20:10.141: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr  6 11:20:10.141: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4538" to be "running"
    Apr  6 11:20:10.146: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.295684ms
    Apr  6 11:20:10.146: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 04/06/23 11:20:10.146
    Apr  6 11:20:10.168: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Apr  6 11:20:10.179: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.475485ms
    Apr  6 11:20:12.186: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017998665s
    Apr  6 11:20:14.187: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018979286s
    Apr  6 11:20:16.194: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.025787182s
    Apr  6 11:20:16.194: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:20:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4538" for this suite. 04/06/23 11:20:16.315
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:16.481
Apr  6 11:20:16.481: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replication-controller 04/06/23 11:20:16.483
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:16.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:16.529
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 04/06/23 11:20:16.555
STEP: waiting for RC to be added 04/06/23 11:20:16.562
STEP: waiting for available Replicas 04/06/23 11:20:16.562
STEP: patching ReplicationController 04/06/23 11:20:18.122
STEP: waiting for RC to be modified 04/06/23 11:20:18.135
STEP: patching ReplicationController status 04/06/23 11:20:18.135
STEP: waiting for RC to be modified 04/06/23 11:20:18.142
STEP: waiting for available Replicas 04/06/23 11:20:18.142
STEP: fetching ReplicationController status 04/06/23 11:20:18.146
STEP: patching ReplicationController scale 04/06/23 11:20:18.15
STEP: waiting for RC to be modified 04/06/23 11:20:18.159
STEP: waiting for ReplicationController's scale to be the max amount 04/06/23 11:20:18.159
STEP: fetching ReplicationController; ensuring that it's patched 04/06/23 11:20:18.954
STEP: updating ReplicationController status 04/06/23 11:20:18.964
STEP: waiting for RC to be modified 04/06/23 11:20:18.973
STEP: listing all ReplicationControllers 04/06/23 11:20:18.973
STEP: checking that ReplicationController has expected values 04/06/23 11:20:18.979
STEP: deleting ReplicationControllers by collection 04/06/23 11:20:18.979
STEP: waiting for ReplicationController to have a DELETED watchEvent 04/06/23 11:20:18.99
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Apr  6 11:20:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3996" for this suite. 04/06/23 11:20:19.064
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":215,"skipped":4140,"failed":0}
------------------------------
â€¢ [2.592 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:16.481
    Apr  6 11:20:16.481: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replication-controller 04/06/23 11:20:16.483
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:16.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:16.529
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 04/06/23 11:20:16.555
    STEP: waiting for RC to be added 04/06/23 11:20:16.562
    STEP: waiting for available Replicas 04/06/23 11:20:16.562
    STEP: patching ReplicationController 04/06/23 11:20:18.122
    STEP: waiting for RC to be modified 04/06/23 11:20:18.135
    STEP: patching ReplicationController status 04/06/23 11:20:18.135
    STEP: waiting for RC to be modified 04/06/23 11:20:18.142
    STEP: waiting for available Replicas 04/06/23 11:20:18.142
    STEP: fetching ReplicationController status 04/06/23 11:20:18.146
    STEP: patching ReplicationController scale 04/06/23 11:20:18.15
    STEP: waiting for RC to be modified 04/06/23 11:20:18.159
    STEP: waiting for ReplicationController's scale to be the max amount 04/06/23 11:20:18.159
    STEP: fetching ReplicationController; ensuring that it's patched 04/06/23 11:20:18.954
    STEP: updating ReplicationController status 04/06/23 11:20:18.964
    STEP: waiting for RC to be modified 04/06/23 11:20:18.973
    STEP: listing all ReplicationControllers 04/06/23 11:20:18.973
    STEP: checking that ReplicationController has expected values 04/06/23 11:20:18.979
    STEP: deleting ReplicationControllers by collection 04/06/23 11:20:18.979
    STEP: waiting for ReplicationController to have a DELETED watchEvent 04/06/23 11:20:18.99
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Apr  6 11:20:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3996" for this suite. 04/06/23 11:20:19.064
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:19.074
Apr  6 11:20:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename watch 04/06/23 11:20:19.075
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:19.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:19.117
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 04/06/23 11:20:19.124
STEP: modifying the configmap once 04/06/23 11:20:19.13
STEP: modifying the configmap a second time 04/06/23 11:20:19.141
STEP: deleting the configmap 04/06/23 11:20:19.151
STEP: creating a watch on configmaps from the resource version returned by the first update 04/06/23 11:20:19.157
STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/06/23 11:20:19.16
Apr  6 11:20:19.161: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1039  20af48f3-2c0f-4b5f-80cd-0e3800cca4c4 29350 0 2023-04-06 11:20:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-06 11:20:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 11:20:19.161: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1039  20af48f3-2c0f-4b5f-80cd-0e3800cca4c4 29351 0 2023-04-06 11:20:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-06 11:20:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Apr  6 11:20:19.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1039" for this suite. 04/06/23 11:20:19.17
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":216,"skipped":4141,"failed":0}
------------------------------
â€¢ [0.121 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:19.074
    Apr  6 11:20:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename watch 04/06/23 11:20:19.075
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:19.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:19.117
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 04/06/23 11:20:19.124
    STEP: modifying the configmap once 04/06/23 11:20:19.13
    STEP: modifying the configmap a second time 04/06/23 11:20:19.141
    STEP: deleting the configmap 04/06/23 11:20:19.151
    STEP: creating a watch on configmaps from the resource version returned by the first update 04/06/23 11:20:19.157
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/06/23 11:20:19.16
    Apr  6 11:20:19.161: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1039  20af48f3-2c0f-4b5f-80cd-0e3800cca4c4 29350 0 2023-04-06 11:20:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-06 11:20:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 11:20:19.161: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1039  20af48f3-2c0f-4b5f-80cd-0e3800cca4c4 29351 0 2023-04-06 11:20:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-06 11:20:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Apr  6 11:20:19.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1039" for this suite. 04/06/23 11:20:19.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:19.203
Apr  6 11:20:19.203: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 11:20:19.204
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:19.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:19.228
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 11:20:19.248
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:20:19.704
STEP: Deploying the webhook pod 04/06/23 11:20:19.713
STEP: Wait for the deployment to be ready 04/06/23 11:20:19.729
Apr  6 11:20:19.739: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/06/23 11:20:21.771
STEP: Verifying the service has paired with the endpoint 04/06/23 11:20:21.785
Apr  6 11:20:22.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 04/06/23 11:20:22.799
STEP: Creating a custom resource definition that should be denied by the webhook 04/06/23 11:20:22.927
Apr  6 11:20:22.927: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:20:23.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8791" for this suite. 04/06/23 11:20:23.076
STEP: Destroying namespace "webhook-8791-markers" for this suite. 04/06/23 11:20:23.082
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":217,"skipped":4146,"failed":0}
------------------------------
â€¢ [3.962 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:19.203
    Apr  6 11:20:19.203: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 11:20:19.204
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:19.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:19.228
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 11:20:19.248
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:20:19.704
    STEP: Deploying the webhook pod 04/06/23 11:20:19.713
    STEP: Wait for the deployment to be ready 04/06/23 11:20:19.729
    Apr  6 11:20:19.739: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/06/23 11:20:21.771
    STEP: Verifying the service has paired with the endpoint 04/06/23 11:20:21.785
    Apr  6 11:20:22.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 04/06/23 11:20:22.799
    STEP: Creating a custom resource definition that should be denied by the webhook 04/06/23 11:20:22.927
    Apr  6 11:20:22.927: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:20:23.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8791" for this suite. 04/06/23 11:20:23.076
    STEP: Destroying namespace "webhook-8791-markers" for this suite. 04/06/23 11:20:23.082
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:23.172
Apr  6 11:20:23.172: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename svcaccounts 04/06/23 11:20:23.173
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:23.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:23.214
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Apr  6 11:20:23.240: INFO: Waiting up to 5m0s for pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1" in namespace "svcaccounts-1743" to be "running"
Apr  6 11:20:23.245: INFO: Pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409686ms
Apr  6 11:20:25.254: INFO: Pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014049892s
Apr  6 11:20:25.254: INFO: Pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1" satisfied condition "running"
STEP: reading a file in the container 04/06/23 11:20:25.254
Apr  6 11:20:25.254: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1743 pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 04/06/23 11:20:25.965
Apr  6 11:20:25.965: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1743 pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 04/06/23 11:20:26.712
Apr  6 11:20:26.712: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1743 pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Apr  6 11:20:27.343: INFO: Got root ca configmap in namespace "svcaccounts-1743"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Apr  6 11:20:27.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1743" for this suite. 04/06/23 11:20:27.384
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":218,"skipped":4176,"failed":0}
------------------------------
â€¢ [4.221 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:23.172
    Apr  6 11:20:23.172: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename svcaccounts 04/06/23 11:20:23.173
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:23.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:23.214
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Apr  6 11:20:23.240: INFO: Waiting up to 5m0s for pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1" in namespace "svcaccounts-1743" to be "running"
    Apr  6 11:20:23.245: INFO: Pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409686ms
    Apr  6 11:20:25.254: INFO: Pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014049892s
    Apr  6 11:20:25.254: INFO: Pod "pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1" satisfied condition "running"
    STEP: reading a file in the container 04/06/23 11:20:25.254
    Apr  6 11:20:25.254: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1743 pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 04/06/23 11:20:25.965
    Apr  6 11:20:25.965: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1743 pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 04/06/23 11:20:26.712
    Apr  6 11:20:26.712: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1743 pod-service-account-e91beb55-115f-49dd-ab59-167eab7388b1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Apr  6 11:20:27.343: INFO: Got root ca configmap in namespace "svcaccounts-1743"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Apr  6 11:20:27.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1743" for this suite. 04/06/23 11:20:27.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:27.398
Apr  6 11:20:27.398: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:20:27.399
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:27.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:27.438
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 04/06/23 11:20:27.445
STEP: Creating a ResourceQuota 04/06/23 11:20:32.451
STEP: Ensuring resource quota status is calculated 04/06/23 11:20:32.458
STEP: Creating a Pod that fits quota 04/06/23 11:20:34.464
STEP: Ensuring ResourceQuota status captures the pod usage 04/06/23 11:20:34.485
STEP: Not allowing a pod to be created that exceeds remaining quota 04/06/23 11:20:36.513
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/06/23 11:20:36.534
STEP: Ensuring a pod cannot update its resource requirements 04/06/23 11:20:36.543
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/06/23 11:20:36.55
STEP: Deleting the pod 04/06/23 11:20:38.557
STEP: Ensuring resource quota status released the pod usage 04/06/23 11:20:38.568
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:20:40.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2917" for this suite. 04/06/23 11:20:40.583
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":219,"skipped":4227,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.192 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:27.398
    Apr  6 11:20:27.398: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:20:27.399
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:27.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:27.438
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 04/06/23 11:20:27.445
    STEP: Creating a ResourceQuota 04/06/23 11:20:32.451
    STEP: Ensuring resource quota status is calculated 04/06/23 11:20:32.458
    STEP: Creating a Pod that fits quota 04/06/23 11:20:34.464
    STEP: Ensuring ResourceQuota status captures the pod usage 04/06/23 11:20:34.485
    STEP: Not allowing a pod to be created that exceeds remaining quota 04/06/23 11:20:36.513
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/06/23 11:20:36.534
    STEP: Ensuring a pod cannot update its resource requirements 04/06/23 11:20:36.543
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/06/23 11:20:36.55
    STEP: Deleting the pod 04/06/23 11:20:38.557
    STEP: Ensuring resource quota status released the pod usage 04/06/23 11:20:38.568
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:20:40.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2917" for this suite. 04/06/23 11:20:40.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:40.594
Apr  6 11:20:40.594: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-pred 04/06/23 11:20:40.595
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:40.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:40.622
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Apr  6 11:20:40.628: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 11:20:40.640: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 11:20:40.645: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
Apr  6 11:20:40.661: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:40.661: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:40.661: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:40.661: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:40.661: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:40.661: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:40.661: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:40.661: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:40.661: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:40.661: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:40.661: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 11:20:40.661: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container e2e ready: true, restart count 0
Apr  6 11:20:40.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:40.661: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:40.661: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:40.661: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
Apr  6 11:20:40.676: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.676: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:40.676: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:40.676: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.676: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:40.676: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:40.676: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:40.676: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:40.676: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:40.676: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.676: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:40.676: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:40.676: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.676: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:40.676: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.676: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:40.676: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:40.676: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:40.676: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
Apr  6 11:20:40.703: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.703: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:40.703: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:40.703: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.703: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 11:20:40.703: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:40.704: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 11:20:40.704: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 11:20:40.704: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 11:20:40.704: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container coredns ready: true, restart count 0
Apr  6 11:20:40.704: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container coredns ready: true, restart count 0
Apr  6 11:20:40.704: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:40.704: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:40.704: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:40.704: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:40.704: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:40.704: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 11:20:40.704: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 11:20:40.704: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:40.704: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:40.704: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container vpn-shoot ready: true, restart count 1
Apr  6 11:20:40.704: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.704: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:40.704: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:40.705: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
Apr  6 11:20:40.721: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.724: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:40.724: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:40.725: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.725: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:40.725: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:40.725: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:40.725: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:40.725: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:40.725: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.725: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:40.725: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:40.725: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.725: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:40.725: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.725: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:40.725: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.725: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:40.725: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:40.725: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
Apr  6 11:20:40.743: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:40.743: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:40.743: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:40.743: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 11:20:40.743: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:40.743: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:40.743: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:40.743: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:40.743: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:40.743: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:40.743: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:40.743: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:40.743: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:40.743: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 04/06/23 11:20:40.743
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1753548abdef3ae9], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 04/06/23 11:20:40.806
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:20:41.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6920" for this suite. 04/06/23 11:20:41.813
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":220,"skipped":4243,"failed":0}
------------------------------
â€¢ [1.230 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:40.594
    Apr  6 11:20:40.594: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-pred 04/06/23 11:20:40.595
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:40.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:40.622
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Apr  6 11:20:40.628: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr  6 11:20:40.640: INFO: Waiting for terminating namespaces to be deleted...
    Apr  6 11:20:40.645: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
    Apr  6 11:20:40.661: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container e2e ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:40.661: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
    Apr  6 11:20:40.676: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.676: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.676: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:40.676: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.676: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.676: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.676: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:40.676: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
    Apr  6 11:20:40.703: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.703: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:40.703: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:40.703: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.703: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Apr  6 11:20:40.703: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container vpn-shoot ready: true, restart count 1
    Apr  6 11:20:40.704: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.704: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:40.704: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:40.705: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
    Apr  6 11:20:40.721: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.724: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:40.724: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.725: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:40.725: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.725: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.725: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.725: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.725: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:40.725: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
    Apr  6 11:20:40.743: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container calico-typha ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:40.743: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:40.743: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 04/06/23 11:20:40.743
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1753548abdef3ae9], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 04/06/23 11:20:40.806
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:20:41.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6920" for this suite. 04/06/23 11:20:41.813
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:41.825
Apr  6 11:20:41.825: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-pred 04/06/23 11:20:41.826
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:41.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:41.856
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Apr  6 11:20:41.864: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 11:20:41.898: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 11:20:41.905: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
Apr  6 11:20:41.927: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:41.927: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:41.927: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:41.927: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:41.927: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:41.927: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:41.927: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:41.927: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:41.927: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:41.927: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:41.927: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 11:20:41.927: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container e2e ready: true, restart count 0
Apr  6 11:20:41.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:41.927: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:41.927: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:41.927: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
Apr  6 11:20:41.945: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.946: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:41.946: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:41.946: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.946: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:41.946: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:41.946: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:41.946: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:41.946: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:41.946: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.946: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:41.946: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:41.946: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.946: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:41.946: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.946: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:41.946: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.946: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:41.946: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:41.946: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
Apr  6 11:20:41.974: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:41.974: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:41.974: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr  6 11:20:41.974: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:41.974: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 11:20:41.974: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 11:20:41.974: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container autoscaler ready: true, restart count 0
Apr  6 11:20:41.974: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container coredns ready: true, restart count 0
Apr  6 11:20:41.974: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container coredns ready: true, restart count 0
Apr  6 11:20:41.974: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:41.974: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:41.974: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:41.974: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:41.974: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:41.974: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 11:20:41.974: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container metrics-server ready: true, restart count 0
Apr  6 11:20:41.974: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:41.974: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:41.974: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container vpn-shoot ready: true, restart count 1
Apr  6 11:20:41.974: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:41.974: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:41.974: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
Apr  6 11:20:41.993: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.993: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:41.993: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:41.993: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.993: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:41.993: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:41.993: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:41.993: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:41.993: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:41.993: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.993: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:41.993: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:41.993: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.993: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:41.993: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:41.993: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:41.993: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:41.993: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:41.993: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 11:20:41.993: INFO: 
Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
Apr  6 11:20:42.020: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container proxy ready: true, restart count 0
Apr  6 11:20:42.020: INFO: 	Container sidecar ready: true, restart count 0
Apr  6 11:20:42.020: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container calico-node ready: true, restart count 0
Apr  6 11:20:42.020: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container calico-typha ready: true, restart count 0
Apr  6 11:20:42.020: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container csi-driver ready: true, restart count 0
Apr  6 11:20:42.020: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Apr  6 11:20:42.020: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr  6 11:20:42.020: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container conntrack-fix ready: true, restart count 0
Apr  6 11:20:42.020: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 11:20:42.020: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container node-exporter ready: true, restart count 0
Apr  6 11:20:42.020: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr  6 11:20:42.020: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
Apr  6 11:20:42.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 11:20:42.020: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st 04/06/23 11:20:48.197
STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d 04/06/23 11:20:48.242
STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 04/06/23 11:20:48.269
STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 11:20:48.302
STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l 04/06/23 11:20:48.32
Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-47z48 requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-5t8ck requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-drlpb requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-f8ckt requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-xnr6q requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.357: INFO: Pod calico-kube-controllers-778f49788f-x8w4d requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod calico-node-2ldzh requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.357: INFO: Pod calico-node-9wkq6 requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.357: INFO: Pod calico-node-chv4m requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.357: INFO: Pod calico-node-fh42t requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.357: INFO: Pod calico-node-n99ct requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod calico-node-vertical-autoscaler-78975f7c69-nmnx9 requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod calico-typha-deploy-64d5844fc8-nljtc requesting resource cpu=320m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.357: INFO: Pod calico-typha-horizontal-autoscaler-6545f79b64-bzbzq requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod calico-typha-vertical-autoscaler-5db4c44555-n6jzx requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod coredns-865d786d7f-l2xgj requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod coredns-865d786d7f-vqssh requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod csi-driver-node-7ljtn requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod csi-driver-node-99vz8 requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.357: INFO: Pod csi-driver-node-jmj9r requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.357: INFO: Pod csi-driver-node-llx9z requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.357: INFO: Pod csi-driver-node-lzjgt requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.357: INFO: Pod metrics-server-6b8d755f56-xknb6 requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod metrics-server-6b8d755f56-zq65x requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod node-exporter-2t52m requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod node-exporter-9qcsn requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.357: INFO: Pod node-exporter-mm8q9 requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.357: INFO: Pod node-exporter-sgvvh requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.357: INFO: Pod node-exporter-x4jr2 requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.357: INFO: Pod node-problem-detector-8zf6h requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.357: INFO: Pod node-problem-detector-99d9x requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.357: INFO: Pod node-problem-detector-bqwqg requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod node-problem-detector-mfcn2 requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.357: INFO: Pod node-problem-detector-x7stm requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.357: INFO: Pod vpn-shoot-5b666d548f-q87zf requesting resource cpu=100m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.357: INFO: Pod sonobuoy requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.358: INFO: Pod sonobuoy-e2e-job-fa29383266594ee9 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
STEP: Starting Pods to consume most of the cluster CPU. 04/06/23 11:20:48.358
Apr  6 11:20:48.358: INFO: Creating a pod which consumes cpu=2452m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
Apr  6 11:20:48.376: INFO: Creating a pod which consumes cpu=2452m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
Apr  6 11:20:48.390: INFO: Creating a pod which consumes cpu=2214m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
Apr  6 11:20:48.407: INFO: Creating a pod which consumes cpu=2452m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
Apr  6 11:20:48.417: INFO: Creating a pod which consumes cpu=2228m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
Apr  6 11:20:48.429: INFO: Waiting up to 5m0s for pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46" in namespace "sched-pred-9767" to be "running"
Apr  6 11:20:48.434: INFO: Pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291721ms
Apr  6 11:20:50.442: INFO: Pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46": Phase="Running", Reason="", readiness=true. Elapsed: 2.01214683s
Apr  6 11:20:50.442: INFO: Pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46" satisfied condition "running"
Apr  6 11:20:50.442: INFO: Waiting up to 5m0s for pod "filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d" in namespace "sched-pred-9767" to be "running"
Apr  6 11:20:50.445: INFO: Pod "filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d": Phase="Running", Reason="", readiness=true. Elapsed: 3.66399ms
Apr  6 11:20:50.445: INFO: Pod "filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d" satisfied condition "running"
Apr  6 11:20:50.445: INFO: Waiting up to 5m0s for pod "filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3" in namespace "sched-pred-9767" to be "running"
Apr  6 11:20:50.450: INFO: Pod "filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.306166ms
Apr  6 11:20:50.450: INFO: Pod "filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3" satisfied condition "running"
Apr  6 11:20:50.450: INFO: Waiting up to 5m0s for pod "filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b" in namespace "sched-pred-9767" to be "running"
Apr  6 11:20:50.453: INFO: Pod "filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b": Phase="Running", Reason="", readiness=true. Elapsed: 3.555097ms
Apr  6 11:20:50.453: INFO: Pod "filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b" satisfied condition "running"
Apr  6 11:20:50.453: INFO: Waiting up to 5m0s for pod "filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f" in namespace "sched-pred-9767" to be "running"
Apr  6 11:20:50.458: INFO: Pod "filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f": Phase="Running", Reason="", readiness=true. Elapsed: 4.389344ms
Apr  6 11:20:50.458: INFO: Pod "filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 04/06/23 11:20:50.458
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548c8517c793], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l] 04/06/23 11:20:50.463
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548cb64d60a5], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.463
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548cb96ac477], Reason = [Created], Message = [Created container filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f] 04/06/23 11:20:50.463
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548cc68ee92c], Reason = [Started], Message = [Started container filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f] 04/06/23 11:20:50.463
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548c81d51266], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46 to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st] 04/06/23 11:20:50.463
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548cb1aa9163], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548cb3ea6df9], Reason = [Created], Message = [Created container filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548cbe3b819c], Reason = [Started], Message = [Started container filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548c83928fc6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3 to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548caa62ce28], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548cab47b025], Reason = [Created], Message = [Created container filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548cb0560b74], Reason = [Started], Message = [Started container filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548c8465af8f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548ca9b29ed3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548caa8a0514], Reason = [Created], Message = [Created container filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b] 04/06/23 11:20:50.464
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548cb0311d6c], Reason = [Started], Message = [Started container filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b] 04/06/23 11:20:50.465
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548c835d6550], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d] 04/06/23 11:20:50.465
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548cab9e3a4f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.465
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548cad710a46], Reason = [Created], Message = [Created container filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d] 04/06/23 11:20:50.465
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548cb68f656f], Reason = [Started], Message = [Started container filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d] 04/06/23 11:20:50.465
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1753548cfebea60c], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.] 04/06/23 11:20:50.478
STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l 04/06/23 11:20:51.494
STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.52
STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st 04/06/23 11:20:51.525
STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.56
STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d 04/06/23 11:20:51.566
STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.591
STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 04/06/23 11:20:51.618
STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.642
STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 11:20:51.647
STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.695
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:20:51.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9767" for this suite. 04/06/23 11:20:51.736
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":221,"skipped":4249,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.919 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:41.825
    Apr  6 11:20:41.825: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-pred 04/06/23 11:20:41.826
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:41.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:41.856
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Apr  6 11:20:41.864: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr  6 11:20:41.898: INFO: Waiting for terminating namespaces to be deleted...
    Apr  6 11:20:41.905: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st before test
    Apr  6 11:20:41.927: INFO: apiserver-proxy-5t8ck from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: calico-node-9wkq6 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: csi-driver-node-llx9z from kube-system started at 2023-04-06 10:13:18 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz from kube-system started at 2023-04-06 10:13:18 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: node-exporter-9qcsn from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: node-problem-detector-mfcn2 from kube-system started at 2023-04-06 10:13:18 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: sonobuoy from sonobuoy started at 2023-04-06 10:17:21 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: sonobuoy-e2e-job-fa29383266594ee9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container e2e ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:41.927: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d before test
    Apr  6 11:20:41.945: INFO: apiserver-proxy-drlpb from kube-system started at 2023-04-06 10:13:17 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.946: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: calico-node-chv4m from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.946: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: csi-driver-node-99vz8 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:41.946: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j from kube-system started at 2023-04-06 10:13:31 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.946: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: node-exporter-mm8q9 from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.946: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: node-problem-detector-99d9x from kube-system started at 2023-04-06 10:13:17 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.946: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.946: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:41.946: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 before test
    Apr  6 11:20:41.974: INFO: apiserver-proxy-f8ckt from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: calico-kube-controllers-778f49788f-x8w4d from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: calico-node-n99ct from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: calico-node-vertical-autoscaler-78975f7c69-nmnx9 from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: calico-typha-horizontal-autoscaler-6545f79b64-bzbzq from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: calico-typha-vertical-autoscaler-5db4c44555-n6jzx from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container autoscaler ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: coredns-865d786d7f-l2xgj from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: coredns-865d786d7f-vqssh from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container coredns ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: csi-driver-node-7ljtn from kube-system started at 2023-04-06 10:12:52 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f from kube-system started at 2023-04-06 10:12:52 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: metrics-server-6b8d755f56-xknb6 from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: metrics-server-6b8d755f56-zq65x from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container metrics-server ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: node-exporter-2t52m from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: node-problem-detector-bqwqg from kube-system started at 2023-04-06 10:12:52 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: vpn-shoot-5b666d548f-q87zf from kube-system started at 2023-04-06 10:13:12 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container vpn-shoot ready: true, restart count 1
    Apr  6 11:20:41.974: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:41.974: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk before test
    Apr  6 11:20:41.993: INFO: apiserver-proxy-47z48 from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.993: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: calico-node-fh42t from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.993: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: csi-driver-node-jmj9r from kube-system started at 2023-04-06 10:13:03 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:41.993: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc from kube-system started at 2023-04-06 10:13:03 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.993: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: node-exporter-sgvvh from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.993: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: node-problem-detector-x7stm from kube-system started at 2023-04-06 10:13:03 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:41.993: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:41.993: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr  6 11:20:41.993: INFO: 
    Logging pods the apiserver thinks is on node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l before test
    Apr  6 11:20:42.020: INFO: apiserver-proxy-xnr6q from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container proxy ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: 	Container sidecar ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: calico-node-2ldzh from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container calico-node ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: calico-typha-deploy-64d5844fc8-nljtc from kube-system started at 2023-04-06 10:13:43 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container calico-typha ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: csi-driver-node-lzjgt from kube-system started at 2023-04-06 10:13:26 +0000 UTC (3 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container csi-driver ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw from kube-system started at 2023-04-06 10:13:26 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container conntrack-fix ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: node-exporter-x4jr2 from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container node-exporter ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: node-problem-detector-8zf6h from kube-system started at 2023-04-06 10:13:26 +0000 UTC (1 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container node-problem-detector ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 from sonobuoy started at 2023-04-06 10:17:32 +0000 UTC (2 container statuses recorded)
    Apr  6 11:20:42.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr  6 11:20:42.020: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st 04/06/23 11:20:48.197
    STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d 04/06/23 11:20:48.242
    STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 04/06/23 11:20:48.269
    STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 11:20:48.302
    STEP: verifying the node has the label node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l 04/06/23 11:20:48.32
    Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-47z48 requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-5t8ck requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-drlpb requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-f8ckt requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod apiserver-proxy-xnr6q requesting resource cpu=40m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.357: INFO: Pod calico-kube-controllers-778f49788f-x8w4d requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod calico-node-2ldzh requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.357: INFO: Pod calico-node-9wkq6 requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.357: INFO: Pod calico-node-chv4m requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.357: INFO: Pod calico-node-fh42t requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.357: INFO: Pod calico-node-n99ct requesting resource cpu=250m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod calico-node-vertical-autoscaler-78975f7c69-nmnx9 requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod calico-typha-deploy-64d5844fc8-nljtc requesting resource cpu=320m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.357: INFO: Pod calico-typha-horizontal-autoscaler-6545f79b64-bzbzq requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod calico-typha-vertical-autoscaler-5db4c44555-n6jzx requesting resource cpu=10m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod coredns-865d786d7f-l2xgj requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod coredns-865d786d7f-vqssh requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod csi-driver-node-7ljtn requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod csi-driver-node-99vz8 requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.357: INFO: Pod csi-driver-node-jmj9r requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.357: INFO: Pod csi-driver-node-llx9z requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.357: INFO: Pod csi-driver-node-lzjgt requesting resource cpu=37m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-7qljw requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-9gd2f requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-djkjz requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-fgd5j requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.357: INFO: Pod kube-proxy-pool-5srtzxdh8p-v1.25.8-kgcgc requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.357: INFO: Pod metrics-server-6b8d755f56-xknb6 requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod metrics-server-6b8d755f56-zq65x requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod node-exporter-2t52m requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod node-exporter-9qcsn requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.357: INFO: Pod node-exporter-mm8q9 requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.357: INFO: Pod node-exporter-sgvvh requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.357: INFO: Pod node-exporter-x4jr2 requesting resource cpu=50m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.357: INFO: Pod node-problem-detector-8zf6h requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.357: INFO: Pod node-problem-detector-99d9x requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.357: INFO: Pod node-problem-detector-bqwqg requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod node-problem-detector-mfcn2 requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.357: INFO: Pod node-problem-detector-x7stm requesting resource cpu=20m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.357: INFO: Pod vpn-shoot-5b666d548f-q87zf requesting resource cpu=100m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.357: INFO: Pod sonobuoy requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.358: INFO: Pod sonobuoy-e2e-job-fa29383266594ee9 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-48nx7 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-7rnkp requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-9lcj9 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-jrn4b requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.358: INFO: Pod sonobuoy-systemd-logs-daemon-set-23438e93172843da-pj4v5 requesting resource cpu=0m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    STEP: Starting Pods to consume most of the cluster CPU. 04/06/23 11:20:48.358
    Apr  6 11:20:48.358: INFO: Creating a pod which consumes cpu=2452m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st
    Apr  6 11:20:48.376: INFO: Creating a pod which consumes cpu=2452m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d
    Apr  6 11:20:48.390: INFO: Creating a pod which consumes cpu=2214m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4
    Apr  6 11:20:48.407: INFO: Creating a pod which consumes cpu=2452m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk
    Apr  6 11:20:48.417: INFO: Creating a pod which consumes cpu=2228m on Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l
    Apr  6 11:20:48.429: INFO: Waiting up to 5m0s for pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46" in namespace "sched-pred-9767" to be "running"
    Apr  6 11:20:48.434: INFO: Pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291721ms
    Apr  6 11:20:50.442: INFO: Pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46": Phase="Running", Reason="", readiness=true. Elapsed: 2.01214683s
    Apr  6 11:20:50.442: INFO: Pod "filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46" satisfied condition "running"
    Apr  6 11:20:50.442: INFO: Waiting up to 5m0s for pod "filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d" in namespace "sched-pred-9767" to be "running"
    Apr  6 11:20:50.445: INFO: Pod "filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d": Phase="Running", Reason="", readiness=true. Elapsed: 3.66399ms
    Apr  6 11:20:50.445: INFO: Pod "filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d" satisfied condition "running"
    Apr  6 11:20:50.445: INFO: Waiting up to 5m0s for pod "filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3" in namespace "sched-pred-9767" to be "running"
    Apr  6 11:20:50.450: INFO: Pod "filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.306166ms
    Apr  6 11:20:50.450: INFO: Pod "filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3" satisfied condition "running"
    Apr  6 11:20:50.450: INFO: Waiting up to 5m0s for pod "filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b" in namespace "sched-pred-9767" to be "running"
    Apr  6 11:20:50.453: INFO: Pod "filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b": Phase="Running", Reason="", readiness=true. Elapsed: 3.555097ms
    Apr  6 11:20:50.453: INFO: Pod "filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b" satisfied condition "running"
    Apr  6 11:20:50.453: INFO: Waiting up to 5m0s for pod "filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f" in namespace "sched-pred-9767" to be "running"
    Apr  6 11:20:50.458: INFO: Pod "filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f": Phase="Running", Reason="", readiness=true. Elapsed: 4.389344ms
    Apr  6 11:20:50.458: INFO: Pod "filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 04/06/23 11:20:50.458
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548c8517c793], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l] 04/06/23 11:20:50.463
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548cb64d60a5], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.463
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548cb96ac477], Reason = [Created], Message = [Created container filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f] 04/06/23 11:20:50.463
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f.1753548cc68ee92c], Reason = [Started], Message = [Started container filler-pod-0f4e1fa8-d781-4367-96c5-e13d38da780f] 04/06/23 11:20:50.463
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548c81d51266], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46 to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st] 04/06/23 11:20:50.463
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548cb1aa9163], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548cb3ea6df9], Reason = [Created], Message = [Created container filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46.1753548cbe3b819c], Reason = [Started], Message = [Started container filler-pod-76851d2f-3503-418f-9e28-cb91e8b91a46] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548c83928fc6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3 to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548caa62ce28], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548cab47b025], Reason = [Created], Message = [Created container filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3.1753548cb0560b74], Reason = [Started], Message = [Started container filler-pod-7b562279-ae06-4409-97c5-9caff319f9b3] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548c8465af8f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548ca9b29ed3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548caa8a0514], Reason = [Created], Message = [Created container filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b] 04/06/23 11:20:50.464
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b.1753548cb0311d6c], Reason = [Started], Message = [Started container filler-pod-ce3ad0fc-2af9-4063-a89b-6136fea8fd4b] 04/06/23 11:20:50.465
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548c835d6550], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9767/filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d to shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d] 04/06/23 11:20:50.465
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548cab9e3a4f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 04/06/23 11:20:50.465
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548cad710a46], Reason = [Created], Message = [Created container filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d] 04/06/23 11:20:50.465
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d.1753548cb68f656f], Reason = [Started], Message = [Started container filler-pod-cf088d1e-004f-48c9-abb8-928e3516361d] 04/06/23 11:20:50.465
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1753548cfebea60c], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.] 04/06/23 11:20:50.478
    STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l 04/06/23 11:20:51.494
    STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.52
    STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st 04/06/23 11:20:51.525
    STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.56
    STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d 04/06/23 11:20:51.566
    STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.591
    STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4 04/06/23 11:20:51.618
    STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.642
    STEP: removing the label node off the node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk 04/06/23 11:20:51.647
    STEP: verifying the node doesn't have the label node 04/06/23 11:20:51.695
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:20:51.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9767" for this suite. 04/06/23 11:20:51.736
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:51.746
Apr  6 11:20:51.746: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename limitrange 04/06/23 11:20:51.749
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:51.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:51.801
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 04/06/23 11:20:51.811
STEP: Setting up watch 04/06/23 11:20:51.811
STEP: Submitting a LimitRange 04/06/23 11:20:51.917
STEP: Verifying LimitRange creation was observed 04/06/23 11:20:51.928
STEP: Fetching the LimitRange to ensure it has proper values 04/06/23 11:20:51.928
Apr  6 11:20:51.932: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr  6 11:20:51.933: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 04/06/23 11:20:51.933
STEP: Ensuring Pod has resource requirements applied from LimitRange 04/06/23 11:20:51.943
Apr  6 11:20:51.947: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr  6 11:20:51.947: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 04/06/23 11:20:51.947
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/06/23 11:20:51.956
Apr  6 11:20:51.961: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr  6 11:20:51.961: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 04/06/23 11:20:51.961
STEP: Failing to create a Pod with more than max resources 04/06/23 11:20:51.967
STEP: Updating a LimitRange 04/06/23 11:20:51.975
STEP: Verifying LimitRange updating is effective 04/06/23 11:20:51.98
STEP: Creating a Pod with less than former min resources 04/06/23 11:20:53.987
STEP: Failing to create a Pod with more than max resources 04/06/23 11:20:53.996
STEP: Deleting a LimitRange 04/06/23 11:20:54.004
STEP: Verifying the LimitRange was deleted 04/06/23 11:20:54.01
Apr  6 11:20:59.018: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 04/06/23 11:20:59.019
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Apr  6 11:20:59.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2865" for this suite. 04/06/23 11:20:59.047
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":222,"skipped":4296,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.309 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:51.746
    Apr  6 11:20:51.746: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename limitrange 04/06/23 11:20:51.749
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:51.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:51.801
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 04/06/23 11:20:51.811
    STEP: Setting up watch 04/06/23 11:20:51.811
    STEP: Submitting a LimitRange 04/06/23 11:20:51.917
    STEP: Verifying LimitRange creation was observed 04/06/23 11:20:51.928
    STEP: Fetching the LimitRange to ensure it has proper values 04/06/23 11:20:51.928
    Apr  6 11:20:51.932: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr  6 11:20:51.933: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 04/06/23 11:20:51.933
    STEP: Ensuring Pod has resource requirements applied from LimitRange 04/06/23 11:20:51.943
    Apr  6 11:20:51.947: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr  6 11:20:51.947: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 04/06/23 11:20:51.947
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/06/23 11:20:51.956
    Apr  6 11:20:51.961: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Apr  6 11:20:51.961: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 04/06/23 11:20:51.961
    STEP: Failing to create a Pod with more than max resources 04/06/23 11:20:51.967
    STEP: Updating a LimitRange 04/06/23 11:20:51.975
    STEP: Verifying LimitRange updating is effective 04/06/23 11:20:51.98
    STEP: Creating a Pod with less than former min resources 04/06/23 11:20:53.987
    STEP: Failing to create a Pod with more than max resources 04/06/23 11:20:53.996
    STEP: Deleting a LimitRange 04/06/23 11:20:54.004
    STEP: Verifying the LimitRange was deleted 04/06/23 11:20:54.01
    Apr  6 11:20:59.018: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 04/06/23 11:20:59.019
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Apr  6 11:20:59.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-2865" for this suite. 04/06/23 11:20:59.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:20:59.056
Apr  6 11:20:59.057: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 11:20:59.059
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:59.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:59.089
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9353 04/06/23 11:20:59.101
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 04/06/23 11:20:59.107
Apr  6 11:20:59.125: INFO: Found 0 stateful pods, waiting for 3
Apr  6 11:21:09.138: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:21:09.138: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:21:09.138: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 04/06/23 11:21:09.155
Apr  6 11:21:09.181: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/06/23 11:21:09.181
STEP: Not applying an update when the partition is greater than the number of replicas 04/06/23 11:21:19.211
STEP: Performing a canary update 04/06/23 11:21:19.212
Apr  6 11:21:19.249: INFO: Updating stateful set ss2
Apr  6 11:21:19.262: INFO: Waiting for Pod statefulset-9353/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 04/06/23 11:21:29.277
Apr  6 11:21:29.322: INFO: Found 1 stateful pods, waiting for 3
Apr  6 11:21:39.337: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:21:39.337: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:21:39.337: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 04/06/23 11:21:39.348
Apr  6 11:21:39.375: INFO: Updating stateful set ss2
Apr  6 11:21:39.392: INFO: Waiting for Pod statefulset-9353/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Apr  6 11:21:49.429: INFO: Updating stateful set ss2
Apr  6 11:21:49.438: INFO: Waiting for StatefulSet statefulset-9353/ss2 to complete update
Apr  6 11:21:49.438: INFO: Waiting for Pod statefulset-9353/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 11:21:59.460: INFO: Deleting all statefulset in ns statefulset-9353
Apr  6 11:21:59.465: INFO: Scaling statefulset ss2 to 0
Apr  6 11:22:09.494: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:22:09.499: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 11:22:09.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9353" for this suite. 04/06/23 11:22:09.529
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":223,"skipped":4301,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.480 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:20:59.056
    Apr  6 11:20:59.057: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 11:20:59.059
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:20:59.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:20:59.089
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9353 04/06/23 11:20:59.101
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 04/06/23 11:20:59.107
    Apr  6 11:20:59.125: INFO: Found 0 stateful pods, waiting for 3
    Apr  6 11:21:09.138: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:21:09.138: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:21:09.138: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 04/06/23 11:21:09.155
    Apr  6 11:21:09.181: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/06/23 11:21:09.181
    STEP: Not applying an update when the partition is greater than the number of replicas 04/06/23 11:21:19.211
    STEP: Performing a canary update 04/06/23 11:21:19.212
    Apr  6 11:21:19.249: INFO: Updating stateful set ss2
    Apr  6 11:21:19.262: INFO: Waiting for Pod statefulset-9353/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 04/06/23 11:21:29.277
    Apr  6 11:21:29.322: INFO: Found 1 stateful pods, waiting for 3
    Apr  6 11:21:39.337: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:21:39.337: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:21:39.337: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 04/06/23 11:21:39.348
    Apr  6 11:21:39.375: INFO: Updating stateful set ss2
    Apr  6 11:21:39.392: INFO: Waiting for Pod statefulset-9353/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Apr  6 11:21:49.429: INFO: Updating stateful set ss2
    Apr  6 11:21:49.438: INFO: Waiting for StatefulSet statefulset-9353/ss2 to complete update
    Apr  6 11:21:49.438: INFO: Waiting for Pod statefulset-9353/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 11:21:59.460: INFO: Deleting all statefulset in ns statefulset-9353
    Apr  6 11:21:59.465: INFO: Scaling statefulset ss2 to 0
    Apr  6 11:22:09.494: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:22:09.499: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 11:22:09.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9353" for this suite. 04/06/23 11:22:09.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:09.538
Apr  6 11:22:09.538: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 11:22:09.54
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:09.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:09.566
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-c217ebc4-0d4f-4d0f-9965-edceaa7246b7 04/06/23 11:22:09.574
STEP: Creating a pod to test consume secrets 04/06/23 11:22:09.58
Apr  6 11:22:09.598: INFO: Waiting up to 5m0s for pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f" in namespace "secrets-3953" to be "Succeeded or Failed"
Apr  6 11:22:09.603: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810694ms
Apr  6 11:22:11.610: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012140873s
Apr  6 11:22:13.610: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011949229s
STEP: Saw pod success 04/06/23 11:22:13.61
Apr  6 11:22:13.610: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f" satisfied condition "Succeeded or Failed"
Apr  6 11:22:13.615: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 11:22:13.667
Apr  6 11:22:13.677: INFO: Waiting for pod pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f to disappear
Apr  6 11:22:13.682: INFO: Pod pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 11:22:13.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3953" for this suite. 04/06/23 11:22:13.696
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":224,"skipped":4317,"failed":0}
------------------------------
â€¢ [4.172 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:09.538
    Apr  6 11:22:09.538: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 11:22:09.54
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:09.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:09.566
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-c217ebc4-0d4f-4d0f-9965-edceaa7246b7 04/06/23 11:22:09.574
    STEP: Creating a pod to test consume secrets 04/06/23 11:22:09.58
    Apr  6 11:22:09.598: INFO: Waiting up to 5m0s for pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f" in namespace "secrets-3953" to be "Succeeded or Failed"
    Apr  6 11:22:09.603: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810694ms
    Apr  6 11:22:11.610: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012140873s
    Apr  6 11:22:13.610: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011949229s
    STEP: Saw pod success 04/06/23 11:22:13.61
    Apr  6 11:22:13.610: INFO: Pod "pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f" satisfied condition "Succeeded or Failed"
    Apr  6 11:22:13.615: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:22:13.667
    Apr  6 11:22:13.677: INFO: Waiting for pod pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f to disappear
    Apr  6 11:22:13.682: INFO: Pod pod-secrets-46d5dc83-09e3-47ef-af38-76727b48db1f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 11:22:13.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3953" for this suite. 04/06/23 11:22:13.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:13.72
Apr  6 11:22:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 11:22:13.722
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:13.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:13.748
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-61445c94-1f79-4557-ae4a-67e2635c0621 04/06/23 11:22:13.763
STEP: Creating the pod 04/06/23 11:22:13.769
Apr  6 11:22:13.784: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47" in namespace "configmap-5346" to be "running"
Apr  6 11:22:13.815: INFO: Pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47": Phase="Pending", Reason="", readiness=false. Elapsed: 30.988571ms
Apr  6 11:22:15.822: INFO: Pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47": Phase="Running", Reason="", readiness=true. Elapsed: 2.037533604s
Apr  6 11:22:15.822: INFO: Pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47" satisfied condition "running"
STEP: Waiting for pod with text data 04/06/23 11:22:15.822
STEP: Waiting for pod with binary data 04/06/23 11:22:15.841
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 11:22:15.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5346" for this suite. 04/06/23 11:22:15.944
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":225,"skipped":4368,"failed":0}
------------------------------
â€¢ [2.242 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:13.72
    Apr  6 11:22:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 11:22:13.722
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:13.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:13.748
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-61445c94-1f79-4557-ae4a-67e2635c0621 04/06/23 11:22:13.763
    STEP: Creating the pod 04/06/23 11:22:13.769
    Apr  6 11:22:13.784: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47" in namespace "configmap-5346" to be "running"
    Apr  6 11:22:13.815: INFO: Pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47": Phase="Pending", Reason="", readiness=false. Elapsed: 30.988571ms
    Apr  6 11:22:15.822: INFO: Pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47": Phase="Running", Reason="", readiness=true. Elapsed: 2.037533604s
    Apr  6 11:22:15.822: INFO: Pod "pod-configmaps-f8c1cf1d-222a-44d4-b111-6a3774a9ad47" satisfied condition "running"
    STEP: Waiting for pod with text data 04/06/23 11:22:15.822
    STEP: Waiting for pod with binary data 04/06/23 11:22:15.841
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 11:22:15.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5346" for this suite. 04/06/23 11:22:15.944
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:15.962
Apr  6 11:22:15.963: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename namespaces 04/06/23 11:22:15.966
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:15.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:16.004
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 04/06/23 11:22:16.027
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:16.043
STEP: Creating a service in the namespace 04/06/23 11:22:16.049
STEP: Deleting the namespace 04/06/23 11:22:16.076
STEP: Waiting for the namespace to be removed. 04/06/23 11:22:16.084
STEP: Recreating the namespace 04/06/23 11:22:22.091
STEP: Verifying there is no service in the namespace 04/06/23 11:22:22.105
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:22:22.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5170" for this suite. 04/06/23 11:22:22.123
STEP: Destroying namespace "nsdeletetest-9762" for this suite. 04/06/23 11:22:22.13
Apr  6 11:22:22.145: INFO: Namespace nsdeletetest-9762 was already deleted
STEP: Destroying namespace "nsdeletetest-8362" for this suite. 04/06/23 11:22:22.145
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":226,"skipped":4371,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.190 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:15.962
    Apr  6 11:22:15.963: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename namespaces 04/06/23 11:22:15.966
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:15.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:16.004
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 04/06/23 11:22:16.027
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:16.043
    STEP: Creating a service in the namespace 04/06/23 11:22:16.049
    STEP: Deleting the namespace 04/06/23 11:22:16.076
    STEP: Waiting for the namespace to be removed. 04/06/23 11:22:16.084
    STEP: Recreating the namespace 04/06/23 11:22:22.091
    STEP: Verifying there is no service in the namespace 04/06/23 11:22:22.105
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:22:22.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5170" for this suite. 04/06/23 11:22:22.123
    STEP: Destroying namespace "nsdeletetest-9762" for this suite. 04/06/23 11:22:22.13
    Apr  6 11:22:22.145: INFO: Namespace nsdeletetest-9762 was already deleted
    STEP: Destroying namespace "nsdeletetest-8362" for this suite. 04/06/23 11:22:22.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:22.155
Apr  6 11:22:22.155: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 11:22:22.157
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:22.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:22.182
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 04/06/23 11:22:22.206
STEP: watching for Pod to be ready 04/06/23 11:22:22.221
Apr  6 11:22:22.226: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions []
Apr  6 11:22:22.226: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
Apr  6 11:22:22.241: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
Apr  6 11:22:22.774: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
Apr  6 11:22:23.971: INFO: Found Pod pod-test in namespace pods-4585 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 04/06/23 11:22:23.976
STEP: getting the Pod and ensuring that it's patched 04/06/23 11:22:23.992
STEP: replacing the Pod's status Ready condition to False 04/06/23 11:22:24.008
STEP: check the Pod again to ensure its Ready conditions are False 04/06/23 11:22:24.031
STEP: deleting the Pod via a Collection with a LabelSelector 04/06/23 11:22:24.031
STEP: watching for the Pod to be deleted 04/06/23 11:22:24.047
Apr  6 11:22:24.051: INFO: observed event type MODIFIED
Apr  6 11:22:26.005: INFO: observed event type MODIFIED
Apr  6 11:22:26.364: INFO: observed event type MODIFIED
Apr  6 11:22:27.017: INFO: observed event type MODIFIED
Apr  6 11:22:27.026: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 11:22:27.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4585" for this suite. 04/06/23 11:22:27.043
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":227,"skipped":4382,"failed":0}
------------------------------
â€¢ [4.896 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:22.155
    Apr  6 11:22:22.155: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 11:22:22.157
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:22.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:22.182
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 04/06/23 11:22:22.206
    STEP: watching for Pod to be ready 04/06/23 11:22:22.221
    Apr  6 11:22:22.226: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Apr  6 11:22:22.226: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
    Apr  6 11:22:22.241: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
    Apr  6 11:22:22.774: INFO: observed Pod pod-test in namespace pods-4585 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
    Apr  6 11:22:23.971: INFO: Found Pod pod-test in namespace pods-4585 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:22:22 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 04/06/23 11:22:23.976
    STEP: getting the Pod and ensuring that it's patched 04/06/23 11:22:23.992
    STEP: replacing the Pod's status Ready condition to False 04/06/23 11:22:24.008
    STEP: check the Pod again to ensure its Ready conditions are False 04/06/23 11:22:24.031
    STEP: deleting the Pod via a Collection with a LabelSelector 04/06/23 11:22:24.031
    STEP: watching for the Pod to be deleted 04/06/23 11:22:24.047
    Apr  6 11:22:24.051: INFO: observed event type MODIFIED
    Apr  6 11:22:26.005: INFO: observed event type MODIFIED
    Apr  6 11:22:26.364: INFO: observed event type MODIFIED
    Apr  6 11:22:27.017: INFO: observed event type MODIFIED
    Apr  6 11:22:27.026: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 11:22:27.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4585" for this suite. 04/06/23 11:22:27.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:27.052
Apr  6 11:22:27.052: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 11:22:27.053
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:27.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:27.075
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Apr  6 11:22:27.102: INFO: Waiting up to 5m0s for pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e" in namespace "pods-2968" to be "running and ready"
Apr  6 11:22:27.112: INFO: Pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.763161ms
Apr  6 11:22:27.112: INFO: The phase of Pod server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:22:29.118: INFO: Pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e": Phase="Running", Reason="", readiness=true. Elapsed: 2.016077161s
Apr  6 11:22:29.118: INFO: The phase of Pod server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e is Running (Ready = true)
Apr  6 11:22:29.118: INFO: Pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e" satisfied condition "running and ready"
Apr  6 11:22:29.146: INFO: Waiting up to 5m0s for pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8" in namespace "pods-2968" to be "Succeeded or Failed"
Apr  6 11:22:29.151: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.687691ms
Apr  6 11:22:31.161: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014653498s
Apr  6 11:22:33.161: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014481352s
STEP: Saw pod success 04/06/23 11:22:33.161
Apr  6 11:22:33.161: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8" satisfied condition "Succeeded or Failed"
Apr  6 11:22:33.166: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8 container env3cont: <nil>
STEP: delete the pod 04/06/23 11:22:33.186
Apr  6 11:22:33.198: INFO: Waiting for pod client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8 to disappear
Apr  6 11:22:33.203: INFO: Pod client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 11:22:33.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2968" for this suite. 04/06/23 11:22:33.212
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":228,"skipped":4387,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.171 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:27.052
    Apr  6 11:22:27.052: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 11:22:27.053
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:27.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:27.075
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Apr  6 11:22:27.102: INFO: Waiting up to 5m0s for pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e" in namespace "pods-2968" to be "running and ready"
    Apr  6 11:22:27.112: INFO: Pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.763161ms
    Apr  6 11:22:27.112: INFO: The phase of Pod server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:22:29.118: INFO: Pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e": Phase="Running", Reason="", readiness=true. Elapsed: 2.016077161s
    Apr  6 11:22:29.118: INFO: The phase of Pod server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e is Running (Ready = true)
    Apr  6 11:22:29.118: INFO: Pod "server-envvars-e778a522-40b0-4f8a-92d5-21b294f8189e" satisfied condition "running and ready"
    Apr  6 11:22:29.146: INFO: Waiting up to 5m0s for pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8" in namespace "pods-2968" to be "Succeeded or Failed"
    Apr  6 11:22:29.151: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.687691ms
    Apr  6 11:22:31.161: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014653498s
    Apr  6 11:22:33.161: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014481352s
    STEP: Saw pod success 04/06/23 11:22:33.161
    Apr  6 11:22:33.161: INFO: Pod "client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8" satisfied condition "Succeeded or Failed"
    Apr  6 11:22:33.166: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8 container env3cont: <nil>
    STEP: delete the pod 04/06/23 11:22:33.186
    Apr  6 11:22:33.198: INFO: Waiting for pod client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8 to disappear
    Apr  6 11:22:33.203: INFO: Pod client-envvars-67123ab5-5a9e-4fe2-8f2f-8b6663908ca8 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 11:22:33.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2968" for this suite. 04/06/23 11:22:33.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:33.223
Apr  6 11:22:33.223: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 11:22:33.224
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:33.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:33.27
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Apr  6 11:22:33.292: INFO: Waiting up to 2m0s for pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" in namespace "var-expansion-9982" to be "container 0 failed with reason CreateContainerConfigError"
Apr  6 11:22:33.298: INFO: Pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.31243ms
Apr  6 11:22:35.306: INFO: Pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014627866s
Apr  6 11:22:35.306: INFO: Pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr  6 11:22:35.306: INFO: Deleting pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" in namespace "var-expansion-9982"
Apr  6 11:22:35.320: INFO: Wait up to 5m0s for pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 11:22:39.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9982" for this suite. 04/06/23 11:22:39.342
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":229,"skipped":4400,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.126 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:33.223
    Apr  6 11:22:33.223: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 11:22:33.224
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:33.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:33.27
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Apr  6 11:22:33.292: INFO: Waiting up to 2m0s for pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" in namespace "var-expansion-9982" to be "container 0 failed with reason CreateContainerConfigError"
    Apr  6 11:22:33.298: INFO: Pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.31243ms
    Apr  6 11:22:35.306: INFO: Pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014627866s
    Apr  6 11:22:35.306: INFO: Pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr  6 11:22:35.306: INFO: Deleting pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" in namespace "var-expansion-9982"
    Apr  6 11:22:35.320: INFO: Wait up to 5m0s for pod "var-expansion-2a38212b-5011-4850-a545-bddaa100e0d1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 11:22:39.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9982" for this suite. 04/06/23 11:22:39.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:39.353
Apr  6 11:22:39.353: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename daemonsets 04/06/23 11:22:39.355
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:39.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:39.376
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 04/06/23 11:22:39.451
STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:22:39.464
Apr  6 11:22:39.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:22:39.480: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:22:40.502: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:22:40.502: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:22:41.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:22:41.503: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/06/23 11:22:41.509
Apr  6 11:22:41.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:22:41.543: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 11:22:42.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:22:42.573: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
Apr  6 11:22:43.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:22:43.562: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 04/06/23 11:22:43.562
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:22:43.589
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-818, will wait for the garbage collector to delete the pods 04/06/23 11:22:43.589
Apr  6 11:22:43.662: INFO: Deleting DaemonSet.extensions daemon-set took: 9.987464ms
Apr  6 11:22:43.763: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.010328ms
Apr  6 11:22:46.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:22:46.168: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr  6 11:22:46.173: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30599"},"items":null}

Apr  6 11:22:46.177: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30599"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:22:46.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-818" for this suite. 04/06/23 11:22:46.228
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":230,"skipped":4421,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.882 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:39.353
    Apr  6 11:22:39.353: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename daemonsets 04/06/23 11:22:39.355
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:39.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:39.376
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 04/06/23 11:22:39.451
    STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:22:39.464
    Apr  6 11:22:39.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:22:39.480: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:22:40.502: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:22:40.502: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:22:41.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:22:41.503: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/06/23 11:22:41.509
    Apr  6 11:22:41.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:22:41.543: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 11:22:42.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:22:42.573: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d is running 0 daemon pod, expected 1
    Apr  6 11:22:43.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:22:43.562: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 04/06/23 11:22:43.562
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:22:43.589
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-818, will wait for the garbage collector to delete the pods 04/06/23 11:22:43.589
    Apr  6 11:22:43.662: INFO: Deleting DaemonSet.extensions daemon-set took: 9.987464ms
    Apr  6 11:22:43.763: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.010328ms
    Apr  6 11:22:46.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:22:46.168: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr  6 11:22:46.173: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30599"},"items":null}

    Apr  6 11:22:46.177: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30599"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:22:46.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-818" for this suite. 04/06/23 11:22:46.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:46.236
Apr  6 11:22:46.236: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename disruption 04/06/23 11:22:46.238
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:46.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:46.267
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 04/06/23 11:22:46.274
STEP: Waiting for the pdb to be processed 04/06/23 11:22:46.284
STEP: updating the pdb 04/06/23 11:22:46.288
STEP: Waiting for the pdb to be processed 04/06/23 11:22:46.305
STEP: patching the pdb 04/06/23 11:22:48.32
STEP: Waiting for the pdb to be processed 04/06/23 11:22:48.34
STEP: Waiting for the pdb to be deleted 04/06/23 11:22:48.351
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Apr  6 11:22:48.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9176" for this suite. 04/06/23 11:22:48.366
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":231,"skipped":4436,"failed":0}
------------------------------
â€¢ [2.137 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:46.236
    Apr  6 11:22:46.236: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename disruption 04/06/23 11:22:46.238
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:46.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:46.267
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 04/06/23 11:22:46.274
    STEP: Waiting for the pdb to be processed 04/06/23 11:22:46.284
    STEP: updating the pdb 04/06/23 11:22:46.288
    STEP: Waiting for the pdb to be processed 04/06/23 11:22:46.305
    STEP: patching the pdb 04/06/23 11:22:48.32
    STEP: Waiting for the pdb to be processed 04/06/23 11:22:48.34
    STEP: Waiting for the pdb to be deleted 04/06/23 11:22:48.351
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Apr  6 11:22:48.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9176" for this suite. 04/06/23 11:22:48.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:48.374
Apr  6 11:22:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename job 04/06/23 11:22:48.376
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:48.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:48.403
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 04/06/23 11:22:48.411
STEP: Ensuring job reaches completions 04/06/23 11:22:48.419
STEP: Ensuring pods with index for job exist 04/06/23 11:22:58.427
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Apr  6 11:22:58.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5667" for this suite. 04/06/23 11:22:58.445
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":232,"skipped":4468,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.078 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:48.374
    Apr  6 11:22:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename job 04/06/23 11:22:48.376
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:48.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:48.403
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 04/06/23 11:22:48.411
    STEP: Ensuring job reaches completions 04/06/23 11:22:48.419
    STEP: Ensuring pods with index for job exist 04/06/23 11:22:58.427
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Apr  6 11:22:58.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5667" for this suite. 04/06/23 11:22:58.445
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:22:58.453
Apr  6 11:22:58.453: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 11:22:58.454
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:58.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:58.501
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Apr  6 11:22:58.523: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: creating the pod 04/06/23 11:22:58.524
STEP: submitting the pod to kubernetes 04/06/23 11:22:58.525
Apr  6 11:22:58.542: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3" in namespace "pods-4019" to be "running and ready"
Apr  6 11:22:58.548: INFO: Pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.696898ms
Apr  6 11:22:58.548: INFO: The phase of Pod pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:23:00.560: INFO: Pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018255631s
Apr  6 11:23:00.561: INFO: The phase of Pod pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3 is Running (Ready = true)
Apr  6 11:23:00.561: INFO: Pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 11:23:00.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4019" for this suite. 04/06/23 11:23:00.885
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":233,"skipped":4468,"failed":0}
------------------------------
â€¢ [2.439 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:22:58.453
    Apr  6 11:22:58.453: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 11:22:58.454
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:22:58.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:22:58.501
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Apr  6 11:22:58.523: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: creating the pod 04/06/23 11:22:58.524
    STEP: submitting the pod to kubernetes 04/06/23 11:22:58.525
    Apr  6 11:22:58.542: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3" in namespace "pods-4019" to be "running and ready"
    Apr  6 11:22:58.548: INFO: Pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.696898ms
    Apr  6 11:22:58.548: INFO: The phase of Pod pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:23:00.560: INFO: Pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018255631s
    Apr  6 11:23:00.561: INFO: The phase of Pod pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3 is Running (Ready = true)
    Apr  6 11:23:00.561: INFO: Pod "pod-exec-websocket-4fb19e59-2440-49c3-901f-1bc47406f8a3" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 11:23:00.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4019" for this suite. 04/06/23 11:23:00.885
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:23:00.893
Apr  6 11:23:00.893: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:23:00.894
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:00.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:00.917
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 04/06/23 11:23:00.924
Apr  6 11:23:00.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19" in namespace "projected-7153" to be "Succeeded or Failed"
Apr  6 11:23:00.950: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19": Phase="Pending", Reason="", readiness=false. Elapsed: 12.264493ms
Apr  6 11:23:02.960: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021622687s
Apr  6 11:23:04.958: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019724367s
STEP: Saw pod success 04/06/23 11:23:04.958
Apr  6 11:23:04.958: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19" satisfied condition "Succeeded or Failed"
Apr  6 11:23:04.962: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19 container client-container: <nil>
STEP: delete the pod 04/06/23 11:23:05.029
Apr  6 11:23:05.040: INFO: Waiting for pod downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19 to disappear
Apr  6 11:23:05.044: INFO: Pod downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 11:23:05.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7153" for this suite. 04/06/23 11:23:05.052
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":234,"skipped":4470,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:23:00.893
    Apr  6 11:23:00.893: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:23:00.894
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:00.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:00.917
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 04/06/23 11:23:00.924
    Apr  6 11:23:00.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19" in namespace "projected-7153" to be "Succeeded or Failed"
    Apr  6 11:23:00.950: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19": Phase="Pending", Reason="", readiness=false. Elapsed: 12.264493ms
    Apr  6 11:23:02.960: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021622687s
    Apr  6 11:23:04.958: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019724367s
    STEP: Saw pod success 04/06/23 11:23:04.958
    Apr  6 11:23:04.958: INFO: Pod "downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19" satisfied condition "Succeeded or Failed"
    Apr  6 11:23:04.962: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19 container client-container: <nil>
    STEP: delete the pod 04/06/23 11:23:05.029
    Apr  6 11:23:05.040: INFO: Waiting for pod downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19 to disappear
    Apr  6 11:23:05.044: INFO: Pod downwardapi-volume-eaaa5c48-d27f-4ce0-a76f-46bddc343f19 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 11:23:05.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7153" for this suite. 04/06/23 11:23:05.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:23:05.066
Apr  6 11:23:05.066: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 11:23:05.068
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:05.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:05.094
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9697 04/06/23 11:23:05.1
STEP: changing the ExternalName service to type=NodePort 04/06/23 11:23:05.11
STEP: creating replication controller externalname-service in namespace services-9697 04/06/23 11:23:05.139
I0406 11:23:05.144921      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9697, replica count: 2
I0406 11:23:08.196963      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 11:23:08.197: INFO: Creating new exec pod
Apr  6 11:23:08.213: INFO: Waiting up to 5m0s for pod "execpod4kgv4" in namespace "services-9697" to be "running"
Apr  6 11:23:08.227: INFO: Pod "execpod4kgv4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.697695ms
Apr  6 11:23:10.235: INFO: Pod "execpod4kgv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.019252504s
Apr  6 11:23:10.235: INFO: Pod "execpod4kgv4" satisfied condition "running"
Apr  6 11:23:11.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr  6 11:23:14.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 11:23:14.423: INFO: stdout: ""
Apr  6 11:23:15.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Apr  6 11:23:16.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 11:23:16.034: INFO: stdout: "externalname-service-xgcws"
Apr  6 11:23:16.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.94.80 80'
Apr  6 11:23:16.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.94.80 80\nConnection to 10.68.94.80 80 port [tcp/http] succeeded!\n"
Apr  6 11:23:16.656: INFO: stdout: "externalname-service-bs7xd"
Apr  6 11:23:16.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 32116'
Apr  6 11:23:17.264: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.236 32116\nConnection to 10.250.2.236 32116 port [tcp/*] succeeded!\n"
Apr  6 11:23:17.264: INFO: stdout: "externalname-service-xgcws"
Apr  6 11:23:17.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.68 32116'
Apr  6 11:23:17.879: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.68 32116\nConnection to 10.250.0.68 32116 port [tcp/*] succeeded!\n"
Apr  6 11:23:17.879: INFO: stdout: "externalname-service-xgcws"
Apr  6 11:23:17.879: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 11:23:17.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9697" for this suite. 04/06/23 11:23:17.926
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":235,"skipped":4517,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.872 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:23:05.066
    Apr  6 11:23:05.066: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 11:23:05.068
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:05.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:05.094
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9697 04/06/23 11:23:05.1
    STEP: changing the ExternalName service to type=NodePort 04/06/23 11:23:05.11
    STEP: creating replication controller externalname-service in namespace services-9697 04/06/23 11:23:05.139
    I0406 11:23:05.144921      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9697, replica count: 2
    I0406 11:23:08.196963      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr  6 11:23:08.197: INFO: Creating new exec pod
    Apr  6 11:23:08.213: INFO: Waiting up to 5m0s for pod "execpod4kgv4" in namespace "services-9697" to be "running"
    Apr  6 11:23:08.227: INFO: Pod "execpod4kgv4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.697695ms
    Apr  6 11:23:10.235: INFO: Pod "execpod4kgv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.019252504s
    Apr  6 11:23:10.235: INFO: Pod "execpod4kgv4" satisfied condition "running"
    Apr  6 11:23:11.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Apr  6 11:23:14.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr  6 11:23:14.423: INFO: stdout: ""
    Apr  6 11:23:15.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Apr  6 11:23:16.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr  6 11:23:16.034: INFO: stdout: "externalname-service-xgcws"
    Apr  6 11:23:16.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.94.80 80'
    Apr  6 11:23:16.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.94.80 80\nConnection to 10.68.94.80 80 port [tcp/http] succeeded!\n"
    Apr  6 11:23:16.656: INFO: stdout: "externalname-service-bs7xd"
    Apr  6 11:23:16.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.2.236 32116'
    Apr  6 11:23:17.264: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.2.236 32116\nConnection to 10.250.2.236 32116 port [tcp/*] succeeded!\n"
    Apr  6 11:23:17.264: INFO: stdout: "externalname-service-xgcws"
    Apr  6 11:23:17.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-9697 exec execpod4kgv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.68 32116'
    Apr  6 11:23:17.879: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.68 32116\nConnection to 10.250.0.68 32116 port [tcp/*] succeeded!\n"
    Apr  6 11:23:17.879: INFO: stdout: "externalname-service-xgcws"
    Apr  6 11:23:17.879: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 11:23:17.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9697" for this suite. 04/06/23 11:23:17.926
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:23:17.941
Apr  6 11:23:17.941: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename events 04/06/23 11:23:17.943
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:17.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:17.974
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 04/06/23 11:23:17.988
STEP: listing all events in all namespaces 04/06/23 11:23:17.994
STEP: patching the test event 04/06/23 11:23:18.002
STEP: fetching the test event 04/06/23 11:23:18.018
STEP: updating the test event 04/06/23 11:23:18.035
STEP: getting the test event 04/06/23 11:23:18.047
STEP: deleting the test event 04/06/23 11:23:18.051
STEP: listing all events in all namespaces 04/06/23 11:23:18.06
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Apr  6 11:23:18.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1696" for this suite. 04/06/23 11:23:18.079
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":236,"skipped":4534,"failed":0}
------------------------------
â€¢ [0.147 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:23:17.941
    Apr  6 11:23:17.941: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename events 04/06/23 11:23:17.943
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:17.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:17.974
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 04/06/23 11:23:17.988
    STEP: listing all events in all namespaces 04/06/23 11:23:17.994
    STEP: patching the test event 04/06/23 11:23:18.002
    STEP: fetching the test event 04/06/23 11:23:18.018
    STEP: updating the test event 04/06/23 11:23:18.035
    STEP: getting the test event 04/06/23 11:23:18.047
    STEP: deleting the test event 04/06/23 11:23:18.051
    STEP: listing all events in all namespaces 04/06/23 11:23:18.06
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Apr  6 11:23:18.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1696" for this suite. 04/06/23 11:23:18.079
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:23:18.091
Apr  6 11:23:18.092: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:23:18.093
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:18.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:18.152
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/06/23 11:23:18.159
Apr  6 11:23:18.160: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/06/23 11:23:35.252
Apr  6 11:23:35.253: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:23:38.524: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:23:52.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4876" for this suite. 04/06/23 11:23:52.497
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":237,"skipped":4536,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.414 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:23:18.091
    Apr  6 11:23:18.092: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:23:18.093
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:18.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:18.152
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/06/23 11:23:18.159
    Apr  6 11:23:18.160: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/06/23 11:23:35.252
    Apr  6 11:23:35.253: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:23:38.524: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:23:52.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4876" for this suite. 04/06/23 11:23:52.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:23:52.508
Apr  6 11:23:52.508: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename cronjob 04/06/23 11:23:52.509
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:52.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:52.548
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 04/06/23 11:23:52.556
STEP: Ensuring more than one job is running at a time 04/06/23 11:23:52.582
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/06/23 11:25:00.588
STEP: Removing cronjob 04/06/23 11:25:00.594
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Apr  6 11:25:00.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4026" for this suite. 04/06/23 11:25:00.623
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":238,"skipped":4578,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.124 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:23:52.508
    Apr  6 11:23:52.508: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename cronjob 04/06/23 11:23:52.509
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:23:52.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:23:52.548
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 04/06/23 11:23:52.556
    STEP: Ensuring more than one job is running at a time 04/06/23 11:23:52.582
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/06/23 11:25:00.588
    STEP: Removing cronjob 04/06/23 11:25:00.594
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Apr  6 11:25:00.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4026" for this suite. 04/06/23 11:25:00.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:25:00.632
Apr  6 11:25:00.632: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 11:25:00.635
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:00.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:00.697
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 04/06/23 11:25:00.702
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_tcp@PTR;sleep 1; done
 04/06/23 11:25:00.727
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_tcp@PTR;sleep 1; done
 04/06/23 11:25:00.727
STEP: creating a pod to probe DNS 04/06/23 11:25:00.727
STEP: submitting the pod to kubernetes 04/06/23 11:25:00.728
Apr  6 11:25:00.765: INFO: Waiting up to 15m0s for pod "dns-test-265e93d3-8417-4903-966d-15db61faae62" in namespace "dns-9223" to be "running"
Apr  6 11:25:00.774: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62": Phase="Pending", Reason="", readiness=false. Elapsed: 8.377795ms
Apr  6 11:25:02.784: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018488191s
Apr  6 11:25:04.785: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62": Phase="Running", Reason="", readiness=true. Elapsed: 4.01948748s
Apr  6 11:25:04.785: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62" satisfied condition "running"
STEP: retrieving the pod 04/06/23 11:25:04.785
STEP: looking for the results for each expected name from probers 04/06/23 11:25:04.791
Apr  6 11:25:04.899: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:04.944: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:04.952: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:04.961: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:05.011: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:05.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:05.031: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:05.124: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:05.213: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Apr  6 11:25:10.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.273: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.282: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.294: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.334: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:10.409: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Apr  6 11:25:15.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.269: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.311: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.324: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.390: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.399: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.410: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.429: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:15.477: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Apr  6 11:25:20.223: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.268: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.287: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.297: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.415: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.431: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.442: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:20.484: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Apr  6 11:25:25.240: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.282: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.291: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.303: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.354: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.376: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.393: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.425: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:25.467: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Apr  6 11:25:30.226: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.300: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.317: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.369: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.380: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.396: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
Apr  6 11:25:30.433: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

Apr  6 11:25:35.424: INFO: DNS probes using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 succeeded

STEP: deleting the pod 04/06/23 11:25:35.424
STEP: deleting the test service 04/06/23 11:25:35.439
STEP: deleting the test headless service 04/06/23 11:25:35.468
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 11:25:35.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9223" for this suite. 04/06/23 11:25:35.496
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":239,"skipped":4593,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.874 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:25:00.632
    Apr  6 11:25:00.632: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 11:25:00.635
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:00.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:00.697
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 04/06/23 11:25:00.702
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_tcp@PTR;sleep 1; done
     04/06/23 11:25:00.727
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9223.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9223.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9223.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.207.67.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.67.207.155_tcp@PTR;sleep 1; done
     04/06/23 11:25:00.727
    STEP: creating a pod to probe DNS 04/06/23 11:25:00.727
    STEP: submitting the pod to kubernetes 04/06/23 11:25:00.728
    Apr  6 11:25:00.765: INFO: Waiting up to 15m0s for pod "dns-test-265e93d3-8417-4903-966d-15db61faae62" in namespace "dns-9223" to be "running"
    Apr  6 11:25:00.774: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62": Phase="Pending", Reason="", readiness=false. Elapsed: 8.377795ms
    Apr  6 11:25:02.784: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018488191s
    Apr  6 11:25:04.785: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62": Phase="Running", Reason="", readiness=true. Elapsed: 4.01948748s
    Apr  6 11:25:04.785: INFO: Pod "dns-test-265e93d3-8417-4903-966d-15db61faae62" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 11:25:04.785
    STEP: looking for the results for each expected name from probers 04/06/23 11:25:04.791
    Apr  6 11:25:04.899: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:04.944: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:04.952: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:04.961: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:05.011: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:05.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:05.031: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:05.124: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:05.213: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Apr  6 11:25:10.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.273: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.282: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.294: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.334: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:10.409: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Apr  6 11:25:15.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.269: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.311: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.324: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.390: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.399: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.410: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.429: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:15.477: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Apr  6 11:25:20.223: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.268: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.287: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.297: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.415: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.431: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.442: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:20.484: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Apr  6 11:25:25.240: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.282: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.291: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.303: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.354: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.376: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.393: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.425: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:25.467: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Apr  6 11:25:30.226: INFO: Unable to read wheezy_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.300: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.317: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.369: INFO: Unable to read jessie_udp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.380: INFO: Unable to read jessie_tcp@dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.396: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local from pod dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62: the server could not find the requested resource (get pods dns-test-265e93d3-8417-4903-966d-15db61faae62)
    Apr  6 11:25:30.433: INFO: Lookups using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 failed for: [wheezy_udp@dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@dns-test-service.dns-9223.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_udp@dns-test-service.dns-9223.svc.cluster.local jessie_tcp@dns-test-service.dns-9223.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9223.svc.cluster.local]

    Apr  6 11:25:35.424: INFO: DNS probes using dns-9223/dns-test-265e93d3-8417-4903-966d-15db61faae62 succeeded

    STEP: deleting the pod 04/06/23 11:25:35.424
    STEP: deleting the test service 04/06/23 11:25:35.439
    STEP: deleting the test headless service 04/06/23 11:25:35.468
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 11:25:35.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9223" for this suite. 04/06/23 11:25:35.496
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:25:35.507
Apr  6 11:25:35.507: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename tables 04/06/23 11:25:35.511
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:35.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:35.541
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Apr  6 11:25:35.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2961" for this suite. 04/06/23 11:25:35.572
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":240,"skipped":4596,"failed":0}
------------------------------
â€¢ [0.072 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:25:35.507
    Apr  6 11:25:35.507: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename tables 04/06/23 11:25:35.511
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:35.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:35.541
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Apr  6 11:25:35.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-2961" for this suite. 04/06/23 11:25:35.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:25:35.58
Apr  6 11:25:35.580: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 11:25:35.582
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:35.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:35.619
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 04/06/23 11:25:35.631
Apr  6 11:25:35.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007" in namespace "downward-api-6099" to be "Succeeded or Failed"
Apr  6 11:25:35.665: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007": Phase="Pending", Reason="", readiness=false. Elapsed: 17.232445ms
Apr  6 11:25:37.677: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029273841s
Apr  6 11:25:39.674: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026997342s
STEP: Saw pod success 04/06/23 11:25:39.675
Apr  6 11:25:39.675: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007" satisfied condition "Succeeded or Failed"
Apr  6 11:25:39.681: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007 container client-container: <nil>
STEP: delete the pod 04/06/23 11:25:39.742
Apr  6 11:25:39.754: INFO: Waiting for pod downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007 to disappear
Apr  6 11:25:39.758: INFO: Pod downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 11:25:39.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6099" for this suite. 04/06/23 11:25:39.768
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":241,"skipped":4609,"failed":0}
------------------------------
â€¢ [4.206 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:25:35.58
    Apr  6 11:25:35.580: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 11:25:35.582
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:35.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:35.619
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 04/06/23 11:25:35.631
    Apr  6 11:25:35.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007" in namespace "downward-api-6099" to be "Succeeded or Failed"
    Apr  6 11:25:35.665: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007": Phase="Pending", Reason="", readiness=false. Elapsed: 17.232445ms
    Apr  6 11:25:37.677: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029273841s
    Apr  6 11:25:39.674: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026997342s
    STEP: Saw pod success 04/06/23 11:25:39.675
    Apr  6 11:25:39.675: INFO: Pod "downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007" satisfied condition "Succeeded or Failed"
    Apr  6 11:25:39.681: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007 container client-container: <nil>
    STEP: delete the pod 04/06/23 11:25:39.742
    Apr  6 11:25:39.754: INFO: Waiting for pod downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007 to disappear
    Apr  6 11:25:39.758: INFO: Pod downwardapi-volume-b68208bd-2be0-40cc-8061-0d9be00b2007 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 11:25:39.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6099" for this suite. 04/06/23 11:25:39.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:25:39.789
Apr  6 11:25:39.789: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 11:25:39.79
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:39.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:39.858
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3817 04/06/23 11:25:39.867
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 04/06/23 11:25:39.882
STEP: Creating pod with conflicting port in namespace statefulset-3817 04/06/23 11:25:39.898
STEP: Waiting until pod test-pod will start running in namespace statefulset-3817 04/06/23 11:25:39.918
Apr  6 11:25:39.918: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3817" to be "running"
Apr  6 11:25:39.923: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.483318ms
Apr  6 11:25:41.930: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011942681s
Apr  6 11:25:41.930: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3817 04/06/23 11:25:41.93
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3817 04/06/23 11:25:41.939
Apr  6 11:25:41.956: INFO: Observed stateful pod in namespace: statefulset-3817, name: ss-0, uid: 42f4986d-542e-4bfc-82ec-87608c651256, status phase: Pending. Waiting for statefulset controller to delete.
Apr  6 11:25:41.967: INFO: Observed stateful pod in namespace: statefulset-3817, name: ss-0, uid: 42f4986d-542e-4bfc-82ec-87608c651256, status phase: Failed. Waiting for statefulset controller to delete.
Apr  6 11:25:41.974: INFO: Observed stateful pod in namespace: statefulset-3817, name: ss-0, uid: 42f4986d-542e-4bfc-82ec-87608c651256, status phase: Failed. Waiting for statefulset controller to delete.
Apr  6 11:25:41.976: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3817
STEP: Removing pod with conflicting port in namespace statefulset-3817 04/06/23 11:25:41.977
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3817 and will be in running state 04/06/23 11:25:41.99
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 11:25:44.002: INFO: Deleting all statefulset in ns statefulset-3817
Apr  6 11:25:44.006: INFO: Scaling statefulset ss to 0
Apr  6 11:25:54.035: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:25:54.040: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 11:25:54.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3817" for this suite. 04/06/23 11:25:54.071
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":242,"skipped":4632,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.288 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:25:39.789
    Apr  6 11:25:39.789: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 11:25:39.79
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:39.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:39.858
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3817 04/06/23 11:25:39.867
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 04/06/23 11:25:39.882
    STEP: Creating pod with conflicting port in namespace statefulset-3817 04/06/23 11:25:39.898
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3817 04/06/23 11:25:39.918
    Apr  6 11:25:39.918: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3817" to be "running"
    Apr  6 11:25:39.923: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.483318ms
    Apr  6 11:25:41.930: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011942681s
    Apr  6 11:25:41.930: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3817 04/06/23 11:25:41.93
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3817 04/06/23 11:25:41.939
    Apr  6 11:25:41.956: INFO: Observed stateful pod in namespace: statefulset-3817, name: ss-0, uid: 42f4986d-542e-4bfc-82ec-87608c651256, status phase: Pending. Waiting for statefulset controller to delete.
    Apr  6 11:25:41.967: INFO: Observed stateful pod in namespace: statefulset-3817, name: ss-0, uid: 42f4986d-542e-4bfc-82ec-87608c651256, status phase: Failed. Waiting for statefulset controller to delete.
    Apr  6 11:25:41.974: INFO: Observed stateful pod in namespace: statefulset-3817, name: ss-0, uid: 42f4986d-542e-4bfc-82ec-87608c651256, status phase: Failed. Waiting for statefulset controller to delete.
    Apr  6 11:25:41.976: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3817
    STEP: Removing pod with conflicting port in namespace statefulset-3817 04/06/23 11:25:41.977
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3817 and will be in running state 04/06/23 11:25:41.99
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 11:25:44.002: INFO: Deleting all statefulset in ns statefulset-3817
    Apr  6 11:25:44.006: INFO: Scaling statefulset ss to 0
    Apr  6 11:25:54.035: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:25:54.040: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 11:25:54.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3817" for this suite. 04/06/23 11:25:54.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:25:54.08
Apr  6 11:25:54.081: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename gc 04/06/23 11:25:54.082
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:54.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:54.105
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 04/06/23 11:25:54.111
STEP: Wait for the Deployment to create new ReplicaSet 04/06/23 11:25:54.117
STEP: delete the deployment 04/06/23 11:25:54.125
STEP: wait for all rs to be garbage collected 04/06/23 11:25:54.133
STEP: expected 0 pods, got 2 pods 04/06/23 11:25:54.144
STEP: Gathering metrics 04/06/23 11:25:54.658
W0406 11:25:54.679176      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 11:25:54.679: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Apr  6 11:25:54.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6603" for this suite. 04/06/23 11:25:54.688
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":243,"skipped":4676,"failed":0}
------------------------------
â€¢ [0.616 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:25:54.08
    Apr  6 11:25:54.081: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename gc 04/06/23 11:25:54.082
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:54.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:54.105
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 04/06/23 11:25:54.111
    STEP: Wait for the Deployment to create new ReplicaSet 04/06/23 11:25:54.117
    STEP: delete the deployment 04/06/23 11:25:54.125
    STEP: wait for all rs to be garbage collected 04/06/23 11:25:54.133
    STEP: expected 0 pods, got 2 pods 04/06/23 11:25:54.144
    STEP: Gathering metrics 04/06/23 11:25:54.658
    W0406 11:25:54.679176      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr  6 11:25:54.679: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Apr  6 11:25:54.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6603" for this suite. 04/06/23 11:25:54.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:25:54.698
Apr  6 11:25:54.698: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:25:54.699
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:54.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:54.724
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-e6da351c-4f4d-47b5-87eb-26383e77b9ee 04/06/23 11:25:54.73
STEP: Creating a pod to test consume secrets 04/06/23 11:25:54.735
Apr  6 11:25:54.746: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c" in namespace "projected-4955" to be "Succeeded or Failed"
Apr  6 11:25:54.766: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.481319ms
Apr  6 11:25:56.771: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025263046s
Apr  6 11:25:58.772: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026042913s
STEP: Saw pod success 04/06/23 11:25:58.772
Apr  6 11:25:58.772: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c" satisfied condition "Succeeded or Failed"
Apr  6 11:25:58.777: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c container projected-secret-volume-test: <nil>
STEP: delete the pod 04/06/23 11:25:58.803
Apr  6 11:25:58.818: INFO: Waiting for pod pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c to disappear
Apr  6 11:25:58.826: INFO: Pod pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Apr  6 11:25:58.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4955" for this suite. 04/06/23 11:25:58.838
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":244,"skipped":4693,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:25:54.698
    Apr  6 11:25:54.698: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:25:54.699
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:54.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:54.724
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-e6da351c-4f4d-47b5-87eb-26383e77b9ee 04/06/23 11:25:54.73
    STEP: Creating a pod to test consume secrets 04/06/23 11:25:54.735
    Apr  6 11:25:54.746: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c" in namespace "projected-4955" to be "Succeeded or Failed"
    Apr  6 11:25:54.766: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.481319ms
    Apr  6 11:25:56.771: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025263046s
    Apr  6 11:25:58.772: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026042913s
    STEP: Saw pod success 04/06/23 11:25:58.772
    Apr  6 11:25:58.772: INFO: Pod "pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c" satisfied condition "Succeeded or Failed"
    Apr  6 11:25:58.777: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:25:58.803
    Apr  6 11:25:58.818: INFO: Waiting for pod pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c to disappear
    Apr  6 11:25:58.826: INFO: Pod pod-projected-secrets-5d7a9843-c433-4569-a43b-a9df4382366c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Apr  6 11:25:58.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4955" for this suite. 04/06/23 11:25:58.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:25:58.853
Apr  6 11:25:58.853: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 11:25:58.854
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:58.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:58.894
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 11:25:58.922
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:25:59.641
STEP: Deploying the webhook pod 04/06/23 11:25:59.652
STEP: Wait for the deployment to be ready 04/06/23 11:25:59.666
Apr  6 11:25:59.678: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 11:26:01.708
STEP: Verifying the service has paired with the endpoint 04/06/23 11:26:01.732
Apr  6 11:26:02.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 04/06/23 11:26:02.739
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/06/23 11:26:02.745
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/06/23 11:26:02.745
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/06/23 11:26:02.745
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/06/23 11:26:02.748
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/06/23 11:26:02.748
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/06/23 11:26:02.751
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:26:02.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5334" for this suite. 04/06/23 11:26:02.761
STEP: Destroying namespace "webhook-5334-markers" for this suite. 04/06/23 11:26:02.768
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":245,"skipped":4721,"failed":0}
------------------------------
â€¢ [3.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:25:58.853
    Apr  6 11:25:58.853: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 11:25:58.854
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:25:58.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:25:58.894
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 11:25:58.922
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:25:59.641
    STEP: Deploying the webhook pod 04/06/23 11:25:59.652
    STEP: Wait for the deployment to be ready 04/06/23 11:25:59.666
    Apr  6 11:25:59.678: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 11:26:01.708
    STEP: Verifying the service has paired with the endpoint 04/06/23 11:26:01.732
    Apr  6 11:26:02.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 04/06/23 11:26:02.739
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/06/23 11:26:02.745
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/06/23 11:26:02.745
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/06/23 11:26:02.745
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/06/23 11:26:02.748
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/06/23 11:26:02.748
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/06/23 11:26:02.751
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:26:02.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5334" for this suite. 04/06/23 11:26:02.761
    STEP: Destroying namespace "webhook-5334-markers" for this suite. 04/06/23 11:26:02.768
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:26:02.848
Apr  6 11:26:02.848: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename taint-single-pod 04/06/23 11:26:02.849
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:26:02.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:26:02.905
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Apr  6 11:26:02.913: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 11:27:03.010: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Apr  6 11:27:03.027: INFO: Starting informer...
STEP: Starting pod... 04/06/23 11:27:03.027
Apr  6 11:27:03.268: INFO: Pod is running on shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d. Tainting Node
STEP: Trying to apply a taint on the Node 04/06/23 11:27:03.268
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:27:03.32
STEP: Waiting short time to make sure Pod is queued for deletion 04/06/23 11:27:03.343
Apr  6 11:27:03.343: INFO: Pod wasn't evicted. Proceeding
Apr  6 11:27:03.343: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:27:03.373
STEP: Waiting some time to make sure that toleration time passed. 04/06/23 11:27:03.39
Apr  6 11:28:18.391: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:28:18.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8752" for this suite. 04/06/23 11:28:18.422
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":246,"skipped":4722,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.582 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:26:02.848
    Apr  6 11:26:02.848: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename taint-single-pod 04/06/23 11:26:02.849
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:26:02.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:26:02.905
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Apr  6 11:26:02.913: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr  6 11:27:03.010: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Apr  6 11:27:03.027: INFO: Starting informer...
    STEP: Starting pod... 04/06/23 11:27:03.027
    Apr  6 11:27:03.268: INFO: Pod is running on shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d. Tainting Node
    STEP: Trying to apply a taint on the Node 04/06/23 11:27:03.268
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:27:03.32
    STEP: Waiting short time to make sure Pod is queued for deletion 04/06/23 11:27:03.343
    Apr  6 11:27:03.343: INFO: Pod wasn't evicted. Proceeding
    Apr  6 11:27:03.343: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:27:03.373
    STEP: Waiting some time to make sure that toleration time passed. 04/06/23 11:27:03.39
    Apr  6 11:28:18.391: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:28:18.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-8752" for this suite. 04/06/23 11:28:18.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:28:18.433
Apr  6 11:28:18.433: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 11:28:18.436
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:18.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:18.465
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5968 04/06/23 11:28:18.472
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-5968 04/06/23 11:28:18.484
Apr  6 11:28:18.499: INFO: Found 0 stateful pods, waiting for 1
Apr  6 11:28:28.509: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 04/06/23 11:28:28.533
STEP: Getting /status 04/06/23 11:28:28.555
Apr  6 11:28:28.562: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 04/06/23 11:28:28.563
Apr  6 11:28:28.584: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 04/06/23 11:28:28.584
Apr  6 11:28:28.592: INFO: Observed &StatefulSet event: ADDED
Apr  6 11:28:28.592: INFO: Found Statefulset ss in namespace statefulset-5968 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr  6 11:28:28.592: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 04/06/23 11:28:28.592
Apr  6 11:28:28.592: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr  6 11:28:28.604: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 04/06/23 11:28:28.604
Apr  6 11:28:28.612: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 11:28:28.612: INFO: Deleting all statefulset in ns statefulset-5968
Apr  6 11:28:28.621: INFO: Scaling statefulset ss to 0
Apr  6 11:28:38.659: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:28:38.665: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 11:28:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5968" for this suite. 04/06/23 11:28:38.7
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":247,"skipped":4735,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.273 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:28:18.433
    Apr  6 11:28:18.433: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 11:28:18.436
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:18.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:18.465
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5968 04/06/23 11:28:18.472
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-5968 04/06/23 11:28:18.484
    Apr  6 11:28:18.499: INFO: Found 0 stateful pods, waiting for 1
    Apr  6 11:28:28.509: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 04/06/23 11:28:28.533
    STEP: Getting /status 04/06/23 11:28:28.555
    Apr  6 11:28:28.562: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 04/06/23 11:28:28.563
    Apr  6 11:28:28.584: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 04/06/23 11:28:28.584
    Apr  6 11:28:28.592: INFO: Observed &StatefulSet event: ADDED
    Apr  6 11:28:28.592: INFO: Found Statefulset ss in namespace statefulset-5968 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr  6 11:28:28.592: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 04/06/23 11:28:28.592
    Apr  6 11:28:28.592: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr  6 11:28:28.604: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 04/06/23 11:28:28.604
    Apr  6 11:28:28.612: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 11:28:28.612: INFO: Deleting all statefulset in ns statefulset-5968
    Apr  6 11:28:28.621: INFO: Scaling statefulset ss to 0
    Apr  6 11:28:38.659: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:28:38.665: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 11:28:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5968" for this suite. 04/06/23 11:28:38.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:28:38.707
Apr  6 11:28:38.708: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 11:28:38.709
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:38.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:38.742
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-8608 04/06/23 11:28:38.75
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[] 04/06/23 11:28:38.765
Apr  6 11:28:38.777: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8608 04/06/23 11:28:38.777
Apr  6 11:28:38.797: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8608" to be "running and ready"
Apr  6 11:28:38.803: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.428824ms
Apr  6 11:28:38.803: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:28:40.812: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014520983s
Apr  6 11:28:40.812: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr  6 11:28:40.812: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[pod1:[100]] 04/06/23 11:28:40.817
Apr  6 11:28:40.835: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8608 04/06/23 11:28:40.835
Apr  6 11:28:40.847: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8608" to be "running and ready"
Apr  6 11:28:40.853: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.887576ms
Apr  6 11:28:40.853: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:28:42.865: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018115769s
Apr  6 11:28:42.865: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr  6 11:28:42.865: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[pod1:[100] pod2:[101]] 04/06/23 11:28:42.87
Apr  6 11:28:42.894: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 04/06/23 11:28:42.894
Apr  6 11:28:42.894: INFO: Creating new exec pod
Apr  6 11:28:42.906: INFO: Waiting up to 5m0s for pod "execpodtpf57" in namespace "services-8608" to be "running"
Apr  6 11:28:42.913: INFO: Pod "execpodtpf57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.514244ms
Apr  6 11:28:44.919: INFO: Pod "execpodtpf57": Phase="Running", Reason="", readiness=true. Elapsed: 2.012833381s
Apr  6 11:28:44.919: INFO: Pod "execpodtpf57" satisfied condition "running"
Apr  6 11:28:45.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Apr  6 11:28:46.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Apr  6 11:28:46.567: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 11:28:46.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.66.97.20 80'
Apr  6 11:28:47.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.66.97.20 80\nConnection to 10.66.97.20 80 port [tcp/http] succeeded!\n"
Apr  6 11:28:47.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 11:28:47.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Apr  6 11:28:47.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Apr  6 11:28:47.785: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Apr  6 11:28:47.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.66.97.20 81'
Apr  6 11:28:48.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.66.97.20 81\nConnection to 10.66.97.20 81 port [tcp/*] succeeded!\n"
Apr  6 11:28:48.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8608 04/06/23 11:28:48.443
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[pod2:[101]] 04/06/23 11:28:48.46
Apr  6 11:28:48.477: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8608 04/06/23 11:28:48.477
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[] 04/06/23 11:28:48.493
Apr  6 11:28:48.504: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 11:28:48.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8608" for this suite. 04/06/23 11:28:48.529
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":248,"skipped":4751,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.832 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:28:38.707
    Apr  6 11:28:38.708: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 11:28:38.709
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:38.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:38.742
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-8608 04/06/23 11:28:38.75
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[] 04/06/23 11:28:38.765
    Apr  6 11:28:38.777: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8608 04/06/23 11:28:38.777
    Apr  6 11:28:38.797: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8608" to be "running and ready"
    Apr  6 11:28:38.803: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.428824ms
    Apr  6 11:28:38.803: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:28:40.812: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014520983s
    Apr  6 11:28:40.812: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr  6 11:28:40.812: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[pod1:[100]] 04/06/23 11:28:40.817
    Apr  6 11:28:40.835: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8608 04/06/23 11:28:40.835
    Apr  6 11:28:40.847: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8608" to be "running and ready"
    Apr  6 11:28:40.853: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.887576ms
    Apr  6 11:28:40.853: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:28:42.865: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018115769s
    Apr  6 11:28:42.865: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr  6 11:28:42.865: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[pod1:[100] pod2:[101]] 04/06/23 11:28:42.87
    Apr  6 11:28:42.894: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 04/06/23 11:28:42.894
    Apr  6 11:28:42.894: INFO: Creating new exec pod
    Apr  6 11:28:42.906: INFO: Waiting up to 5m0s for pod "execpodtpf57" in namespace "services-8608" to be "running"
    Apr  6 11:28:42.913: INFO: Pod "execpodtpf57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.514244ms
    Apr  6 11:28:44.919: INFO: Pod "execpodtpf57": Phase="Running", Reason="", readiness=true. Elapsed: 2.012833381s
    Apr  6 11:28:44.919: INFO: Pod "execpodtpf57" satisfied condition "running"
    Apr  6 11:28:45.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Apr  6 11:28:46.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Apr  6 11:28:46.567: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 11:28:46.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.66.97.20 80'
    Apr  6 11:28:47.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.66.97.20 80\nConnection to 10.66.97.20 80 port [tcp/http] succeeded!\n"
    Apr  6 11:28:47.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 11:28:47.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Apr  6 11:28:47.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Apr  6 11:28:47.785: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Apr  6 11:28:47.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=services-8608 exec execpodtpf57 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.66.97.20 81'
    Apr  6 11:28:48.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.66.97.20 81\nConnection to 10.66.97.20 81 port [tcp/*] succeeded!\n"
    Apr  6 11:28:48.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-8608 04/06/23 11:28:48.443
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[pod2:[101]] 04/06/23 11:28:48.46
    Apr  6 11:28:48.477: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8608 04/06/23 11:28:48.477
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8608 to expose endpoints map[] 04/06/23 11:28:48.493
    Apr  6 11:28:48.504: INFO: successfully validated that service multi-endpoint-test in namespace services-8608 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 11:28:48.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8608" for this suite. 04/06/23 11:28:48.529
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:28:48.544
Apr  6 11:28:48.544: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:28:48.547
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:48.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:48.58
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-84c3e9dc-7076-4740-87ea-6ae47dc246c6 04/06/23 11:28:48.602
STEP: Creating a pod to test consume configMaps 04/06/23 11:28:48.62
Apr  6 11:28:48.637: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83" in namespace "projected-6822" to be "Succeeded or Failed"
Apr  6 11:28:48.643: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154384ms
Apr  6 11:28:50.650: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013707587s
Apr  6 11:28:52.650: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013610857s
STEP: Saw pod success 04/06/23 11:28:52.661
Apr  6 11:28:52.661: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83" satisfied condition "Succeeded or Failed"
Apr  6 11:28:52.667: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:28:52.686
Apr  6 11:28:52.699: INFO: Waiting for pod pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83 to disappear
Apr  6 11:28:52.703: INFO: Pod pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 11:28:52.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6822" for this suite. 04/06/23 11:28:52.729
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":249,"skipped":4758,"failed":0}
------------------------------
â€¢ [4.192 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:28:48.544
    Apr  6 11:28:48.544: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:28:48.547
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:48.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:48.58
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-84c3e9dc-7076-4740-87ea-6ae47dc246c6 04/06/23 11:28:48.602
    STEP: Creating a pod to test consume configMaps 04/06/23 11:28:48.62
    Apr  6 11:28:48.637: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83" in namespace "projected-6822" to be "Succeeded or Failed"
    Apr  6 11:28:48.643: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154384ms
    Apr  6 11:28:50.650: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013707587s
    Apr  6 11:28:52.650: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013610857s
    STEP: Saw pod success 04/06/23 11:28:52.661
    Apr  6 11:28:52.661: INFO: Pod "pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83" satisfied condition "Succeeded or Failed"
    Apr  6 11:28:52.667: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:28:52.686
    Apr  6 11:28:52.699: INFO: Waiting for pod pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83 to disappear
    Apr  6 11:28:52.703: INFO: Pod pod-projected-configmaps-21d1aa32-5a0e-4f3e-97c4-cbae509dea83 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 11:28:52.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6822" for this suite. 04/06/23 11:28:52.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:28:52.738
Apr  6 11:28:52.738: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename job 04/06/23 11:28:52.739
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:52.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:52.78
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 04/06/23 11:28:52.792
STEP: Ensure pods equal to paralellism count is attached to the job 04/06/23 11:28:52.8
STEP: patching /status 04/06/23 11:28:54.811
STEP: updating /status 04/06/23 11:28:54.818
STEP: get /status 04/06/23 11:28:54.851
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Apr  6 11:28:54.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3421" for this suite. 04/06/23 11:28:54.864
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":250,"skipped":4774,"failed":0}
------------------------------
â€¢ [2.131 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:28:52.738
    Apr  6 11:28:52.738: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename job 04/06/23 11:28:52.739
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:52.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:52.78
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 04/06/23 11:28:52.792
    STEP: Ensure pods equal to paralellism count is attached to the job 04/06/23 11:28:52.8
    STEP: patching /status 04/06/23 11:28:54.811
    STEP: updating /status 04/06/23 11:28:54.818
    STEP: get /status 04/06/23 11:28:54.851
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Apr  6 11:28:54.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3421" for this suite. 04/06/23 11:28:54.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:28:54.871
Apr  6 11:28:54.871: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 11:28:54.872
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:54.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:54.896
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-b70a8d43-cda8-43ce-a22d-857ae6cbb9f5 04/06/23 11:28:54.902
STEP: Creating a pod to test consume secrets 04/06/23 11:28:54.907
Apr  6 11:28:54.918: INFO: Waiting up to 5m0s for pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4" in namespace "secrets-889" to be "Succeeded or Failed"
Apr  6 11:28:54.921: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512106ms
Apr  6 11:28:56.933: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015381843s
Apr  6 11:28:58.932: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014428779s
STEP: Saw pod success 04/06/23 11:28:58.932
Apr  6 11:28:58.932: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4" satisfied condition "Succeeded or Failed"
Apr  6 11:28:58.938: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4 container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 11:28:58.951
Apr  6 11:28:58.967: INFO: Waiting for pod pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4 to disappear
Apr  6 11:28:58.972: INFO: Pod pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 11:28:58.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-889" for this suite. 04/06/23 11:28:58.997
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":251,"skipped":4795,"failed":0}
------------------------------
â€¢ [4.137 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:28:54.871
    Apr  6 11:28:54.871: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 11:28:54.872
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:54.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:54.896
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-b70a8d43-cda8-43ce-a22d-857ae6cbb9f5 04/06/23 11:28:54.902
    STEP: Creating a pod to test consume secrets 04/06/23 11:28:54.907
    Apr  6 11:28:54.918: INFO: Waiting up to 5m0s for pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4" in namespace "secrets-889" to be "Succeeded or Failed"
    Apr  6 11:28:54.921: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512106ms
    Apr  6 11:28:56.933: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015381843s
    Apr  6 11:28:58.932: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014428779s
    STEP: Saw pod success 04/06/23 11:28:58.932
    Apr  6 11:28:58.932: INFO: Pod "pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4" satisfied condition "Succeeded or Failed"
    Apr  6 11:28:58.938: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4 container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:28:58.951
    Apr  6 11:28:58.967: INFO: Waiting for pod pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4 to disappear
    Apr  6 11:28:58.972: INFO: Pod pod-secrets-78c1fa9e-fbb1-48f8-bca9-fababe6f92a4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 11:28:58.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-889" for this suite. 04/06/23 11:28:58.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:28:59.009
Apr  6 11:28:59.009: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 11:28:59.01
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:59.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:59.034
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 04/06/23 11:28:59.04
Apr  6 11:28:59.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375" in namespace "downward-api-5536" to be "Succeeded or Failed"
Apr  6 11:28:59.055: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958684ms
Apr  6 11:29:01.064: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012599737s
Apr  6 11:29:03.062: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010713819s
STEP: Saw pod success 04/06/23 11:29:03.062
Apr  6 11:29:03.062: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375" satisfied condition "Succeeded or Failed"
Apr  6 11:29:03.067: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375 container client-container: <nil>
STEP: delete the pod 04/06/23 11:29:03.123
Apr  6 11:29:03.136: INFO: Waiting for pod downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375 to disappear
Apr  6 11:29:03.141: INFO: Pod downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 11:29:03.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5536" for this suite. 04/06/23 11:29:03.168
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":252,"skipped":4801,"failed":0}
------------------------------
â€¢ [4.169 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:28:59.009
    Apr  6 11:28:59.009: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 11:28:59.01
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:28:59.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:28:59.034
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 04/06/23 11:28:59.04
    Apr  6 11:28:59.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375" in namespace "downward-api-5536" to be "Succeeded or Failed"
    Apr  6 11:28:59.055: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958684ms
    Apr  6 11:29:01.064: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012599737s
    Apr  6 11:29:03.062: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010713819s
    STEP: Saw pod success 04/06/23 11:29:03.062
    Apr  6 11:29:03.062: INFO: Pod "downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375" satisfied condition "Succeeded or Failed"
    Apr  6 11:29:03.067: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375 container client-container: <nil>
    STEP: delete the pod 04/06/23 11:29:03.123
    Apr  6 11:29:03.136: INFO: Waiting for pod downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375 to disappear
    Apr  6 11:29:03.141: INFO: Pod downwardapi-volume-e61dfb54-6f96-4520-a10a-0e418fc81375 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 11:29:03.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5536" for this suite. 04/06/23 11:29:03.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:29:03.18
Apr  6 11:29:03.180: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubelet-test 04/06/23 11:29:03.182
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:29:03.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:29:03.227
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 04/06/23 11:29:03.247
Apr  6 11:29:03.248: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f" in namespace "kubelet-test-8469" to be "completed"
Apr  6 11:29:03.255: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.682818ms
Apr  6 11:29:05.263: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015276389s
Apr  6 11:29:07.270: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022382825s
Apr  6 11:29:07.270: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Apr  6 11:29:07.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8469" for this suite. 04/06/23 11:29:07.322
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":253,"skipped":4809,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:29:03.18
    Apr  6 11:29:03.180: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubelet-test 04/06/23 11:29:03.182
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:29:03.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:29:03.227
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 04/06/23 11:29:03.247
    Apr  6 11:29:03.248: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f" in namespace "kubelet-test-8469" to be "completed"
    Apr  6 11:29:03.255: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.682818ms
    Apr  6 11:29:05.263: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015276389s
    Apr  6 11:29:07.270: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022382825s
    Apr  6 11:29:07.270: INFO: Pod "agnhost-host-aliasesc42be1e8-edad-4bff-b317-65d6fc9a1a3f" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Apr  6 11:29:07.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8469" for this suite. 04/06/23 11:29:07.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:29:07.34
Apr  6 11:29:07.340: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:29:07.341
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:29:07.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:29:07.374
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 04/06/23 11:29:07.385
Apr  6 11:29:07.408: INFO: Waiting up to 5m0s for pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80" in namespace "emptydir-1363" to be "Succeeded or Failed"
Apr  6 11:29:07.414: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882311ms
Apr  6 11:29:09.420: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012745693s
Apr  6 11:29:11.421: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013703715s
STEP: Saw pod success 04/06/23 11:29:11.421
Apr  6 11:29:11.421: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80" satisfied condition "Succeeded or Failed"
Apr  6 11:29:11.432: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-a59343ec-5810-4bf4-881b-caa58c3c8b80 container test-container: <nil>
STEP: delete the pod 04/06/23 11:29:11.472
Apr  6 11:29:11.492: INFO: Waiting for pod pod-a59343ec-5810-4bf4-881b-caa58c3c8b80 to disappear
Apr  6 11:29:11.506: INFO: Pod pod-a59343ec-5810-4bf4-881b-caa58c3c8b80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:29:11.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1363" for this suite. 04/06/23 11:29:11.525
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":254,"skipped":4818,"failed":0}
------------------------------
â€¢ [4.193 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:29:07.34
    Apr  6 11:29:07.340: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:29:07.341
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:29:07.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:29:07.374
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/06/23 11:29:07.385
    Apr  6 11:29:07.408: INFO: Waiting up to 5m0s for pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80" in namespace "emptydir-1363" to be "Succeeded or Failed"
    Apr  6 11:29:07.414: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882311ms
    Apr  6 11:29:09.420: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012745693s
    Apr  6 11:29:11.421: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013703715s
    STEP: Saw pod success 04/06/23 11:29:11.421
    Apr  6 11:29:11.421: INFO: Pod "pod-a59343ec-5810-4bf4-881b-caa58c3c8b80" satisfied condition "Succeeded or Failed"
    Apr  6 11:29:11.432: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-a59343ec-5810-4bf4-881b-caa58c3c8b80 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:29:11.472
    Apr  6 11:29:11.492: INFO: Waiting for pod pod-a59343ec-5810-4bf4-881b-caa58c3c8b80 to disappear
    Apr  6 11:29:11.506: INFO: Pod pod-a59343ec-5810-4bf4-881b-caa58c3c8b80 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:29:11.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1363" for this suite. 04/06/23 11:29:11.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:29:11.534
Apr  6 11:29:11.534: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 11:29:11.535
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:29:11.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:29:11.56
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4731 04/06/23 11:29:11.566
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-4731 04/06/23 11:29:11.572
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4731 04/06/23 11:29:11.579
Apr  6 11:29:11.593: INFO: Found 0 stateful pods, waiting for 1
Apr  6 11:29:21.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/06/23 11:29:21.603
Apr  6 11:29:21.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:29:22.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:29:22.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:29:22.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:29:22.203: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  6 11:29:32.216: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:29:32.216: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:29:32.246: INFO: POD   NODE                                                           PHASE    GRACE  CONDITIONS
Apr  6 11:29:32.246: INFO: ss-0  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  }]
Apr  6 11:29:32.246: INFO: 
Apr  6 11:29:32.246: INFO: StatefulSet ss has not reached scale 3, at 1
Apr  6 11:29:33.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992971283s
Apr  6 11:29:34.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973584733s
Apr  6 11:29:35.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966559091s
Apr  6 11:29:36.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.957418922s
Apr  6 11:29:37.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.948170109s
Apr  6 11:29:38.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.940716813s
Apr  6 11:29:39.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.928407306s
Apr  6 11:29:40.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.921648588s
Apr  6 11:29:41.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 915.006571ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4731 04/06/23 11:29:42.332
Apr  6 11:29:42.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:29:43.004: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 11:29:43.004: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:29:43.004: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 11:29:43.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:29:43.716: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  6 11:29:43.716: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:29:43.716: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 11:29:43.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:29:44.518: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  6 11:29:44.518: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:29:44.518: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 11:29:44.525: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:29:44.525: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:29:44.525: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 04/06/23 11:29:44.525
Apr  6 11:29:44.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:29:45.146: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:29:45.146: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:29:45.146: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:29:45.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:29:45.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:29:45.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:29:45.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:29:45.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:29:46.337: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:29:46.337: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:29:46.337: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:29:46.337: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:29:46.345: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr  6 11:29:56.367: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:29:56.367: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:29:56.367: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:29:56.384: INFO: POD   NODE                                                           PHASE    GRACE  CONDITIONS
Apr  6 11:29:56.384: INFO: ss-0  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  }]
Apr  6 11:29:56.384: INFO: ss-1  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
Apr  6 11:29:56.384: INFO: ss-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
Apr  6 11:29:56.384: INFO: 
Apr  6 11:29:56.384: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  6 11:29:57.397: INFO: POD   NODE                                                           PHASE    GRACE  CONDITIONS
Apr  6 11:29:57.397: INFO: ss-0  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  }]
Apr  6 11:29:57.397: INFO: ss-1  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
Apr  6 11:29:57.397: INFO: ss-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
Apr  6 11:29:57.397: INFO: 
Apr  6 11:29:57.397: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  6 11:29:58.405: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.981691973s
Apr  6 11:29:59.410: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.974403086s
Apr  6 11:30:00.418: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.968845447s
Apr  6 11:30:01.433: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.959359827s
Apr  6 11:30:02.440: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.945713394s
Apr  6 11:30:03.445: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.939058693s
Apr  6 11:30:04.452: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.933275026s
Apr  6 11:30:05.464: INFO: Verifying statefulset ss doesn't scale past 0 for another 925.833685ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4731 04/06/23 11:30:06.464
Apr  6 11:30:06.471: INFO: Scaling statefulset ss to 0
Apr  6 11:30:06.489: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 11:30:06.494: INFO: Deleting all statefulset in ns statefulset-4731
Apr  6 11:30:06.499: INFO: Scaling statefulset ss to 0
Apr  6 11:30:06.528: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:30:06.540: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 11:30:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4731" for this suite. 04/06/23 11:30:06.577
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":255,"skipped":4823,"failed":0}
------------------------------
â€¢ [SLOW TEST] [55.058 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:29:11.534
    Apr  6 11:29:11.534: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 11:29:11.535
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:29:11.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:29:11.56
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4731 04/06/23 11:29:11.566
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-4731 04/06/23 11:29:11.572
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4731 04/06/23 11:29:11.579
    Apr  6 11:29:11.593: INFO: Found 0 stateful pods, waiting for 1
    Apr  6 11:29:21.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/06/23 11:29:21.603
    Apr  6 11:29:21.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:29:22.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:29:22.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:29:22.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:29:22.203: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr  6 11:29:32.216: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:29:32.216: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:29:32.246: INFO: POD   NODE                                                           PHASE    GRACE  CONDITIONS
    Apr  6 11:29:32.246: INFO: ss-0  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  }]
    Apr  6 11:29:32.246: INFO: 
    Apr  6 11:29:32.246: INFO: StatefulSet ss has not reached scale 3, at 1
    Apr  6 11:29:33.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992971283s
    Apr  6 11:29:34.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973584733s
    Apr  6 11:29:35.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966559091s
    Apr  6 11:29:36.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.957418922s
    Apr  6 11:29:37.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.948170109s
    Apr  6 11:29:38.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.940716813s
    Apr  6 11:29:39.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.928407306s
    Apr  6 11:29:40.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.921648588s
    Apr  6 11:29:41.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 915.006571ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4731 04/06/23 11:29:42.332
    Apr  6 11:29:42.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:29:43.004: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr  6 11:29:43.004: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:29:43.004: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr  6 11:29:43.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:29:43.716: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr  6 11:29:43.716: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:29:43.716: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr  6 11:29:43.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:29:44.518: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr  6 11:29:44.518: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:29:44.518: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr  6 11:29:44.525: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:29:44.525: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:29:44.525: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 04/06/23 11:29:44.525
    Apr  6 11:29:44.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:29:45.146: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:29:45.146: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:29:45.146: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:29:45.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:29:45.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:29:45.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:29:45.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:29:45.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-4731 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:29:46.337: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:29:46.337: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:29:46.337: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:29:46.337: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:29:46.345: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Apr  6 11:29:56.367: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:29:56.367: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:29:56.367: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:29:56.384: INFO: POD   NODE                                                           PHASE    GRACE  CONDITIONS
    Apr  6 11:29:56.384: INFO: ss-0  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  }]
    Apr  6 11:29:56.384: INFO: ss-1  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
    Apr  6 11:29:56.384: INFO: ss-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
    Apr  6 11:29:56.384: INFO: 
    Apr  6 11:29:56.384: INFO: StatefulSet ss has not reached scale 0, at 3
    Apr  6 11:29:57.397: INFO: POD   NODE                                                           PHASE    GRACE  CONDITIONS
    Apr  6 11:29:57.397: INFO: ss-0  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:11 +0000 UTC  }]
    Apr  6 11:29:57.397: INFO: ss-1  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
    Apr  6 11:29:57.397: INFO: ss-2  shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-06 11:29:32 +0000 UTC  }]
    Apr  6 11:29:57.397: INFO: 
    Apr  6 11:29:57.397: INFO: StatefulSet ss has not reached scale 0, at 3
    Apr  6 11:29:58.405: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.981691973s
    Apr  6 11:29:59.410: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.974403086s
    Apr  6 11:30:00.418: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.968845447s
    Apr  6 11:30:01.433: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.959359827s
    Apr  6 11:30:02.440: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.945713394s
    Apr  6 11:30:03.445: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.939058693s
    Apr  6 11:30:04.452: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.933275026s
    Apr  6 11:30:05.464: INFO: Verifying statefulset ss doesn't scale past 0 for another 925.833685ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4731 04/06/23 11:30:06.464
    Apr  6 11:30:06.471: INFO: Scaling statefulset ss to 0
    Apr  6 11:30:06.489: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 11:30:06.494: INFO: Deleting all statefulset in ns statefulset-4731
    Apr  6 11:30:06.499: INFO: Scaling statefulset ss to 0
    Apr  6 11:30:06.528: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:30:06.540: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 11:30:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4731" for this suite. 04/06/23 11:30:06.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:06.593
Apr  6 11:30:06.593: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:30:06.595
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:06.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:06.619
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 04/06/23 11:30:06.627
Apr  6 11:30:06.644: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b" in namespace "emptydir-2034" to be "running"
Apr  6 11:30:06.649: INFO: Pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515633ms
Apr  6 11:30:08.657: INFO: Pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b": Phase="Running", Reason="", readiness=false. Elapsed: 2.012866298s
Apr  6 11:30:08.657: INFO: Pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b" satisfied condition "running"
STEP: Reading file content from the nginx-container 04/06/23 11:30:08.657
Apr  6 11:30:08.657: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2034 PodName:pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:30:08.657: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:30:08.658: INFO: ExecWithOptions: Clientset creation
Apr  6 11:30:08.658: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/emptydir-2034/pods/pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Apr  6 11:30:09.106: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:30:09.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2034" for this suite. 04/06/23 11:30:09.126
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":256,"skipped":4828,"failed":0}
------------------------------
â€¢ [2.540 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:06.593
    Apr  6 11:30:06.593: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:30:06.595
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:06.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:06.619
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 04/06/23 11:30:06.627
    Apr  6 11:30:06.644: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b" in namespace "emptydir-2034" to be "running"
    Apr  6 11:30:06.649: INFO: Pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515633ms
    Apr  6 11:30:08.657: INFO: Pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b": Phase="Running", Reason="", readiness=false. Elapsed: 2.012866298s
    Apr  6 11:30:08.657: INFO: Pod "pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b" satisfied condition "running"
    STEP: Reading file content from the nginx-container 04/06/23 11:30:08.657
    Apr  6 11:30:08.657: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2034 PodName:pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:30:08.657: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:30:08.658: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:30:08.658: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/emptydir-2034/pods/pod-sharedvolume-c43ede2f-e023-40bf-a3dd-6c168f25bc5b/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Apr  6 11:30:09.106: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:30:09.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2034" for this suite. 04/06/23 11:30:09.126
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:09.133
Apr  6 11:30:09.133: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 11:30:09.135
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:09.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:09.153
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 04/06/23 11:30:09.158
Apr  6 11:30:09.178: INFO: Waiting up to 5m0s for pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787" in namespace "var-expansion-1143" to be "Succeeded or Failed"
Apr  6 11:30:09.182: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787": Phase="Pending", Reason="", readiness=false. Elapsed: 4.415483ms
Apr  6 11:30:11.191: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013699473s
Apr  6 11:30:13.189: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011201072s
STEP: Saw pod success 04/06/23 11:30:13.189
Apr  6 11:30:13.189: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787" satisfied condition "Succeeded or Failed"
Apr  6 11:30:13.194: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787 container dapi-container: <nil>
STEP: delete the pod 04/06/23 11:30:13.209
Apr  6 11:30:13.228: INFO: Waiting for pod var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787 to disappear
Apr  6 11:30:13.232: INFO: Pod var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 11:30:13.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1143" for this suite. 04/06/23 11:30:13.243
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":257,"skipped":4831,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:09.133
    Apr  6 11:30:09.133: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 11:30:09.135
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:09.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:09.153
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 04/06/23 11:30:09.158
    Apr  6 11:30:09.178: INFO: Waiting up to 5m0s for pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787" in namespace "var-expansion-1143" to be "Succeeded or Failed"
    Apr  6 11:30:09.182: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787": Phase="Pending", Reason="", readiness=false. Elapsed: 4.415483ms
    Apr  6 11:30:11.191: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013699473s
    Apr  6 11:30:13.189: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011201072s
    STEP: Saw pod success 04/06/23 11:30:13.189
    Apr  6 11:30:13.189: INFO: Pod "var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787" satisfied condition "Succeeded or Failed"
    Apr  6 11:30:13.194: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787 container dapi-container: <nil>
    STEP: delete the pod 04/06/23 11:30:13.209
    Apr  6 11:30:13.228: INFO: Waiting for pod var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787 to disappear
    Apr  6 11:30:13.232: INFO: Pod var-expansion-0a8e6488-a4e6-478b-bff2-2129c7cd4787 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 11:30:13.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1143" for this suite. 04/06/23 11:30:13.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:13.251
Apr  6 11:30:13.252: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename disruption 04/06/23 11:30:13.253
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:13.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:13.298
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:13.305
Apr  6 11:30:13.306: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename disruption-2 04/06/23 11:30:13.307
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:13.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:13.36
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 04/06/23 11:30:13.377
STEP: Waiting for the pdb to be processed 04/06/23 11:30:15.404
STEP: Waiting for the pdb to be processed 04/06/23 11:30:15.416
STEP: listing a collection of PDBs across all namespaces 04/06/23 11:30:15.42
STEP: listing a collection of PDBs in namespace disruption-7808 04/06/23 11:30:15.429
STEP: deleting a collection of PDBs 04/06/23 11:30:15.434
STEP: Waiting for the PDB collection to be deleted 04/06/23 11:30:15.446
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Apr  6 11:30:15.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-9738" for this suite. 04/06/23 11:30:15.479
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Apr  6 11:30:15.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7808" for this suite. 04/06/23 11:30:15.492
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":258,"skipped":4841,"failed":0}
------------------------------
â€¢ [2.253 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:13.251
    Apr  6 11:30:13.252: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename disruption 04/06/23 11:30:13.253
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:13.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:13.298
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:13.305
    Apr  6 11:30:13.306: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename disruption-2 04/06/23 11:30:13.307
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:13.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:13.36
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 04/06/23 11:30:13.377
    STEP: Waiting for the pdb to be processed 04/06/23 11:30:15.404
    STEP: Waiting for the pdb to be processed 04/06/23 11:30:15.416
    STEP: listing a collection of PDBs across all namespaces 04/06/23 11:30:15.42
    STEP: listing a collection of PDBs in namespace disruption-7808 04/06/23 11:30:15.429
    STEP: deleting a collection of PDBs 04/06/23 11:30:15.434
    STEP: Waiting for the PDB collection to be deleted 04/06/23 11:30:15.446
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Apr  6 11:30:15.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-9738" for this suite. 04/06/23 11:30:15.479
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Apr  6 11:30:15.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7808" for this suite. 04/06/23 11:30:15.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:15.506
Apr  6 11:30:15.506: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replicaset 04/06/23 11:30:15.507
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:15.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:15.535
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Apr  6 11:30:15.562: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 11:30:20.568: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/06/23 11:30:20.568
STEP: Scaling up "test-rs" replicaset  04/06/23 11:30:20.568
Apr  6 11:30:20.577: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 04/06/23 11:30:20.578
W0406 11:30:20.587417      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr  6 11:30:20.602: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 11:30:20.602: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 11:30:20.620: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 11:30:20.620: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
Apr  6 11:30:21.548: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 2, AvailableReplicas 2
Apr  6 11:30:22.180: INFO: observed Replicaset test-rs in namespace replicaset-2087 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Apr  6 11:30:22.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2087" for this suite. 04/06/23 11:30:22.208
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":259,"skipped":4863,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.709 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:15.506
    Apr  6 11:30:15.506: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replicaset 04/06/23 11:30:15.507
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:15.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:15.535
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Apr  6 11:30:15.562: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr  6 11:30:20.568: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/06/23 11:30:20.568
    STEP: Scaling up "test-rs" replicaset  04/06/23 11:30:20.568
    Apr  6 11:30:20.577: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 04/06/23 11:30:20.578
    W0406 11:30:20.587417      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr  6 11:30:20.602: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
    Apr  6 11:30:20.602: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
    Apr  6 11:30:20.620: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
    Apr  6 11:30:20.620: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 1, AvailableReplicas 1
    Apr  6 11:30:21.548: INFO: observed ReplicaSet test-rs in namespace replicaset-2087 with ReadyReplicas 2, AvailableReplicas 2
    Apr  6 11:30:22.180: INFO: observed Replicaset test-rs in namespace replicaset-2087 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Apr  6 11:30:22.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2087" for this suite. 04/06/23 11:30:22.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:22.22
Apr  6 11:30:22.220: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 11:30:22.221
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:22.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:22.244
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Apr  6 11:30:22.264: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr  6 11:30:27.275: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/06/23 11:30:27.275
Apr  6 11:30:27.275: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr  6 11:30:29.284: INFO: Creating deployment "test-rollover-deployment"
Apr  6 11:30:29.297: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr  6 11:30:31.312: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr  6 11:30:31.335: INFO: Ensure that both replica sets have 1 created replica
Apr  6 11:30:31.345: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr  6 11:30:31.358: INFO: Updating deployment test-rollover-deployment
Apr  6 11:30:31.358: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr  6 11:30:33.373: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr  6 11:30:33.385: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr  6 11:30:33.410: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 11:30:33.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:30:35.424: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 11:30:35.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:30:37.423: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 11:30:37.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:30:39.422: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 11:30:39.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:30:41.427: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 11:30:41.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:30:43.426: INFO: 
Apr  6 11:30:43.426: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 11:30:43.443: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7007  b55b5213-c6d5-4854-b6df-92f44ebf8d32 33344 2 2023-04-06 11:30:29 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00541f188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-06 11:30:29 +0000 UTC,LastTransitionTime:2023-04-06 11:30:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-04-06 11:30:42 +0000 UTC,LastTransitionTime:2023-04-06 11:30:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 11:30:43.449: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7007  9a8b8fb1-57c4-42eb-abed-eefc610e79e7 33337 2 2023-04-06 11:30:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b55b5213-c6d5-4854-b6df-92f44ebf8d32 0xc005473c07 0xc005473c08}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b55b5213-c6d5-4854-b6df-92f44ebf8d32\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005473cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:30:43.449: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr  6 11:30:43.449: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7007  5d488c03-07de-4921-945f-bfb855f62156 33343 2 2023-04-06 11:30:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b55b5213-c6d5-4854-b6df-92f44ebf8d32 0xc0054739b7 0xc0054739b8}] [] [{e2e.test Update apps/v1 2023-04-06 11:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b55b5213-c6d5-4854-b6df-92f44ebf8d32\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005473a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:30:43.449: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7007  35df6f85-13ac-4ed6-a4e2-76c9c7bc7d38 33284 2 2023-04-06 11:30:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b55b5213-c6d5-4854-b6df-92f44ebf8d32 0xc005473ae7 0xc005473ae8}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b55b5213-c6d5-4854-b6df-92f44ebf8d32\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005473b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:30:43.460: INFO: Pod "test-rollover-deployment-6d45fd857b-r2tqn" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-r2tqn test-rollover-deployment-6d45fd857b- deployment-7007  24c1fa12-7bd1-4f47-b74f-76b7099585e0 33297 0 2023-04-06 11:30:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:f028806ed83109af265adb8d591f69c3299890caadd81c63f35f864990fb74e1 cni.projectcalico.org/podIP:10.96.2.156/32 cni.projectcalico.org/podIPs:10.96.2.156/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 9a8b8fb1-57c4-42eb-abed-eefc610e79e7 0xc00541f527 0xc00541f528}] [] [{Go-http-client Update v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a8b8fb1-57c4-42eb-abed-eefc610e79e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:30:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8c4f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8c4f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.156,StartTime:2023-04-06 11:30:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:30:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9c163cd3654738746e2de8270a7c85ae047856386d522f19f6d0d7c53a4f7607,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 11:30:43.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7007" for this suite. 04/06/23 11:30:43.473
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":260,"skipped":4869,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.265 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:22.22
    Apr  6 11:30:22.220: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 11:30:22.221
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:22.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:22.244
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Apr  6 11:30:22.264: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Apr  6 11:30:27.275: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/06/23 11:30:27.275
    Apr  6 11:30:27.275: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Apr  6 11:30:29.284: INFO: Creating deployment "test-rollover-deployment"
    Apr  6 11:30:29.297: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Apr  6 11:30:31.312: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Apr  6 11:30:31.335: INFO: Ensure that both replica sets have 1 created replica
    Apr  6 11:30:31.345: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Apr  6 11:30:31.358: INFO: Updating deployment test-rollover-deployment
    Apr  6 11:30:31.358: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Apr  6 11:30:33.373: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Apr  6 11:30:33.385: INFO: Make sure deployment "test-rollover-deployment" is complete
    Apr  6 11:30:33.410: INFO: all replica sets need to contain the pod-template-hash label
    Apr  6 11:30:33.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:30:35.424: INFO: all replica sets need to contain the pod-template-hash label
    Apr  6 11:30:35.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:30:37.423: INFO: all replica sets need to contain the pod-template-hash label
    Apr  6 11:30:37.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:30:39.422: INFO: all replica sets need to contain the pod-template-hash label
    Apr  6 11:30:39.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:30:41.427: INFO: all replica sets need to contain the pod-template-hash label
    Apr  6 11:30:41.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 30, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 30, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:30:43.426: INFO: 
    Apr  6 11:30:43.426: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 11:30:43.443: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7007  b55b5213-c6d5-4854-b6df-92f44ebf8d32 33344 2 2023-04-06 11:30:29 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00541f188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-06 11:30:29 +0000 UTC,LastTransitionTime:2023-04-06 11:30:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-04-06 11:30:42 +0000 UTC,LastTransitionTime:2023-04-06 11:30:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr  6 11:30:43.449: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7007  9a8b8fb1-57c4-42eb-abed-eefc610e79e7 33337 2 2023-04-06 11:30:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b55b5213-c6d5-4854-b6df-92f44ebf8d32 0xc005473c07 0xc005473c08}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b55b5213-c6d5-4854-b6df-92f44ebf8d32\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005473cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:30:43.449: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Apr  6 11:30:43.449: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7007  5d488c03-07de-4921-945f-bfb855f62156 33343 2 2023-04-06 11:30:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b55b5213-c6d5-4854-b6df-92f44ebf8d32 0xc0054739b7 0xc0054739b8}] [] [{e2e.test Update apps/v1 2023-04-06 11:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b55b5213-c6d5-4854-b6df-92f44ebf8d32\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005473a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:30:43.449: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7007  35df6f85-13ac-4ed6-a4e2-76c9c7bc7d38 33284 2 2023-04-06 11:30:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b55b5213-c6d5-4854-b6df-92f44ebf8d32 0xc005473ae7 0xc005473ae8}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b55b5213-c6d5-4854-b6df-92f44ebf8d32\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005473b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:30:43.460: INFO: Pod "test-rollover-deployment-6d45fd857b-r2tqn" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-r2tqn test-rollover-deployment-6d45fd857b- deployment-7007  24c1fa12-7bd1-4f47-b74f-76b7099585e0 33297 0 2023-04-06 11:30:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:f028806ed83109af265adb8d591f69c3299890caadd81c63f35f864990fb74e1 cni.projectcalico.org/podIP:10.96.2.156/32 cni.projectcalico.org/podIPs:10.96.2.156/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 9a8b8fb1-57c4-42eb-abed-eefc610e79e7 0xc00541f527 0xc00541f528}] [] [{Go-http-client Update v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-06 11:30:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a8b8fb1-57c4-42eb-abed-eefc610e79e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:30:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8c4f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8c4f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:30:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.156,StartTime:2023-04-06 11:30:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:30:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://9c163cd3654738746e2de8270a7c85ae047856386d522f19f6d0d7c53a4f7607,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 11:30:43.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7007" for this suite. 04/06/23 11:30:43.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:43.485
Apr  6 11:30:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-runtime 04/06/23 11:30:43.487
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:43.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:43.517
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 04/06/23 11:30:43.542
STEP: wait for the container to reach Failed 04/06/23 11:30:43.559
STEP: get the container status 04/06/23 11:30:47.594
STEP: the container should be terminated 04/06/23 11:30:47.6
STEP: the termination message should be set 04/06/23 11:30:47.601
Apr  6 11:30:47.601: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/06/23 11:30:47.601
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Apr  6 11:30:47.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7711" for this suite. 04/06/23 11:30:47.625
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":261,"skipped":4878,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:43.485
    Apr  6 11:30:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-runtime 04/06/23 11:30:43.487
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:43.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:43.517
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 04/06/23 11:30:43.542
    STEP: wait for the container to reach Failed 04/06/23 11:30:43.559
    STEP: get the container status 04/06/23 11:30:47.594
    STEP: the container should be terminated 04/06/23 11:30:47.6
    STEP: the termination message should be set 04/06/23 11:30:47.601
    Apr  6 11:30:47.601: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/06/23 11:30:47.601
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Apr  6 11:30:47.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7711" for this suite. 04/06/23 11:30:47.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:47.631
Apr  6 11:30:47.631: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename watch 04/06/23 11:30:47.632
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:47.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:47.662
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 04/06/23 11:30:47.668
STEP: creating a new configmap 04/06/23 11:30:47.67
STEP: modifying the configmap once 04/06/23 11:30:47.674
STEP: changing the label value of the configmap 04/06/23 11:30:47.683
STEP: Expecting to observe a delete notification for the watched object 04/06/23 11:30:47.691
Apr  6 11:30:47.691: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33382 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 11:30:47.692: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33383 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 11:30:47.692: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33384 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 04/06/23 11:30:47.692
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/06/23 11:30:47.707
STEP: changing the label value of the configmap back 04/06/23 11:30:57.708
STEP: modifying the configmap a third time 04/06/23 11:30:57.718
STEP: deleting the configmap 04/06/23 11:30:57.726
STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/06/23 11:30:57.731
Apr  6 11:30:57.731: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33440 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 11:30:57.732: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33441 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 11:30:57.732: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33442 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Apr  6 11:30:57.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3556" for this suite. 04/06/23 11:30:57.741
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":262,"skipped":4883,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.115 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:47.631
    Apr  6 11:30:47.631: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename watch 04/06/23 11:30:47.632
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:47.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:47.662
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 04/06/23 11:30:47.668
    STEP: creating a new configmap 04/06/23 11:30:47.67
    STEP: modifying the configmap once 04/06/23 11:30:47.674
    STEP: changing the label value of the configmap 04/06/23 11:30:47.683
    STEP: Expecting to observe a delete notification for the watched object 04/06/23 11:30:47.691
    Apr  6 11:30:47.691: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33382 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 11:30:47.692: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33383 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 11:30:47.692: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33384 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 04/06/23 11:30:47.692
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/06/23 11:30:47.707
    STEP: changing the label value of the configmap back 04/06/23 11:30:57.708
    STEP: modifying the configmap a third time 04/06/23 11:30:57.718
    STEP: deleting the configmap 04/06/23 11:30:57.726
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/06/23 11:30:57.731
    Apr  6 11:30:57.731: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33440 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 11:30:57.732: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33441 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 11:30:57.732: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3556  667edc92-26f5-4d07-b96a-3f065d9c0dba 33442 0 2023-04-06 11:30:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-06 11:30:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Apr  6 11:30:57.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3556" for this suite. 04/06/23 11:30:57.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:30:57.752
Apr  6 11:30:57.752: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 11:30:57.754
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:57.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:57.773
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 04/06/23 11:30:57.786
Apr  6 11:30:57.809: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9793" to be "running and ready"
Apr  6 11:30:57.823: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.495907ms
Apr  6 11:30:57.823: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:30:59.831: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018403649s
Apr  6 11:30:59.831: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr  6 11:30:59.831: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 04/06/23 11:30:59.837
Apr  6 11:30:59.848: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9793" to be "running and ready"
Apr  6 11:30:59.854: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42106ms
Apr  6 11:30:59.854: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:31:01.861: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012401197s
Apr  6 11:31:01.861: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Apr  6 11:31:01.861: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/06/23 11:31:01.866
Apr  6 11:31:01.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 11:31:01.880: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 11:31:03.881: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 11:31:03.891: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 11:31:05.881: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 11:31:05.890: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 04/06/23 11:31:05.89
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Apr  6 11:31:05.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9793" for this suite. 04/06/23 11:31:06.009
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":263,"skipped":4895,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.276 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:30:57.752
    Apr  6 11:30:57.752: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 11:30:57.754
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:30:57.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:30:57.773
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 04/06/23 11:30:57.786
    Apr  6 11:30:57.809: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9793" to be "running and ready"
    Apr  6 11:30:57.823: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.495907ms
    Apr  6 11:30:57.823: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:30:59.831: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018403649s
    Apr  6 11:30:59.831: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr  6 11:30:59.831: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 04/06/23 11:30:59.837
    Apr  6 11:30:59.848: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9793" to be "running and ready"
    Apr  6 11:30:59.854: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42106ms
    Apr  6 11:30:59.854: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:31:01.861: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012401197s
    Apr  6 11:31:01.861: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Apr  6 11:31:01.861: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/06/23 11:31:01.866
    Apr  6 11:31:01.874: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr  6 11:31:01.880: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr  6 11:31:03.881: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr  6 11:31:03.891: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr  6 11:31:05.881: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr  6 11:31:05.890: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 04/06/23 11:31:05.89
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Apr  6 11:31:05.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9793" for this suite. 04/06/23 11:31:06.009
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:06.028
Apr  6 11:31:06.029: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 11:31:06.03
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:06.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:06.067
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 11:31:06.107
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:31:06.706
STEP: Deploying the webhook pod 04/06/23 11:31:06.74
STEP: Wait for the deployment to be ready 04/06/23 11:31:06.76
Apr  6 11:31:06.779: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 11:31:08.802
STEP: Verifying the service has paired with the endpoint 04/06/23 11:31:08.828
Apr  6 11:31:09.828: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/06/23 11:31:09.84
STEP: create a configmap that should be updated by the webhook 04/06/23 11:31:10.024
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:31:10.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9517" for this suite. 04/06/23 11:31:10.267
STEP: Destroying namespace "webhook-9517-markers" for this suite. 04/06/23 11:31:10.275
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":264,"skipped":4896,"failed":0}
------------------------------
â€¢ [4.289 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:06.028
    Apr  6 11:31:06.029: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 11:31:06.03
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:06.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:06.067
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 11:31:06.107
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:31:06.706
    STEP: Deploying the webhook pod 04/06/23 11:31:06.74
    STEP: Wait for the deployment to be ready 04/06/23 11:31:06.76
    Apr  6 11:31:06.779: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 11:31:08.802
    STEP: Verifying the service has paired with the endpoint 04/06/23 11:31:08.828
    Apr  6 11:31:09.828: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/06/23 11:31:09.84
    STEP: create a configmap that should be updated by the webhook 04/06/23 11:31:10.024
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:31:10.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9517" for this suite. 04/06/23 11:31:10.267
    STEP: Destroying namespace "webhook-9517-markers" for this suite. 04/06/23 11:31:10.275
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:10.323
Apr  6 11:31:10.323: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 11:31:10.324
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:10.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:10.346
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-e2ff9307-47e2-4d46-bb53-0e970cf28fb0 04/06/23 11:31:10.354
STEP: Creating a pod to test consume configMaps 04/06/23 11:31:10.36
Apr  6 11:31:10.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc" in namespace "configmap-7853" to be "Succeeded or Failed"
Apr  6 11:31:10.377: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493243ms
Apr  6 11:31:12.383: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011259229s
Apr  6 11:31:14.385: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012649497s
STEP: Saw pod success 04/06/23 11:31:14.385
Apr  6 11:31:14.385: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc" satisfied condition "Succeeded or Failed"
Apr  6 11:31:14.391: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:31:14.413
Apr  6 11:31:14.426: INFO: Waiting for pod pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc to disappear
Apr  6 11:31:14.434: INFO: Pod pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 11:31:14.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7853" for this suite. 04/06/23 11:31:14.445
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":4914,"failed":0}
------------------------------
â€¢ [4.130 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:10.323
    Apr  6 11:31:10.323: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 11:31:10.324
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:10.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:10.346
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-e2ff9307-47e2-4d46-bb53-0e970cf28fb0 04/06/23 11:31:10.354
    STEP: Creating a pod to test consume configMaps 04/06/23 11:31:10.36
    Apr  6 11:31:10.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc" in namespace "configmap-7853" to be "Succeeded or Failed"
    Apr  6 11:31:10.377: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493243ms
    Apr  6 11:31:12.383: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011259229s
    Apr  6 11:31:14.385: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012649497s
    STEP: Saw pod success 04/06/23 11:31:14.385
    Apr  6 11:31:14.385: INFO: Pod "pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc" satisfied condition "Succeeded or Failed"
    Apr  6 11:31:14.391: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:31:14.413
    Apr  6 11:31:14.426: INFO: Waiting for pod pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc to disappear
    Apr  6 11:31:14.434: INFO: Pod pod-configmaps-e7e7e68b-6f30-4030-a226-98193cddc1cc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 11:31:14.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7853" for this suite. 04/06/23 11:31:14.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:14.454
Apr  6 11:31:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename endpointslice 04/06/23 11:31:14.455
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:14.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:14.482
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 04/06/23 11:31:14.487
STEP: getting /apis/discovery.k8s.io 04/06/23 11:31:14.495
STEP: getting /apis/discovery.k8s.iov1 04/06/23 11:31:14.497
STEP: creating 04/06/23 11:31:14.5
STEP: getting 04/06/23 11:31:14.526
STEP: listing 04/06/23 11:31:14.531
STEP: watching 04/06/23 11:31:14.537
Apr  6 11:31:14.537: INFO: starting watch
STEP: cluster-wide listing 04/06/23 11:31:14.542
STEP: cluster-wide watching 04/06/23 11:31:14.551
Apr  6 11:31:14.551: INFO: starting watch
STEP: patching 04/06/23 11:31:14.554
STEP: updating 04/06/23 11:31:14.562
Apr  6 11:31:14.574: INFO: waiting for watch events with expected annotations
Apr  6 11:31:14.574: INFO: saw patched and updated annotations
STEP: deleting 04/06/23 11:31:14.574
STEP: deleting a collection 04/06/23 11:31:14.587
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Apr  6 11:31:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-363" for this suite. 04/06/23 11:31:14.62
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":266,"skipped":4921,"failed":0}
------------------------------
â€¢ [0.200 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:14.454
    Apr  6 11:31:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename endpointslice 04/06/23 11:31:14.455
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:14.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:14.482
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 04/06/23 11:31:14.487
    STEP: getting /apis/discovery.k8s.io 04/06/23 11:31:14.495
    STEP: getting /apis/discovery.k8s.iov1 04/06/23 11:31:14.497
    STEP: creating 04/06/23 11:31:14.5
    STEP: getting 04/06/23 11:31:14.526
    STEP: listing 04/06/23 11:31:14.531
    STEP: watching 04/06/23 11:31:14.537
    Apr  6 11:31:14.537: INFO: starting watch
    STEP: cluster-wide listing 04/06/23 11:31:14.542
    STEP: cluster-wide watching 04/06/23 11:31:14.551
    Apr  6 11:31:14.551: INFO: starting watch
    STEP: patching 04/06/23 11:31:14.554
    STEP: updating 04/06/23 11:31:14.562
    Apr  6 11:31:14.574: INFO: waiting for watch events with expected annotations
    Apr  6 11:31:14.574: INFO: saw patched and updated annotations
    STEP: deleting 04/06/23 11:31:14.574
    STEP: deleting a collection 04/06/23 11:31:14.587
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Apr  6 11:31:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-363" for this suite. 04/06/23 11:31:14.62
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:14.655
Apr  6 11:31:14.655: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename watch 04/06/23 11:31:14.657
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:14.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:14.682
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 04/06/23 11:31:14.687
STEP: creating a new configmap 04/06/23 11:31:14.691
STEP: modifying the configmap once 04/06/23 11:31:14.697
STEP: closing the watch once it receives two notifications 04/06/23 11:31:14.707
Apr  6 11:31:14.707: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33609 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 11:31:14.708: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33610 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 04/06/23 11:31:14.708
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/06/23 11:31:14.719
STEP: deleting the configmap 04/06/23 11:31:14.722
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/06/23 11:31:14.727
Apr  6 11:31:14.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33611 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 11:31:14.728: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33612 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Apr  6 11:31:14.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9711" for this suite. 04/06/23 11:31:14.735
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":267,"skipped":4924,"failed":0}
------------------------------
â€¢ [0.085 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:14.655
    Apr  6 11:31:14.655: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename watch 04/06/23 11:31:14.657
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:14.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:14.682
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 04/06/23 11:31:14.687
    STEP: creating a new configmap 04/06/23 11:31:14.691
    STEP: modifying the configmap once 04/06/23 11:31:14.697
    STEP: closing the watch once it receives two notifications 04/06/23 11:31:14.707
    Apr  6 11:31:14.707: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33609 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 11:31:14.708: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33610 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 04/06/23 11:31:14.708
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/06/23 11:31:14.719
    STEP: deleting the configmap 04/06/23 11:31:14.722
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/06/23 11:31:14.727
    Apr  6 11:31:14.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33611 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr  6 11:31:14.728: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9711  10cbef65-28a0-4ae6-ab9e-978b60c6b864 33612 0 2023-04-06 11:31:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-06 11:31:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Apr  6 11:31:14.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9711" for this suite. 04/06/23 11:31:14.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:14.741
Apr  6 11:31:14.741: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sysctl 04/06/23 11:31:14.742
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:14.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:14.762
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/06/23 11:31:14.768
STEP: Watching for error events or started pod 04/06/23 11:31:14.777
STEP: Waiting for pod completion 04/06/23 11:31:16.791
Apr  6 11:31:16.798: INFO: Waiting up to 3m0s for pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82" in namespace "sysctl-34" to be "completed"
Apr  6 11:31:16.807: INFO: Pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82": Phase="Pending", Reason="", readiness=false. Elapsed: 5.989747ms
Apr  6 11:31:18.816: INFO: Pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014880675s
Apr  6 11:31:18.824: INFO: Pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82" satisfied condition "completed"
STEP: Checking that the pod succeeded 04/06/23 11:31:18.83
STEP: Getting logs from the pod 04/06/23 11:31:18.83
STEP: Checking that the sysctl is actually updated 04/06/23 11:31:18.845
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Apr  6 11:31:18.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-34" for this suite. 04/06/23 11:31:18.857
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":268,"skipped":4929,"failed":0}
------------------------------
â€¢ [4.124 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:14.741
    Apr  6 11:31:14.741: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sysctl 04/06/23 11:31:14.742
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:14.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:14.762
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/06/23 11:31:14.768
    STEP: Watching for error events or started pod 04/06/23 11:31:14.777
    STEP: Waiting for pod completion 04/06/23 11:31:16.791
    Apr  6 11:31:16.798: INFO: Waiting up to 3m0s for pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82" in namespace "sysctl-34" to be "completed"
    Apr  6 11:31:16.807: INFO: Pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82": Phase="Pending", Reason="", readiness=false. Elapsed: 5.989747ms
    Apr  6 11:31:18.816: INFO: Pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014880675s
    Apr  6 11:31:18.824: INFO: Pod "sysctl-ff1196ad-2fc0-4a73-9778-e1941f899c82" satisfied condition "completed"
    STEP: Checking that the pod succeeded 04/06/23 11:31:18.83
    STEP: Getting logs from the pod 04/06/23 11:31:18.83
    STEP: Checking that the sysctl is actually updated 04/06/23 11:31:18.845
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Apr  6 11:31:18.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-34" for this suite. 04/06/23 11:31:18.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:18.867
Apr  6 11:31:18.867: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename daemonsets 04/06/23 11:31:18.868
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:18.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:18.908
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Apr  6 11:31:18.959: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:31:18.966
Apr  6 11:31:18.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:31:18.982: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:31:20.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:31:20.003: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:31:21.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:31:21.022: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image. 04/06/23 11:31:21.04
STEP: Check that daemon pods images are updated. 04/06/23 11:31:21.06
Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:24.082: INFO: Pod daemon-set-bstjf is not available
Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:25.089: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:25.089: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:25.089: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:26.083: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:26.083: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:26.083: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:26.083: INFO: Pod daemon-set-xn8rf is not available
Apr  6 11:31:27.084: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:27.084: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:27.084: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:27.084: INFO: Pod daemon-set-xn8rf is not available
Apr  6 11:31:28.085: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:28.085: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:29.087: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:29.087: INFO: Pod daemon-set-h55qt is not available
Apr  6 11:31:29.087: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:30.086: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:31.084: INFO: Pod daemon-set-7t29l is not available
Apr  6 11:31:31.084: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:32.083: INFO: Pod daemon-set-7t29l is not available
Apr  6 11:31:32.083: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Apr  6 11:31:35.125: INFO: Pod daemon-set-vmqzj is not available
STEP: Check that daemon pods are still running on every node of the cluster. 04/06/23 11:31:35.135
Apr  6 11:31:35.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:31:35.155: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:31:36.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:31:36.172: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:31:37.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:31:37.216: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:31:37.265
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4023, will wait for the garbage collector to delete the pods 04/06/23 11:31:37.265
Apr  6 11:31:37.340: INFO: Deleting DaemonSet.extensions daemon-set took: 11.684551ms
Apr  6 11:31:37.444: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.602949ms
Apr  6 11:31:39.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:31:39.351: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr  6 11:31:39.355: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33878"},"items":null}

Apr  6 11:31:39.360: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33878"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:31:39.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4023" for this suite. 04/06/23 11:31:39.404
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":269,"skipped":4960,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.545 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:18.867
    Apr  6 11:31:18.867: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename daemonsets 04/06/23 11:31:18.868
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:18.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:18.908
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Apr  6 11:31:18.959: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:31:18.966
    Apr  6 11:31:18.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:31:18.982: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:31:20.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:31:20.003: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:31:21.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:31:21.022: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Update daemon pods image. 04/06/23 11:31:21.04
    STEP: Check that daemon pods images are updated. 04/06/23 11:31:21.06
    Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:21.066: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:22.084: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:23.091: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:24.082: INFO: Pod daemon-set-bstjf is not available
    Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-nsmqz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:24.082: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:25.089: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:25.089: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:25.089: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:26.083: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:26.083: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:26.083: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:26.083: INFO: Pod daemon-set-xn8rf is not available
    Apr  6 11:31:27.084: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:27.084: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:27.084: INFO: Wrong image for pod: daemon-set-wpm5c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:27.084: INFO: Pod daemon-set-xn8rf is not available
    Apr  6 11:31:28.085: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:28.085: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:29.087: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:29.087: INFO: Pod daemon-set-h55qt is not available
    Apr  6 11:31:29.087: INFO: Wrong image for pod: daemon-set-nwslj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:30.086: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:31.084: INFO: Pod daemon-set-7t29l is not available
    Apr  6 11:31:31.084: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:32.083: INFO: Pod daemon-set-7t29l is not available
    Apr  6 11:31:32.083: INFO: Wrong image for pod: daemon-set-fmqnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Apr  6 11:31:35.125: INFO: Pod daemon-set-vmqzj is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 04/06/23 11:31:35.135
    Apr  6 11:31:35.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:31:35.155: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:31:36.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:31:36.172: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:31:37.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:31:37.216: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:31:37.265
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4023, will wait for the garbage collector to delete the pods 04/06/23 11:31:37.265
    Apr  6 11:31:37.340: INFO: Deleting DaemonSet.extensions daemon-set took: 11.684551ms
    Apr  6 11:31:37.444: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.602949ms
    Apr  6 11:31:39.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:31:39.351: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr  6 11:31:39.355: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33878"},"items":null}

    Apr  6 11:31:39.360: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33878"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:31:39.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4023" for this suite. 04/06/23 11:31:39.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:39.415
Apr  6 11:31:39.415: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 11:31:39.416
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:39.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:39.438
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-6c590fe4-ffeb-463e-828b-3ee3bec2305f 04/06/23 11:31:39.454
STEP: Creating secret with name s-test-opt-upd-7e88500b-df73-48a7-b248-925b42c81648 04/06/23 11:31:39.46
STEP: Creating the pod 04/06/23 11:31:39.464
Apr  6 11:31:39.477: INFO: Waiting up to 5m0s for pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179" in namespace "secrets-495" to be "running and ready"
Apr  6 11:31:39.485: INFO: Pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179": Phase="Pending", Reason="", readiness=false. Elapsed: 8.294478ms
Apr  6 11:31:39.486: INFO: The phase of Pod pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:31:41.494: INFO: Pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179": Phase="Running", Reason="", readiness=true. Elapsed: 2.016996199s
Apr  6 11:31:41.494: INFO: The phase of Pod pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179 is Running (Ready = true)
Apr  6 11:31:41.494: INFO: Pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-6c590fe4-ffeb-463e-828b-3ee3bec2305f 04/06/23 11:31:41.69
STEP: Updating secret s-test-opt-upd-7e88500b-df73-48a7-b248-925b42c81648 04/06/23 11:31:41.702
STEP: Creating secret with name s-test-opt-create-59f284b5-f7e3-449c-adb3-104903e12bcf 04/06/23 11:31:41.709
STEP: waiting to observe update in volume 04/06/23 11:31:41.722
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 11:31:43.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-495" for this suite. 04/06/23 11:31:43.862
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":270,"skipped":4985,"failed":0}
------------------------------
â€¢ [4.453 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:39.415
    Apr  6 11:31:39.415: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 11:31:39.416
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:39.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:39.438
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-6c590fe4-ffeb-463e-828b-3ee3bec2305f 04/06/23 11:31:39.454
    STEP: Creating secret with name s-test-opt-upd-7e88500b-df73-48a7-b248-925b42c81648 04/06/23 11:31:39.46
    STEP: Creating the pod 04/06/23 11:31:39.464
    Apr  6 11:31:39.477: INFO: Waiting up to 5m0s for pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179" in namespace "secrets-495" to be "running and ready"
    Apr  6 11:31:39.485: INFO: Pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179": Phase="Pending", Reason="", readiness=false. Elapsed: 8.294478ms
    Apr  6 11:31:39.486: INFO: The phase of Pod pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:31:41.494: INFO: Pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179": Phase="Running", Reason="", readiness=true. Elapsed: 2.016996199s
    Apr  6 11:31:41.494: INFO: The phase of Pod pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179 is Running (Ready = true)
    Apr  6 11:31:41.494: INFO: Pod "pod-secrets-41519328-a4de-46fb-ab72-0746b20e5179" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-6c590fe4-ffeb-463e-828b-3ee3bec2305f 04/06/23 11:31:41.69
    STEP: Updating secret s-test-opt-upd-7e88500b-df73-48a7-b248-925b42c81648 04/06/23 11:31:41.702
    STEP: Creating secret with name s-test-opt-create-59f284b5-f7e3-449c-adb3-104903e12bcf 04/06/23 11:31:41.709
    STEP: waiting to observe update in volume 04/06/23 11:31:41.722
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 11:31:43.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-495" for this suite. 04/06/23 11:31:43.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:43.869
Apr  6 11:31:43.869: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 11:31:43.87
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:43.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:43.9
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 04/06/23 11:31:43.907
Apr  6 11:31:43.907: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1886 proxy --unix-socket=/tmp/kubectl-proxy-unix1168802594/test'
STEP: retrieving proxy /api/ output 04/06/23 11:31:43.998
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 11:31:43.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1886" for this suite. 04/06/23 11:31:44.009
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":271,"skipped":4990,"failed":0}
------------------------------
â€¢ [0.147 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:43.869
    Apr  6 11:31:43.869: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 11:31:43.87
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:43.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:43.9
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 04/06/23 11:31:43.907
    Apr  6 11:31:43.907: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-1886 proxy --unix-socket=/tmp/kubectl-proxy-unix1168802594/test'
    STEP: retrieving proxy /api/ output 04/06/23 11:31:43.998
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 11:31:43.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1886" for this suite. 04/06/23 11:31:44.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:44.019
Apr  6 11:31:44.020: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:31:44.021
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:44.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:44.039
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-04c40622-088b-4bd5-8d5c-8f3eca6f275f 04/06/23 11:31:44.045
STEP: Creating a pod to test consume configMaps 04/06/23 11:31:44.049
Apr  6 11:31:44.060: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411" in namespace "projected-1848" to be "Succeeded or Failed"
Apr  6 11:31:44.063: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525776ms
Apr  6 11:31:46.071: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011435477s
Apr  6 11:31:48.071: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011179248s
STEP: Saw pod success 04/06/23 11:31:48.071
Apr  6 11:31:48.071: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411" satisfied condition "Succeeded or Failed"
Apr  6 11:31:48.081: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:31:48.095
Apr  6 11:31:48.106: INFO: Waiting for pod pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411 to disappear
Apr  6 11:31:48.112: INFO: Pod pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 11:31:48.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1848" for this suite. 04/06/23 11:31:48.122
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":272,"skipped":5034,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:44.019
    Apr  6 11:31:44.020: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:31:44.021
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:44.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:44.039
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-04c40622-088b-4bd5-8d5c-8f3eca6f275f 04/06/23 11:31:44.045
    STEP: Creating a pod to test consume configMaps 04/06/23 11:31:44.049
    Apr  6 11:31:44.060: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411" in namespace "projected-1848" to be "Succeeded or Failed"
    Apr  6 11:31:44.063: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525776ms
    Apr  6 11:31:46.071: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011435477s
    Apr  6 11:31:48.071: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011179248s
    STEP: Saw pod success 04/06/23 11:31:48.071
    Apr  6 11:31:48.071: INFO: Pod "pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411" satisfied condition "Succeeded or Failed"
    Apr  6 11:31:48.081: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:31:48.095
    Apr  6 11:31:48.106: INFO: Waiting for pod pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411 to disappear
    Apr  6 11:31:48.112: INFO: Pod pod-projected-configmaps-84cc2eda-cae8-428c-a350-c596c8a7c411 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 11:31:48.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1848" for this suite. 04/06/23 11:31:48.122
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:31:48.13
Apr  6 11:31:48.130: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename hostport 04/06/23 11:31:48.132
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:48.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:48.151
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/06/23 11:31:48.165
Apr  6 11:31:48.178: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-433" to be "running and ready"
Apr  6 11:31:48.188: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.564255ms
Apr  6 11:31:48.188: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:31:50.197: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018637075s
Apr  6 11:31:50.200: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr  6 11:31:50.200: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.3.220 on the node which pod1 resides and expect scheduled 04/06/23 11:31:50.201
Apr  6 11:31:50.215: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-433" to be "running and ready"
Apr  6 11:31:50.221: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328112ms
Apr  6 11:31:50.223: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:31:52.255: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.038177191s
Apr  6 11:31:52.255: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr  6 11:31:52.255: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.3.220 but use UDP protocol on the node which pod2 resides 04/06/23 11:31:52.255
Apr  6 11:31:52.264: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-433" to be "running and ready"
Apr  6 11:31:52.272: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.222269ms
Apr  6 11:31:52.272: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:31:54.279: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014774827s
Apr  6 11:31:54.279: INFO: The phase of Pod pod3 is Running (Ready = true)
Apr  6 11:31:54.279: INFO: Pod "pod3" satisfied condition "running and ready"
Apr  6 11:31:54.294: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-433" to be "running and ready"
Apr  6 11:31:54.299: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014805ms
Apr  6 11:31:54.299: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:31:56.305: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011508954s
Apr  6 11:31:56.305: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Apr  6 11:31:56.305: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/06/23 11:31:56.309
Apr  6 11:31:56.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.3.220 http://127.0.0.1:54323/hostname] Namespace:hostport-433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:31:56.310: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:31:56.311: INFO: ExecWithOptions: Clientset creation
Apr  6 11:31:56.311: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/hostport-433/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.3.220+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.220, port: 54323 04/06/23 11:31:56.845
Apr  6 11:31:56.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.3.220:54323/hostname] Namespace:hostport-433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:31:56.845: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:31:56.846: INFO: ExecWithOptions: Clientset creation
Apr  6 11:31:56.846: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/hostport-433/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.3.220%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.220, port: 54323 UDP 04/06/23 11:31:57.168
Apr  6 11:31:57.169: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.3.220 54323] Namespace:hostport-433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:31:57.169: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:31:57.170: INFO: ExecWithOptions: Clientset creation
Apr  6 11:31:57.170: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/hostport-433/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.3.220+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Apr  6 11:32:02.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-433" for this suite. 04/06/23 11:32:02.588
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":273,"skipped":5034,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.466 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:31:48.13
    Apr  6 11:31:48.130: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename hostport 04/06/23 11:31:48.132
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:31:48.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:31:48.151
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/06/23 11:31:48.165
    Apr  6 11:31:48.178: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-433" to be "running and ready"
    Apr  6 11:31:48.188: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.564255ms
    Apr  6 11:31:48.188: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:31:50.197: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018637075s
    Apr  6 11:31:50.200: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr  6 11:31:50.200: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.3.220 on the node which pod1 resides and expect scheduled 04/06/23 11:31:50.201
    Apr  6 11:31:50.215: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-433" to be "running and ready"
    Apr  6 11:31:50.221: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328112ms
    Apr  6 11:31:50.223: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:31:52.255: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.038177191s
    Apr  6 11:31:52.255: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr  6 11:31:52.255: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.3.220 but use UDP protocol on the node which pod2 resides 04/06/23 11:31:52.255
    Apr  6 11:31:52.264: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-433" to be "running and ready"
    Apr  6 11:31:52.272: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.222269ms
    Apr  6 11:31:52.272: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:31:54.279: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014774827s
    Apr  6 11:31:54.279: INFO: The phase of Pod pod3 is Running (Ready = true)
    Apr  6 11:31:54.279: INFO: Pod "pod3" satisfied condition "running and ready"
    Apr  6 11:31:54.294: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-433" to be "running and ready"
    Apr  6 11:31:54.299: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014805ms
    Apr  6 11:31:54.299: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:31:56.305: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011508954s
    Apr  6 11:31:56.305: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Apr  6 11:31:56.305: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/06/23 11:31:56.309
    Apr  6 11:31:56.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.3.220 http://127.0.0.1:54323/hostname] Namespace:hostport-433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:31:56.310: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:31:56.311: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:31:56.311: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/hostport-433/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.250.3.220+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.220, port: 54323 04/06/23 11:31:56.845
    Apr  6 11:31:56.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.3.220:54323/hostname] Namespace:hostport-433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:31:56.845: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:31:56.846: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:31:56.846: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/hostport-433/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.250.3.220%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.220, port: 54323 UDP 04/06/23 11:31:57.168
    Apr  6 11:31:57.169: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.250.3.220 54323] Namespace:hostport-433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:31:57.169: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:31:57.170: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:31:57.170: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/hostport-433/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.250.3.220+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Apr  6 11:32:02.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-433" for this suite. 04/06/23 11:32:02.588
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:32:02.597
Apr  6 11:32:02.597: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename proxy 04/06/23 11:32:02.599
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:32:02.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:32:02.659
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Apr  6 11:32:02.667: INFO: Creating pod...
Apr  6 11:32:02.683: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9255" to be "running"
Apr  6 11:32:02.688: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748595ms
Apr  6 11:32:04.696: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012235688s
Apr  6 11:32:04.696: INFO: Pod "agnhost" satisfied condition "running"
Apr  6 11:32:04.696: INFO: Creating service...
Apr  6 11:32:04.709: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/DELETE
Apr  6 11:32:04.813: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr  6 11:32:04.813: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/GET
Apr  6 11:32:04.858: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr  6 11:32:04.858: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/HEAD
Apr  6 11:32:04.865: INFO: http.Client request:HEAD | StatusCode:200
Apr  6 11:32:04.865: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/OPTIONS
Apr  6 11:32:04.873: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr  6 11:32:04.873: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/PATCH
Apr  6 11:32:04.880: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr  6 11:32:04.880: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/POST
Apr  6 11:32:04.889: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr  6 11:32:04.889: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/PUT
Apr  6 11:32:04.905: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr  6 11:32:04.905: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/DELETE
Apr  6 11:32:04.913: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr  6 11:32:04.913: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/GET
Apr  6 11:32:04.921: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr  6 11:32:04.921: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/HEAD
Apr  6 11:32:04.928: INFO: http.Client request:HEAD | StatusCode:200
Apr  6 11:32:04.928: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/OPTIONS
Apr  6 11:32:04.935: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr  6 11:32:04.935: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/PATCH
Apr  6 11:32:04.941: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr  6 11:32:04.942: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/POST
Apr  6 11:32:04.948: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr  6 11:32:04.948: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/PUT
Apr  6 11:32:04.964: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Apr  6 11:32:04.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9255" for this suite. 04/06/23 11:32:04.981
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":274,"skipped":5038,"failed":0}
------------------------------
â€¢ [2.411 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:32:02.597
    Apr  6 11:32:02.597: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename proxy 04/06/23 11:32:02.599
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:32:02.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:32:02.659
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Apr  6 11:32:02.667: INFO: Creating pod...
    Apr  6 11:32:02.683: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9255" to be "running"
    Apr  6 11:32:02.688: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748595ms
    Apr  6 11:32:04.696: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012235688s
    Apr  6 11:32:04.696: INFO: Pod "agnhost" satisfied condition "running"
    Apr  6 11:32:04.696: INFO: Creating service...
    Apr  6 11:32:04.709: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/DELETE
    Apr  6 11:32:04.813: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr  6 11:32:04.813: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/GET
    Apr  6 11:32:04.858: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr  6 11:32:04.858: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/HEAD
    Apr  6 11:32:04.865: INFO: http.Client request:HEAD | StatusCode:200
    Apr  6 11:32:04.865: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/OPTIONS
    Apr  6 11:32:04.873: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr  6 11:32:04.873: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/PATCH
    Apr  6 11:32:04.880: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr  6 11:32:04.880: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/POST
    Apr  6 11:32:04.889: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr  6 11:32:04.889: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/pods/agnhost/proxy/some/path/with/PUT
    Apr  6 11:32:04.905: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr  6 11:32:04.905: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/DELETE
    Apr  6 11:32:04.913: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr  6 11:32:04.913: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/GET
    Apr  6 11:32:04.921: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr  6 11:32:04.921: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/HEAD
    Apr  6 11:32:04.928: INFO: http.Client request:HEAD | StatusCode:200
    Apr  6 11:32:04.928: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/OPTIONS
    Apr  6 11:32:04.935: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr  6 11:32:04.935: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/PATCH
    Apr  6 11:32:04.941: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr  6 11:32:04.942: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/POST
    Apr  6 11:32:04.948: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr  6 11:32:04.948: INFO: Starting http.Client for https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/proxy-9255/services/test-service/proxy/some/path/with/PUT
    Apr  6 11:32:04.964: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Apr  6 11:32:04.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9255" for this suite. 04/06/23 11:32:04.981
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:32:05.009
Apr  6 11:32:05.009: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 11:32:05.01
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:32:05.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:32:05.034
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-9653/secret-test-929c3667-17df-467e-b740-ce581d3b27b2 04/06/23 11:32:05.04
STEP: Creating a pod to test consume secrets 04/06/23 11:32:05.047
Apr  6 11:32:05.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2" in namespace "secrets-9653" to be "Succeeded or Failed"
Apr  6 11:32:05.069: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.896855ms
Apr  6 11:32:07.076: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013851103s
Apr  6 11:32:09.078: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01541015s
STEP: Saw pod success 04/06/23 11:32:09.078
Apr  6 11:32:09.078: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2" satisfied condition "Succeeded or Failed"
Apr  6 11:32:09.082: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2 container env-test: <nil>
STEP: delete the pod 04/06/23 11:32:09.092
Apr  6 11:32:09.114: INFO: Waiting for pod pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2 to disappear
Apr  6 11:32:09.128: INFO: Pod pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Apr  6 11:32:09.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9653" for this suite. 04/06/23 11:32:09.137
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":275,"skipped":5039,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:32:05.009
    Apr  6 11:32:05.009: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 11:32:05.01
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:32:05.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:32:05.034
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-9653/secret-test-929c3667-17df-467e-b740-ce581d3b27b2 04/06/23 11:32:05.04
    STEP: Creating a pod to test consume secrets 04/06/23 11:32:05.047
    Apr  6 11:32:05.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2" in namespace "secrets-9653" to be "Succeeded or Failed"
    Apr  6 11:32:05.069: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.896855ms
    Apr  6 11:32:07.076: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013851103s
    Apr  6 11:32:09.078: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01541015s
    STEP: Saw pod success 04/06/23 11:32:09.078
    Apr  6 11:32:09.078: INFO: Pod "pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2" satisfied condition "Succeeded or Failed"
    Apr  6 11:32:09.082: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2 container env-test: <nil>
    STEP: delete the pod 04/06/23 11:32:09.092
    Apr  6 11:32:09.114: INFO: Waiting for pod pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2 to disappear
    Apr  6 11:32:09.128: INFO: Pod pod-configmaps-6cc6661a-141d-473a-9cee-d42312e95af2 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 11:32:09.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9653" for this suite. 04/06/23 11:32:09.137
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:32:09.147
Apr  6 11:32:09.147: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-probe 04/06/23 11:32:09.148
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:32:09.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:32:09.169
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 in namespace container-probe-6225 04/06/23 11:32:09.175
Apr  6 11:32:09.184: INFO: Waiting up to 5m0s for pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453" in namespace "container-probe-6225" to be "not pending"
Apr  6 11:32:09.189: INFO: Pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453": Phase="Pending", Reason="", readiness=false. Elapsed: 4.955738ms
Apr  6 11:32:11.210: INFO: Pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453": Phase="Running", Reason="", readiness=true. Elapsed: 2.02602219s
Apr  6 11:32:11.210: INFO: Pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453" satisfied condition "not pending"
Apr  6 11:32:11.210: INFO: Started pod busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 in namespace container-probe-6225
STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 11:32:11.21
Apr  6 11:32:11.216: INFO: Initial restart count of pod busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 is 0
Apr  6 11:33:01.487: INFO: Restart count of pod container-probe-6225/busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 is now 1 (50.270872849s elapsed)
STEP: deleting the pod 04/06/23 11:33:01.487
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Apr  6 11:33:01.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6225" for this suite. 04/06/23 11:33:01.522
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":276,"skipped":5041,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.381 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:32:09.147
    Apr  6 11:32:09.147: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-probe 04/06/23 11:32:09.148
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:32:09.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:32:09.169
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 in namespace container-probe-6225 04/06/23 11:32:09.175
    Apr  6 11:32:09.184: INFO: Waiting up to 5m0s for pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453" in namespace "container-probe-6225" to be "not pending"
    Apr  6 11:32:09.189: INFO: Pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453": Phase="Pending", Reason="", readiness=false. Elapsed: 4.955738ms
    Apr  6 11:32:11.210: INFO: Pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453": Phase="Running", Reason="", readiness=true. Elapsed: 2.02602219s
    Apr  6 11:32:11.210: INFO: Pod "busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453" satisfied condition "not pending"
    Apr  6 11:32:11.210: INFO: Started pod busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 in namespace container-probe-6225
    STEP: checking the pod's current state and verifying that restartCount is present 04/06/23 11:32:11.21
    Apr  6 11:32:11.216: INFO: Initial restart count of pod busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 is 0
    Apr  6 11:33:01.487: INFO: Restart count of pod container-probe-6225/busybox-4cae7834-234b-41b9-bcad-cc8c3ce48453 is now 1 (50.270872849s elapsed)
    STEP: deleting the pod 04/06/23 11:33:01.487
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Apr  6 11:33:01.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6225" for this suite. 04/06/23 11:33:01.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:33:01.529
Apr  6 11:33:01.529: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 11:33:01.53
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:33:01.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:33:01.56
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2248 04/06/23 11:33:01.567
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 04/06/23 11:33:01.573
Apr  6 11:33:01.584: INFO: Found 0 stateful pods, waiting for 3
Apr  6 11:33:11.594: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:33:11.594: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:33:11.594: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:33:11.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:33:12.134: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:33:12.134: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:33:12.134: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 04/06/23 11:33:22.163
Apr  6 11:33:22.198: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/06/23 11:33:22.198
STEP: Updating Pods in reverse ordinal order 04/06/23 11:33:32.237
Apr  6 11:33:32.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:33:32.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 11:33:32.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:33:32.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 11:33:42.853: INFO: Waiting for StatefulSet statefulset-2248/ss2 to complete update
Apr  6 11:33:42.853: INFO: Waiting for Pod statefulset-2248/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Rolling back to a previous revision 04/06/23 11:33:52.872
Apr  6 11:33:52.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:33:53.711: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:33:53.711: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:33:53.711: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:34:03.767: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 04/06/23 11:34:13.796
Apr  6 11:34:13.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:34:14.465: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 11:34:14.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:34:14.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 11:34:24.506: INFO: Deleting all statefulset in ns statefulset-2248
Apr  6 11:34:24.510: INFO: Scaling statefulset ss2 to 0
Apr  6 11:34:34.536: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:34:34.540: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 11:34:34.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2248" for this suite. 04/06/23 11:34:34.568
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":277,"skipped":5046,"failed":0}
------------------------------
â€¢ [SLOW TEST] [93.046 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:33:01.529
    Apr  6 11:33:01.529: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 11:33:01.53
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:33:01.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:33:01.56
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2248 04/06/23 11:33:01.567
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 04/06/23 11:33:01.573
    Apr  6 11:33:01.584: INFO: Found 0 stateful pods, waiting for 3
    Apr  6 11:33:11.594: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:33:11.594: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:33:11.594: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:33:11.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:33:12.134: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:33:12.134: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:33:12.134: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 04/06/23 11:33:22.163
    Apr  6 11:33:22.198: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/06/23 11:33:22.198
    STEP: Updating Pods in reverse ordinal order 04/06/23 11:33:32.237
    Apr  6 11:33:32.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:33:32.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr  6 11:33:32.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:33:32.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr  6 11:33:42.853: INFO: Waiting for StatefulSet statefulset-2248/ss2 to complete update
    Apr  6 11:33:42.853: INFO: Waiting for Pod statefulset-2248/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Rolling back to a previous revision 04/06/23 11:33:52.872
    Apr  6 11:33:52.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:33:53.711: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:33:53.711: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:33:53.711: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:34:03.767: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 04/06/23 11:34:13.796
    Apr  6 11:34:13.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-2248 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:34:14.465: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr  6 11:34:14.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:34:14.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 11:34:24.506: INFO: Deleting all statefulset in ns statefulset-2248
    Apr  6 11:34:24.510: INFO: Scaling statefulset ss2 to 0
    Apr  6 11:34:34.536: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:34:34.540: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 11:34:34.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2248" for this suite. 04/06/23 11:34:34.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:34:34.576
Apr  6 11:34:34.577: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:34:34.578
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:34.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:34.611
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-9950e29d-d44a-47a3-a668-c1e24ef26cac 04/06/23 11:34:34.629
STEP: Creating a pod to test consume secrets 04/06/23 11:34:34.635
Apr  6 11:34:34.662: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5" in namespace "projected-1770" to be "Succeeded or Failed"
Apr  6 11:34:34.667: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.117944ms
Apr  6 11:34:36.674: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012531431s
Apr  6 11:34:38.676: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01433647s
STEP: Saw pod success 04/06/23 11:34:38.676
Apr  6 11:34:38.676: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5" satisfied condition "Succeeded or Failed"
Apr  6 11:34:38.686: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/06/23 11:34:38.707
Apr  6 11:34:38.721: INFO: Waiting for pod pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5 to disappear
Apr  6 11:34:38.737: INFO: Pod pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Apr  6 11:34:38.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1770" for this suite. 04/06/23 11:34:38.753
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":278,"skipped":5071,"failed":0}
------------------------------
â€¢ [4.185 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:34:34.576
    Apr  6 11:34:34.577: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:34:34.578
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:34.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:34.611
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-9950e29d-d44a-47a3-a668-c1e24ef26cac 04/06/23 11:34:34.629
    STEP: Creating a pod to test consume secrets 04/06/23 11:34:34.635
    Apr  6 11:34:34.662: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5" in namespace "projected-1770" to be "Succeeded or Failed"
    Apr  6 11:34:34.667: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.117944ms
    Apr  6 11:34:36.674: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012531431s
    Apr  6 11:34:38.676: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01433647s
    STEP: Saw pod success 04/06/23 11:34:38.676
    Apr  6 11:34:38.676: INFO: Pod "pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5" satisfied condition "Succeeded or Failed"
    Apr  6 11:34:38.686: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:34:38.707
    Apr  6 11:34:38.721: INFO: Waiting for pod pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5 to disappear
    Apr  6 11:34:38.737: INFO: Pod pod-projected-secrets-61bd8cbc-8c83-42fc-8be6-7cdb87d5d4f5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Apr  6 11:34:38.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1770" for this suite. 04/06/23 11:34:38.753
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:34:38.762
Apr  6 11:34:38.762: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename discovery 04/06/23 11:34:38.764
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:38.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:38.793
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 04/06/23 11:34:38.803
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Apr  6 11:34:39.498: INFO: Checking APIGroup: apiregistration.k8s.io
Apr  6 11:34:39.515: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr  6 11:34:39.516: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Apr  6 11:34:39.516: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr  6 11:34:39.516: INFO: Checking APIGroup: apps
Apr  6 11:34:39.520: INFO: PreferredVersion.GroupVersion: apps/v1
Apr  6 11:34:39.520: INFO: Versions found [{apps/v1 v1}]
Apr  6 11:34:39.520: INFO: apps/v1 matches apps/v1
Apr  6 11:34:39.520: INFO: Checking APIGroup: events.k8s.io
Apr  6 11:34:39.526: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr  6 11:34:39.526: INFO: Versions found [{events.k8s.io/v1 v1}]
Apr  6 11:34:39.526: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr  6 11:34:39.526: INFO: Checking APIGroup: authentication.k8s.io
Apr  6 11:34:39.529: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr  6 11:34:39.529: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Apr  6 11:34:39.529: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr  6 11:34:39.529: INFO: Checking APIGroup: authorization.k8s.io
Apr  6 11:34:39.532: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr  6 11:34:39.532: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Apr  6 11:34:39.532: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr  6 11:34:39.532: INFO: Checking APIGroup: autoscaling
Apr  6 11:34:39.534: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Apr  6 11:34:39.534: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Apr  6 11:34:39.534: INFO: autoscaling/v2 matches autoscaling/v2
Apr  6 11:34:39.534: INFO: Checking APIGroup: batch
Apr  6 11:34:39.537: INFO: PreferredVersion.GroupVersion: batch/v1
Apr  6 11:34:39.537: INFO: Versions found [{batch/v1 v1}]
Apr  6 11:34:39.537: INFO: batch/v1 matches batch/v1
Apr  6 11:34:39.537: INFO: Checking APIGroup: certificates.k8s.io
Apr  6 11:34:39.540: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr  6 11:34:39.540: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Apr  6 11:34:39.540: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr  6 11:34:39.540: INFO: Checking APIGroup: networking.k8s.io
Apr  6 11:34:39.545: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr  6 11:34:39.545: INFO: Versions found [{networking.k8s.io/v1 v1}]
Apr  6 11:34:39.545: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr  6 11:34:39.545: INFO: Checking APIGroup: policy
Apr  6 11:34:39.548: INFO: PreferredVersion.GroupVersion: policy/v1
Apr  6 11:34:39.548: INFO: Versions found [{policy/v1 v1}]
Apr  6 11:34:39.548: INFO: policy/v1 matches policy/v1
Apr  6 11:34:39.548: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr  6 11:34:39.552: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr  6 11:34:39.552: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Apr  6 11:34:39.552: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr  6 11:34:39.552: INFO: Checking APIGroup: storage.k8s.io
Apr  6 11:34:39.555: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr  6 11:34:39.555: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr  6 11:34:39.555: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr  6 11:34:39.555: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr  6 11:34:39.558: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr  6 11:34:39.558: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Apr  6 11:34:39.558: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr  6 11:34:39.558: INFO: Checking APIGroup: apiextensions.k8s.io
Apr  6 11:34:39.561: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr  6 11:34:39.561: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Apr  6 11:34:39.561: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr  6 11:34:39.561: INFO: Checking APIGroup: scheduling.k8s.io
Apr  6 11:34:39.564: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr  6 11:34:39.564: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Apr  6 11:34:39.564: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr  6 11:34:39.564: INFO: Checking APIGroup: coordination.k8s.io
Apr  6 11:34:39.567: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr  6 11:34:39.567: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Apr  6 11:34:39.567: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr  6 11:34:39.567: INFO: Checking APIGroup: node.k8s.io
Apr  6 11:34:39.570: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr  6 11:34:39.570: INFO: Versions found [{node.k8s.io/v1 v1}]
Apr  6 11:34:39.570: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr  6 11:34:39.570: INFO: Checking APIGroup: discovery.k8s.io
Apr  6 11:34:39.573: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Apr  6 11:34:39.573: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Apr  6 11:34:39.573: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Apr  6 11:34:39.573: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr  6 11:34:39.575: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Apr  6 11:34:39.575: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Apr  6 11:34:39.575: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Apr  6 11:34:39.575: INFO: Checking APIGroup: crd.projectcalico.org
Apr  6 11:34:39.578: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Apr  6 11:34:39.578: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Apr  6 11:34:39.578: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Apr  6 11:34:39.578: INFO: Checking APIGroup: snapshot.storage.k8s.io
Apr  6 11:34:39.580: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Apr  6 11:34:39.580: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Apr  6 11:34:39.580: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Apr  6 11:34:39.580: INFO: Checking APIGroup: metrics.k8s.io
Apr  6 11:34:39.583: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Apr  6 11:34:39.583: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Apr  6 11:34:39.583: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Apr  6 11:34:39.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1662" for this suite. 04/06/23 11:34:39.594
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":279,"skipped":5074,"failed":0}
------------------------------
â€¢ [0.839 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:34:38.762
    Apr  6 11:34:38.762: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename discovery 04/06/23 11:34:38.764
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:38.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:38.793
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 04/06/23 11:34:38.803
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Apr  6 11:34:39.498: INFO: Checking APIGroup: apiregistration.k8s.io
    Apr  6 11:34:39.515: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Apr  6 11:34:39.516: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Apr  6 11:34:39.516: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Apr  6 11:34:39.516: INFO: Checking APIGroup: apps
    Apr  6 11:34:39.520: INFO: PreferredVersion.GroupVersion: apps/v1
    Apr  6 11:34:39.520: INFO: Versions found [{apps/v1 v1}]
    Apr  6 11:34:39.520: INFO: apps/v1 matches apps/v1
    Apr  6 11:34:39.520: INFO: Checking APIGroup: events.k8s.io
    Apr  6 11:34:39.526: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Apr  6 11:34:39.526: INFO: Versions found [{events.k8s.io/v1 v1}]
    Apr  6 11:34:39.526: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Apr  6 11:34:39.526: INFO: Checking APIGroup: authentication.k8s.io
    Apr  6 11:34:39.529: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Apr  6 11:34:39.529: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Apr  6 11:34:39.529: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Apr  6 11:34:39.529: INFO: Checking APIGroup: authorization.k8s.io
    Apr  6 11:34:39.532: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Apr  6 11:34:39.532: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Apr  6 11:34:39.532: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Apr  6 11:34:39.532: INFO: Checking APIGroup: autoscaling
    Apr  6 11:34:39.534: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Apr  6 11:34:39.534: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Apr  6 11:34:39.534: INFO: autoscaling/v2 matches autoscaling/v2
    Apr  6 11:34:39.534: INFO: Checking APIGroup: batch
    Apr  6 11:34:39.537: INFO: PreferredVersion.GroupVersion: batch/v1
    Apr  6 11:34:39.537: INFO: Versions found [{batch/v1 v1}]
    Apr  6 11:34:39.537: INFO: batch/v1 matches batch/v1
    Apr  6 11:34:39.537: INFO: Checking APIGroup: certificates.k8s.io
    Apr  6 11:34:39.540: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Apr  6 11:34:39.540: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Apr  6 11:34:39.540: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Apr  6 11:34:39.540: INFO: Checking APIGroup: networking.k8s.io
    Apr  6 11:34:39.545: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Apr  6 11:34:39.545: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Apr  6 11:34:39.545: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Apr  6 11:34:39.545: INFO: Checking APIGroup: policy
    Apr  6 11:34:39.548: INFO: PreferredVersion.GroupVersion: policy/v1
    Apr  6 11:34:39.548: INFO: Versions found [{policy/v1 v1}]
    Apr  6 11:34:39.548: INFO: policy/v1 matches policy/v1
    Apr  6 11:34:39.548: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Apr  6 11:34:39.552: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Apr  6 11:34:39.552: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Apr  6 11:34:39.552: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Apr  6 11:34:39.552: INFO: Checking APIGroup: storage.k8s.io
    Apr  6 11:34:39.555: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Apr  6 11:34:39.555: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Apr  6 11:34:39.555: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Apr  6 11:34:39.555: INFO: Checking APIGroup: admissionregistration.k8s.io
    Apr  6 11:34:39.558: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Apr  6 11:34:39.558: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Apr  6 11:34:39.558: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Apr  6 11:34:39.558: INFO: Checking APIGroup: apiextensions.k8s.io
    Apr  6 11:34:39.561: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Apr  6 11:34:39.561: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Apr  6 11:34:39.561: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Apr  6 11:34:39.561: INFO: Checking APIGroup: scheduling.k8s.io
    Apr  6 11:34:39.564: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Apr  6 11:34:39.564: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Apr  6 11:34:39.564: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Apr  6 11:34:39.564: INFO: Checking APIGroup: coordination.k8s.io
    Apr  6 11:34:39.567: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Apr  6 11:34:39.567: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Apr  6 11:34:39.567: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Apr  6 11:34:39.567: INFO: Checking APIGroup: node.k8s.io
    Apr  6 11:34:39.570: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Apr  6 11:34:39.570: INFO: Versions found [{node.k8s.io/v1 v1}]
    Apr  6 11:34:39.570: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Apr  6 11:34:39.570: INFO: Checking APIGroup: discovery.k8s.io
    Apr  6 11:34:39.573: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Apr  6 11:34:39.573: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Apr  6 11:34:39.573: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Apr  6 11:34:39.573: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Apr  6 11:34:39.575: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Apr  6 11:34:39.575: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Apr  6 11:34:39.575: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Apr  6 11:34:39.575: INFO: Checking APIGroup: crd.projectcalico.org
    Apr  6 11:34:39.578: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Apr  6 11:34:39.578: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Apr  6 11:34:39.578: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Apr  6 11:34:39.578: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Apr  6 11:34:39.580: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Apr  6 11:34:39.580: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Apr  6 11:34:39.580: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Apr  6 11:34:39.580: INFO: Checking APIGroup: metrics.k8s.io
    Apr  6 11:34:39.583: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Apr  6 11:34:39.583: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Apr  6 11:34:39.583: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Apr  6 11:34:39.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-1662" for this suite. 04/06/23 11:34:39.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:34:39.604
Apr  6 11:34:39.604: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename runtimeclass 04/06/23 11:34:39.607
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:39.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:39.635
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 04/06/23 11:34:39.642
STEP: getting /apis/node.k8s.io 04/06/23 11:34:39.648
STEP: getting /apis/node.k8s.io/v1 04/06/23 11:34:39.651
STEP: creating 04/06/23 11:34:39.668
STEP: watching 04/06/23 11:34:39.705
Apr  6 11:34:39.705: INFO: starting watch
STEP: getting 04/06/23 11:34:39.712
STEP: listing 04/06/23 11:34:39.717
STEP: patching 04/06/23 11:34:39.721
STEP: updating 04/06/23 11:34:39.729
Apr  6 11:34:39.735: INFO: waiting for watch events with expected annotations
STEP: deleting 04/06/23 11:34:39.745
STEP: deleting a collection 04/06/23 11:34:39.771
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Apr  6 11:34:39.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5171" for this suite. 04/06/23 11:34:39.804
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":280,"skipped":5142,"failed":0}
------------------------------
â€¢ [0.207 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:34:39.604
    Apr  6 11:34:39.604: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename runtimeclass 04/06/23 11:34:39.607
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:39.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:39.635
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 04/06/23 11:34:39.642
    STEP: getting /apis/node.k8s.io 04/06/23 11:34:39.648
    STEP: getting /apis/node.k8s.io/v1 04/06/23 11:34:39.651
    STEP: creating 04/06/23 11:34:39.668
    STEP: watching 04/06/23 11:34:39.705
    Apr  6 11:34:39.705: INFO: starting watch
    STEP: getting 04/06/23 11:34:39.712
    STEP: listing 04/06/23 11:34:39.717
    STEP: patching 04/06/23 11:34:39.721
    STEP: updating 04/06/23 11:34:39.729
    Apr  6 11:34:39.735: INFO: waiting for watch events with expected annotations
    STEP: deleting 04/06/23 11:34:39.745
    STEP: deleting a collection 04/06/23 11:34:39.771
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Apr  6 11:34:39.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5171" for this suite. 04/06/23 11:34:39.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:34:39.82
Apr  6 11:34:39.821: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:34:39.822
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:39.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:39.849
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 04/06/23 11:34:39.858
Apr  6 11:34:39.873: INFO: Waiting up to 5m0s for pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3" in namespace "emptydir-8611" to be "Succeeded or Failed"
Apr  6 11:34:39.881: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.853435ms
Apr  6 11:34:41.900: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027298682s
Apr  6 11:34:43.904: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030986537s
STEP: Saw pod success 04/06/23 11:34:43.904
Apr  6 11:34:43.904: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3" satisfied condition "Succeeded or Failed"
Apr  6 11:34:43.913: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3 container test-container: <nil>
STEP: delete the pod 04/06/23 11:34:43.975
Apr  6 11:34:43.989: INFO: Waiting for pod pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3 to disappear
Apr  6 11:34:43.994: INFO: Pod pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:34:43.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8611" for this suite. 04/06/23 11:34:44.005
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":281,"skipped":5178,"failed":0}
------------------------------
â€¢ [4.201 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:34:39.82
    Apr  6 11:34:39.821: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:34:39.822
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:39.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:39.849
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/06/23 11:34:39.858
    Apr  6 11:34:39.873: INFO: Waiting up to 5m0s for pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3" in namespace "emptydir-8611" to be "Succeeded or Failed"
    Apr  6 11:34:39.881: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.853435ms
    Apr  6 11:34:41.900: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027298682s
    Apr  6 11:34:43.904: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030986537s
    STEP: Saw pod success 04/06/23 11:34:43.904
    Apr  6 11:34:43.904: INFO: Pod "pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3" satisfied condition "Succeeded or Failed"
    Apr  6 11:34:43.913: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:34:43.975
    Apr  6 11:34:43.989: INFO: Waiting for pod pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3 to disappear
    Apr  6 11:34:43.994: INFO: Pod pod-18bd8c24-001b-4ae0-bf23-5c9753f21cf3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:34:43.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8611" for this suite. 04/06/23 11:34:44.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:34:44.022
Apr  6 11:34:44.022: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 11:34:44.023
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:44.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:44.048
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 11:34:44.064
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:34:44.644
STEP: Deploying the webhook pod 04/06/23 11:34:44.653
STEP: Wait for the deployment to be ready 04/06/23 11:34:44.663
Apr  6 11:34:44.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 11:34:46.693
STEP: Verifying the service has paired with the endpoint 04/06/23 11:34:46.706
Apr  6 11:34:47.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Apr  6 11:34:47.721: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1575-crds.webhook.example.com via the AdmissionRegistration API 04/06/23 11:34:48.243
STEP: Creating a custom resource that should be mutated by the webhook 04/06/23 11:34:48.364
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:34:51.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8236" for this suite. 04/06/23 11:34:51.046
STEP: Destroying namespace "webhook-8236-markers" for this suite. 04/06/23 11:34:51.051
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":282,"skipped":5184,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.081 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:34:44.022
    Apr  6 11:34:44.022: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 11:34:44.023
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:44.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:44.048
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 11:34:44.064
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:34:44.644
    STEP: Deploying the webhook pod 04/06/23 11:34:44.653
    STEP: Wait for the deployment to be ready 04/06/23 11:34:44.663
    Apr  6 11:34:44.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 11:34:46.693
    STEP: Verifying the service has paired with the endpoint 04/06/23 11:34:46.706
    Apr  6 11:34:47.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Apr  6 11:34:47.721: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1575-crds.webhook.example.com via the AdmissionRegistration API 04/06/23 11:34:48.243
    STEP: Creating a custom resource that should be mutated by the webhook 04/06/23 11:34:48.364
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:34:51.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8236" for this suite. 04/06/23 11:34:51.046
    STEP: Destroying namespace "webhook-8236-markers" for this suite. 04/06/23 11:34:51.051
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:34:51.104
Apr  6 11:34:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:34:51.107
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:51.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:51.141
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 04/06/23 11:34:51.148
STEP: Creating a ResourceQuota 04/06/23 11:34:56.155
STEP: Ensuring resource quota status is calculated 04/06/23 11:34:56.161
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:34:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-968" for this suite. 04/06/23 11:34:58.179
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":283,"skipped":5192,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.080 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:34:51.104
    Apr  6 11:34:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:34:51.107
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:51.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:51.141
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 04/06/23 11:34:51.148
    STEP: Creating a ResourceQuota 04/06/23 11:34:56.155
    STEP: Ensuring resource quota status is calculated 04/06/23 11:34:56.161
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:34:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-968" for this suite. 04/06/23 11:34:58.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:34:58.185
Apr  6 11:34:58.185: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename prestop 04/06/23 11:34:58.187
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:58.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:58.225
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9985 04/06/23 11:34:58.229
STEP: Waiting for pods to come up. 04/06/23 11:34:58.242
Apr  6 11:34:58.242: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9985" to be "running"
Apr  6 11:34:58.254: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.290606ms
Apr  6 11:35:00.261: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.018188546s
Apr  6 11:35:00.261: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9985 04/06/23 11:35:00.266
Apr  6 11:35:00.279: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9985" to be "running"
Apr  6 11:35:00.283: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.545029ms
Apr  6 11:35:02.291: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.012388163s
Apr  6 11:35:02.291: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 04/06/23 11:35:02.291
Apr  6 11:35:07.414: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 04/06/23 11:35:07.414
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Apr  6 11:35:07.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9985" for this suite. 04/06/23 11:35:07.434
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":284,"skipped":5202,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.256 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:34:58.185
    Apr  6 11:34:58.185: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename prestop 04/06/23 11:34:58.187
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:34:58.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:34:58.225
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9985 04/06/23 11:34:58.229
    STEP: Waiting for pods to come up. 04/06/23 11:34:58.242
    Apr  6 11:34:58.242: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9985" to be "running"
    Apr  6 11:34:58.254: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.290606ms
    Apr  6 11:35:00.261: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.018188546s
    Apr  6 11:35:00.261: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9985 04/06/23 11:35:00.266
    Apr  6 11:35:00.279: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9985" to be "running"
    Apr  6 11:35:00.283: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.545029ms
    Apr  6 11:35:02.291: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.012388163s
    Apr  6 11:35:02.291: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 04/06/23 11:35:02.291
    Apr  6 11:35:07.414: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 04/06/23 11:35:07.414
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Apr  6 11:35:07.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9985" for this suite. 04/06/23 11:35:07.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:35:07.442
Apr  6 11:35:07.442: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir-wrapper 04/06/23 11:35:07.459
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:07.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:07.491
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 04/06/23 11:35:07.499
STEP: Creating RC which spawns configmap-volume pods 04/06/23 11:35:07.749
Apr  6 11:35:07.817: INFO: Pod name wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784: Found 1 pods out of 5
Apr  6 11:35:12.836: INFO: Pod name wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/06/23 11:35:12.836
Apr  6 11:35:12.836: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-k26xw" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:12.844: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-k26xw": Phase="Running", Reason="", readiness=true. Elapsed: 7.908669ms
Apr  6 11:35:12.844: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-k26xw" satisfied condition "running"
Apr  6 11:35:12.844: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lcs5f" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:12.866: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lcs5f": Phase="Running", Reason="", readiness=true. Elapsed: 21.170034ms
Apr  6 11:35:12.866: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lcs5f" satisfied condition "running"
Apr  6 11:35:12.866: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lr6bb" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:12.876: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lr6bb": Phase="Running", Reason="", readiness=true. Elapsed: 10.532148ms
Apr  6 11:35:12.876: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lr6bb" satisfied condition "running"
Apr  6 11:35:12.876: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-p9bk7" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:12.884: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-p9bk7": Phase="Running", Reason="", readiness=true. Elapsed: 8.281902ms
Apr  6 11:35:12.884: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-p9bk7" satisfied condition "running"
Apr  6 11:35:12.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-snpqt" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:12.907: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-snpqt": Phase="Running", Reason="", readiness=true. Elapsed: 22.640859ms
Apr  6 11:35:12.907: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-snpqt" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784 in namespace emptydir-wrapper-6015, will wait for the garbage collector to delete the pods 04/06/23 11:35:12.907
Apr  6 11:35:12.975: INFO: Deleting ReplicationController wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784 took: 8.133646ms
Apr  6 11:35:13.075: INFO: Terminating ReplicationController wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784 pods took: 100.376176ms
STEP: Creating RC which spawns configmap-volume pods 04/06/23 11:35:14.983
Apr  6 11:35:15.014: INFO: Pod name wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b: Found 0 pods out of 5
Apr  6 11:35:20.028: INFO: Pod name wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/06/23 11:35:20.028
Apr  6 11:35:20.028: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-qmsff" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:20.036: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-qmsff": Phase="Running", Reason="", readiness=true. Elapsed: 7.938612ms
Apr  6 11:35:20.036: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-qmsff" satisfied condition "running"
Apr  6 11:35:20.036: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-t6htj" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:20.041: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-t6htj": Phase="Running", Reason="", readiness=true. Elapsed: 4.897192ms
Apr  6 11:35:20.042: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-t6htj" satisfied condition "running"
Apr  6 11:35:20.042: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-tgv28" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:20.052: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-tgv28": Phase="Running", Reason="", readiness=true. Elapsed: 10.692404ms
Apr  6 11:35:20.052: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-tgv28" satisfied condition "running"
Apr  6 11:35:20.052: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-v2pxt" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:20.057: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-v2pxt": Phase="Running", Reason="", readiness=true. Elapsed: 4.99402ms
Apr  6 11:35:20.057: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-v2pxt" satisfied condition "running"
Apr  6 11:35:20.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-x95rn" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:20.063: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-x95rn": Phase="Running", Reason="", readiness=true. Elapsed: 5.857491ms
Apr  6 11:35:20.063: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-x95rn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b in namespace emptydir-wrapper-6015, will wait for the garbage collector to delete the pods 04/06/23 11:35:20.063
Apr  6 11:35:20.127: INFO: Deleting ReplicationController wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b took: 6.512011ms
Apr  6 11:35:20.227: INFO: Terminating ReplicationController wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b pods took: 100.272728ms
STEP: Creating RC which spawns configmap-volume pods 04/06/23 11:35:21.257
Apr  6 11:35:21.275: INFO: Pod name wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88: Found 0 pods out of 5
Apr  6 11:35:26.306: INFO: Pod name wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/06/23 11:35:26.306
Apr  6 11:35:26.313: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-8pf8d" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:26.326: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-8pf8d": Phase="Running", Reason="", readiness=true. Elapsed: 11.098499ms
Apr  6 11:35:26.326: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-8pf8d" satisfied condition "running"
Apr  6 11:35:26.326: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-kbmc5" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:26.337: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-kbmc5": Phase="Running", Reason="", readiness=true. Elapsed: 10.929122ms
Apr  6 11:35:26.337: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-kbmc5" satisfied condition "running"
Apr  6 11:35:26.337: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-ljjd4" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:26.350: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-ljjd4": Phase="Running", Reason="", readiness=true. Elapsed: 12.30422ms
Apr  6 11:35:26.350: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-ljjd4" satisfied condition "running"
Apr  6 11:35:26.350: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-mkkqc" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:26.355: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-mkkqc": Phase="Running", Reason="", readiness=true. Elapsed: 5.681667ms
Apr  6 11:35:26.355: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-mkkqc" satisfied condition "running"
Apr  6 11:35:26.355: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-x268x" in namespace "emptydir-wrapper-6015" to be "running"
Apr  6 11:35:26.366: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-x268x": Phase="Running", Reason="", readiness=true. Elapsed: 10.329369ms
Apr  6 11:35:26.366: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-x268x" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88 in namespace emptydir-wrapper-6015, will wait for the garbage collector to delete the pods 04/06/23 11:35:26.366
Apr  6 11:35:26.433: INFO: Deleting ReplicationController wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88 took: 8.731464ms
Apr  6 11:35:26.536: INFO: Terminating ReplicationController wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88 pods took: 102.762545ms
STEP: Cleaning up the configMaps 04/06/23 11:35:28.537
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Apr  6 11:35:28.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6015" for this suite. 04/06/23 11:35:28.999
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":285,"skipped":5210,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.564 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:35:07.442
    Apr  6 11:35:07.442: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir-wrapper 04/06/23 11:35:07.459
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:07.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:07.491
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 04/06/23 11:35:07.499
    STEP: Creating RC which spawns configmap-volume pods 04/06/23 11:35:07.749
    Apr  6 11:35:07.817: INFO: Pod name wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784: Found 1 pods out of 5
    Apr  6 11:35:12.836: INFO: Pod name wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/06/23 11:35:12.836
    Apr  6 11:35:12.836: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-k26xw" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:12.844: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-k26xw": Phase="Running", Reason="", readiness=true. Elapsed: 7.908669ms
    Apr  6 11:35:12.844: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-k26xw" satisfied condition "running"
    Apr  6 11:35:12.844: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lcs5f" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:12.866: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lcs5f": Phase="Running", Reason="", readiness=true. Elapsed: 21.170034ms
    Apr  6 11:35:12.866: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lcs5f" satisfied condition "running"
    Apr  6 11:35:12.866: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lr6bb" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:12.876: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lr6bb": Phase="Running", Reason="", readiness=true. Elapsed: 10.532148ms
    Apr  6 11:35:12.876: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-lr6bb" satisfied condition "running"
    Apr  6 11:35:12.876: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-p9bk7" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:12.884: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-p9bk7": Phase="Running", Reason="", readiness=true. Elapsed: 8.281902ms
    Apr  6 11:35:12.884: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-p9bk7" satisfied condition "running"
    Apr  6 11:35:12.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-snpqt" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:12.907: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-snpqt": Phase="Running", Reason="", readiness=true. Elapsed: 22.640859ms
    Apr  6 11:35:12.907: INFO: Pod "wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784-snpqt" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784 in namespace emptydir-wrapper-6015, will wait for the garbage collector to delete the pods 04/06/23 11:35:12.907
    Apr  6 11:35:12.975: INFO: Deleting ReplicationController wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784 took: 8.133646ms
    Apr  6 11:35:13.075: INFO: Terminating ReplicationController wrapped-volume-race-35024492-8430-46f2-9ae4-01303c842784 pods took: 100.376176ms
    STEP: Creating RC which spawns configmap-volume pods 04/06/23 11:35:14.983
    Apr  6 11:35:15.014: INFO: Pod name wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b: Found 0 pods out of 5
    Apr  6 11:35:20.028: INFO: Pod name wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/06/23 11:35:20.028
    Apr  6 11:35:20.028: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-qmsff" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:20.036: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-qmsff": Phase="Running", Reason="", readiness=true. Elapsed: 7.938612ms
    Apr  6 11:35:20.036: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-qmsff" satisfied condition "running"
    Apr  6 11:35:20.036: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-t6htj" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:20.041: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-t6htj": Phase="Running", Reason="", readiness=true. Elapsed: 4.897192ms
    Apr  6 11:35:20.042: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-t6htj" satisfied condition "running"
    Apr  6 11:35:20.042: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-tgv28" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:20.052: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-tgv28": Phase="Running", Reason="", readiness=true. Elapsed: 10.692404ms
    Apr  6 11:35:20.052: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-tgv28" satisfied condition "running"
    Apr  6 11:35:20.052: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-v2pxt" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:20.057: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-v2pxt": Phase="Running", Reason="", readiness=true. Elapsed: 4.99402ms
    Apr  6 11:35:20.057: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-v2pxt" satisfied condition "running"
    Apr  6 11:35:20.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-x95rn" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:20.063: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-x95rn": Phase="Running", Reason="", readiness=true. Elapsed: 5.857491ms
    Apr  6 11:35:20.063: INFO: Pod "wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b-x95rn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b in namespace emptydir-wrapper-6015, will wait for the garbage collector to delete the pods 04/06/23 11:35:20.063
    Apr  6 11:35:20.127: INFO: Deleting ReplicationController wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b took: 6.512011ms
    Apr  6 11:35:20.227: INFO: Terminating ReplicationController wrapped-volume-race-6e9dacc6-ec74-4d84-8322-11d03645912b pods took: 100.272728ms
    STEP: Creating RC which spawns configmap-volume pods 04/06/23 11:35:21.257
    Apr  6 11:35:21.275: INFO: Pod name wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88: Found 0 pods out of 5
    Apr  6 11:35:26.306: INFO: Pod name wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/06/23 11:35:26.306
    Apr  6 11:35:26.313: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-8pf8d" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:26.326: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-8pf8d": Phase="Running", Reason="", readiness=true. Elapsed: 11.098499ms
    Apr  6 11:35:26.326: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-8pf8d" satisfied condition "running"
    Apr  6 11:35:26.326: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-kbmc5" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:26.337: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-kbmc5": Phase="Running", Reason="", readiness=true. Elapsed: 10.929122ms
    Apr  6 11:35:26.337: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-kbmc5" satisfied condition "running"
    Apr  6 11:35:26.337: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-ljjd4" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:26.350: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-ljjd4": Phase="Running", Reason="", readiness=true. Elapsed: 12.30422ms
    Apr  6 11:35:26.350: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-ljjd4" satisfied condition "running"
    Apr  6 11:35:26.350: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-mkkqc" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:26.355: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-mkkqc": Phase="Running", Reason="", readiness=true. Elapsed: 5.681667ms
    Apr  6 11:35:26.355: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-mkkqc" satisfied condition "running"
    Apr  6 11:35:26.355: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-x268x" in namespace "emptydir-wrapper-6015" to be "running"
    Apr  6 11:35:26.366: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-x268x": Phase="Running", Reason="", readiness=true. Elapsed: 10.329369ms
    Apr  6 11:35:26.366: INFO: Pod "wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88-x268x" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88 in namespace emptydir-wrapper-6015, will wait for the garbage collector to delete the pods 04/06/23 11:35:26.366
    Apr  6 11:35:26.433: INFO: Deleting ReplicationController wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88 took: 8.731464ms
    Apr  6 11:35:26.536: INFO: Terminating ReplicationController wrapped-volume-race-28ff72e8-2d47-42e8-954c-585b98472d88 pods took: 102.762545ms
    STEP: Cleaning up the configMaps 04/06/23 11:35:28.537
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:35:28.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6015" for this suite. 04/06/23 11:35:28.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:35:29.01
Apr  6 11:35:29.010: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:35:29.011
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:29.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:29.048
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 04/06/23 11:35:29.054
Apr  6 11:35:29.068: INFO: Waiting up to 5m0s for pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364" in namespace "emptydir-8074" to be "Succeeded or Failed"
Apr  6 11:35:29.078: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385438ms
Apr  6 11:35:31.086: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01736449s
Apr  6 11:35:33.085: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017213668s
STEP: Saw pod success 04/06/23 11:35:33.086
Apr  6 11:35:33.086: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364" satisfied condition "Succeeded or Failed"
Apr  6 11:35:33.091: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-d7443cdb-7640-4d8c-9c5e-59255383e364 container test-container: <nil>
STEP: delete the pod 04/06/23 11:35:33.147
Apr  6 11:35:33.158: INFO: Waiting for pod pod-d7443cdb-7640-4d8c-9c5e-59255383e364 to disappear
Apr  6 11:35:33.162: INFO: Pod pod-d7443cdb-7640-4d8c-9c5e-59255383e364 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:35:33.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8074" for this suite. 04/06/23 11:35:33.183
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":286,"skipped":5235,"failed":0}
------------------------------
â€¢ [4.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:35:29.01
    Apr  6 11:35:29.010: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:35:29.011
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:29.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:29.048
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/06/23 11:35:29.054
    Apr  6 11:35:29.068: INFO: Waiting up to 5m0s for pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364" in namespace "emptydir-8074" to be "Succeeded or Failed"
    Apr  6 11:35:29.078: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385438ms
    Apr  6 11:35:31.086: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01736449s
    Apr  6 11:35:33.085: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017213668s
    STEP: Saw pod success 04/06/23 11:35:33.086
    Apr  6 11:35:33.086: INFO: Pod "pod-d7443cdb-7640-4d8c-9c5e-59255383e364" satisfied condition "Succeeded or Failed"
    Apr  6 11:35:33.091: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-d7443cdb-7640-4d8c-9c5e-59255383e364 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:35:33.147
    Apr  6 11:35:33.158: INFO: Waiting for pod pod-d7443cdb-7640-4d8c-9c5e-59255383e364 to disappear
    Apr  6 11:35:33.162: INFO: Pod pod-d7443cdb-7640-4d8c-9c5e-59255383e364 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:35:33.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8074" for this suite. 04/06/23 11:35:33.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:35:33.191
Apr  6 11:35:33.191: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubelet-test 04/06/23 11:35:33.193
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:33.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:33.242
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Apr  6 11:35:33.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7981" for this suite. 04/06/23 11:35:33.285
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":287,"skipped":5259,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:35:33.191
    Apr  6 11:35:33.191: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubelet-test 04/06/23 11:35:33.193
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:33.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:33.242
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Apr  6 11:35:33.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7981" for this suite. 04/06/23 11:35:33.285
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:35:33.294
Apr  6 11:35:33.294: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename taint-multiple-pods 04/06/23 11:35:33.295
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:33.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:33.332
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Apr  6 11:35:33.340: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 11:36:33.474: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Apr  6 11:36:33.480: INFO: Starting informer...
STEP: Starting pods... 04/06/23 11:36:33.48
Apr  6 11:36:33.518: INFO: Pod1 is running on shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk. Tainting Node
Apr  6 11:36:33.734: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9837" to be "running"
Apr  6 11:36:33.740: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040075ms
Apr  6 11:36:35.748: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.0144777s
Apr  6 11:36:35.748: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Apr  6 11:36:35.748: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9837" to be "running"
Apr  6 11:36:35.753: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.563464ms
Apr  6 11:36:35.753: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Apr  6 11:36:35.753: INFO: Pod2 is running on shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk. Tainting Node
STEP: Trying to apply a taint on the Node 04/06/23 11:36:35.753
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:36:35.778
STEP: Waiting for Pod1 and Pod2 to be deleted 04/06/23 11:36:35.787
Apr  6 11:36:42.241: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr  6 11:37:01.617: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:37:01.644
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:37:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9837" for this suite. 04/06/23 11:37:01.657
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":288,"skipped":5262,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.370 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:35:33.294
    Apr  6 11:35:33.294: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename taint-multiple-pods 04/06/23 11:35:33.295
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:35:33.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:35:33.332
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Apr  6 11:35:33.340: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr  6 11:36:33.474: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Apr  6 11:36:33.480: INFO: Starting informer...
    STEP: Starting pods... 04/06/23 11:36:33.48
    Apr  6 11:36:33.518: INFO: Pod1 is running on shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk. Tainting Node
    Apr  6 11:36:33.734: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9837" to be "running"
    Apr  6 11:36:33.740: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040075ms
    Apr  6 11:36:35.748: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.0144777s
    Apr  6 11:36:35.748: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Apr  6 11:36:35.748: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9837" to be "running"
    Apr  6 11:36:35.753: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.563464ms
    Apr  6 11:36:35.753: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Apr  6 11:36:35.753: INFO: Pod2 is running on shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk. Tainting Node
    STEP: Trying to apply a taint on the Node 04/06/23 11:36:35.753
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:36:35.778
    STEP: Waiting for Pod1 and Pod2 to be deleted 04/06/23 11:36:35.787
    Apr  6 11:36:42.241: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Apr  6 11:37:01.617: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/06/23 11:37:01.644
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:37:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-9837" for this suite. 04/06/23 11:37:01.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:37:01.665
Apr  6 11:37:01.665: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename init-container 04/06/23 11:37:01.666
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:37:01.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:37:01.689
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 04/06/23 11:37:01.709
Apr  6 11:37:01.709: INFO: PodSpec: initContainers in spec.initContainers
Apr  6 11:37:49.083: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9778aa84-7eb9-4d8a-8824-4770391ac7a9", GenerateName:"", Namespace:"init-container-7430", SelfLink:"", UID:"2f4d2b3b-e1ab-4be5-82d6-156f412c7699", ResourceVersion:"36187", Generation:0, CreationTimestamp:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"709415663"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"72321803a52b0f71a498ce8fc5b9bf880d9e710bbfa6a7c730602c25ba583f73", "cni.projectcalico.org/podIP":"10.96.2.171/32", "cni.projectcalico.org/podIPs":"10.96.2.171/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f87500), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 6, 11, 37, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f87530), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 6, 11, 37, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f87590), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-gmnj4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002794920), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gmnj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gmnj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gmnj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003b667d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00029cc40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003b66850)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003b66870)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003b66878), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003b6687c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005418d60), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.68", PodIP:"10.96.2.171", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.96.2.171"}}, StartTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00029cd90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00029ce00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://b5e0e79c705e878c1c85c4d5240b5dfd854845a20385bd624a12ce9428fe64f6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027949a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002794980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc003b668ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Apr  6 11:37:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7430" for this suite. 04/06/23 11:37:49.116
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":289,"skipped":5270,"failed":0}
------------------------------
â€¢ [SLOW TEST] [47.457 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:37:01.665
    Apr  6 11:37:01.665: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename init-container 04/06/23 11:37:01.666
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:37:01.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:37:01.689
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 04/06/23 11:37:01.709
    Apr  6 11:37:01.709: INFO: PodSpec: initContainers in spec.initContainers
    Apr  6 11:37:49.083: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9778aa84-7eb9-4d8a-8824-4770391ac7a9", GenerateName:"", Namespace:"init-container-7430", SelfLink:"", UID:"2f4d2b3b-e1ab-4be5-82d6-156f412c7699", ResourceVersion:"36187", Generation:0, CreationTimestamp:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"709415663"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"72321803a52b0f71a498ce8fc5b9bf880d9e710bbfa6a7c730602c25ba583f73", "cni.projectcalico.org/podIP":"10.96.2.171/32", "cni.projectcalico.org/podIPs":"10.96.2.171/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f87500), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 6, 11, 37, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f87530), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 6, 11, 37, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f87590), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-gmnj4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002794920), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gmnj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gmnj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gmnj4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003b667d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00029cc40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003b66850)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003b66870)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003b66878), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003b6687c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005418d60), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.68", PodIP:"10.96.2.171", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.96.2.171"}}, StartTime:time.Date(2023, time.April, 6, 11, 37, 1, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00029cd90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00029ce00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://b5e0e79c705e878c1c85c4d5240b5dfd854845a20385bd624a12ce9428fe64f6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027949a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002794980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc003b668ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Apr  6 11:37:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7430" for this suite. 04/06/23 11:37:49.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:37:49.123
Apr  6 11:37:49.123: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename gc 04/06/23 11:37:49.128
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:37:49.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:37:49.158
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 04/06/23 11:37:49.172
STEP: delete the rc 04/06/23 11:37:54.195
STEP: wait for the rc to be deleted 04/06/23 11:37:54.208
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/06/23 11:37:59.216
STEP: Gathering metrics 04/06/23 11:38:29.239
W0406 11:38:29.259216      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 11:38:29.259: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr  6 11:38:29.259: INFO: Deleting pod "simpletest.rc-2lpmq" in namespace "gc-362"
Apr  6 11:38:29.274: INFO: Deleting pod "simpletest.rc-2vl2w" in namespace "gc-362"
Apr  6 11:38:29.287: INFO: Deleting pod "simpletest.rc-4d8jw" in namespace "gc-362"
Apr  6 11:38:29.298: INFO: Deleting pod "simpletest.rc-4qgrx" in namespace "gc-362"
Apr  6 11:38:29.310: INFO: Deleting pod "simpletest.rc-56krm" in namespace "gc-362"
Apr  6 11:38:29.319: INFO: Deleting pod "simpletest.rc-5dm2d" in namespace "gc-362"
Apr  6 11:38:29.328: INFO: Deleting pod "simpletest.rc-6bxqm" in namespace "gc-362"
Apr  6 11:38:29.350: INFO: Deleting pod "simpletest.rc-6dsst" in namespace "gc-362"
Apr  6 11:38:29.365: INFO: Deleting pod "simpletest.rc-6hfb2" in namespace "gc-362"
Apr  6 11:38:29.374: INFO: Deleting pod "simpletest.rc-6m9w6" in namespace "gc-362"
Apr  6 11:38:29.382: INFO: Deleting pod "simpletest.rc-6n8j6" in namespace "gc-362"
Apr  6 11:38:29.393: INFO: Deleting pod "simpletest.rc-6p7jt" in namespace "gc-362"
Apr  6 11:38:29.406: INFO: Deleting pod "simpletest.rc-6zpwr" in namespace "gc-362"
Apr  6 11:38:29.417: INFO: Deleting pod "simpletest.rc-7ddwv" in namespace "gc-362"
Apr  6 11:38:29.428: INFO: Deleting pod "simpletest.rc-7ppwk" in namespace "gc-362"
Apr  6 11:38:29.458: INFO: Deleting pod "simpletest.rc-8gdjr" in namespace "gc-362"
Apr  6 11:38:29.474: INFO: Deleting pod "simpletest.rc-8l4wn" in namespace "gc-362"
Apr  6 11:38:29.485: INFO: Deleting pod "simpletest.rc-8tspf" in namespace "gc-362"
Apr  6 11:38:29.496: INFO: Deleting pod "simpletest.rc-9c4sm" in namespace "gc-362"
Apr  6 11:38:29.510: INFO: Deleting pod "simpletest.rc-9gz9s" in namespace "gc-362"
Apr  6 11:38:29.521: INFO: Deleting pod "simpletest.rc-9kh4h" in namespace "gc-362"
Apr  6 11:38:29.533: INFO: Deleting pod "simpletest.rc-9whbl" in namespace "gc-362"
Apr  6 11:38:29.550: INFO: Deleting pod "simpletest.rc-b2942" in namespace "gc-362"
Apr  6 11:38:29.571: INFO: Deleting pod "simpletest.rc-b65mf" in namespace "gc-362"
Apr  6 11:38:29.606: INFO: Deleting pod "simpletest.rc-c5lcs" in namespace "gc-362"
Apr  6 11:38:29.623: INFO: Deleting pod "simpletest.rc-cfhb2" in namespace "gc-362"
Apr  6 11:38:29.630: INFO: Deleting pod "simpletest.rc-cvj7k" in namespace "gc-362"
Apr  6 11:38:29.646: INFO: Deleting pod "simpletest.rc-dgm7d" in namespace "gc-362"
Apr  6 11:38:29.658: INFO: Deleting pod "simpletest.rc-drt86" in namespace "gc-362"
Apr  6 11:38:29.674: INFO: Deleting pod "simpletest.rc-f47s6" in namespace "gc-362"
Apr  6 11:38:29.692: INFO: Deleting pod "simpletest.rc-fblnd" in namespace "gc-362"
Apr  6 11:38:29.708: INFO: Deleting pod "simpletest.rc-fg7mn" in namespace "gc-362"
Apr  6 11:38:29.726: INFO: Deleting pod "simpletest.rc-fgbld" in namespace "gc-362"
Apr  6 11:38:29.751: INFO: Deleting pod "simpletest.rc-fmnt8" in namespace "gc-362"
Apr  6 11:38:29.772: INFO: Deleting pod "simpletest.rc-fzwsb" in namespace "gc-362"
Apr  6 11:38:29.787: INFO: Deleting pod "simpletest.rc-hcmf7" in namespace "gc-362"
Apr  6 11:38:29.802: INFO: Deleting pod "simpletest.rc-j4gsz" in namespace "gc-362"
Apr  6 11:38:29.825: INFO: Deleting pod "simpletest.rc-j8zv2" in namespace "gc-362"
Apr  6 11:38:29.854: INFO: Deleting pod "simpletest.rc-jc6ts" in namespace "gc-362"
Apr  6 11:38:29.862: INFO: Deleting pod "simpletest.rc-kbw8l" in namespace "gc-362"
Apr  6 11:38:29.873: INFO: Deleting pod "simpletest.rc-kcc6x" in namespace "gc-362"
Apr  6 11:38:29.887: INFO: Deleting pod "simpletest.rc-kghxh" in namespace "gc-362"
Apr  6 11:38:29.899: INFO: Deleting pod "simpletest.rc-kgtwk" in namespace "gc-362"
Apr  6 11:38:29.912: INFO: Deleting pod "simpletest.rc-kxwrd" in namespace "gc-362"
Apr  6 11:38:29.923: INFO: Deleting pod "simpletest.rc-ls742" in namespace "gc-362"
Apr  6 11:38:29.934: INFO: Deleting pod "simpletest.rc-lvq6j" in namespace "gc-362"
Apr  6 11:38:29.952: INFO: Deleting pod "simpletest.rc-mpr7x" in namespace "gc-362"
Apr  6 11:38:29.962: INFO: Deleting pod "simpletest.rc-mrd67" in namespace "gc-362"
Apr  6 11:38:29.982: INFO: Deleting pod "simpletest.rc-mvfcc" in namespace "gc-362"
Apr  6 11:38:30.022: INFO: Deleting pod "simpletest.rc-n5q2j" in namespace "gc-362"
Apr  6 11:38:30.040: INFO: Deleting pod "simpletest.rc-n7ghx" in namespace "gc-362"
Apr  6 11:38:30.064: INFO: Deleting pod "simpletest.rc-ntv5l" in namespace "gc-362"
Apr  6 11:38:30.075: INFO: Deleting pod "simpletest.rc-nv98p" in namespace "gc-362"
Apr  6 11:38:30.082: INFO: Deleting pod "simpletest.rc-nvgmc" in namespace "gc-362"
Apr  6 11:38:30.090: INFO: Deleting pod "simpletest.rc-phc6n" in namespace "gc-362"
Apr  6 11:38:30.098: INFO: Deleting pod "simpletest.rc-phpfd" in namespace "gc-362"
Apr  6 11:38:30.105: INFO: Deleting pod "simpletest.rc-pq5c7" in namespace "gc-362"
Apr  6 11:38:30.112: INFO: Deleting pod "simpletest.rc-pqnxm" in namespace "gc-362"
Apr  6 11:38:30.122: INFO: Deleting pod "simpletest.rc-pwq76" in namespace "gc-362"
Apr  6 11:38:30.132: INFO: Deleting pod "simpletest.rc-q5s96" in namespace "gc-362"
Apr  6 11:38:30.140: INFO: Deleting pod "simpletest.rc-q7qdn" in namespace "gc-362"
Apr  6 11:38:30.206: INFO: Deleting pod "simpletest.rc-qddqm" in namespace "gc-362"
Apr  6 11:38:30.214: INFO: Deleting pod "simpletest.rc-qjmrh" in namespace "gc-362"
Apr  6 11:38:30.235: INFO: Deleting pod "simpletest.rc-qnqzr" in namespace "gc-362"
Apr  6 11:38:30.251: INFO: Deleting pod "simpletest.rc-qwl2m" in namespace "gc-362"
Apr  6 11:38:30.259: INFO: Deleting pod "simpletest.rc-r27nd" in namespace "gc-362"
Apr  6 11:38:30.268: INFO: Deleting pod "simpletest.rc-r7q9t" in namespace "gc-362"
Apr  6 11:38:30.278: INFO: Deleting pod "simpletest.rc-rg7n5" in namespace "gc-362"
Apr  6 11:38:30.287: INFO: Deleting pod "simpletest.rc-rgq8m" in namespace "gc-362"
Apr  6 11:38:30.296: INFO: Deleting pod "simpletest.rc-rj5pm" in namespace "gc-362"
Apr  6 11:38:30.329: INFO: Deleting pod "simpletest.rc-rql2c" in namespace "gc-362"
Apr  6 11:38:30.376: INFO: Deleting pod "simpletest.rc-s2fzf" in namespace "gc-362"
Apr  6 11:38:30.444: INFO: Deleting pod "simpletest.rc-scrt8" in namespace "gc-362"
Apr  6 11:38:30.488: INFO: Deleting pod "simpletest.rc-sg5hg" in namespace "gc-362"
Apr  6 11:38:30.526: INFO: Deleting pod "simpletest.rc-slvjl" in namespace "gc-362"
Apr  6 11:38:30.576: INFO: Deleting pod "simpletest.rc-sz5hp" in namespace "gc-362"
Apr  6 11:38:30.631: INFO: Deleting pod "simpletest.rc-tbf2z" in namespace "gc-362"
Apr  6 11:38:30.678: INFO: Deleting pod "simpletest.rc-tlfxl" in namespace "gc-362"
Apr  6 11:38:30.733: INFO: Deleting pod "simpletest.rc-tqwfr" in namespace "gc-362"
Apr  6 11:38:30.780: INFO: Deleting pod "simpletest.rc-trcrk" in namespace "gc-362"
Apr  6 11:38:30.839: INFO: Deleting pod "simpletest.rc-tt68f" in namespace "gc-362"
Apr  6 11:38:30.879: INFO: Deleting pod "simpletest.rc-v4fc6" in namespace "gc-362"
Apr  6 11:38:30.929: INFO: Deleting pod "simpletest.rc-vf7v6" in namespace "gc-362"
Apr  6 11:38:30.980: INFO: Deleting pod "simpletest.rc-vg98z" in namespace "gc-362"
Apr  6 11:38:31.029: INFO: Deleting pod "simpletest.rc-vjx2v" in namespace "gc-362"
Apr  6 11:38:31.097: INFO: Deleting pod "simpletest.rc-vkrn2" in namespace "gc-362"
Apr  6 11:38:31.130: INFO: Deleting pod "simpletest.rc-vm2zj" in namespace "gc-362"
Apr  6 11:38:31.181: INFO: Deleting pod "simpletest.rc-wnjvw" in namespace "gc-362"
Apr  6 11:38:31.231: INFO: Deleting pod "simpletest.rc-wqwt9" in namespace "gc-362"
Apr  6 11:38:31.279: INFO: Deleting pod "simpletest.rc-wvs7n" in namespace "gc-362"
Apr  6 11:38:31.331: INFO: Deleting pod "simpletest.rc-ww25m" in namespace "gc-362"
Apr  6 11:38:31.383: INFO: Deleting pod "simpletest.rc-wzw5t" in namespace "gc-362"
Apr  6 11:38:31.448: INFO: Deleting pod "simpletest.rc-xdw4x" in namespace "gc-362"
Apr  6 11:38:31.488: INFO: Deleting pod "simpletest.rc-xkkn6" in namespace "gc-362"
Apr  6 11:38:31.537: INFO: Deleting pod "simpletest.rc-xpc4f" in namespace "gc-362"
Apr  6 11:38:31.582: INFO: Deleting pod "simpletest.rc-z7tfg" in namespace "gc-362"
Apr  6 11:38:31.634: INFO: Deleting pod "simpletest.rc-zbvv6" in namespace "gc-362"
Apr  6 11:38:31.679: INFO: Deleting pod "simpletest.rc-zlqgm" in namespace "gc-362"
Apr  6 11:38:31.747: INFO: Deleting pod "simpletest.rc-zm9wn" in namespace "gc-362"
Apr  6 11:38:31.782: INFO: Deleting pod "simpletest.rc-zwgp8" in namespace "gc-362"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Apr  6 11:38:31.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-362" for this suite. 04/06/23 11:38:31.876
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":290,"skipped":5279,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.802 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:37:49.123
    Apr  6 11:37:49.123: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename gc 04/06/23 11:37:49.128
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:37:49.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:37:49.158
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 04/06/23 11:37:49.172
    STEP: delete the rc 04/06/23 11:37:54.195
    STEP: wait for the rc to be deleted 04/06/23 11:37:54.208
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/06/23 11:37:59.216
    STEP: Gathering metrics 04/06/23 11:38:29.239
    W0406 11:38:29.259216      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr  6 11:38:29.259: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr  6 11:38:29.259: INFO: Deleting pod "simpletest.rc-2lpmq" in namespace "gc-362"
    Apr  6 11:38:29.274: INFO: Deleting pod "simpletest.rc-2vl2w" in namespace "gc-362"
    Apr  6 11:38:29.287: INFO: Deleting pod "simpletest.rc-4d8jw" in namespace "gc-362"
    Apr  6 11:38:29.298: INFO: Deleting pod "simpletest.rc-4qgrx" in namespace "gc-362"
    Apr  6 11:38:29.310: INFO: Deleting pod "simpletest.rc-56krm" in namespace "gc-362"
    Apr  6 11:38:29.319: INFO: Deleting pod "simpletest.rc-5dm2d" in namespace "gc-362"
    Apr  6 11:38:29.328: INFO: Deleting pod "simpletest.rc-6bxqm" in namespace "gc-362"
    Apr  6 11:38:29.350: INFO: Deleting pod "simpletest.rc-6dsst" in namespace "gc-362"
    Apr  6 11:38:29.365: INFO: Deleting pod "simpletest.rc-6hfb2" in namespace "gc-362"
    Apr  6 11:38:29.374: INFO: Deleting pod "simpletest.rc-6m9w6" in namespace "gc-362"
    Apr  6 11:38:29.382: INFO: Deleting pod "simpletest.rc-6n8j6" in namespace "gc-362"
    Apr  6 11:38:29.393: INFO: Deleting pod "simpletest.rc-6p7jt" in namespace "gc-362"
    Apr  6 11:38:29.406: INFO: Deleting pod "simpletest.rc-6zpwr" in namespace "gc-362"
    Apr  6 11:38:29.417: INFO: Deleting pod "simpletest.rc-7ddwv" in namespace "gc-362"
    Apr  6 11:38:29.428: INFO: Deleting pod "simpletest.rc-7ppwk" in namespace "gc-362"
    Apr  6 11:38:29.458: INFO: Deleting pod "simpletest.rc-8gdjr" in namespace "gc-362"
    Apr  6 11:38:29.474: INFO: Deleting pod "simpletest.rc-8l4wn" in namespace "gc-362"
    Apr  6 11:38:29.485: INFO: Deleting pod "simpletest.rc-8tspf" in namespace "gc-362"
    Apr  6 11:38:29.496: INFO: Deleting pod "simpletest.rc-9c4sm" in namespace "gc-362"
    Apr  6 11:38:29.510: INFO: Deleting pod "simpletest.rc-9gz9s" in namespace "gc-362"
    Apr  6 11:38:29.521: INFO: Deleting pod "simpletest.rc-9kh4h" in namespace "gc-362"
    Apr  6 11:38:29.533: INFO: Deleting pod "simpletest.rc-9whbl" in namespace "gc-362"
    Apr  6 11:38:29.550: INFO: Deleting pod "simpletest.rc-b2942" in namespace "gc-362"
    Apr  6 11:38:29.571: INFO: Deleting pod "simpletest.rc-b65mf" in namespace "gc-362"
    Apr  6 11:38:29.606: INFO: Deleting pod "simpletest.rc-c5lcs" in namespace "gc-362"
    Apr  6 11:38:29.623: INFO: Deleting pod "simpletest.rc-cfhb2" in namespace "gc-362"
    Apr  6 11:38:29.630: INFO: Deleting pod "simpletest.rc-cvj7k" in namespace "gc-362"
    Apr  6 11:38:29.646: INFO: Deleting pod "simpletest.rc-dgm7d" in namespace "gc-362"
    Apr  6 11:38:29.658: INFO: Deleting pod "simpletest.rc-drt86" in namespace "gc-362"
    Apr  6 11:38:29.674: INFO: Deleting pod "simpletest.rc-f47s6" in namespace "gc-362"
    Apr  6 11:38:29.692: INFO: Deleting pod "simpletest.rc-fblnd" in namespace "gc-362"
    Apr  6 11:38:29.708: INFO: Deleting pod "simpletest.rc-fg7mn" in namespace "gc-362"
    Apr  6 11:38:29.726: INFO: Deleting pod "simpletest.rc-fgbld" in namespace "gc-362"
    Apr  6 11:38:29.751: INFO: Deleting pod "simpletest.rc-fmnt8" in namespace "gc-362"
    Apr  6 11:38:29.772: INFO: Deleting pod "simpletest.rc-fzwsb" in namespace "gc-362"
    Apr  6 11:38:29.787: INFO: Deleting pod "simpletest.rc-hcmf7" in namespace "gc-362"
    Apr  6 11:38:29.802: INFO: Deleting pod "simpletest.rc-j4gsz" in namespace "gc-362"
    Apr  6 11:38:29.825: INFO: Deleting pod "simpletest.rc-j8zv2" in namespace "gc-362"
    Apr  6 11:38:29.854: INFO: Deleting pod "simpletest.rc-jc6ts" in namespace "gc-362"
    Apr  6 11:38:29.862: INFO: Deleting pod "simpletest.rc-kbw8l" in namespace "gc-362"
    Apr  6 11:38:29.873: INFO: Deleting pod "simpletest.rc-kcc6x" in namespace "gc-362"
    Apr  6 11:38:29.887: INFO: Deleting pod "simpletest.rc-kghxh" in namespace "gc-362"
    Apr  6 11:38:29.899: INFO: Deleting pod "simpletest.rc-kgtwk" in namespace "gc-362"
    Apr  6 11:38:29.912: INFO: Deleting pod "simpletest.rc-kxwrd" in namespace "gc-362"
    Apr  6 11:38:29.923: INFO: Deleting pod "simpletest.rc-ls742" in namespace "gc-362"
    Apr  6 11:38:29.934: INFO: Deleting pod "simpletest.rc-lvq6j" in namespace "gc-362"
    Apr  6 11:38:29.952: INFO: Deleting pod "simpletest.rc-mpr7x" in namespace "gc-362"
    Apr  6 11:38:29.962: INFO: Deleting pod "simpletest.rc-mrd67" in namespace "gc-362"
    Apr  6 11:38:29.982: INFO: Deleting pod "simpletest.rc-mvfcc" in namespace "gc-362"
    Apr  6 11:38:30.022: INFO: Deleting pod "simpletest.rc-n5q2j" in namespace "gc-362"
    Apr  6 11:38:30.040: INFO: Deleting pod "simpletest.rc-n7ghx" in namespace "gc-362"
    Apr  6 11:38:30.064: INFO: Deleting pod "simpletest.rc-ntv5l" in namespace "gc-362"
    Apr  6 11:38:30.075: INFO: Deleting pod "simpletest.rc-nv98p" in namespace "gc-362"
    Apr  6 11:38:30.082: INFO: Deleting pod "simpletest.rc-nvgmc" in namespace "gc-362"
    Apr  6 11:38:30.090: INFO: Deleting pod "simpletest.rc-phc6n" in namespace "gc-362"
    Apr  6 11:38:30.098: INFO: Deleting pod "simpletest.rc-phpfd" in namespace "gc-362"
    Apr  6 11:38:30.105: INFO: Deleting pod "simpletest.rc-pq5c7" in namespace "gc-362"
    Apr  6 11:38:30.112: INFO: Deleting pod "simpletest.rc-pqnxm" in namespace "gc-362"
    Apr  6 11:38:30.122: INFO: Deleting pod "simpletest.rc-pwq76" in namespace "gc-362"
    Apr  6 11:38:30.132: INFO: Deleting pod "simpletest.rc-q5s96" in namespace "gc-362"
    Apr  6 11:38:30.140: INFO: Deleting pod "simpletest.rc-q7qdn" in namespace "gc-362"
    Apr  6 11:38:30.206: INFO: Deleting pod "simpletest.rc-qddqm" in namespace "gc-362"
    Apr  6 11:38:30.214: INFO: Deleting pod "simpletest.rc-qjmrh" in namespace "gc-362"
    Apr  6 11:38:30.235: INFO: Deleting pod "simpletest.rc-qnqzr" in namespace "gc-362"
    Apr  6 11:38:30.251: INFO: Deleting pod "simpletest.rc-qwl2m" in namespace "gc-362"
    Apr  6 11:38:30.259: INFO: Deleting pod "simpletest.rc-r27nd" in namespace "gc-362"
    Apr  6 11:38:30.268: INFO: Deleting pod "simpletest.rc-r7q9t" in namespace "gc-362"
    Apr  6 11:38:30.278: INFO: Deleting pod "simpletest.rc-rg7n5" in namespace "gc-362"
    Apr  6 11:38:30.287: INFO: Deleting pod "simpletest.rc-rgq8m" in namespace "gc-362"
    Apr  6 11:38:30.296: INFO: Deleting pod "simpletest.rc-rj5pm" in namespace "gc-362"
    Apr  6 11:38:30.329: INFO: Deleting pod "simpletest.rc-rql2c" in namespace "gc-362"
    Apr  6 11:38:30.376: INFO: Deleting pod "simpletest.rc-s2fzf" in namespace "gc-362"
    Apr  6 11:38:30.444: INFO: Deleting pod "simpletest.rc-scrt8" in namespace "gc-362"
    Apr  6 11:38:30.488: INFO: Deleting pod "simpletest.rc-sg5hg" in namespace "gc-362"
    Apr  6 11:38:30.526: INFO: Deleting pod "simpletest.rc-slvjl" in namespace "gc-362"
    Apr  6 11:38:30.576: INFO: Deleting pod "simpletest.rc-sz5hp" in namespace "gc-362"
    Apr  6 11:38:30.631: INFO: Deleting pod "simpletest.rc-tbf2z" in namespace "gc-362"
    Apr  6 11:38:30.678: INFO: Deleting pod "simpletest.rc-tlfxl" in namespace "gc-362"
    Apr  6 11:38:30.733: INFO: Deleting pod "simpletest.rc-tqwfr" in namespace "gc-362"
    Apr  6 11:38:30.780: INFO: Deleting pod "simpletest.rc-trcrk" in namespace "gc-362"
    Apr  6 11:38:30.839: INFO: Deleting pod "simpletest.rc-tt68f" in namespace "gc-362"
    Apr  6 11:38:30.879: INFO: Deleting pod "simpletest.rc-v4fc6" in namespace "gc-362"
    Apr  6 11:38:30.929: INFO: Deleting pod "simpletest.rc-vf7v6" in namespace "gc-362"
    Apr  6 11:38:30.980: INFO: Deleting pod "simpletest.rc-vg98z" in namespace "gc-362"
    Apr  6 11:38:31.029: INFO: Deleting pod "simpletest.rc-vjx2v" in namespace "gc-362"
    Apr  6 11:38:31.097: INFO: Deleting pod "simpletest.rc-vkrn2" in namespace "gc-362"
    Apr  6 11:38:31.130: INFO: Deleting pod "simpletest.rc-vm2zj" in namespace "gc-362"
    Apr  6 11:38:31.181: INFO: Deleting pod "simpletest.rc-wnjvw" in namespace "gc-362"
    Apr  6 11:38:31.231: INFO: Deleting pod "simpletest.rc-wqwt9" in namespace "gc-362"
    Apr  6 11:38:31.279: INFO: Deleting pod "simpletest.rc-wvs7n" in namespace "gc-362"
    Apr  6 11:38:31.331: INFO: Deleting pod "simpletest.rc-ww25m" in namespace "gc-362"
    Apr  6 11:38:31.383: INFO: Deleting pod "simpletest.rc-wzw5t" in namespace "gc-362"
    Apr  6 11:38:31.448: INFO: Deleting pod "simpletest.rc-xdw4x" in namespace "gc-362"
    Apr  6 11:38:31.488: INFO: Deleting pod "simpletest.rc-xkkn6" in namespace "gc-362"
    Apr  6 11:38:31.537: INFO: Deleting pod "simpletest.rc-xpc4f" in namespace "gc-362"
    Apr  6 11:38:31.582: INFO: Deleting pod "simpletest.rc-z7tfg" in namespace "gc-362"
    Apr  6 11:38:31.634: INFO: Deleting pod "simpletest.rc-zbvv6" in namespace "gc-362"
    Apr  6 11:38:31.679: INFO: Deleting pod "simpletest.rc-zlqgm" in namespace "gc-362"
    Apr  6 11:38:31.747: INFO: Deleting pod "simpletest.rc-zm9wn" in namespace "gc-362"
    Apr  6 11:38:31.782: INFO: Deleting pod "simpletest.rc-zwgp8" in namespace "gc-362"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Apr  6 11:38:31.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-362" for this suite. 04/06/23 11:38:31.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:38:31.926
Apr  6 11:38:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 11:38:31.928
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:31.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:31.953
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 04/06/23 11:38:31.963
Apr  6 11:38:31.990: INFO: Waiting up to 5m0s for pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2" in namespace "downward-api-1579" to be "running and ready"
Apr  6 11:38:31.997: INFO: Pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.325897ms
Apr  6 11:38:31.997: INFO: The phase of Pod annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:38:34.004: INFO: Pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013850197s
Apr  6 11:38:34.004: INFO: The phase of Pod annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2 is Running (Ready = true)
Apr  6 11:38:34.004: INFO: Pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2" satisfied condition "running and ready"
Apr  6 11:38:34.540: INFO: Successfully updated pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 11:38:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1579" for this suite. 04/06/23 11:38:38.596
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":291,"skipped":5284,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.683 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:38:31.926
    Apr  6 11:38:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 11:38:31.928
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:31.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:31.953
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 04/06/23 11:38:31.963
    Apr  6 11:38:31.990: INFO: Waiting up to 5m0s for pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2" in namespace "downward-api-1579" to be "running and ready"
    Apr  6 11:38:31.997: INFO: Pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.325897ms
    Apr  6 11:38:31.997: INFO: The phase of Pod annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:38:34.004: INFO: Pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013850197s
    Apr  6 11:38:34.004: INFO: The phase of Pod annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2 is Running (Ready = true)
    Apr  6 11:38:34.004: INFO: Pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2" satisfied condition "running and ready"
    Apr  6 11:38:34.540: INFO: Successfully updated pod "annotationupdate0f78f51a-4d68-4c11-86eb-6a40260023d2"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 11:38:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1579" for this suite. 04/06/23 11:38:38.596
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:38:38.61
Apr  6 11:38:38.610: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename job 04/06/23 11:38:38.611
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:38.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:38.645
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 04/06/23 11:38:38.652
STEP: Ensuring active pods == parallelism 04/06/23 11:38:38.659
STEP: Orphaning one of the Job's Pods 04/06/23 11:38:40.666
Apr  6 11:38:41.195: INFO: Successfully updated pod "adopt-release-tsw2z"
STEP: Checking that the Job readopts the Pod 04/06/23 11:38:41.195
Apr  6 11:38:41.195: INFO: Waiting up to 15m0s for pod "adopt-release-tsw2z" in namespace "job-2543" to be "adopted"
Apr  6 11:38:41.201: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 6.425741ms
Apr  6 11:38:43.216: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.021026852s
Apr  6 11:38:43.216: INFO: Pod "adopt-release-tsw2z" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 04/06/23 11:38:43.216
Apr  6 11:38:43.736: INFO: Successfully updated pod "adopt-release-tsw2z"
STEP: Checking that the Job releases the Pod 04/06/23 11:38:43.736
Apr  6 11:38:43.736: INFO: Waiting up to 15m0s for pod "adopt-release-tsw2z" in namespace "job-2543" to be "released"
Apr  6 11:38:43.741: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 5.100915ms
Apr  6 11:38:45.752: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.016197761s
Apr  6 11:38:45.752: INFO: Pod "adopt-release-tsw2z" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Apr  6 11:38:45.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2543" for this suite. 04/06/23 11:38:45.766
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":292,"skipped":5285,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.165 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:38:38.61
    Apr  6 11:38:38.610: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename job 04/06/23 11:38:38.611
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:38.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:38.645
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 04/06/23 11:38:38.652
    STEP: Ensuring active pods == parallelism 04/06/23 11:38:38.659
    STEP: Orphaning one of the Job's Pods 04/06/23 11:38:40.666
    Apr  6 11:38:41.195: INFO: Successfully updated pod "adopt-release-tsw2z"
    STEP: Checking that the Job readopts the Pod 04/06/23 11:38:41.195
    Apr  6 11:38:41.195: INFO: Waiting up to 15m0s for pod "adopt-release-tsw2z" in namespace "job-2543" to be "adopted"
    Apr  6 11:38:41.201: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 6.425741ms
    Apr  6 11:38:43.216: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.021026852s
    Apr  6 11:38:43.216: INFO: Pod "adopt-release-tsw2z" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 04/06/23 11:38:43.216
    Apr  6 11:38:43.736: INFO: Successfully updated pod "adopt-release-tsw2z"
    STEP: Checking that the Job releases the Pod 04/06/23 11:38:43.736
    Apr  6 11:38:43.736: INFO: Waiting up to 15m0s for pod "adopt-release-tsw2z" in namespace "job-2543" to be "released"
    Apr  6 11:38:43.741: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 5.100915ms
    Apr  6 11:38:45.752: INFO: Pod "adopt-release-tsw2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.016197761s
    Apr  6 11:38:45.752: INFO: Pod "adopt-release-tsw2z" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Apr  6 11:38:45.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2543" for this suite. 04/06/23 11:38:45.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:38:45.775
Apr  6 11:38:45.776: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 11:38:45.777
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:45.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:45.8
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 11:38:45.827
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:38:46.335
STEP: Deploying the webhook pod 04/06/23 11:38:46.344
STEP: Wait for the deployment to be ready 04/06/23 11:38:46.363
Apr  6 11:38:46.372: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 11:38:48.389
STEP: Verifying the service has paired with the endpoint 04/06/23 11:38:48.403
Apr  6 11:38:49.403: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/06/23 11:38:49.419
STEP: create a pod that should be updated by the webhook 04/06/23 11:38:49.555
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:38:49.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7289" for this suite. 04/06/23 11:38:49.676
STEP: Destroying namespace "webhook-7289-markers" for this suite. 04/06/23 11:38:49.684
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":293,"skipped":5296,"failed":0}
------------------------------
â€¢ [4.006 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:38:45.775
    Apr  6 11:38:45.776: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 11:38:45.777
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:45.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:45.8
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 11:38:45.827
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:38:46.335
    STEP: Deploying the webhook pod 04/06/23 11:38:46.344
    STEP: Wait for the deployment to be ready 04/06/23 11:38:46.363
    Apr  6 11:38:46.372: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 11:38:48.389
    STEP: Verifying the service has paired with the endpoint 04/06/23 11:38:48.403
    Apr  6 11:38:49.403: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/06/23 11:38:49.419
    STEP: create a pod that should be updated by the webhook 04/06/23 11:38:49.555
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:38:49.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7289" for this suite. 04/06/23 11:38:49.676
    STEP: Destroying namespace "webhook-7289-markers" for this suite. 04/06/23 11:38:49.684
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:38:49.802
Apr  6 11:38:49.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 11:38:49.803
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:49.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:49.848
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 04/06/23 11:38:49.858
Apr  6 11:38:49.876: INFO: Waiting up to 5m0s for pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e" in namespace "downward-api-857" to be "running and ready"
Apr  6 11:38:49.886: INFO: Pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.324245ms
Apr  6 11:38:49.886: INFO: The phase of Pod labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:38:51.898: INFO: Pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e": Phase="Running", Reason="", readiness=true. Elapsed: 2.022423741s
Apr  6 11:38:51.898: INFO: The phase of Pod labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e is Running (Ready = true)
Apr  6 11:38:51.898: INFO: Pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e" satisfied condition "running and ready"
Apr  6 11:38:52.448: INFO: Successfully updated pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 11:38:56.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-857" for this suite. 04/06/23 11:38:56.505
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":294,"skipped":5311,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.710 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:38:49.802
    Apr  6 11:38:49.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 11:38:49.803
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:49.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:49.848
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 04/06/23 11:38:49.858
    Apr  6 11:38:49.876: INFO: Waiting up to 5m0s for pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e" in namespace "downward-api-857" to be "running and ready"
    Apr  6 11:38:49.886: INFO: Pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.324245ms
    Apr  6 11:38:49.886: INFO: The phase of Pod labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:38:51.898: INFO: Pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e": Phase="Running", Reason="", readiness=true. Elapsed: 2.022423741s
    Apr  6 11:38:51.898: INFO: The phase of Pod labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e is Running (Ready = true)
    Apr  6 11:38:51.898: INFO: Pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e" satisfied condition "running and ready"
    Apr  6 11:38:52.448: INFO: Successfully updated pod "labelsupdated5feca0d-b315-40ce-8edb-5e4c78c62f1e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 11:38:56.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-857" for this suite. 04/06/23 11:38:56.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:38:56.52
Apr  6 11:38:56.520: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename runtimeclass 04/06/23 11:38:56.521
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:56.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:56.544
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2704-delete-me 04/06/23 11:38:56.559
STEP: Waiting for the RuntimeClass to disappear 04/06/23 11:38:56.566
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Apr  6 11:38:56.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2704" for this suite. 04/06/23 11:38:56.588
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":295,"skipped":5355,"failed":0}
------------------------------
â€¢ [0.073 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:38:56.52
    Apr  6 11:38:56.520: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename runtimeclass 04/06/23 11:38:56.521
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:56.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:56.544
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2704-delete-me 04/06/23 11:38:56.559
    STEP: Waiting for the RuntimeClass to disappear 04/06/23 11:38:56.566
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Apr  6 11:38:56.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2704" for this suite. 04/06/23 11:38:56.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:38:56.595
Apr  6 11:38:56.595: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 11:38:56.596
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:56.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:56.625
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 11:38:56.641
Apr  6 11:38:56.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4808 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Apr  6 11:38:56.828: INFO: stderr: ""
Apr  6 11:38:56.828: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 04/06/23 11:38:56.828
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Apr  6 11:38:56.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4808 delete pods e2e-test-httpd-pod'
Apr  6 11:38:59.224: INFO: stderr: ""
Apr  6 11:38:59.224: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 11:38:59.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4808" for this suite. 04/06/23 11:38:59.235
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":296,"skipped":5371,"failed":0}
------------------------------
â€¢ [2.645 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:38:56.595
    Apr  6 11:38:56.595: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 11:38:56.596
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:56.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:56.625
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 11:38:56.641
    Apr  6 11:38:56.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4808 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Apr  6 11:38:56.828: INFO: stderr: ""
    Apr  6 11:38:56.828: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 04/06/23 11:38:56.828
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Apr  6 11:38:56.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4808 delete pods e2e-test-httpd-pod'
    Apr  6 11:38:59.224: INFO: stderr: ""
    Apr  6 11:38:59.224: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 11:38:59.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4808" for this suite. 04/06/23 11:38:59.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:38:59.241
Apr  6 11:38:59.241: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 11:38:59.242
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:59.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:59.26
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 04/06/23 11:38:59.265
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local;sleep 1; done
 04/06/23 11:38:59.269
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local;sleep 1; done
 04/06/23 11:38:59.27
STEP: creating a pod to probe DNS 04/06/23 11:38:59.27
STEP: submitting the pod to kubernetes 04/06/23 11:38:59.27
Apr  6 11:38:59.283: INFO: Waiting up to 15m0s for pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90" in namespace "dns-266" to be "running"
Apr  6 11:38:59.290: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961064ms
Apr  6 11:39:01.309: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025515006s
Apr  6 11:39:03.300: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016404175s
Apr  6 11:39:05.298: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01496343s
Apr  6 11:39:07.296: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Running", Reason="", readiness=true. Elapsed: 8.012811565s
Apr  6 11:39:07.296: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90" satisfied condition "running"
STEP: retrieving the pod 04/06/23 11:39:07.296
STEP: looking for the results for each expected name from probers 04/06/23 11:39:07.301
Apr  6 11:39:07.425: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.469: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.483: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.492: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.502: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.511: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.520: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.530: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:07.530: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

Apr  6 11:39:12.543: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.586: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.596: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.604: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.613: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.623: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.637: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.653: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:12.653: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

Apr  6 11:39:17.542: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.586: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.595: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.603: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.612: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.620: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.628: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.637: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:17.637: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

Apr  6 11:39:22.542: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.585: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.605: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.614: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.623: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.633: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.646: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.657: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:22.657: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

Apr  6 11:39:27.544: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.594: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.632: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.661: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.680: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.690: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.698: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
Apr  6 11:39:27.698: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

Apr  6 11:39:32.677: INFO: DNS probes using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 succeeded

STEP: deleting the pod 04/06/23 11:39:32.677
STEP: deleting the test headless service 04/06/23 11:39:32.692
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 11:39:32.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-266" for this suite. 04/06/23 11:39:32.737
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":297,"skipped":5391,"failed":0}
------------------------------
â€¢ [SLOW TEST] [33.512 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:38:59.241
    Apr  6 11:38:59.241: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 11:38:59.242
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:38:59.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:38:59.26
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 04/06/23 11:38:59.265
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local;sleep 1; done
     04/06/23 11:38:59.269
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-266.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-266.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local;sleep 1; done
     04/06/23 11:38:59.27
    STEP: creating a pod to probe DNS 04/06/23 11:38:59.27
    STEP: submitting the pod to kubernetes 04/06/23 11:38:59.27
    Apr  6 11:38:59.283: INFO: Waiting up to 15m0s for pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90" in namespace "dns-266" to be "running"
    Apr  6 11:38:59.290: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961064ms
    Apr  6 11:39:01.309: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025515006s
    Apr  6 11:39:03.300: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016404175s
    Apr  6 11:39:05.298: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01496343s
    Apr  6 11:39:07.296: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90": Phase="Running", Reason="", readiness=true. Elapsed: 8.012811565s
    Apr  6 11:39:07.296: INFO: Pod "dns-test-6c1de76c-0400-4087-b552-101b6ce95e90" satisfied condition "running"
    STEP: retrieving the pod 04/06/23 11:39:07.296
    STEP: looking for the results for each expected name from probers 04/06/23 11:39:07.301
    Apr  6 11:39:07.425: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.469: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.483: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.492: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.502: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.511: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.520: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.530: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:07.530: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

    Apr  6 11:39:12.543: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.586: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.596: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.604: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.613: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.623: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.637: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.653: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:12.653: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

    Apr  6 11:39:17.542: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.586: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.595: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.603: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.612: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.620: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.628: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.637: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:17.637: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

    Apr  6 11:39:22.542: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.585: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.605: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.614: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.623: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.633: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.646: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.657: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:22.657: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

    Apr  6 11:39:27.544: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.594: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.632: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.661: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.680: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.690: INFO: Unable to read jessie_udp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.698: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local from pod dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90: the server could not find the requested resource (get pods dns-test-6c1de76c-0400-4087-b552-101b6ce95e90)
    Apr  6 11:39:27.698: INFO: Lookups using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local wheezy_udp@dns-test-service-2.dns-266.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-266.svc.cluster.local jessie_udp@dns-test-service-2.dns-266.svc.cluster.local jessie_tcp@dns-test-service-2.dns-266.svc.cluster.local]

    Apr  6 11:39:32.677: INFO: DNS probes using dns-266/dns-test-6c1de76c-0400-4087-b552-101b6ce95e90 succeeded

    STEP: deleting the pod 04/06/23 11:39:32.677
    STEP: deleting the test headless service 04/06/23 11:39:32.692
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 11:39:32.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-266" for this suite. 04/06/23 11:39:32.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:39:32.77
Apr  6 11:39:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 11:39:32.771
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:32.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:32.809
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 04/06/23 11:39:32.831
Apr  6 11:39:32.851: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9157" to be "running and ready"
Apr  6 11:39:32.858: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.25471ms
Apr  6 11:39:32.858: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:39:34.864: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013333016s
Apr  6 11:39:34.864: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr  6 11:39:34.864: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 04/06/23 11:39:34.88
Apr  6 11:39:34.894: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9157" to be "running and ready"
Apr  6 11:39:34.898: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964106ms
Apr  6 11:39:34.898: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:39:36.906: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012354378s
Apr  6 11:39:36.907: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Apr  6 11:39:36.907: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/06/23 11:39:36.911
Apr  6 11:39:36.919: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 11:39:36.927: INFO: Pod pod-with-prestop-http-hook still exists
Apr  6 11:39:38.928: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 11:39:38.937: INFO: Pod pod-with-prestop-http-hook still exists
Apr  6 11:39:40.927: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 11:39:40.933: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 04/06/23 11:39:40.933
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Apr  6 11:39:40.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9157" for this suite. 04/06/23 11:39:40.985
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":298,"skipped":5402,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.222 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:39:32.77
    Apr  6 11:39:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 11:39:32.771
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:32.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:32.809
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 04/06/23 11:39:32.831
    Apr  6 11:39:32.851: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9157" to be "running and ready"
    Apr  6 11:39:32.858: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.25471ms
    Apr  6 11:39:32.858: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:39:34.864: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013333016s
    Apr  6 11:39:34.864: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr  6 11:39:34.864: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 04/06/23 11:39:34.88
    Apr  6 11:39:34.894: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9157" to be "running and ready"
    Apr  6 11:39:34.898: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964106ms
    Apr  6 11:39:34.898: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:39:36.906: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012354378s
    Apr  6 11:39:36.907: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Apr  6 11:39:36.907: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/06/23 11:39:36.911
    Apr  6 11:39:36.919: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr  6 11:39:36.927: INFO: Pod pod-with-prestop-http-hook still exists
    Apr  6 11:39:38.928: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr  6 11:39:38.937: INFO: Pod pod-with-prestop-http-hook still exists
    Apr  6 11:39:40.927: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr  6 11:39:40.933: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 04/06/23 11:39:40.933
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Apr  6 11:39:40.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9157" for this suite. 04/06/23 11:39:40.985
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:39:40.993
Apr  6 11:39:40.993: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename ephemeral-containers-test 04/06/23 11:39:40.994
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:41.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:41.017
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 04/06/23 11:39:41.023
Apr  6 11:39:41.041: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4490" to be "running and ready"
Apr  6 11:39:41.046: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37068ms
Apr  6 11:39:41.046: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:39:43.054: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013420145s
Apr  6 11:39:43.054: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Apr  6 11:39:43.054: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 04/06/23 11:39:43.059
Apr  6 11:39:43.080: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4490" to be "container debugger running"
Apr  6 11:39:43.085: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.139518ms
Apr  6 11:39:45.119: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.039153332s
Apr  6 11:39:47.094: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014282902s
Apr  6 11:39:47.094: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 04/06/23 11:39:47.094
Apr  6 11:39:47.095: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4490 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:39:47.095: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:39:47.096: INFO: ExecWithOptions: Clientset creation
Apr  6 11:39:47.096: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/ephemeral-containers-test-4490/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Apr  6 11:39:47.541: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Apr  6 11:39:47.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-4490" for this suite. 04/06/23 11:39:47.599
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":299,"skipped":5405,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.615 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:39:40.993
    Apr  6 11:39:40.993: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename ephemeral-containers-test 04/06/23 11:39:40.994
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:41.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:41.017
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 04/06/23 11:39:41.023
    Apr  6 11:39:41.041: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4490" to be "running and ready"
    Apr  6 11:39:41.046: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37068ms
    Apr  6 11:39:41.046: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:39:43.054: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013420145s
    Apr  6 11:39:43.054: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Apr  6 11:39:43.054: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 04/06/23 11:39:43.059
    Apr  6 11:39:43.080: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4490" to be "container debugger running"
    Apr  6 11:39:43.085: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.139518ms
    Apr  6 11:39:45.119: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.039153332s
    Apr  6 11:39:47.094: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014282902s
    Apr  6 11:39:47.094: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 04/06/23 11:39:47.094
    Apr  6 11:39:47.095: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4490 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:39:47.095: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:39:47.096: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:39:47.096: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/ephemeral-containers-test-4490/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Apr  6 11:39:47.541: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Apr  6 11:39:47.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-4490" for this suite. 04/06/23 11:39:47.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:39:47.608
Apr  6 11:39:47.608: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 11:39:47.609
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:47.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:47.637
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 11:39:47.662
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:39:47.936
STEP: Deploying the webhook pod 04/06/23 11:39:47.946
STEP: Wait for the deployment to be ready 04/06/23 11:39:47.959
Apr  6 11:39:47.972: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 11:39:49.996
STEP: Verifying the service has paired with the endpoint 04/06/23 11:39:50.009
Apr  6 11:39:51.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Apr  6 11:39:51.016: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1084-crds.webhook.example.com via the AdmissionRegistration API 04/06/23 11:39:51.536
STEP: Creating a custom resource while v1 is storage version 04/06/23 11:39:51.675
STEP: Patching Custom Resource Definition to set v2 as storage 04/06/23 11:39:53.972
STEP: Patching the custom resource while v2 is storage version 04/06/23 11:39:53.992
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:39:54.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7878" for this suite. 04/06/23 11:39:54.589
STEP: Destroying namespace "webhook-7878-markers" for this suite. 04/06/23 11:39:54.596
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":300,"skipped":5439,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.048 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:39:47.608
    Apr  6 11:39:47.608: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 11:39:47.609
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:47.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:47.637
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 11:39:47.662
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:39:47.936
    STEP: Deploying the webhook pod 04/06/23 11:39:47.946
    STEP: Wait for the deployment to be ready 04/06/23 11:39:47.959
    Apr  6 11:39:47.972: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 11:39:49.996
    STEP: Verifying the service has paired with the endpoint 04/06/23 11:39:50.009
    Apr  6 11:39:51.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Apr  6 11:39:51.016: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1084-crds.webhook.example.com via the AdmissionRegistration API 04/06/23 11:39:51.536
    STEP: Creating a custom resource while v1 is storage version 04/06/23 11:39:51.675
    STEP: Patching Custom Resource Definition to set v2 as storage 04/06/23 11:39:53.972
    STEP: Patching the custom resource while v2 is storage version 04/06/23 11:39:53.992
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:39:54.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7878" for this suite. 04/06/23 11:39:54.589
    STEP: Destroying namespace "webhook-7878-markers" for this suite. 04/06/23 11:39:54.596
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:39:54.659
Apr  6 11:39:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 11:39:54.661
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:54.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:54.715
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Apr  6 11:39:54.723: INFO: Creating deployment "webserver-deployment"
Apr  6 11:39:54.729: INFO: Waiting for observed generation 1
Apr  6 11:39:56.740: INFO: Waiting for all required pods to come up
Apr  6 11:39:56.749: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 04/06/23 11:39:56.75
Apr  6 11:39:56.751: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-cj27d" in namespace "deployment-580" to be "running"
Apr  6 11:39:56.751: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-q2mtk" in namespace "deployment-580" to be "running"
Apr  6 11:39:56.758: INFO: Pod "webserver-deployment-845c8977d9-q2mtk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.003646ms
Apr  6 11:39:56.758: INFO: Pod "webserver-deployment-845c8977d9-cj27d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.074964ms
Apr  6 11:39:58.764: INFO: Pod "webserver-deployment-845c8977d9-cj27d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013712766s
Apr  6 11:39:58.764: INFO: Pod "webserver-deployment-845c8977d9-cj27d" satisfied condition "running"
Apr  6 11:39:58.765: INFO: Pod "webserver-deployment-845c8977d9-q2mtk": Phase="Running", Reason="", readiness=true. Elapsed: 2.0137522s
Apr  6 11:39:58.765: INFO: Pod "webserver-deployment-845c8977d9-q2mtk" satisfied condition "running"
Apr  6 11:39:58.765: INFO: Waiting for deployment "webserver-deployment" to complete
Apr  6 11:39:58.775: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr  6 11:39:58.793: INFO: Updating deployment webserver-deployment
Apr  6 11:39:58.793: INFO: Waiting for observed generation 2
Apr  6 11:40:00.810: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr  6 11:40:00.815: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr  6 11:40:00.819: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr  6 11:40:00.833: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr  6 11:40:00.833: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr  6 11:40:00.838: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr  6 11:40:00.846: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr  6 11:40:00.846: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr  6 11:40:00.858: INFO: Updating deployment webserver-deployment
Apr  6 11:40:00.858: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr  6 11:40:00.867: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr  6 11:40:00.870: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 11:40:00.880: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-580  b940180e-9e03-4113-a10d-efe1dca2502f 37973 3 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026d43b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-04-06 11:39:58 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-06 11:40:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr  6 11:40:00.885: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-580  c9611d8e-4a02-4db6-9afe-6e0c3584b656 37971 3 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b940180e-9e03-4113-a10d-efe1dca2502f 0xc003c70e37 0xc003c70e38}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b940180e-9e03-4113-a10d-efe1dca2502f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c71018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:40:00.886: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr  6 11:40:00.886: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-580  de10b022-8948-4706-8e3c-ac46b4f1e9ef 37970 3 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b940180e-9e03-4113-a10d-efe1dca2502f 0xc003c71077 0xc003c71078}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b940180e-9e03-4113-a10d-efe1dca2502f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c71118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr  6 11:40:00.910: INFO: Pod "webserver-deployment-69b7448995-5v2pf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5v2pf webserver-deployment-69b7448995- deployment-580  7c90c093-38e7-4bbc-9757-d1ea4e1300f4 37992 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c715b7 0xc003c715b8}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhf6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhf6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-9tjrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9tjrp webserver-deployment-69b7448995- deployment-580  2f278b21-cd82-4fa8-be85-ebc75f102858 37967 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b008bac15523022b12381c132ec6e4d6d80e2c123787c20e5ae27712877d9fe9 cni.projectcalico.org/podIP:10.96.0.55/32 cni.projectcalico.org/podIPs:10.96.0.55/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71740 0xc003c71741}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzq75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzq75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:10.96.0.55,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-mvpdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mvpdt webserver-deployment-69b7448995- deployment-580  a32e40a0-b635-436e-89cd-d657b81eaeb6 37979 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71970 0xc003c71971}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tjmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tjmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-mx2w4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mx2w4 webserver-deployment-69b7448995- deployment-580  0f468548-af8e-4cf6-95b0-7911e42bc3db 37953 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:546e9bd589304b3a9de8675e3b0852f2e1f6336ab0bc518988e46e7926ae5d1c cni.projectcalico.org/podIP:10.96.3.57/32 cni.projectcalico.org/podIPs:10.96.3.57/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71af0 0xc003c71af1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t6tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t6tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.236,PodIP:,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-pj8gl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pj8gl webserver-deployment-69b7448995- deployment-580  b5abe36f-bc14-491d-a59a-95d486dc043e 37964 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a6e70ac558727e7e01fd893e446ef1a4c28cf1aca5cad586e002fd80320f7ebc cni.projectcalico.org/podIP:10.96.1.190/32 cni.projectcalico.org/podIPs:10.96.1.190/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71d17 0xc003c71d18}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzhmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzhmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.190,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.912: INFO: Pod "webserver-deployment-69b7448995-qj588" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qj588 webserver-deployment-69b7448995- deployment-580  7b1c094b-e035-4d61-ad0d-b20c3a0bf1bd 37985 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71f40 0xc003c71f41}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-975vn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-975vn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.912: INFO: Pod "webserver-deployment-69b7448995-z4qlc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-z4qlc webserver-deployment-69b7448995- deployment-580  e60e40ed-8783-4fee-9e4d-12f49257b2d8 37954 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1c26e47cd3087a0297c928b6401d7ff2d39f63ca168f8b2ace143ade1f30a1a7 cni.projectcalico.org/podIP:10.96.4.59/32 cni.projectcalico.org/podIPs:10.96.4.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc00367a0d0 0xc00367a0d1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8f5sh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8f5sh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.220,PodIP:,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.912: INFO: Pod "webserver-deployment-69b7448995-zgqmq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zgqmq webserver-deployment-69b7448995- deployment-580  a315810e-60ac-4552-b3e6-acd4afb8ef40 37966 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ef75a9b336fae3032a11c457d49080d777f80994af63820f2432c95a7cc48696 cni.projectcalico.org/podIP:10.96.2.199/32 cni.projectcalico.org/podIPs:10.96.2.199/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc00367a307 0xc00367a308}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slq85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slq85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.199,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.913: INFO: Pod "webserver-deployment-845c8977d9-4ns4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4ns4v webserver-deployment-845c8977d9- deployment-580  0049fab1-d8c2-4c5a-b06d-b19e242cc443 37990 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367a540 0xc00367a541}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltkzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltkzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.913: INFO: Pod "webserver-deployment-845c8977d9-6mxdx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6mxdx webserver-deployment-845c8977d9- deployment-580  9d651528-a5e8-44d8-a885-4bab40565153 37980 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367a677 0xc00367a678}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scvdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scvdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.913: INFO: Pod "webserver-deployment-845c8977d9-9vvwp" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vvwp webserver-deployment-845c8977d9- deployment-580  0beaed6c-0170-4074-b871-e82c24cbc125 37880 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:13b7889b02e238f8b75154fb41261d3baaf90ad0dfc4f87a3b7f2a4ab67e3dfb cni.projectcalico.org/podIP:10.96.3.56/32 cni.projectcalico.org/podIPs:10.96.3.56/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367a800 0xc00367a801}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nbcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nbcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.236,PodIP:10.96.3.56,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d0a715f912b6e383017e35dcc64aeb5aeb0bbae7fea904c32ed84bc15c8dafa8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.914: INFO: Pod "webserver-deployment-845c8977d9-bpqnk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bpqnk webserver-deployment-845c8977d9- deployment-580  35cdc63d-93a0-4b60-b2df-2e5e64fdea0b 37871 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:136ec92865314f583fd92ede0756665c1f30ef88ab4a8158443a877540823358 cni.projectcalico.org/podIP:10.96.0.53/32 cni.projectcalico.org/podIPs:10.96.0.53/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367aa40 0xc00367aa41}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8kqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8kqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:10.96.0.53,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cf5af1c06606ed4f4652075542afb76af3784a763fa5119c1dc76f8764ac2730,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.914: INFO: Pod "webserver-deployment-845c8977d9-d2sn6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-d2sn6 webserver-deployment-845c8977d9- deployment-580  5021bff8-e1c8-48f9-ba83-4a41d22dba32 37874 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:72b670a8dbbc6d462d3c3d7b2fc5e9958b10add41bef572b5502a9958e2aa0b1 cni.projectcalico.org/podIP:10.96.0.54/32 cni.projectcalico.org/podIPs:10.96.0.54/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367ac70 0xc00367ac71}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vd4nc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vd4nc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:10.96.0.54,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0fe73438ade61a66a322edfceacce32f2756ecfec1d986f92a896075a391b7f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.915: INFO: Pod "webserver-deployment-845c8977d9-gnfhq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gnfhq webserver-deployment-845c8977d9- deployment-580  e1003632-c171-4ddc-8df0-1ad4f3d2b046 37987 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367aea0 0xc00367aea1}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2tln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2tln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.915: INFO: Pod "webserver-deployment-845c8977d9-klrrl" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-klrrl webserver-deployment-845c8977d9- deployment-580  27755c76-31e4-42af-b64f-9f50ff02c23a 37991 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367aff0 0xc00367aff1}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqrkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqrkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.915: INFO: Pod "webserver-deployment-845c8977d9-lzlf2" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lzlf2 webserver-deployment-845c8977d9- deployment-580  2dec8f2e-5f3c-417d-8d3f-cc41c7e860c9 37889 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9aaf6cc7744bd7f0379c5c5a08d7b4fab076677dc99088ba90190b6eeb32ed4c cni.projectcalico.org/podIP:10.96.1.189/32 cni.projectcalico.org/podIPs:10.96.1.189/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b170 0xc00367b171}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntjtx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntjtx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.189,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bc1ef62bdbdad60a5c08c72f35f302ce2a2d81962e2009fbf2b156fbd8c8fec1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.916: INFO: Pod "webserver-deployment-845c8977d9-mck24" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mck24 webserver-deployment-845c8977d9- deployment-580  68393426-cd1f-4315-98d5-d626c0ab71e0 37895 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7799107f57ae4e229a767c2a20e295e89390a7559f7abcd82a3c7d3267636319 cni.projectcalico.org/podIP:10.96.2.198/32 cni.projectcalico.org/podIPs:10.96.2.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b3a0 0xc00367b3a1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gj9mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gj9mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.198,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://023ea429f857f3f35a24fe322723b5b11e5ea7ba2c51d6b0be9c62d0bfe89409,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.916: INFO: Pod "webserver-deployment-845c8977d9-q2mtk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-q2mtk webserver-deployment-845c8977d9- deployment-580  a41f71ac-a0c7-4317-b733-e8e6e9ac9fec 37899 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f1248a4c5a686c7818986bbb7fa7ead80029ac620e2a0ea3735d3093e5c41894 cni.projectcalico.org/podIP:10.96.4.57/32 cni.projectcalico.org/podIPs:10.96.4.57/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b5e0 0xc00367b5e1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.4.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8phbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8phbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.220,PodIP:10.96.4.57,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f0cbcc67420ce25ee145d1ac68fca8fe7dbb482270af8a1fe44f75dc4d0fcb0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.4.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.916: INFO: Pod "webserver-deployment-845c8977d9-ssl97" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ssl97 webserver-deployment-845c8977d9- deployment-580  074e5192-6b72-4df2-9935-36fface43d33 37883 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c15359c2e32a3b135ff62abf8bcf9971ac105675fca13678171265811f0df5f cni.projectcalico.org/podIP:10.96.3.55/32 cni.projectcalico.org/podIPs:10.96.3.55/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b810 0xc00367b811}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9fjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9fjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.236,PodIP:10.96.3.55,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ed78dc532a5f94976a337a1f8aedcd461ce095a8c49fae9d8e40a2997e54b61b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-vknz8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vknz8 webserver-deployment-845c8977d9- deployment-580  720527f3-9196-469a-8270-d45575050b15 37892 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:049a03a3ac9007e1639cba739c7cf3fe45f8b688fa5a00bb82310811c800697f cni.projectcalico.org/podIP:10.96.2.197/32 cni.projectcalico.org/podIPs:10.96.2.197/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367bb20 0xc00367bb21}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vl64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vl64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.197,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0dabcdd462931913020a9725df39c166955696c948acef068aa05e1cb0355a2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-wktj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wktj5 webserver-deployment-845c8977d9- deployment-580  aab9b1fd-cf64-434f-84b7-c1f67a5a916d 37981 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc003d8a0e0 0xc003d8a0e1}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vw7pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vw7pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-z7srx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7srx webserver-deployment-845c8977d9- deployment-580  4690efbd-53f1-4d5b-95f9-bb330c5f363e 37989 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc003d8a240 0xc003d8a241}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9fnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9fnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:,StartTime:2023-04-06 11:40:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-zx444" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zx444 webserver-deployment-845c8977d9- deployment-580  5a92fbbf-8a0d-427b-9e9b-eaf0fb3aea45 37988 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc003d8a3f7 0xc003d8a3f8}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj6rm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj6rm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 11:40:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-580" for this suite. 04/06/23 11:40:00.926
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":301,"skipped":5439,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.275 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:39:54.659
    Apr  6 11:39:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 11:39:54.661
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:39:54.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:39:54.715
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Apr  6 11:39:54.723: INFO: Creating deployment "webserver-deployment"
    Apr  6 11:39:54.729: INFO: Waiting for observed generation 1
    Apr  6 11:39:56.740: INFO: Waiting for all required pods to come up
    Apr  6 11:39:56.749: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 04/06/23 11:39:56.75
    Apr  6 11:39:56.751: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-cj27d" in namespace "deployment-580" to be "running"
    Apr  6 11:39:56.751: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-q2mtk" in namespace "deployment-580" to be "running"
    Apr  6 11:39:56.758: INFO: Pod "webserver-deployment-845c8977d9-q2mtk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.003646ms
    Apr  6 11:39:56.758: INFO: Pod "webserver-deployment-845c8977d9-cj27d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.074964ms
    Apr  6 11:39:58.764: INFO: Pod "webserver-deployment-845c8977d9-cj27d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013712766s
    Apr  6 11:39:58.764: INFO: Pod "webserver-deployment-845c8977d9-cj27d" satisfied condition "running"
    Apr  6 11:39:58.765: INFO: Pod "webserver-deployment-845c8977d9-q2mtk": Phase="Running", Reason="", readiness=true. Elapsed: 2.0137522s
    Apr  6 11:39:58.765: INFO: Pod "webserver-deployment-845c8977d9-q2mtk" satisfied condition "running"
    Apr  6 11:39:58.765: INFO: Waiting for deployment "webserver-deployment" to complete
    Apr  6 11:39:58.775: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Apr  6 11:39:58.793: INFO: Updating deployment webserver-deployment
    Apr  6 11:39:58.793: INFO: Waiting for observed generation 2
    Apr  6 11:40:00.810: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Apr  6 11:40:00.815: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Apr  6 11:40:00.819: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr  6 11:40:00.833: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Apr  6 11:40:00.833: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Apr  6 11:40:00.838: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr  6 11:40:00.846: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Apr  6 11:40:00.846: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Apr  6 11:40:00.858: INFO: Updating deployment webserver-deployment
    Apr  6 11:40:00.858: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Apr  6 11:40:00.867: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Apr  6 11:40:00.870: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 11:40:00.880: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-580  b940180e-9e03-4113-a10d-efe1dca2502f 37973 3 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026d43b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-04-06 11:39:58 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-06 11:40:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Apr  6 11:40:00.885: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-580  c9611d8e-4a02-4db6-9afe-6e0c3584b656 37971 3 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b940180e-9e03-4113-a10d-efe1dca2502f 0xc003c70e37 0xc003c70e38}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b940180e-9e03-4113-a10d-efe1dca2502f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c71018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:40:00.886: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Apr  6 11:40:00.886: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-580  de10b022-8948-4706-8e3c-ac46b4f1e9ef 37970 3 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b940180e-9e03-4113-a10d-efe1dca2502f 0xc003c71077 0xc003c71078}] [] [{kube-controller-manager Update apps/v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b940180e-9e03-4113-a10d-efe1dca2502f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c71118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Apr  6 11:40:00.910: INFO: Pod "webserver-deployment-69b7448995-5v2pf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5v2pf webserver-deployment-69b7448995- deployment-580  7c90c093-38e7-4bbc-9757-d1ea4e1300f4 37992 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c715b7 0xc003c715b8}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhf6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhf6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-9tjrp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9tjrp webserver-deployment-69b7448995- deployment-580  2f278b21-cd82-4fa8-be85-ebc75f102858 37967 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b008bac15523022b12381c132ec6e4d6d80e2c123787c20e5ae27712877d9fe9 cni.projectcalico.org/podIP:10.96.0.55/32 cni.projectcalico.org/podIPs:10.96.0.55/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71740 0xc003c71741}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzq75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzq75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:10.96.0.55,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-mvpdt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mvpdt webserver-deployment-69b7448995- deployment-580  a32e40a0-b635-436e-89cd-d657b81eaeb6 37979 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71970 0xc003c71971}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tjmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tjmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-mx2w4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mx2w4 webserver-deployment-69b7448995- deployment-580  0f468548-af8e-4cf6-95b0-7911e42bc3db 37953 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:546e9bd589304b3a9de8675e3b0852f2e1f6336ab0bc518988e46e7926ae5d1c cni.projectcalico.org/podIP:10.96.3.57/32 cni.projectcalico.org/podIPs:10.96.3.57/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71af0 0xc003c71af1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t6tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t6tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.236,PodIP:,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.911: INFO: Pod "webserver-deployment-69b7448995-pj8gl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pj8gl webserver-deployment-69b7448995- deployment-580  b5abe36f-bc14-491d-a59a-95d486dc043e 37964 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a6e70ac558727e7e01fd893e446ef1a4c28cf1aca5cad586e002fd80320f7ebc cni.projectcalico.org/podIP:10.96.1.190/32 cni.projectcalico.org/podIPs:10.96.1.190/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71d17 0xc003c71d18}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzhmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzhmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.190,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.912: INFO: Pod "webserver-deployment-69b7448995-qj588" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qj588 webserver-deployment-69b7448995- deployment-580  7b1c094b-e035-4d61-ad0d-b20c3a0bf1bd 37985 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc003c71f40 0xc003c71f41}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-975vn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-975vn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.912: INFO: Pod "webserver-deployment-69b7448995-z4qlc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-z4qlc webserver-deployment-69b7448995- deployment-580  e60e40ed-8783-4fee-9e4d-12f49257b2d8 37954 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1c26e47cd3087a0297c928b6401d7ff2d39f63ca168f8b2ace143ade1f30a1a7 cni.projectcalico.org/podIP:10.96.4.59/32 cni.projectcalico.org/podIPs:10.96.4.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc00367a0d0 0xc00367a0d1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8f5sh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8f5sh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.220,PodIP:,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.912: INFO: Pod "webserver-deployment-69b7448995-zgqmq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zgqmq webserver-deployment-69b7448995- deployment-580  a315810e-60ac-4552-b3e6-acd4afb8ef40 37966 0 2023-04-06 11:39:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ef75a9b336fae3032a11c457d49080d777f80994af63820f2432c95a7cc48696 cni.projectcalico.org/podIP:10.96.2.199/32 cni.projectcalico.org/podIPs:10.96.2.199/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c9611d8e-4a02-4db6-9afe-6e0c3584b656 0xc00367a307 0xc00367a308}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9611d8e-4a02-4db6-9afe-6e0c3584b656\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slq85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slq85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.199,StartTime:2023-04-06 11:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.913: INFO: Pod "webserver-deployment-845c8977d9-4ns4v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4ns4v webserver-deployment-845c8977d9- deployment-580  0049fab1-d8c2-4c5a-b06d-b19e242cc443 37990 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367a540 0xc00367a541}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltkzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltkzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.913: INFO: Pod "webserver-deployment-845c8977d9-6mxdx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6mxdx webserver-deployment-845c8977d9- deployment-580  9d651528-a5e8-44d8-a885-4bab40565153 37980 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367a677 0xc00367a678}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scvdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scvdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.913: INFO: Pod "webserver-deployment-845c8977d9-9vvwp" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vvwp webserver-deployment-845c8977d9- deployment-580  0beaed6c-0170-4074-b871-e82c24cbc125 37880 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:13b7889b02e238f8b75154fb41261d3baaf90ad0dfc4f87a3b7f2a4ab67e3dfb cni.projectcalico.org/podIP:10.96.3.56/32 cni.projectcalico.org/podIPs:10.96.3.56/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367a800 0xc00367a801}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nbcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nbcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.236,PodIP:10.96.3.56,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d0a715f912b6e383017e35dcc64aeb5aeb0bbae7fea904c32ed84bc15c8dafa8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.914: INFO: Pod "webserver-deployment-845c8977d9-bpqnk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bpqnk webserver-deployment-845c8977d9- deployment-580  35cdc63d-93a0-4b60-b2df-2e5e64fdea0b 37871 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:136ec92865314f583fd92ede0756665c1f30ef88ab4a8158443a877540823358 cni.projectcalico.org/podIP:10.96.0.53/32 cni.projectcalico.org/podIPs:10.96.0.53/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367aa40 0xc00367aa41}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8kqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8kqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:10.96.0.53,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cf5af1c06606ed4f4652075542afb76af3784a763fa5119c1dc76f8764ac2730,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.914: INFO: Pod "webserver-deployment-845c8977d9-d2sn6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-d2sn6 webserver-deployment-845c8977d9- deployment-580  5021bff8-e1c8-48f9-ba83-4a41d22dba32 37874 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:72b670a8dbbc6d462d3c3d7b2fc5e9958b10add41bef572b5502a9958e2aa0b1 cni.projectcalico.org/podIP:10.96.0.54/32 cni.projectcalico.org/podIPs:10.96.0.54/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367ac70 0xc00367ac71}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.0.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vd4nc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vd4nc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-d2kf4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.89,PodIP:10.96.0.54,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0fe73438ade61a66a322edfceacce32f2756ecfec1d986f92a896075a391b7f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.0.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.915: INFO: Pod "webserver-deployment-845c8977d9-gnfhq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gnfhq webserver-deployment-845c8977d9- deployment-580  e1003632-c171-4ddc-8df0-1ad4f3d2b046 37987 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367aea0 0xc00367aea1}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2tln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2tln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.915: INFO: Pod "webserver-deployment-845c8977d9-klrrl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-klrrl webserver-deployment-845c8977d9- deployment-580  27755c76-31e4-42af-b64f-9f50ff02c23a 37991 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367aff0 0xc00367aff1}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqrkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqrkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.915: INFO: Pod "webserver-deployment-845c8977d9-lzlf2" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lzlf2 webserver-deployment-845c8977d9- deployment-580  2dec8f2e-5f3c-417d-8d3f-cc41c7e860c9 37889 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9aaf6cc7744bd7f0379c5c5a08d7b4fab076677dc99088ba90190b6eeb32ed4c cni.projectcalico.org/podIP:10.96.1.189/32 cni.projectcalico.org/podIPs:10.96.1.189/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b170 0xc00367b171}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.1.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntjtx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntjtx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:10.96.1.189,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bc1ef62bdbdad60a5c08c72f35f302ce2a2d81962e2009fbf2b156fbd8c8fec1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.1.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.916: INFO: Pod "webserver-deployment-845c8977d9-mck24" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mck24 webserver-deployment-845c8977d9- deployment-580  68393426-cd1f-4315-98d5-d626c0ab71e0 37895 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7799107f57ae4e229a767c2a20e295e89390a7559f7abcd82a3c7d3267636319 cni.projectcalico.org/podIP:10.96.2.198/32 cni.projectcalico.org/podIPs:10.96.2.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b3a0 0xc00367b3a1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gj9mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gj9mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.198,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://023ea429f857f3f35a24fe322723b5b11e5ea7ba2c51d6b0be9c62d0bfe89409,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.916: INFO: Pod "webserver-deployment-845c8977d9-q2mtk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-q2mtk webserver-deployment-845c8977d9- deployment-580  a41f71ac-a0c7-4317-b733-e8e6e9ac9fec 37899 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f1248a4c5a686c7818986bbb7fa7ead80029ac620e2a0ea3735d3093e5c41894 cni.projectcalico.org/podIP:10.96.4.57/32 cni.projectcalico.org/podIPs:10.96.4.57/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b5e0 0xc00367b5e1}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.4.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8phbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8phbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.220,PodIP:10.96.4.57,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f0cbcc67420ce25ee145d1ac68fca8fe7dbb482270af8a1fe44f75dc4d0fcb0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.4.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.916: INFO: Pod "webserver-deployment-845c8977d9-ssl97" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ssl97 webserver-deployment-845c8977d9- deployment-580  074e5192-6b72-4df2-9935-36fface43d33 37883 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c15359c2e32a3b135ff62abf8bcf9971ac105675fca13678171265811f0df5f cni.projectcalico.org/podIP:10.96.3.55/32 cni.projectcalico.org/podIPs:10.96.3.55/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367b810 0xc00367b811}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.3.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9fjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9fjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.2.236,PodIP:10.96.3.55,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ed78dc532a5f94976a337a1f8aedcd461ce095a8c49fae9d8e40a2997e54b61b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.3.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-vknz8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vknz8 webserver-deployment-845c8977d9- deployment-580  720527f3-9196-469a-8270-d45575050b15 37892 0 2023-04-06 11:39:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:049a03a3ac9007e1639cba739c7cf3fe45f8b688fa5a00bb82310811c800697f cni.projectcalico.org/podIP:10.96.2.197/32 cni.projectcalico.org/podIPs:10.96.2.197/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc00367bb20 0xc00367bb21}] [] [{kube-controller-manager Update v1 2023-04-06 11:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-04-06 11:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-06 11:39:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.96.2.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vl64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vl64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:39:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.68,PodIP:10.96.2.197,StartTime:2023-04-06 11:39:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-06 11:39:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0dabcdd462931913020a9725df39c166955696c948acef068aa05e1cb0355a2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.96.2.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-wktj5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wktj5 webserver-deployment-845c8977d9- deployment-580  aab9b1fd-cf64-434f-84b7-c1f67a5a916d 37981 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc003d8a0e0 0xc003d8a0e1}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vw7pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vw7pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-z7srx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7srx webserver-deployment-845c8977d9- deployment-580  4690efbd-53f1-4d5b-95f9-bb330c5f363e 37989 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc003d8a240 0xc003d8a241}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9fnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9fnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-06 11:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.148,PodIP:,StartTime:2023-04-06 11:40:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:40:00.917: INFO: Pod "webserver-deployment-845c8977d9-zx444" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zx444 webserver-deployment-845c8977d9- deployment-580  5a92fbbf-8a0d-427b-9e9b-eaf0fb3aea45 37988 0 2023-04-06 11:40:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 de10b022-8948-4706-8e3c-ac46b4f1e9ef 0xc003d8a3f7 0xc003d8a3f8}] [] [{kube-controller-manager Update v1 2023-04-06 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de10b022-8948-4706-8e3c-ac46b4f1e9ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj6rm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj6rm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 11:40:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-580" for this suite. 04/06/23 11:40:00.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:40:00.937
Apr  6 11:40:00.937: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 11:40:00.939
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:00.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:00.965
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 04/06/23 11:40:00.976
Apr  6 11:40:00.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr  6 11:40:01.142: INFO: stderr: ""
Apr  6 11:40:01.143: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 04/06/23 11:40:01.143
Apr  6 11:40:01.143: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr  6 11:40:01.143: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8203" to be "running and ready, or succeeded"
Apr  6 11:40:01.149: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.852118ms
Apr  6 11:40:01.149: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d' to be 'Running' but was 'Pending'
Apr  6 11:40:03.159: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.015803874s
Apr  6 11:40:03.159: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr  6 11:40:03.159: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 04/06/23 11:40:03.159
Apr  6 11:40:03.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator'
Apr  6 11:40:03.334: INFO: stderr: ""
Apr  6 11:40:03.334: INFO: stdout: "I0406 11:40:02.320961       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/69v 377\nI0406 11:40:02.521462       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/zghn 331\nI0406 11:40:02.721989       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/5gkw 207\nI0406 11:40:02.921503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/j2s 486\nI0406 11:40:03.121565       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/s2s 398\nI0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\n"
STEP: limiting log lines 04/06/23 11:40:03.334
Apr  6 11:40:03.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --tail=1'
Apr  6 11:40:03.545: INFO: stderr: ""
Apr  6 11:40:03.545: INFO: stdout: "I0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\n"
Apr  6 11:40:03.545: INFO: got output "I0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\n"
STEP: limiting log bytes 04/06/23 11:40:03.545
Apr  6 11:40:03.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --limit-bytes=1'
Apr  6 11:40:03.706: INFO: stderr: ""
Apr  6 11:40:03.706: INFO: stdout: "I"
Apr  6 11:40:03.706: INFO: got output "I"
STEP: exposing timestamps 04/06/23 11:40:03.706
Apr  6 11:40:03.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --tail=1 --timestamps'
Apr  6 11:40:03.917: INFO: stderr: ""
Apr  6 11:40:03.917: INFO: stdout: "2023-04-06T11:40:03.722090110Z I0406 11:40:03.721902       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z7s 356\n"
Apr  6 11:40:03.917: INFO: got output "2023-04-06T11:40:03.722090110Z I0406 11:40:03.721902       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z7s 356\n"
STEP: restricting to a time range 04/06/23 11:40:03.917
Apr  6 11:40:06.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --since=1s'
Apr  6 11:40:06.625: INFO: stderr: ""
Apr  6 11:40:06.625: INFO: stdout: "I0406 11:40:05.721807       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/222h 421\nI0406 11:40:05.921145       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/b6w 590\nI0406 11:40:06.125343       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/b78 489\nI0406 11:40:06.321741       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/hqj 440\nI0406 11:40:06.529448       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/rp48 455\n"
Apr  6 11:40:06.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --since=24h'
Apr  6 11:40:06.855: INFO: stderr: ""
Apr  6 11:40:06.855: INFO: stdout: "I0406 11:40:02.320961       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/69v 377\nI0406 11:40:02.521462       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/zghn 331\nI0406 11:40:02.721989       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/5gkw 207\nI0406 11:40:02.921503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/j2s 486\nI0406 11:40:03.121565       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/s2s 398\nI0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\nI0406 11:40:03.521499       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/c777 544\nI0406 11:40:03.721902       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z7s 356\nI0406 11:40:03.921112       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/jtv 327\nI0406 11:40:04.121592       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/xgd 209\nI0406 11:40:04.321945       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/954 470\nI0406 11:40:04.521471       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/tzz 254\nI0406 11:40:04.721919       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/466 222\nI0406 11:40:04.921335       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/v6dz 283\nI0406 11:40:05.121841       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/rgq 573\nI0406 11:40:05.323289       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/r5b 422\nI0406 11:40:05.521413       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/pj4 393\nI0406 11:40:05.721807       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/222h 421\nI0406 11:40:05.921145       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/b6w 590\nI0406 11:40:06.125343       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/b78 489\nI0406 11:40:06.321741       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/hqj 440\nI0406 11:40:06.529448       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/rp48 455\nI0406 11:40:06.730476       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/47tj 335\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Apr  6 11:40:06.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 delete pod logs-generator'
Apr  6 11:40:07.982: INFO: stderr: ""
Apr  6 11:40:07.982: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 11:40:07.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8203" for this suite. 04/06/23 11:40:07.991
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":302,"skipped":5456,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.060 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:40:00.937
    Apr  6 11:40:00.937: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 11:40:00.939
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:00.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:00.965
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 04/06/23 11:40:00.976
    Apr  6 11:40:00.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Apr  6 11:40:01.142: INFO: stderr: ""
    Apr  6 11:40:01.143: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 04/06/23 11:40:01.143
    Apr  6 11:40:01.143: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Apr  6 11:40:01.143: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8203" to be "running and ready, or succeeded"
    Apr  6 11:40:01.149: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.852118ms
    Apr  6 11:40:01.149: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d' to be 'Running' but was 'Pending'
    Apr  6 11:40:03.159: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.015803874s
    Apr  6 11:40:03.159: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Apr  6 11:40:03.159: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 04/06/23 11:40:03.159
    Apr  6 11:40:03.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator'
    Apr  6 11:40:03.334: INFO: stderr: ""
    Apr  6 11:40:03.334: INFO: stdout: "I0406 11:40:02.320961       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/69v 377\nI0406 11:40:02.521462       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/zghn 331\nI0406 11:40:02.721989       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/5gkw 207\nI0406 11:40:02.921503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/j2s 486\nI0406 11:40:03.121565       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/s2s 398\nI0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\n"
    STEP: limiting log lines 04/06/23 11:40:03.334
    Apr  6 11:40:03.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --tail=1'
    Apr  6 11:40:03.545: INFO: stderr: ""
    Apr  6 11:40:03.545: INFO: stdout: "I0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\n"
    Apr  6 11:40:03.545: INFO: got output "I0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\n"
    STEP: limiting log bytes 04/06/23 11:40:03.545
    Apr  6 11:40:03.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --limit-bytes=1'
    Apr  6 11:40:03.706: INFO: stderr: ""
    Apr  6 11:40:03.706: INFO: stdout: "I"
    Apr  6 11:40:03.706: INFO: got output "I"
    STEP: exposing timestamps 04/06/23 11:40:03.706
    Apr  6 11:40:03.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --tail=1 --timestamps'
    Apr  6 11:40:03.917: INFO: stderr: ""
    Apr  6 11:40:03.917: INFO: stdout: "2023-04-06T11:40:03.722090110Z I0406 11:40:03.721902       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z7s 356\n"
    Apr  6 11:40:03.917: INFO: got output "2023-04-06T11:40:03.722090110Z I0406 11:40:03.721902       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z7s 356\n"
    STEP: restricting to a time range 04/06/23 11:40:03.917
    Apr  6 11:40:06.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --since=1s'
    Apr  6 11:40:06.625: INFO: stderr: ""
    Apr  6 11:40:06.625: INFO: stdout: "I0406 11:40:05.721807       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/222h 421\nI0406 11:40:05.921145       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/b6w 590\nI0406 11:40:06.125343       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/b78 489\nI0406 11:40:06.321741       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/hqj 440\nI0406 11:40:06.529448       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/rp48 455\n"
    Apr  6 11:40:06.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 logs logs-generator logs-generator --since=24h'
    Apr  6 11:40:06.855: INFO: stderr: ""
    Apr  6 11:40:06.855: INFO: stdout: "I0406 11:40:02.320961       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/69v 377\nI0406 11:40:02.521462       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/zghn 331\nI0406 11:40:02.721989       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/5gkw 207\nI0406 11:40:02.921503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/j2s 486\nI0406 11:40:03.121565       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/s2s 398\nI0406 11:40:03.322057       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/854 267\nI0406 11:40:03.521499       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/c777 544\nI0406 11:40:03.721902       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/z7s 356\nI0406 11:40:03.921112       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/jtv 327\nI0406 11:40:04.121592       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/xgd 209\nI0406 11:40:04.321945       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/954 470\nI0406 11:40:04.521471       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/tzz 254\nI0406 11:40:04.721919       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/466 222\nI0406 11:40:04.921335       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/v6dz 283\nI0406 11:40:05.121841       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/rgq 573\nI0406 11:40:05.323289       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/r5b 422\nI0406 11:40:05.521413       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/pj4 393\nI0406 11:40:05.721807       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/222h 421\nI0406 11:40:05.921145       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/b6w 590\nI0406 11:40:06.125343       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/b78 489\nI0406 11:40:06.321741       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/hqj 440\nI0406 11:40:06.529448       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/rp48 455\nI0406 11:40:06.730476       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/47tj 335\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Apr  6 11:40:06.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-8203 delete pod logs-generator'
    Apr  6 11:40:07.982: INFO: stderr: ""
    Apr  6 11:40:07.982: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 11:40:07.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8203" for this suite. 04/06/23 11:40:07.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:40:08.007
Apr  6 11:40:08.008: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename replicaset 04/06/23 11:40:08.009
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:08.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:08.07
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/06/23 11:40:08.089
Apr  6 11:40:08.110: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/06/23 11:40:08.11
Apr  6 11:40:08.110: INFO: Waiting up to 5m0s for pod "test-rs-g2rzz" in namespace "replicaset-77" to be "running"
Apr  6 11:40:08.121: INFO: Pod "test-rs-g2rzz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.832198ms
Apr  6 11:40:10.128: INFO: Pod "test-rs-g2rzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017744595s
Apr  6 11:40:10.128: INFO: Pod "test-rs-g2rzz" satisfied condition "running"
STEP: getting scale subresource 04/06/23 11:40:10.128
STEP: updating a scale subresource 04/06/23 11:40:10.132
STEP: verifying the replicaset Spec.Replicas was modified 04/06/23 11:40:10.139
STEP: Patch a scale subresource 04/06/23 11:40:10.143
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Apr  6 11:40:10.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-77" for this suite. 04/06/23 11:40:10.158
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":303,"skipped":5472,"failed":0}
------------------------------
â€¢ [2.157 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:40:08.007
    Apr  6 11:40:08.008: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename replicaset 04/06/23 11:40:08.009
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:08.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:08.07
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/06/23 11:40:08.089
    Apr  6 11:40:08.110: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/06/23 11:40:08.11
    Apr  6 11:40:08.110: INFO: Waiting up to 5m0s for pod "test-rs-g2rzz" in namespace "replicaset-77" to be "running"
    Apr  6 11:40:08.121: INFO: Pod "test-rs-g2rzz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.832198ms
    Apr  6 11:40:10.128: INFO: Pod "test-rs-g2rzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017744595s
    Apr  6 11:40:10.128: INFO: Pod "test-rs-g2rzz" satisfied condition "running"
    STEP: getting scale subresource 04/06/23 11:40:10.128
    STEP: updating a scale subresource 04/06/23 11:40:10.132
    STEP: verifying the replicaset Spec.Replicas was modified 04/06/23 11:40:10.139
    STEP: Patch a scale subresource 04/06/23 11:40:10.143
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Apr  6 11:40:10.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-77" for this suite. 04/06/23 11:40:10.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:40:10.17
Apr  6 11:40:10.170: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename containers 04/06/23 11:40:10.173
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:10.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:10.194
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 04/06/23 11:40:10.2
Apr  6 11:40:10.211: INFO: Waiting up to 5m0s for pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16" in namespace "containers-9016" to be "Succeeded or Failed"
Apr  6 11:40:10.217: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.461562ms
Apr  6 11:40:12.224: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012941911s
Apr  6 11:40:14.224: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012546015s
STEP: Saw pod success 04/06/23 11:40:14.224
Apr  6 11:40:14.224: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16" satisfied condition "Succeeded or Failed"
Apr  6 11:40:14.233: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-containers-d366c57f-6c94-4128-885c-35e7c0139c16 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:40:14.284
Apr  6 11:40:14.302: INFO: Waiting for pod client-containers-d366c57f-6c94-4128-885c-35e7c0139c16 to disappear
Apr  6 11:40:14.311: INFO: Pod client-containers-d366c57f-6c94-4128-885c-35e7c0139c16 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Apr  6 11:40:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9016" for this suite. 04/06/23 11:40:14.326
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":304,"skipped":5480,"failed":0}
------------------------------
â€¢ [4.162 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:40:10.17
    Apr  6 11:40:10.170: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename containers 04/06/23 11:40:10.173
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:10.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:10.194
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 04/06/23 11:40:10.2
    Apr  6 11:40:10.211: INFO: Waiting up to 5m0s for pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16" in namespace "containers-9016" to be "Succeeded or Failed"
    Apr  6 11:40:10.217: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.461562ms
    Apr  6 11:40:12.224: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012941911s
    Apr  6 11:40:14.224: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012546015s
    STEP: Saw pod success 04/06/23 11:40:14.224
    Apr  6 11:40:14.224: INFO: Pod "client-containers-d366c57f-6c94-4128-885c-35e7c0139c16" satisfied condition "Succeeded or Failed"
    Apr  6 11:40:14.233: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-containers-d366c57f-6c94-4128-885c-35e7c0139c16 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:40:14.284
    Apr  6 11:40:14.302: INFO: Waiting for pod client-containers-d366c57f-6c94-4128-885c-35e7c0139c16 to disappear
    Apr  6 11:40:14.311: INFO: Pod client-containers-d366c57f-6c94-4128-885c-35e7c0139c16 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Apr  6 11:40:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9016" for this suite. 04/06/23 11:40:14.326
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:40:14.334
Apr  6 11:40:14.334: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename gc 04/06/23 11:40:14.335
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:14.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:14.359
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 04/06/23 11:40:14.375
STEP: delete the rc 04/06/23 11:40:19.39
STEP: wait for the rc to be deleted 04/06/23 11:40:19.398
Apr  6 11:40:20.426: INFO: 91 pods remaining
Apr  6 11:40:20.426: INFO: 91 pods has nil DeletionTimestamp
Apr  6 11:40:20.426: INFO: 
Apr  6 11:40:21.431: INFO: 70 pods remaining
Apr  6 11:40:21.431: INFO: 70 pods has nil DeletionTimestamp
Apr  6 11:40:21.431: INFO: 
Apr  6 11:40:22.435: INFO: 70 pods remaining
Apr  6 11:40:22.435: INFO: 70 pods has nil DeletionTimestamp
Apr  6 11:40:22.435: INFO: 
Apr  6 11:40:23.459: INFO: 40 pods remaining
Apr  6 11:40:23.459: INFO: 40 pods has nil DeletionTimestamp
Apr  6 11:40:23.460: INFO: 
Apr  6 11:40:24.417: INFO: 40 pods remaining
Apr  6 11:40:24.418: INFO: 40 pods has nil DeletionTimestamp
Apr  6 11:40:24.418: INFO: 
Apr  6 11:40:25.422: INFO: 11 pods remaining
Apr  6 11:40:25.422: INFO: 11 pods has nil DeletionTimestamp
Apr  6 11:40:25.422: INFO: 
STEP: Gathering metrics 04/06/23 11:40:26.409
W0406 11:40:26.442516      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 11:40:26.442: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Apr  6 11:40:26.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3566" for this suite. 04/06/23 11:40:26.452
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":305,"skipped":5483,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.126 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:40:14.334
    Apr  6 11:40:14.334: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename gc 04/06/23 11:40:14.335
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:14.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:14.359
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 04/06/23 11:40:14.375
    STEP: delete the rc 04/06/23 11:40:19.39
    STEP: wait for the rc to be deleted 04/06/23 11:40:19.398
    Apr  6 11:40:20.426: INFO: 91 pods remaining
    Apr  6 11:40:20.426: INFO: 91 pods has nil DeletionTimestamp
    Apr  6 11:40:20.426: INFO: 
    Apr  6 11:40:21.431: INFO: 70 pods remaining
    Apr  6 11:40:21.431: INFO: 70 pods has nil DeletionTimestamp
    Apr  6 11:40:21.431: INFO: 
    Apr  6 11:40:22.435: INFO: 70 pods remaining
    Apr  6 11:40:22.435: INFO: 70 pods has nil DeletionTimestamp
    Apr  6 11:40:22.435: INFO: 
    Apr  6 11:40:23.459: INFO: 40 pods remaining
    Apr  6 11:40:23.459: INFO: 40 pods has nil DeletionTimestamp
    Apr  6 11:40:23.460: INFO: 
    Apr  6 11:40:24.417: INFO: 40 pods remaining
    Apr  6 11:40:24.418: INFO: 40 pods has nil DeletionTimestamp
    Apr  6 11:40:24.418: INFO: 
    Apr  6 11:40:25.422: INFO: 11 pods remaining
    Apr  6 11:40:25.422: INFO: 11 pods has nil DeletionTimestamp
    Apr  6 11:40:25.422: INFO: 
    STEP: Gathering metrics 04/06/23 11:40:26.409
    W0406 11:40:26.442516      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr  6 11:40:26.442: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Apr  6 11:40:26.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3566" for this suite. 04/06/23 11:40:26.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:40:26.464
Apr  6 11:40:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:40:26.474
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:26.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:26.514
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 04/06/23 11:40:26.521
STEP: Creating a ResourceQuota 04/06/23 11:40:31.532
STEP: Ensuring resource quota status is calculated 04/06/23 11:40:31.544
STEP: Creating a Service 04/06/23 11:40:33.554
STEP: Creating a NodePort Service 04/06/23 11:40:33.577
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/06/23 11:40:33.6
STEP: Ensuring resource quota status captures service creation 04/06/23 11:40:33.63
STEP: Deleting Services 04/06/23 11:40:35.637
STEP: Ensuring resource quota status released usage 04/06/23 11:40:35.694
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:40:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6050" for this suite. 04/06/23 11:40:37.712
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":306,"skipped":5550,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.253 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:40:26.464
    Apr  6 11:40:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:40:26.474
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:26.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:26.514
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 04/06/23 11:40:26.521
    STEP: Creating a ResourceQuota 04/06/23 11:40:31.532
    STEP: Ensuring resource quota status is calculated 04/06/23 11:40:31.544
    STEP: Creating a Service 04/06/23 11:40:33.554
    STEP: Creating a NodePort Service 04/06/23 11:40:33.577
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/06/23 11:40:33.6
    STEP: Ensuring resource quota status captures service creation 04/06/23 11:40:33.63
    STEP: Deleting Services 04/06/23 11:40:35.637
    STEP: Ensuring resource quota status released usage 04/06/23 11:40:35.694
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:40:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6050" for this suite. 04/06/23 11:40:37.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:40:37.718
Apr  6 11:40:37.718: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename aggregator 04/06/23 11:40:37.719
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:37.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:37.746
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Apr  6 11:40:37.754: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 04/06/23 11:40:37.755
Apr  6 11:40:38.598: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr  6 11:40:40.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:42.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:44.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:46.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:48.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:50.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:52.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:54.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:56.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:40:58.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:41:00.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:41:02.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 11:41:04.910: INFO: Waited 242.234937ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 04/06/23 11:41:05.575
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/06/23 11:41:05.581
STEP: List APIServices 04/06/23 11:41:05.59
Apr  6 11:41:05.616: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Apr  6 11:41:05.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7221" for this suite. 04/06/23 11:41:05.942
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":307,"skipped":5557,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.236 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:40:37.718
    Apr  6 11:40:37.718: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename aggregator 04/06/23 11:40:37.719
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:40:37.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:40:37.746
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Apr  6 11:40:37.754: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 04/06/23 11:40:37.755
    Apr  6 11:40:38.598: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Apr  6 11:40:40.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:42.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:44.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:46.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:48.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:50.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:52.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:54.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:56.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:40:58.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:41:00.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:41:02.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 6, 11, 40, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr  6 11:41:04.910: INFO: Waited 242.234937ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 04/06/23 11:41:05.575
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/06/23 11:41:05.581
    STEP: List APIServices 04/06/23 11:41:05.59
    Apr  6 11:41:05.616: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Apr  6 11:41:05.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-7221" for this suite. 04/06/23 11:41:05.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:41:05.955
Apr  6 11:41:05.955: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename cronjob 04/06/23 11:41:05.956
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:41:05.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:41:05.994
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 04/06/23 11:41:06.002
STEP: Ensuring no jobs are scheduled 04/06/23 11:41:06.011
STEP: Ensuring no job exists by listing jobs explicitly 04/06/23 11:46:06.045
STEP: Removing cronjob 04/06/23 11:46:06.056
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Apr  6 11:46:06.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9779" for this suite. 04/06/23 11:46:06.073
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":308,"skipped":5568,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.126 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:41:05.955
    Apr  6 11:41:05.955: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename cronjob 04/06/23 11:41:05.956
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:41:05.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:41:05.994
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 04/06/23 11:41:06.002
    STEP: Ensuring no jobs are scheduled 04/06/23 11:41:06.011
    STEP: Ensuring no job exists by listing jobs explicitly 04/06/23 11:46:06.045
    STEP: Removing cronjob 04/06/23 11:46:06.056
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Apr  6 11:46:06.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9779" for this suite. 04/06/23 11:46:06.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:46:06.082
Apr  6 11:46:06.083: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:46:06.084
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:06.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:06.125
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Apr  6 11:46:06.144: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/06/23 11:46:09.625
Apr  6 11:46:09.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 create -f -'
Apr  6 11:46:10.737: INFO: stderr: ""
Apr  6 11:46:10.737: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr  6 11:46:10.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 delete e2e-test-crd-publish-openapi-2990-crds test-cr'
Apr  6 11:46:10.881: INFO: stderr: ""
Apr  6 11:46:10.881: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr  6 11:46:10.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 apply -f -'
Apr  6 11:46:11.676: INFO: stderr: ""
Apr  6 11:46:11.677: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr  6 11:46:11.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 delete e2e-test-crd-publish-openapi-2990-crds test-cr'
Apr  6 11:46:11.866: INFO: stderr: ""
Apr  6 11:46:11.866: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/06/23 11:46:11.866
Apr  6 11:46:11.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 explain e2e-test-crd-publish-openapi-2990-crds'
Apr  6 11:46:12.334: INFO: stderr: ""
Apr  6 11:46:12.334: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2990-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:46:15.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3590" for this suite. 04/06/23 11:46:15.752
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":309,"skipped":5603,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.686 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:46:06.082
    Apr  6 11:46:06.083: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:46:06.084
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:06.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:06.125
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Apr  6 11:46:06.144: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/06/23 11:46:09.625
    Apr  6 11:46:09.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 create -f -'
    Apr  6 11:46:10.737: INFO: stderr: ""
    Apr  6 11:46:10.737: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr  6 11:46:10.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 delete e2e-test-crd-publish-openapi-2990-crds test-cr'
    Apr  6 11:46:10.881: INFO: stderr: ""
    Apr  6 11:46:10.881: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Apr  6 11:46:10.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 apply -f -'
    Apr  6 11:46:11.676: INFO: stderr: ""
    Apr  6 11:46:11.677: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr  6 11:46:11.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 --namespace=crd-publish-openapi-3590 delete e2e-test-crd-publish-openapi-2990-crds test-cr'
    Apr  6 11:46:11.866: INFO: stderr: ""
    Apr  6 11:46:11.866: INFO: stdout: "e2e-test-crd-publish-openapi-2990-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/06/23 11:46:11.866
    Apr  6 11:46:11.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=crd-publish-openapi-3590 explain e2e-test-crd-publish-openapi-2990-crds'
    Apr  6 11:46:12.334: INFO: stderr: ""
    Apr  6 11:46:12.334: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2990-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:46:15.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3590" for this suite. 04/06/23 11:46:15.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:46:15.772
Apr  6 11:46:15.772: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename security-context-test 04/06/23 11:46:15.775
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:15.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:15.84
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Apr  6 11:46:15.867: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b" in namespace "security-context-test-415" to be "Succeeded or Failed"
Apr  6 11:46:15.876: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.538068ms
Apr  6 11:46:17.886: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019194792s
Apr  6 11:46:19.908: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04100209s
Apr  6 11:46:19.908: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Apr  6 11:46:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-415" for this suite. 04/06/23 11:46:19.929
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":310,"skipped":5632,"failed":0}
------------------------------
â€¢ [4.180 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:46:15.772
    Apr  6 11:46:15.772: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename security-context-test 04/06/23 11:46:15.775
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:15.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:15.84
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Apr  6 11:46:15.867: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b" in namespace "security-context-test-415" to be "Succeeded or Failed"
    Apr  6 11:46:15.876: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.538068ms
    Apr  6 11:46:17.886: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019194792s
    Apr  6 11:46:19.908: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04100209s
    Apr  6 11:46:19.908: INFO: Pod "busybox-user-65534-3a915115-044d-4100-a0d9-11057c5cb95b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Apr  6 11:46:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-415" for this suite. 04/06/23 11:46:19.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:46:19.954
Apr  6 11:46:19.954: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 11:46:19.956
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:19.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:19.984
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 11:46:19.992
Apr  6 11:46:19.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr  6 11:46:20.259: INFO: stderr: ""
Apr  6 11:46:20.259: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 04/06/23 11:46:20.259
STEP: verifying the pod e2e-test-httpd-pod was created 04/06/23 11:46:25.311
Apr  6 11:46:25.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 get pod e2e-test-httpd-pod -o json'
Apr  6 11:46:25.494: INFO: stderr: ""
Apr  6 11:46:25.494: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a73926becb155179396b62374cede43f80f95ad9f9e76c1e2d15a00938c7b9f1\",\n            \"cni.projectcalico.org/podIP\": \"10.96.1.214/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.96.1.214/32\"\n        },\n        \"creationTimestamp\": \"2023-04-06T11:46:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4266\",\n        \"resourceVersion\": \"40570\",\n        \"uid\": \"1e325d9a-ff38-47cb-aef8-16c4e7f7a7e9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5t9ct\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5t9ct\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6ab67440bc03ae26a4ff5dffdb06a3ac7f8fd630b74173243008501a2afe1e9b\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-06T11:46:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.1.148\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.96.1.214\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.96.1.214\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-06T11:46:20Z\"\n    }\n}\n"
STEP: replace the image in the pod 04/06/23 11:46:25.495
Apr  6 11:46:25.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 replace -f -'
Apr  6 11:46:26.400: INFO: stderr: ""
Apr  6 11:46:26.401: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 04/06/23 11:46:26.401
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Apr  6 11:46:26.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 delete pods e2e-test-httpd-pod'
Apr  6 11:46:28.152: INFO: stderr: ""
Apr  6 11:46:28.152: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 11:46:28.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4266" for this suite. 04/06/23 11:46:28.162
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":311,"skipped":5662,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.215 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:46:19.954
    Apr  6 11:46:19.954: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 11:46:19.956
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:19.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:19.984
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 04/06/23 11:46:19.992
    Apr  6 11:46:19.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr  6 11:46:20.259: INFO: stderr: ""
    Apr  6 11:46:20.259: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 04/06/23 11:46:20.259
    STEP: verifying the pod e2e-test-httpd-pod was created 04/06/23 11:46:25.311
    Apr  6 11:46:25.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 get pod e2e-test-httpd-pod -o json'
    Apr  6 11:46:25.494: INFO: stderr: ""
    Apr  6 11:46:25.494: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a73926becb155179396b62374cede43f80f95ad9f9e76c1e2d15a00938c7b9f1\",\n            \"cni.projectcalico.org/podIP\": \"10.96.1.214/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.96.1.214/32\"\n        },\n        \"creationTimestamp\": \"2023-04-06T11:46:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4266\",\n        \"resourceVersion\": \"40570\",\n        \"uid\": \"1e325d9a-ff38-47cb-aef8-16c4e7f7a7e9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud\"\n                    }\n                ],\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5t9ct\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5t9ct\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-06T11:46:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6ab67440bc03ae26a4ff5dffdb06a3ac7f8fd630b74173243008501a2afe1e9b\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-06T11:46:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.1.148\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.96.1.214\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.96.1.214\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-06T11:46:20Z\"\n    }\n}\n"
    STEP: replace the image in the pod 04/06/23 11:46:25.495
    Apr  6 11:46:25.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 replace -f -'
    Apr  6 11:46:26.400: INFO: stderr: ""
    Apr  6 11:46:26.401: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 04/06/23 11:46:26.401
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Apr  6 11:46:26.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4266 delete pods e2e-test-httpd-pod'
    Apr  6 11:46:28.152: INFO: stderr: ""
    Apr  6 11:46:28.152: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 11:46:28.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4266" for this suite. 04/06/23 11:46:28.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:46:28.171
Apr  6 11:46:28.171: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 11:46:28.172
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:28.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:28.2
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 04/06/23 11:46:28.205
Apr  6 11:46:28.205: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9758 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 04/06/23 11:46:28.307
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 11:46:28.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9758" for this suite. 04/06/23 11:46:28.353
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":312,"skipped":5677,"failed":0}
------------------------------
â€¢ [0.193 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:46:28.171
    Apr  6 11:46:28.171: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 11:46:28.172
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:28.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:28.2
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 04/06/23 11:46:28.205
    Apr  6 11:46:28.205: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-9758 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 04/06/23 11:46:28.307
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 11:46:28.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9758" for this suite. 04/06/23 11:46:28.353
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:46:28.364
Apr  6 11:46:28.364: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 11:46:28.366
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:28.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:28.399
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 04/06/23 11:46:28.404
STEP: submitting the pod to kubernetes 04/06/23 11:46:28.404
Apr  6 11:46:28.416: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" in namespace "pods-328" to be "running and ready"
Apr  6 11:46:28.419: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627328ms
Apr  6 11:46:28.419: INFO: The phase of Pod pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:46:30.430: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013972287s
Apr  6 11:46:30.430: INFO: The phase of Pod pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1 is Running (Ready = true)
Apr  6 11:46:30.430: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/06/23 11:46:30.44
STEP: updating the pod 04/06/23 11:46:30.446
Apr  6 11:46:30.959: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1"
Apr  6 11:46:30.959: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" in namespace "pods-328" to be "terminated with reason DeadlineExceeded"
Apr  6 11:46:30.964: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=true. Elapsed: 5.194488ms
Apr  6 11:46:32.970: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010872198s
Apr  6 11:46:34.971: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=false. Elapsed: 4.012290761s
Apr  6 11:46:36.971: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.012407272s
Apr  6 11:46:36.971: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Apr  6 11:46:36.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-328" for this suite. 04/06/23 11:46:36.981
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":313,"skipped":5679,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.626 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:46:28.364
    Apr  6 11:46:28.364: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 11:46:28.366
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:28.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:28.399
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 04/06/23 11:46:28.404
    STEP: submitting the pod to kubernetes 04/06/23 11:46:28.404
    Apr  6 11:46:28.416: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" in namespace "pods-328" to be "running and ready"
    Apr  6 11:46:28.419: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627328ms
    Apr  6 11:46:28.419: INFO: The phase of Pod pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:46:30.430: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013972287s
    Apr  6 11:46:30.430: INFO: The phase of Pod pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1 is Running (Ready = true)
    Apr  6 11:46:30.430: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/06/23 11:46:30.44
    STEP: updating the pod 04/06/23 11:46:30.446
    Apr  6 11:46:30.959: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1"
    Apr  6 11:46:30.959: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" in namespace "pods-328" to be "terminated with reason DeadlineExceeded"
    Apr  6 11:46:30.964: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=true. Elapsed: 5.194488ms
    Apr  6 11:46:32.970: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010872198s
    Apr  6 11:46:34.971: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Running", Reason="", readiness=false. Elapsed: 4.012290761s
    Apr  6 11:46:36.971: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.012407272s
    Apr  6 11:46:36.971: INFO: Pod "pod-update-activedeadlineseconds-ce86c8af-694d-4dfc-85f5-6447cdca53f1" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Apr  6 11:46:36.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-328" for this suite. 04/06/23 11:46:36.981
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:46:36.99
Apr  6 11:46:36.990: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pod-network-test 04/06/23 11:46:36.991
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:37.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:37.03
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7612 04/06/23 11:46:37.034
STEP: creating a selector 04/06/23 11:46:37.034
STEP: Creating the service pods in kubernetes 04/06/23 11:46:37.035
Apr  6 11:46:37.035: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 11:46:37.110: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7612" to be "running and ready"
Apr  6 11:46:37.114: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672159ms
Apr  6 11:46:37.115: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:46:39.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010647521s
Apr  6 11:46:39.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:41.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011855844s
Apr  6 11:46:41.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:43.121: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011533912s
Apr  6 11:46:43.121: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:45.142: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.032107418s
Apr  6 11:46:45.142: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:47.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010007986s
Apr  6 11:46:47.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:49.123: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013651196s
Apr  6 11:46:49.133: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:51.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01230082s
Apr  6 11:46:51.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:53.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010489127s
Apr  6 11:46:53.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:55.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012653821s
Apr  6 11:46:55.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:57.121: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011754889s
Apr  6 11:46:57.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr  6 11:46:59.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012137212s
Apr  6 11:46:59.122: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr  6 11:46:59.122: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr  6 11:46:59.126: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7612" to be "running and ready"
Apr  6 11:46:59.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.499157ms
Apr  6 11:46:59.130: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr  6 11:46:59.131: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr  6 11:46:59.134: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7612" to be "running and ready"
Apr  6 11:46:59.137: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.186041ms
Apr  6 11:46:59.137: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr  6 11:46:59.137: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr  6 11:46:59.142: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7612" to be "running and ready"
Apr  6 11:46:59.147: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.015372ms
Apr  6 11:46:59.147: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr  6 11:46:59.147: INFO: Pod "netserver-3" satisfied condition "running and ready"
Apr  6 11:46:59.150: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7612" to be "running and ready"
Apr  6 11:46:59.154: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.115983ms
Apr  6 11:46:59.154: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Apr  6 11:46:59.154: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 04/06/23 11:46:59.159
Apr  6 11:46:59.166: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7612" to be "running"
Apr  6 11:46:59.169: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.223481ms
Apr  6 11:47:01.178: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011235159s
Apr  6 11:47:01.178: INFO: Pod "test-container-pod" satisfied condition "running"
Apr  6 11:47:01.186: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Apr  6 11:47:01.186: INFO: Breadth first check of 10.96.3.77 on host 10.250.2.236...
Apr  6 11:47:01.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.3.77&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:47:01.194: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:01.195: INFO: ExecWithOptions: Clientset creation
Apr  6 11:47:01.195: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.3.77%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 11:47:01.701: INFO: Waiting for responses: map[]
Apr  6 11:47:01.701: INFO: reached 10.96.3.77 after 0/1 tries
Apr  6 11:47:01.702: INFO: Breadth first check of 10.96.2.224 on host 10.250.0.68...
Apr  6 11:47:01.708: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.2.224&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:47:01.708: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:01.709: INFO: ExecWithOptions: Clientset creation
Apr  6 11:47:01.709: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.2.224%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 11:47:02.207: INFO: Waiting for responses: map[]
Apr  6 11:47:02.207: INFO: reached 10.96.2.224 after 0/1 tries
Apr  6 11:47:02.207: INFO: Breadth first check of 10.96.0.76 on host 10.250.2.89...
Apr  6 11:47:02.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.0.76&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:47:02.211: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:02.212: INFO: ExecWithOptions: Clientset creation
Apr  6 11:47:02.212: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.0.76%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 11:47:02.718: INFO: Waiting for responses: map[]
Apr  6 11:47:02.718: INFO: reached 10.96.0.76 after 0/1 tries
Apr  6 11:47:02.718: INFO: Breadth first check of 10.96.1.215 on host 10.250.1.148...
Apr  6 11:47:02.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.1.215&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:47:02.724: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:02.726: INFO: ExecWithOptions: Clientset creation
Apr  6 11:47:02.726: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.1.215%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 11:47:03.098: INFO: Waiting for responses: map[]
Apr  6 11:47:03.098: INFO: reached 10.96.1.215 after 0/1 tries
Apr  6 11:47:03.098: INFO: Breadth first check of 10.96.4.80 on host 10.250.3.220...
Apr  6 11:47:03.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.4.80&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:47:03.103: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:03.104: INFO: ExecWithOptions: Clientset creation
Apr  6 11:47:03.104: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.4.80%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr  6 11:47:03.658: INFO: Waiting for responses: map[]
Apr  6 11:47:03.658: INFO: reached 10.96.4.80 after 0/1 tries
Apr  6 11:47:03.658: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Apr  6 11:47:03.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7612" for this suite. 04/06/23 11:47:03.671
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":314,"skipped":5683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.694 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:46:36.99
    Apr  6 11:46:36.990: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pod-network-test 04/06/23 11:46:36.991
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:46:37.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:46:37.03
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7612 04/06/23 11:46:37.034
    STEP: creating a selector 04/06/23 11:46:37.034
    STEP: Creating the service pods in kubernetes 04/06/23 11:46:37.035
    Apr  6 11:46:37.035: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr  6 11:46:37.110: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7612" to be "running and ready"
    Apr  6 11:46:37.114: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672159ms
    Apr  6 11:46:37.115: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:46:39.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010647521s
    Apr  6 11:46:39.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:41.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011855844s
    Apr  6 11:46:41.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:43.121: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011533912s
    Apr  6 11:46:43.121: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:45.142: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.032107418s
    Apr  6 11:46:45.142: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:47.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010007986s
    Apr  6 11:46:47.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:49.123: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013651196s
    Apr  6 11:46:49.133: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:51.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01230082s
    Apr  6 11:46:51.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:53.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010489127s
    Apr  6 11:46:53.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:55.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012653821s
    Apr  6 11:46:55.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:57.121: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011754889s
    Apr  6 11:46:57.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr  6 11:46:59.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012137212s
    Apr  6 11:46:59.122: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr  6 11:46:59.122: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr  6 11:46:59.126: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7612" to be "running and ready"
    Apr  6 11:46:59.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.499157ms
    Apr  6 11:46:59.130: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr  6 11:46:59.131: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr  6 11:46:59.134: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7612" to be "running and ready"
    Apr  6 11:46:59.137: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.186041ms
    Apr  6 11:46:59.137: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr  6 11:46:59.137: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr  6 11:46:59.142: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7612" to be "running and ready"
    Apr  6 11:46:59.147: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.015372ms
    Apr  6 11:46:59.147: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr  6 11:46:59.147: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Apr  6 11:46:59.150: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7612" to be "running and ready"
    Apr  6 11:46:59.154: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.115983ms
    Apr  6 11:46:59.154: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Apr  6 11:46:59.154: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 04/06/23 11:46:59.159
    Apr  6 11:46:59.166: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7612" to be "running"
    Apr  6 11:46:59.169: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.223481ms
    Apr  6 11:47:01.178: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011235159s
    Apr  6 11:47:01.178: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr  6 11:47:01.186: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Apr  6 11:47:01.186: INFO: Breadth first check of 10.96.3.77 on host 10.250.2.236...
    Apr  6 11:47:01.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.3.77&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:47:01.194: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:01.195: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:47:01.195: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.3.77%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 11:47:01.701: INFO: Waiting for responses: map[]
    Apr  6 11:47:01.701: INFO: reached 10.96.3.77 after 0/1 tries
    Apr  6 11:47:01.702: INFO: Breadth first check of 10.96.2.224 on host 10.250.0.68...
    Apr  6 11:47:01.708: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.2.224&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:47:01.708: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:01.709: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:47:01.709: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.2.224%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 11:47:02.207: INFO: Waiting for responses: map[]
    Apr  6 11:47:02.207: INFO: reached 10.96.2.224 after 0/1 tries
    Apr  6 11:47:02.207: INFO: Breadth first check of 10.96.0.76 on host 10.250.2.89...
    Apr  6 11:47:02.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.0.76&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:47:02.211: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:02.212: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:47:02.212: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.0.76%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 11:47:02.718: INFO: Waiting for responses: map[]
    Apr  6 11:47:02.718: INFO: reached 10.96.0.76 after 0/1 tries
    Apr  6 11:47:02.718: INFO: Breadth first check of 10.96.1.215 on host 10.250.1.148...
    Apr  6 11:47:02.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.1.215&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:47:02.724: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:02.726: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:47:02.726: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.1.215%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 11:47:03.098: INFO: Waiting for responses: map[]
    Apr  6 11:47:03.098: INFO: reached 10.96.1.215 after 0/1 tries
    Apr  6 11:47:03.098: INFO: Breadth first check of 10.96.4.80 on host 10.250.3.220...
    Apr  6 11:47:03.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.96.1.216:9080/dial?request=hostname&protocol=http&host=10.96.4.80&port=8083&tries=1'] Namespace:pod-network-test-7612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:47:03.103: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:03.104: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:47:03.104: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/pod-network-test-7612/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.96.1.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.96.4.80%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr  6 11:47:03.658: INFO: Waiting for responses: map[]
    Apr  6 11:47:03.658: INFO: reached 10.96.4.80 after 0/1 tries
    Apr  6 11:47:03.658: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Apr  6 11:47:03.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7612" for this suite. 04/06/23 11:47:03.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:47:03.688
Apr  6 11:47:03.689: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:47:03.691
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:03.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:03.8
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/06/23 11:47:03.835
Apr  6 11:47:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:08.709: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:47:26.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7103" for this suite. 04/06/23 11:47:26.982
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":315,"skipped":5694,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.308 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:47:03.688
    Apr  6 11:47:03.689: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename crd-publish-openapi 04/06/23 11:47:03.691
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:03.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:03.8
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/06/23 11:47:03.835
    Apr  6 11:47:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:08.709: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:47:26.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7103" for this suite. 04/06/23 11:47:26.982
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:47:26.996
Apr  6 11:47:26.997: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:47:26.998
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:27.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:27.029
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 04/06/23 11:47:27.038
Apr  6 11:47:27.054: INFO: Waiting up to 5m0s for pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2" in namespace "emptydir-8891" to be "Succeeded or Failed"
Apr  6 11:47:27.059: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878814ms
Apr  6 11:47:29.067: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012869558s
Apr  6 11:47:31.066: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012353057s
STEP: Saw pod success 04/06/23 11:47:31.066
Apr  6 11:47:31.066: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2" satisfied condition "Succeeded or Failed"
Apr  6 11:47:31.072: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-18a35f60-1520-4953-9f3b-523f3aca04e2 container test-container: <nil>
STEP: delete the pod 04/06/23 11:47:31.092
Apr  6 11:47:31.120: INFO: Waiting for pod pod-18a35f60-1520-4953-9f3b-523f3aca04e2 to disappear
Apr  6 11:47:31.125: INFO: Pod pod-18a35f60-1520-4953-9f3b-523f3aca04e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:47:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8891" for this suite. 04/06/23 11:47:31.151
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":316,"skipped":5697,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:47:26.996
    Apr  6 11:47:26.997: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:47:26.998
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:27.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:27.029
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/06/23 11:47:27.038
    Apr  6 11:47:27.054: INFO: Waiting up to 5m0s for pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2" in namespace "emptydir-8891" to be "Succeeded or Failed"
    Apr  6 11:47:27.059: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878814ms
    Apr  6 11:47:29.067: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012869558s
    Apr  6 11:47:31.066: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012353057s
    STEP: Saw pod success 04/06/23 11:47:31.066
    Apr  6 11:47:31.066: INFO: Pod "pod-18a35f60-1520-4953-9f3b-523f3aca04e2" satisfied condition "Succeeded or Failed"
    Apr  6 11:47:31.072: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-18a35f60-1520-4953-9f3b-523f3aca04e2 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:47:31.092
    Apr  6 11:47:31.120: INFO: Waiting for pod pod-18a35f60-1520-4953-9f3b-523f3aca04e2 to disappear
    Apr  6 11:47:31.125: INFO: Pod pod-18a35f60-1520-4953-9f3b-523f3aca04e2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:47:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8891" for this suite. 04/06/23 11:47:31.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:47:31.166
Apr  6 11:47:31.166: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename security-context-test 04/06/23 11:47:31.179
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:31.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:31.214
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Apr  6 11:47:31.236: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388" in namespace "security-context-test-1794" to be "Succeeded or Failed"
Apr  6 11:47:31.242: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": Phase="Pending", Reason="", readiness=false. Elapsed: 5.536568ms
Apr  6 11:47:33.249: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012714591s
Apr  6 11:47:35.252: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015981593s
Apr  6 11:47:35.253: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388" satisfied condition "Succeeded or Failed"
Apr  6 11:47:35.304: INFO: Got logs for pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Apr  6 11:47:35.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1794" for this suite. 04/06/23 11:47:35.313
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":317,"skipped":5708,"failed":0}
------------------------------
â€¢ [4.158 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:47:31.166
    Apr  6 11:47:31.166: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename security-context-test 04/06/23 11:47:31.179
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:31.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:31.214
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Apr  6 11:47:31.236: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388" in namespace "security-context-test-1794" to be "Succeeded or Failed"
    Apr  6 11:47:31.242: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": Phase="Pending", Reason="", readiness=false. Elapsed: 5.536568ms
    Apr  6 11:47:33.249: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012714591s
    Apr  6 11:47:35.252: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015981593s
    Apr  6 11:47:35.253: INFO: Pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388" satisfied condition "Succeeded or Failed"
    Apr  6 11:47:35.304: INFO: Got logs for pod "busybox-privileged-false-5b3ac5b3-5e78-4d3e-b2b4-3f5730dc2388": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Apr  6 11:47:35.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1794" for this suite. 04/06/23 11:47:35.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:47:35.324
Apr  6 11:47:35.324: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename dns 04/06/23 11:47:35.325
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:35.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:35.355
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/06/23 11:47:35.366
Apr  6 11:47:35.390: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4805  8cc41a08-105b-4bf3-a833-57fbf8434bd3 41017 0 2023-04-06 11:47:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-06 11:47:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n9qbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n9qbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 11:47:35.391: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4805" to be "running and ready"
Apr  6 11:47:35.399: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.438454ms
Apr  6 11:47:35.399: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:47:37.409: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.018054895s
Apr  6 11:47:37.409: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Apr  6 11:47:37.409: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 04/06/23 11:47:37.409
Apr  6 11:47:37.409: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4805 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:47:37.409: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:37.409: INFO: ExecWithOptions: Clientset creation
Apr  6 11:47:37.410: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/dns-4805/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 04/06/23 11:47:37.969
Apr  6 11:47:37.969: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4805 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:47:37.969: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:47:37.970: INFO: ExecWithOptions: Clientset creation
Apr  6 11:47:37.970: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/dns-4805/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr  6 11:47:38.565: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Apr  6 11:47:38.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4805" for this suite. 04/06/23 11:47:38.587
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":318,"skipped":5721,"failed":0}
------------------------------
â€¢ [3.271 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:47:35.324
    Apr  6 11:47:35.324: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename dns 04/06/23 11:47:35.325
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:35.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:35.355
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/06/23 11:47:35.366
    Apr  6 11:47:35.390: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4805  8cc41a08-105b-4bf3-a833-57fbf8434bd3 41017 0 2023-04-06 11:47:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-06 11:47:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n9qbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n9qbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr  6 11:47:35.391: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4805" to be "running and ready"
    Apr  6 11:47:35.399: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.438454ms
    Apr  6 11:47:35.399: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:47:37.409: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.018054895s
    Apr  6 11:47:37.409: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Apr  6 11:47:37.409: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 04/06/23 11:47:37.409
    Apr  6 11:47:37.409: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4805 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:47:37.409: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:37.409: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:47:37.410: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/dns-4805/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 04/06/23 11:47:37.969
    Apr  6 11:47:37.969: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4805 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:47:37.969: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:47:37.970: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:47:37.970: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/dns-4805/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr  6 11:47:38.565: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Apr  6 11:47:38.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4805" for this suite. 04/06/23 11:47:38.587
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:47:38.595
Apr  6 11:47:38.596: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename webhook 04/06/23 11:47:38.597
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:38.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:38.631
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 04/06/23 11:47:38.67
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:47:39.111
STEP: Deploying the webhook pod 04/06/23 11:47:39.139
STEP: Wait for the deployment to be ready 04/06/23 11:47:39.156
Apr  6 11:47:39.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/06/23 11:47:41.191
STEP: Verifying the service has paired with the endpoint 04/06/23 11:47:41.207
Apr  6 11:47:42.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/06/23 11:47:42.216
STEP: create a namespace for the webhook 04/06/23 11:47:42.358
STEP: create a configmap should be unconditionally rejected by the webhook 04/06/23 11:47:42.368
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:47:42.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7583" for this suite. 04/06/23 11:47:42.516
STEP: Destroying namespace "webhook-7583-markers" for this suite. 04/06/23 11:47:42.523
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":319,"skipped":5722,"failed":0}
------------------------------
â€¢ [3.971 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:47:38.595
    Apr  6 11:47:38.596: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename webhook 04/06/23 11:47:38.597
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:38.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:38.631
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 04/06/23 11:47:38.67
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/06/23 11:47:39.111
    STEP: Deploying the webhook pod 04/06/23 11:47:39.139
    STEP: Wait for the deployment to be ready 04/06/23 11:47:39.156
    Apr  6 11:47:39.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/06/23 11:47:41.191
    STEP: Verifying the service has paired with the endpoint 04/06/23 11:47:41.207
    Apr  6 11:47:42.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/06/23 11:47:42.216
    STEP: create a namespace for the webhook 04/06/23 11:47:42.358
    STEP: create a configmap should be unconditionally rejected by the webhook 04/06/23 11:47:42.368
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:47:42.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7583" for this suite. 04/06/23 11:47:42.516
    STEP: Destroying namespace "webhook-7583-markers" for this suite. 04/06/23 11:47:42.523
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:47:42.569
Apr  6 11:47:42.570: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename gc 04/06/23 11:47:42.571
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:42.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:42.606
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 04/06/23 11:47:42.631
STEP: create the rc2 04/06/23 11:47:42.649
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/06/23 11:47:47.665
STEP: delete the rc simpletest-rc-to-be-deleted 04/06/23 11:47:48.49
STEP: wait for the rc to be deleted 04/06/23 11:47:48.509
Apr  6 11:47:53.553: INFO: 72 pods remaining
Apr  6 11:47:53.554: INFO: 72 pods has nil DeletionTimestamp
Apr  6 11:47:53.554: INFO: 
STEP: Gathering metrics 04/06/23 11:47:58.546
W0406 11:47:58.580318      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Apr  6 11:47:58.580: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr  6 11:47:58.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-22v59" in namespace "gc-1201"
Apr  6 11:47:58.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-2njrn" in namespace "gc-1201"
Apr  6 11:47:58.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qh4d" in namespace "gc-1201"
Apr  6 11:47:58.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rlkl" in namespace "gc-1201"
Apr  6 11:47:58.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-52wps" in namespace "gc-1201"
Apr  6 11:47:58.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xd89" in namespace "gc-1201"
Apr  6 11:47:58.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-65lsh" in namespace "gc-1201"
Apr  6 11:47:58.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-66zdc" in namespace "gc-1201"
Apr  6 11:47:58.713: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sbnw" in namespace "gc-1201"
Apr  6 11:47:58.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-6shf9" in namespace "gc-1201"
Apr  6 11:47:58.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zsns" in namespace "gc-1201"
Apr  6 11:47:58.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-77ds4" in namespace "gc-1201"
Apr  6 11:47:58.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-7htcd" in namespace "gc-1201"
Apr  6 11:47:58.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nvs2" in namespace "gc-1201"
Apr  6 11:47:58.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vl9n" in namespace "gc-1201"
Apr  6 11:47:58.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xkkn" in namespace "gc-1201"
Apr  6 11:47:58.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xs8d" in namespace "gc-1201"
Apr  6 11:47:58.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gzpk" in namespace "gc-1201"
Apr  6 11:47:58.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-8q7qx" in namespace "gc-1201"
Apr  6 11:47:58.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-92w79" in namespace "gc-1201"
Apr  6 11:47:58.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-99c6z" in namespace "gc-1201"
Apr  6 11:47:58.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ch2m" in namespace "gc-1201"
Apr  6 11:47:58.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gtxw" in namespace "gc-1201"
Apr  6 11:47:58.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6m8q" in namespace "gc-1201"
Apr  6 11:47:58.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6rx2" in namespace "gc-1201"
Apr  6 11:47:58.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7bwj" in namespace "gc-1201"
Apr  6 11:47:58.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-c884h" in namespace "gc-1201"
Apr  6 11:47:58.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-cd8p9" in namespace "gc-1201"
Apr  6 11:47:58.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctlzm" in namespace "gc-1201"
Apr  6 11:47:58.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-db7ws" in namespace "gc-1201"
Apr  6 11:47:58.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcfhh" in namespace "gc-1201"
Apr  6 11:47:59.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfd95" in namespace "gc-1201"
Apr  6 11:47:59.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-dftmh" in namespace "gc-1201"
Apr  6 11:47:59.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlp5j" in namespace "gc-1201"
Apr  6 11:47:59.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5b6r" in namespace "gc-1201"
Apr  6 11:47:59.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7v5r" in namespace "gc-1201"
Apr  6 11:47:59.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-flgjn" in namespace "gc-1201"
Apr  6 11:47:59.134: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm72w" in namespace "gc-1201"
Apr  6 11:47:59.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs6sn" in namespace "gc-1201"
Apr  6 11:47:59.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv7lq" in namespace "gc-1201"
Apr  6 11:47:59.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-gr87g" in namespace "gc-1201"
Apr  6 11:47:59.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtbb8" in namespace "gc-1201"
Apr  6 11:47:59.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvjqm" in namespace "gc-1201"
Apr  6 11:47:59.220: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2pd4" in namespace "gc-1201"
Apr  6 11:47:59.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6892" in namespace "gc-1201"
Apr  6 11:47:59.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9sgw" in namespace "gc-1201"
Apr  6 11:47:59.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-hf65j" in namespace "gc-1201"
Apr  6 11:47:59.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjg8x" in namespace "gc-1201"
Apr  6 11:47:59.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkzr5" in namespace "gc-1201"
Apr  6 11:47:59.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzjhj" in namespace "gc-1201"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Apr  6 11:47:59.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1201" for this suite. 04/06/23 11:47:59.326
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":320,"skipped":5743,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.765 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:47:42.569
    Apr  6 11:47:42.570: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename gc 04/06/23 11:47:42.571
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:42.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:42.606
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 04/06/23 11:47:42.631
    STEP: create the rc2 04/06/23 11:47:42.649
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/06/23 11:47:47.665
    STEP: delete the rc simpletest-rc-to-be-deleted 04/06/23 11:47:48.49
    STEP: wait for the rc to be deleted 04/06/23 11:47:48.509
    Apr  6 11:47:53.553: INFO: 72 pods remaining
    Apr  6 11:47:53.554: INFO: 72 pods has nil DeletionTimestamp
    Apr  6 11:47:53.554: INFO: 
    STEP: Gathering metrics 04/06/23 11:47:58.546
    W0406 11:47:58.580318      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Apr  6 11:47:58.580: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr  6 11:47:58.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-22v59" in namespace "gc-1201"
    Apr  6 11:47:58.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-2njrn" in namespace "gc-1201"
    Apr  6 11:47:58.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qh4d" in namespace "gc-1201"
    Apr  6 11:47:58.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rlkl" in namespace "gc-1201"
    Apr  6 11:47:58.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-52wps" in namespace "gc-1201"
    Apr  6 11:47:58.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xd89" in namespace "gc-1201"
    Apr  6 11:47:58.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-65lsh" in namespace "gc-1201"
    Apr  6 11:47:58.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-66zdc" in namespace "gc-1201"
    Apr  6 11:47:58.713: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sbnw" in namespace "gc-1201"
    Apr  6 11:47:58.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-6shf9" in namespace "gc-1201"
    Apr  6 11:47:58.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zsns" in namespace "gc-1201"
    Apr  6 11:47:58.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-77ds4" in namespace "gc-1201"
    Apr  6 11:47:58.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-7htcd" in namespace "gc-1201"
    Apr  6 11:47:58.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nvs2" in namespace "gc-1201"
    Apr  6 11:47:58.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vl9n" in namespace "gc-1201"
    Apr  6 11:47:58.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xkkn" in namespace "gc-1201"
    Apr  6 11:47:58.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xs8d" in namespace "gc-1201"
    Apr  6 11:47:58.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gzpk" in namespace "gc-1201"
    Apr  6 11:47:58.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-8q7qx" in namespace "gc-1201"
    Apr  6 11:47:58.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-92w79" in namespace "gc-1201"
    Apr  6 11:47:58.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-99c6z" in namespace "gc-1201"
    Apr  6 11:47:58.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ch2m" in namespace "gc-1201"
    Apr  6 11:47:58.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gtxw" in namespace "gc-1201"
    Apr  6 11:47:58.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6m8q" in namespace "gc-1201"
    Apr  6 11:47:58.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6rx2" in namespace "gc-1201"
    Apr  6 11:47:58.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7bwj" in namespace "gc-1201"
    Apr  6 11:47:58.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-c884h" in namespace "gc-1201"
    Apr  6 11:47:58.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-cd8p9" in namespace "gc-1201"
    Apr  6 11:47:58.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctlzm" in namespace "gc-1201"
    Apr  6 11:47:58.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-db7ws" in namespace "gc-1201"
    Apr  6 11:47:58.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcfhh" in namespace "gc-1201"
    Apr  6 11:47:59.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfd95" in namespace "gc-1201"
    Apr  6 11:47:59.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-dftmh" in namespace "gc-1201"
    Apr  6 11:47:59.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlp5j" in namespace "gc-1201"
    Apr  6 11:47:59.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5b6r" in namespace "gc-1201"
    Apr  6 11:47:59.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7v5r" in namespace "gc-1201"
    Apr  6 11:47:59.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-flgjn" in namespace "gc-1201"
    Apr  6 11:47:59.134: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm72w" in namespace "gc-1201"
    Apr  6 11:47:59.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs6sn" in namespace "gc-1201"
    Apr  6 11:47:59.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv7lq" in namespace "gc-1201"
    Apr  6 11:47:59.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-gr87g" in namespace "gc-1201"
    Apr  6 11:47:59.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtbb8" in namespace "gc-1201"
    Apr  6 11:47:59.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvjqm" in namespace "gc-1201"
    Apr  6 11:47:59.220: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2pd4" in namespace "gc-1201"
    Apr  6 11:47:59.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6892" in namespace "gc-1201"
    Apr  6 11:47:59.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9sgw" in namespace "gc-1201"
    Apr  6 11:47:59.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-hf65j" in namespace "gc-1201"
    Apr  6 11:47:59.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjg8x" in namespace "gc-1201"
    Apr  6 11:47:59.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkzr5" in namespace "gc-1201"
    Apr  6 11:47:59.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzjhj" in namespace "gc-1201"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Apr  6 11:47:59.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1201" for this suite. 04/06/23 11:47:59.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:47:59.385
Apr  6 11:47:59.385: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:47:59.387
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:59.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:59.421
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 04/06/23 11:47:59.426
STEP: Creating a ResourceQuota 04/06/23 11:48:04.433
STEP: Ensuring resource quota status is calculated 04/06/23 11:48:04.439
STEP: Creating a ReplicaSet 04/06/23 11:48:06.447
STEP: Ensuring resource quota status captures replicaset creation 04/06/23 11:48:06.466
STEP: Deleting a ReplicaSet 04/06/23 11:48:08.475
STEP: Ensuring resource quota status released usage 04/06/23 11:48:08.482
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:48:10.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4417" for this suite. 04/06/23 11:48:10.507
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":321,"skipped":5769,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.132 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:47:59.385
    Apr  6 11:47:59.385: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:47:59.387
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:47:59.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:47:59.421
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 04/06/23 11:47:59.426
    STEP: Creating a ResourceQuota 04/06/23 11:48:04.433
    STEP: Ensuring resource quota status is calculated 04/06/23 11:48:04.439
    STEP: Creating a ReplicaSet 04/06/23 11:48:06.447
    STEP: Ensuring resource quota status captures replicaset creation 04/06/23 11:48:06.466
    STEP: Deleting a ReplicaSet 04/06/23 11:48:08.475
    STEP: Ensuring resource quota status released usage 04/06/23 11:48:08.482
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:48:10.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4417" for this suite. 04/06/23 11:48:10.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:10.535
Apr  6 11:48:10.535: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename configmap 04/06/23 11:48:10.536
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:10.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:10.557
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-c4ab0552-a730-405c-a3d3-bad572e6540f 04/06/23 11:48:10.562
STEP: Creating a pod to test consume configMaps 04/06/23 11:48:10.574
Apr  6 11:48:10.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e" in namespace "configmap-6350" to be "Succeeded or Failed"
Apr  6 11:48:10.591: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574174ms
Apr  6 11:48:12.600: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013927181s
Apr  6 11:48:14.607: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020683463s
STEP: Saw pod success 04/06/23 11:48:14.607
Apr  6 11:48:14.607: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e" satisfied condition "Succeeded or Failed"
Apr  6 11:48:14.616: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e container configmap-volume-test: <nil>
STEP: delete the pod 04/06/23 11:48:14.704
Apr  6 11:48:14.715: INFO: Waiting for pod pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e to disappear
Apr  6 11:48:14.720: INFO: Pod pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Apr  6 11:48:14.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6350" for this suite. 04/06/23 11:48:14.736
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":322,"skipped":5852,"failed":0}
------------------------------
â€¢ [4.207 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:10.535
    Apr  6 11:48:10.535: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename configmap 04/06/23 11:48:10.536
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:10.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:10.557
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-c4ab0552-a730-405c-a3d3-bad572e6540f 04/06/23 11:48:10.562
    STEP: Creating a pod to test consume configMaps 04/06/23 11:48:10.574
    Apr  6 11:48:10.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e" in namespace "configmap-6350" to be "Succeeded or Failed"
    Apr  6 11:48:10.591: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574174ms
    Apr  6 11:48:12.600: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013927181s
    Apr  6 11:48:14.607: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020683463s
    STEP: Saw pod success 04/06/23 11:48:14.607
    Apr  6 11:48:14.607: INFO: Pod "pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e" satisfied condition "Succeeded or Failed"
    Apr  6 11:48:14.616: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e container configmap-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:48:14.704
    Apr  6 11:48:14.715: INFO: Waiting for pod pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e to disappear
    Apr  6 11:48:14.720: INFO: Pod pod-configmaps-d633adc4-4884-451d-8af2-d12df0061c8e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Apr  6 11:48:14.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6350" for this suite. 04/06/23 11:48:14.736
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:14.742
Apr  6 11:48:14.742: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename subpath 04/06/23 11:48:14.743
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:14.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:14.772
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/06/23 11:48:14.785
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-4chx 04/06/23 11:48:14.806
STEP: Creating a pod to test atomic-volume-subpath 04/06/23 11:48:14.806
Apr  6 11:48:14.818: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4chx" in namespace "subpath-6237" to be "Succeeded or Failed"
Apr  6 11:48:14.832: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Pending", Reason="", readiness=false. Elapsed: 13.587567ms
Apr  6 11:48:16.841: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 2.02208745s
Apr  6 11:48:18.842: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 4.023515929s
Apr  6 11:48:20.842: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 6.023586903s
Apr  6 11:48:22.842: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 8.02307202s
Apr  6 11:48:24.847: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 10.028911024s
Apr  6 11:48:26.839: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 12.02071924s
Apr  6 11:48:28.839: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 14.020952559s
Apr  6 11:48:30.843: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 16.024475177s
Apr  6 11:48:32.840: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 18.021385861s
Apr  6 11:48:34.838: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 20.019564226s
Apr  6 11:48:36.872: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=false. Elapsed: 22.053314048s
Apr  6 11:48:38.841: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022140196s
STEP: Saw pod success 04/06/23 11:48:38.841
Apr  6 11:48:38.845: INFO: Pod "pod-subpath-test-projected-4chx" satisfied condition "Succeeded or Failed"
Apr  6 11:48:38.852: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-subpath-test-projected-4chx container test-container-subpath-projected-4chx: <nil>
STEP: delete the pod 04/06/23 11:48:38.867
Apr  6 11:48:38.884: INFO: Waiting for pod pod-subpath-test-projected-4chx to disappear
Apr  6 11:48:38.892: INFO: Pod pod-subpath-test-projected-4chx no longer exists
STEP: Deleting pod pod-subpath-test-projected-4chx 04/06/23 11:48:38.892
Apr  6 11:48:38.892: INFO: Deleting pod "pod-subpath-test-projected-4chx" in namespace "subpath-6237"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Apr  6 11:48:38.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6237" for this suite. 04/06/23 11:48:38.911
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":323,"skipped":5854,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.180 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:14.742
    Apr  6 11:48:14.742: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename subpath 04/06/23 11:48:14.743
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:14.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:14.772
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/06/23 11:48:14.785
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-4chx 04/06/23 11:48:14.806
    STEP: Creating a pod to test atomic-volume-subpath 04/06/23 11:48:14.806
    Apr  6 11:48:14.818: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4chx" in namespace "subpath-6237" to be "Succeeded or Failed"
    Apr  6 11:48:14.832: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Pending", Reason="", readiness=false. Elapsed: 13.587567ms
    Apr  6 11:48:16.841: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 2.02208745s
    Apr  6 11:48:18.842: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 4.023515929s
    Apr  6 11:48:20.842: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 6.023586903s
    Apr  6 11:48:22.842: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 8.02307202s
    Apr  6 11:48:24.847: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 10.028911024s
    Apr  6 11:48:26.839: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 12.02071924s
    Apr  6 11:48:28.839: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 14.020952559s
    Apr  6 11:48:30.843: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 16.024475177s
    Apr  6 11:48:32.840: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 18.021385861s
    Apr  6 11:48:34.838: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=true. Elapsed: 20.019564226s
    Apr  6 11:48:36.872: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Running", Reason="", readiness=false. Elapsed: 22.053314048s
    Apr  6 11:48:38.841: INFO: Pod "pod-subpath-test-projected-4chx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022140196s
    STEP: Saw pod success 04/06/23 11:48:38.841
    Apr  6 11:48:38.845: INFO: Pod "pod-subpath-test-projected-4chx" satisfied condition "Succeeded or Failed"
    Apr  6 11:48:38.852: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-subpath-test-projected-4chx container test-container-subpath-projected-4chx: <nil>
    STEP: delete the pod 04/06/23 11:48:38.867
    Apr  6 11:48:38.884: INFO: Waiting for pod pod-subpath-test-projected-4chx to disappear
    Apr  6 11:48:38.892: INFO: Pod pod-subpath-test-projected-4chx no longer exists
    STEP: Deleting pod pod-subpath-test-projected-4chx 04/06/23 11:48:38.892
    Apr  6 11:48:38.892: INFO: Deleting pod "pod-subpath-test-projected-4chx" in namespace "subpath-6237"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Apr  6 11:48:38.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6237" for this suite. 04/06/23 11:48:38.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:38.927
Apr  6 11:48:38.930: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename events 04/06/23 11:48:38.932
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:38.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:38.967
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 04/06/23 11:48:38.987
Apr  6 11:48:38.995: INFO: created test-event-1
Apr  6 11:48:39.010: INFO: created test-event-2
Apr  6 11:48:39.015: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 04/06/23 11:48:39.015
STEP: delete collection of events 04/06/23 11:48:39.021
Apr  6 11:48:39.021: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/06/23 11:48:39.046
Apr  6 11:48:39.046: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Apr  6 11:48:39.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1329" for this suite. 04/06/23 11:48:39.06
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":324,"skipped":5862,"failed":0}
------------------------------
â€¢ [0.138 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:38.927
    Apr  6 11:48:38.930: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename events 04/06/23 11:48:38.932
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:38.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:38.967
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 04/06/23 11:48:38.987
    Apr  6 11:48:38.995: INFO: created test-event-1
    Apr  6 11:48:39.010: INFO: created test-event-2
    Apr  6 11:48:39.015: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 04/06/23 11:48:39.015
    STEP: delete collection of events 04/06/23 11:48:39.021
    Apr  6 11:48:39.021: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/06/23 11:48:39.046
    Apr  6 11:48:39.046: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Apr  6 11:48:39.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1329" for this suite. 04/06/23 11:48:39.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:39.067
Apr  6 11:48:39.067: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename deployment 04/06/23 11:48:39.068
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:39.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:39.09
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 04/06/23 11:48:39.112
STEP: waiting for Deployment to be created 04/06/23 11:48:39.117
STEP: waiting for all Replicas to be Ready 04/06/23 11:48:39.119
Apr  6 11:48:39.122: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:39.122: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:39.127: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:39.127: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:39.134: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:39.135: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:39.152: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:39.152: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr  6 11:48:40.505: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr  6 11:48:40.505: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr  6 11:48:41.026: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 04/06/23 11:48:41.027
W0406 11:48:41.039505      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr  6 11:48:41.044: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 04/06/23 11:48:41.044
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.060: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.060: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:41.066: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:41.066: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:41.090: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:41.090: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:42.549: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:42.549: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:42.559: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
STEP: listing Deployments 04/06/23 11:48:42.559
Apr  6 11:48:42.569: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 04/06/23 11:48:42.569
Apr  6 11:48:42.596: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 04/06/23 11:48:42.596
Apr  6 11:48:42.619: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:42.619: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:42.619: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:42.622: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:42.638: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:42.638: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:44.030: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:44.084: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:44.112: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:44.124: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr  6 11:48:45.589: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 04/06/23 11:48:45.607
STEP: fetching the DeploymentStatus 04/06/23 11:48:45.63
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3
STEP: deleting the Deployment 04/06/23 11:48:45.644
Apr  6 11:48:45.663: INFO: observed event type MODIFIED
Apr  6 11:48:45.663: INFO: observed event type MODIFIED
Apr  6 11:48:45.663: INFO: observed event type MODIFIED
Apr  6 11:48:45.663: INFO: observed event type MODIFIED
Apr  6 11:48:45.663: INFO: observed event type MODIFIED
Apr  6 11:48:45.663: INFO: observed event type MODIFIED
Apr  6 11:48:45.663: INFO: observed event type MODIFIED
Apr  6 11:48:45.664: INFO: observed event type MODIFIED
Apr  6 11:48:45.664: INFO: observed event type MODIFIED
Apr  6 11:48:45.664: INFO: observed event type MODIFIED
Apr  6 11:48:45.664: INFO: observed event type MODIFIED
Apr  6 11:48:45.665: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr  6 11:48:45.672: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Apr  6 11:48:45.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4901" for this suite. 04/06/23 11:48:45.71
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":325,"skipped":5873,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.658 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:39.067
    Apr  6 11:48:39.067: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename deployment 04/06/23 11:48:39.068
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:39.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:39.09
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 04/06/23 11:48:39.112
    STEP: waiting for Deployment to be created 04/06/23 11:48:39.117
    STEP: waiting for all Replicas to be Ready 04/06/23 11:48:39.119
    Apr  6 11:48:39.122: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:39.122: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:39.127: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:39.127: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:39.134: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:39.135: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:39.152: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:39.152: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr  6 11:48:40.505: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr  6 11:48:40.505: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr  6 11:48:41.026: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 04/06/23 11:48:41.027
    W0406 11:48:41.039505      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr  6 11:48:41.044: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 04/06/23 11:48:41.044
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 0
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.048: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.060: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.060: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:41.066: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:41.066: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:41.090: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:41.090: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:42.549: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:42.549: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:42.559: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    STEP: listing Deployments 04/06/23 11:48:42.559
    Apr  6 11:48:42.569: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 04/06/23 11:48:42.569
    Apr  6 11:48:42.596: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 04/06/23 11:48:42.596
    Apr  6 11:48:42.619: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:42.619: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:42.619: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:42.622: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:42.638: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:42.638: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:44.030: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:44.084: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:44.112: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:44.124: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr  6 11:48:45.589: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 04/06/23 11:48:45.607
    STEP: fetching the DeploymentStatus 04/06/23 11:48:45.63
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 1
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 2
    Apr  6 11:48:45.644: INFO: observed Deployment test-deployment in namespace deployment-4901 with ReadyReplicas 3
    STEP: deleting the Deployment 04/06/23 11:48:45.644
    Apr  6 11:48:45.663: INFO: observed event type MODIFIED
    Apr  6 11:48:45.663: INFO: observed event type MODIFIED
    Apr  6 11:48:45.663: INFO: observed event type MODIFIED
    Apr  6 11:48:45.663: INFO: observed event type MODIFIED
    Apr  6 11:48:45.663: INFO: observed event type MODIFIED
    Apr  6 11:48:45.663: INFO: observed event type MODIFIED
    Apr  6 11:48:45.663: INFO: observed event type MODIFIED
    Apr  6 11:48:45.664: INFO: observed event type MODIFIED
    Apr  6 11:48:45.664: INFO: observed event type MODIFIED
    Apr  6 11:48:45.664: INFO: observed event type MODIFIED
    Apr  6 11:48:45.664: INFO: observed event type MODIFIED
    Apr  6 11:48:45.665: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr  6 11:48:45.672: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Apr  6 11:48:45.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4901" for this suite. 04/06/23 11:48:45.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:45.73
Apr  6 11:48:45.731: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename daemonsets 04/06/23 11:48:45.737
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:45.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:45.789
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 04/06/23 11:48:45.894
STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:48:45.904
Apr  6 11:48:45.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:48:45.920: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:48:46.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:48:46.957: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:48:47.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr  6 11:48:47.942: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
Apr  6 11:48:48.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Apr  6 11:48:48.947: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status 04/06/23 11:48:48.952
Apr  6 11:48:48.977: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 04/06/23 11:48:48.977
Apr  6 11:48:48.994: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 04/06/23 11:48:48.994
Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: ADDED
Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.002: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.005: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.007: INFO: Found daemon set daemon-set in namespace daemonsets-601 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 11:48:49.007: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 04/06/23 11:48:49.008
STEP: watching for the daemon set status to be patched 04/06/23 11:48:49.035
Apr  6 11:48:49.041: INFO: Observed &DaemonSet event: ADDED
Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.043: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.043: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.043: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.043: INFO: Observed daemon set daemon-set in namespace daemonsets-601 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr  6 11:48:49.044: INFO: Observed &DaemonSet event: MODIFIED
Apr  6 11:48:49.044: INFO: Found daemon set daemon-set in namespace daemonsets-601 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Apr  6 11:48:49.044: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:48:49.054
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-601, will wait for the garbage collector to delete the pods 04/06/23 11:48:49.054
Apr  6 11:48:49.131: INFO: Deleting DaemonSet.extensions daemon-set took: 20.356065ms
Apr  6 11:48:49.231: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.237982ms
Apr  6 11:48:51.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr  6 11:48:51.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr  6 11:48:51.247: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42581"},"items":null}

Apr  6 11:48:51.252: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42581"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:48:51.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-601" for this suite. 04/06/23 11:48:51.326
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":326,"skipped":5934,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.603 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:45.73
    Apr  6 11:48:45.731: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename daemonsets 04/06/23 11:48:45.737
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:45.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:45.789
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 04/06/23 11:48:45.894
    STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:48:45.904
    Apr  6 11:48:45.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:48:45.920: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:48:46.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:48:46.957: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:48:47.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr  6 11:48:47.942: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-tfv6l is running 0 daemon pod, expected 1
    Apr  6 11:48:48.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Apr  6 11:48:48.947: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Getting /status 04/06/23 11:48:48.952
    Apr  6 11:48:48.977: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 04/06/23 11:48:48.977
    Apr  6 11:48:48.994: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 04/06/23 11:48:48.994
    Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: ADDED
    Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.001: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.002: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.005: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.007: INFO: Found daemon set daemon-set in namespace daemonsets-601 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr  6 11:48:49.007: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 04/06/23 11:48:49.008
    STEP: watching for the daemon set status to be patched 04/06/23 11:48:49.035
    Apr  6 11:48:49.041: INFO: Observed &DaemonSet event: ADDED
    Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.042: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.043: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.043: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.043: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.043: INFO: Observed daemon set daemon-set in namespace daemonsets-601 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr  6 11:48:49.044: INFO: Observed &DaemonSet event: MODIFIED
    Apr  6 11:48:49.044: INFO: Found daemon set daemon-set in namespace daemonsets-601 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Apr  6 11:48:49.044: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 04/06/23 11:48:49.054
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-601, will wait for the garbage collector to delete the pods 04/06/23 11:48:49.054
    Apr  6 11:48:49.131: INFO: Deleting DaemonSet.extensions daemon-set took: 20.356065ms
    Apr  6 11:48:49.231: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.237982ms
    Apr  6 11:48:51.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr  6 11:48:51.242: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr  6 11:48:51.247: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42581"},"items":null}

    Apr  6 11:48:51.252: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42581"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:48:51.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-601" for this suite. 04/06/23 11:48:51.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:51.334
Apr  6 11:48:51.334: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 11:48:51.335
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:51.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:51.364
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 04/06/23 11:48:51.398
STEP: waiting for available Endpoint 04/06/23 11:48:51.42
STEP: listing all Endpoints 04/06/23 11:48:51.424
STEP: updating the Endpoint 04/06/23 11:48:51.429
STEP: fetching the Endpoint 04/06/23 11:48:51.443
STEP: patching the Endpoint 04/06/23 11:48:51.448
STEP: fetching the Endpoint 04/06/23 11:48:51.459
STEP: deleting the Endpoint by Collection 04/06/23 11:48:51.464
STEP: waiting for Endpoint deletion 04/06/23 11:48:51.47
STEP: fetching the Endpoint 04/06/23 11:48:51.473
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 11:48:51.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5511" for this suite. 04/06/23 11:48:51.496
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":327,"skipped":5939,"failed":0}
------------------------------
â€¢ [0.170 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:51.334
    Apr  6 11:48:51.334: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 11:48:51.335
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:51.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:51.364
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 04/06/23 11:48:51.398
    STEP: waiting for available Endpoint 04/06/23 11:48:51.42
    STEP: listing all Endpoints 04/06/23 11:48:51.424
    STEP: updating the Endpoint 04/06/23 11:48:51.429
    STEP: fetching the Endpoint 04/06/23 11:48:51.443
    STEP: patching the Endpoint 04/06/23 11:48:51.448
    STEP: fetching the Endpoint 04/06/23 11:48:51.459
    STEP: deleting the Endpoint by Collection 04/06/23 11:48:51.464
    STEP: waiting for Endpoint deletion 04/06/23 11:48:51.47
    STEP: fetching the Endpoint 04/06/23 11:48:51.473
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 11:48:51.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5511" for this suite. 04/06/23 11:48:51.496
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:51.505
Apr  6 11:48:51.506: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 11:48:51.507
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:51.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:51.548
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 04/06/23 11:48:51.555
Apr  6 11:48:51.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2" in namespace "downward-api-6603" to be "Succeeded or Failed"
Apr  6 11:48:51.582: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.328073ms
Apr  6 11:48:53.598: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.031415971s
Apr  6 11:48:55.588: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Running", Reason="", readiness=false. Elapsed: 4.021709736s
Apr  6 11:48:57.589: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02218132s
STEP: Saw pod success 04/06/23 11:48:57.589
Apr  6 11:48:57.589: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2" satisfied condition "Succeeded or Failed"
Apr  6 11:48:57.593: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2 container client-container: <nil>
STEP: delete the pod 04/06/23 11:48:57.607
Apr  6 11:48:57.621: INFO: Waiting for pod downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2 to disappear
Apr  6 11:48:57.625: INFO: Pod downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 11:48:57.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6603" for this suite. 04/06/23 11:48:57.631
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":328,"skipped":5939,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.134 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:51.505
    Apr  6 11:48:51.506: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 11:48:51.507
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:51.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:51.548
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 04/06/23 11:48:51.555
    Apr  6 11:48:51.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2" in namespace "downward-api-6603" to be "Succeeded or Failed"
    Apr  6 11:48:51.582: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.328073ms
    Apr  6 11:48:53.598: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.031415971s
    Apr  6 11:48:55.588: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Running", Reason="", readiness=false. Elapsed: 4.021709736s
    Apr  6 11:48:57.589: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02218132s
    STEP: Saw pod success 04/06/23 11:48:57.589
    Apr  6 11:48:57.589: INFO: Pod "downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2" satisfied condition "Succeeded or Failed"
    Apr  6 11:48:57.593: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2 container client-container: <nil>
    STEP: delete the pod 04/06/23 11:48:57.607
    Apr  6 11:48:57.621: INFO: Waiting for pod downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2 to disappear
    Apr  6 11:48:57.625: INFO: Pod downwardapi-volume-db749cd9-35c8-46af-918d-133201d721e2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 11:48:57.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6603" for this suite. 04/06/23 11:48:57.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:48:57.642
Apr  6 11:48:57.651: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename downward-api 04/06/23 11:48:57.652
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:57.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:57.675
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 04/06/23 11:48:57.684
Apr  6 11:48:57.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08" in namespace "downward-api-2547" to be "Succeeded or Failed"
Apr  6 11:48:57.699: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061211ms
Apr  6 11:48:59.723: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028365032s
Apr  6 11:49:01.707: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012771626s
STEP: Saw pod success 04/06/23 11:49:01.707
Apr  6 11:49:01.707: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08" satisfied condition "Succeeded or Failed"
Apr  6 11:49:01.713: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08 container client-container: <nil>
STEP: delete the pod 04/06/23 11:49:01.729
Apr  6 11:49:01.741: INFO: Waiting for pod downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08 to disappear
Apr  6 11:49:01.746: INFO: Pod downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Apr  6 11:49:01.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2547" for this suite. 04/06/23 11:49:01.755
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":329,"skipped":5952,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:48:57.642
    Apr  6 11:48:57.651: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename downward-api 04/06/23 11:48:57.652
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:48:57.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:48:57.675
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 04/06/23 11:48:57.684
    Apr  6 11:48:57.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08" in namespace "downward-api-2547" to be "Succeeded or Failed"
    Apr  6 11:48:57.699: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061211ms
    Apr  6 11:48:59.723: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028365032s
    Apr  6 11:49:01.707: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012771626s
    STEP: Saw pod success 04/06/23 11:49:01.707
    Apr  6 11:49:01.707: INFO: Pod "downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08" satisfied condition "Succeeded or Failed"
    Apr  6 11:49:01.713: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08 container client-container: <nil>
    STEP: delete the pod 04/06/23 11:49:01.729
    Apr  6 11:49:01.741: INFO: Waiting for pod downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08 to disappear
    Apr  6 11:49:01.746: INFO: Pod downwardapi-volume-e80e56c1-99d2-41d2-baa1-4e37c12b0b08 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Apr  6 11:49:01.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2547" for this suite. 04/06/23 11:49:01.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:01.765
Apr  6 11:49:01.765: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename disruption 04/06/23 11:49:01.767
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:01.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:01.803
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 04/06/23 11:49:01.812
STEP: Waiting for the pdb to be processed 04/06/23 11:49:01.824
STEP: First trying to evict a pod which shouldn't be evictable 04/06/23 11:49:03.845
STEP: Waiting for all pods to be running 04/06/23 11:49:03.845
Apr  6 11:49:03.852: INFO: pods: 0 < 3
STEP: locating a running pod 04/06/23 11:49:05.863
STEP: Updating the pdb to allow a pod to be evicted 04/06/23 11:49:05.885
STEP: Waiting for the pdb to be processed 04/06/23 11:49:05.899
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/06/23 11:49:05.905
STEP: Waiting for all pods to be running 04/06/23 11:49:05.905
STEP: Waiting for the pdb to observed all healthy pods 04/06/23 11:49:05.911
STEP: Patching the pdb to disallow a pod to be evicted 04/06/23 11:49:05.934
STEP: Waiting for the pdb to be processed 04/06/23 11:49:05.953
STEP: Waiting for all pods to be running 04/06/23 11:49:05.962
Apr  6 11:49:05.972: INFO: running pods: 2 < 3
STEP: locating a running pod 04/06/23 11:49:07.989
STEP: Deleting the pdb to allow a pod to be evicted 04/06/23 11:49:08.005
STEP: Waiting for the pdb to be deleted 04/06/23 11:49:08.013
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/06/23 11:49:08.017
STEP: Waiting for all pods to be running 04/06/23 11:49:08.017
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Apr  6 11:49:08.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3270" for this suite. 04/06/23 11:49:08.05
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":330,"skipped":6004,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.308 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:01.765
    Apr  6 11:49:01.765: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename disruption 04/06/23 11:49:01.767
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:01.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:01.803
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 04/06/23 11:49:01.812
    STEP: Waiting for the pdb to be processed 04/06/23 11:49:01.824
    STEP: First trying to evict a pod which shouldn't be evictable 04/06/23 11:49:03.845
    STEP: Waiting for all pods to be running 04/06/23 11:49:03.845
    Apr  6 11:49:03.852: INFO: pods: 0 < 3
    STEP: locating a running pod 04/06/23 11:49:05.863
    STEP: Updating the pdb to allow a pod to be evicted 04/06/23 11:49:05.885
    STEP: Waiting for the pdb to be processed 04/06/23 11:49:05.899
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/06/23 11:49:05.905
    STEP: Waiting for all pods to be running 04/06/23 11:49:05.905
    STEP: Waiting for the pdb to observed all healthy pods 04/06/23 11:49:05.911
    STEP: Patching the pdb to disallow a pod to be evicted 04/06/23 11:49:05.934
    STEP: Waiting for the pdb to be processed 04/06/23 11:49:05.953
    STEP: Waiting for all pods to be running 04/06/23 11:49:05.962
    Apr  6 11:49:05.972: INFO: running pods: 2 < 3
    STEP: locating a running pod 04/06/23 11:49:07.989
    STEP: Deleting the pdb to allow a pod to be evicted 04/06/23 11:49:08.005
    STEP: Waiting for the pdb to be deleted 04/06/23 11:49:08.013
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/06/23 11:49:08.017
    STEP: Waiting for all pods to be running 04/06/23 11:49:08.017
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Apr  6 11:49:08.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3270" for this suite. 04/06/23 11:49:08.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:08.076
Apr  6 11:49:08.078: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/06/23 11:49:08.08
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:08.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:08.119
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 04/06/23 11:49:08.129
STEP: Creating hostNetwork=false pod 04/06/23 11:49:08.129
Apr  6 11:49:08.149: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4603" to be "running and ready"
Apr  6 11:49:08.153: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524ms
Apr  6 11:49:08.153: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:49:10.163: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013980029s
Apr  6 11:49:10.177: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:49:12.161: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011612611s
Apr  6 11:49:12.161: INFO: The phase of Pod test-pod is Running (Ready = true)
Apr  6 11:49:12.161: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 04/06/23 11:49:12.167
Apr  6 11:49:12.179: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4603" to be "running and ready"
Apr  6 11:49:12.184: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.70824ms
Apr  6 11:49:12.185: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:49:14.199: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019939937s
Apr  6 11:49:14.199: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Apr  6 11:49:14.199: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 04/06/23 11:49:14.204
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/06/23 11:49:14.204
Apr  6 11:49:14.205: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:14.205: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:14.205: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:14.205: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr  6 11:49:14.695: INFO: Exec stderr: ""
Apr  6 11:49:14.709: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:14.711: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:14.715: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:14.718: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr  6 11:49:15.227: INFO: Exec stderr: ""
Apr  6 11:49:15.227: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:15.227: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:15.227: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:15.228: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr  6 11:49:15.747: INFO: Exec stderr: ""
Apr  6 11:49:15.747: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:15.747: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:15.748: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:15.748: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr  6 11:49:16.136: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/06/23 11:49:16.136
Apr  6 11:49:16.136: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:16.136: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:16.137: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:16.137: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr  6 11:49:16.590: INFO: Exec stderr: ""
Apr  6 11:49:16.590: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:16.590: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:16.591: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:16.591: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr  6 11:49:17.139: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/06/23 11:49:17.139
Apr  6 11:49:17.139: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:17.139: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:17.140: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:17.140: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr  6 11:49:17.707: INFO: Exec stderr: ""
Apr  6 11:49:17.707: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:17.707: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:17.709: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:17.709: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr  6 11:49:17.990: INFO: Exec stderr: ""
Apr  6 11:49:17.990: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:17.990: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:17.991: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:17.991: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr  6 11:49:18.472: INFO: Exec stderr: ""
Apr  6 11:49:18.472: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr  6 11:49:18.472: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
Apr  6 11:49:18.473: INFO: ExecWithOptions: Clientset creation
Apr  6 11:49:18.473: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr  6 11:49:19.068: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Apr  6 11:49:19.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4603" for this suite. 04/06/23 11:49:19.077
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":331,"skipped":6060,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.007 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:08.076
    Apr  6 11:49:08.078: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/06/23 11:49:08.08
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:08.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:08.119
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 04/06/23 11:49:08.129
    STEP: Creating hostNetwork=false pod 04/06/23 11:49:08.129
    Apr  6 11:49:08.149: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4603" to be "running and ready"
    Apr  6 11:49:08.153: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524ms
    Apr  6 11:49:08.153: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:49:10.163: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013980029s
    Apr  6 11:49:10.177: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:49:12.161: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011612611s
    Apr  6 11:49:12.161: INFO: The phase of Pod test-pod is Running (Ready = true)
    Apr  6 11:49:12.161: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 04/06/23 11:49:12.167
    Apr  6 11:49:12.179: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4603" to be "running and ready"
    Apr  6 11:49:12.184: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.70824ms
    Apr  6 11:49:12.185: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:49:14.199: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019939937s
    Apr  6 11:49:14.199: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Apr  6 11:49:14.199: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 04/06/23 11:49:14.204
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/06/23 11:49:14.204
    Apr  6 11:49:14.205: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:14.205: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:14.205: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:14.205: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr  6 11:49:14.695: INFO: Exec stderr: ""
    Apr  6 11:49:14.709: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:14.711: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:14.715: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:14.718: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr  6 11:49:15.227: INFO: Exec stderr: ""
    Apr  6 11:49:15.227: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:15.227: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:15.227: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:15.228: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr  6 11:49:15.747: INFO: Exec stderr: ""
    Apr  6 11:49:15.747: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:15.747: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:15.748: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:15.748: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr  6 11:49:16.136: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/06/23 11:49:16.136
    Apr  6 11:49:16.136: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:16.136: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:16.137: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:16.137: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr  6 11:49:16.590: INFO: Exec stderr: ""
    Apr  6 11:49:16.590: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:16.590: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:16.591: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:16.591: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr  6 11:49:17.139: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/06/23 11:49:17.139
    Apr  6 11:49:17.139: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:17.139: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:17.140: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:17.140: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr  6 11:49:17.707: INFO: Exec stderr: ""
    Apr  6 11:49:17.707: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:17.707: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:17.709: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:17.709: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr  6 11:49:17.990: INFO: Exec stderr: ""
    Apr  6 11:49:17.990: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:17.990: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:17.991: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:17.991: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr  6 11:49:18.472: INFO: Exec stderr: ""
    Apr  6 11:49:18.472: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4603 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr  6 11:49:18.472: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    Apr  6 11:49:18.473: INFO: ExecWithOptions: Clientset creation
    Apr  6 11:49:18.473: INFO: ExecWithOptions: execute(POST https://api.cl-0czl78yf.4f62e10b2e.internal.dev.ske.eu01.stackit.cloud:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr  6 11:49:19.068: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Apr  6 11:49:19.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4603" for this suite. 04/06/23 11:49:19.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:19.083
Apr  6 11:49:19.083: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 11:49:19.084
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:19.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:19.114
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-35d95b1e-cc09-4a0d-b012-9b24498bde67 04/06/23 11:49:19.119
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Apr  6 11:49:19.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3165" for this suite. 04/06/23 11:49:19.134
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":332,"skipped":6073,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:19.083
    Apr  6 11:49:19.083: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 11:49:19.084
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:19.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:19.114
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-35d95b1e-cc09-4a0d-b012-9b24498bde67 04/06/23 11:49:19.119
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 11:49:19.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3165" for this suite. 04/06/23 11:49:19.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:19.142
Apr  6 11:49:19.142: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename disruption 04/06/23 11:49:19.143
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:19.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:19.161
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 04/06/23 11:49:19.172
STEP: Updating PodDisruptionBudget status 04/06/23 11:49:21.184
STEP: Waiting for all pods to be running 04/06/23 11:49:21.196
Apr  6 11:49:21.203: INFO: running pods: 0 < 1
STEP: locating a running pod 04/06/23 11:49:23.211
STEP: Waiting for the pdb to be processed 04/06/23 11:49:23.231
STEP: Patching PodDisruptionBudget status 04/06/23 11:49:23.241
STEP: Waiting for the pdb to be processed 04/06/23 11:49:23.254
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Apr  6 11:49:23.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9790" for this suite. 04/06/23 11:49:23.267
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":333,"skipped":6100,"failed":0}
------------------------------
â€¢ [4.132 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:19.142
    Apr  6 11:49:19.142: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename disruption 04/06/23 11:49:19.143
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:19.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:19.161
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 04/06/23 11:49:19.172
    STEP: Updating PodDisruptionBudget status 04/06/23 11:49:21.184
    STEP: Waiting for all pods to be running 04/06/23 11:49:21.196
    Apr  6 11:49:21.203: INFO: running pods: 0 < 1
    STEP: locating a running pod 04/06/23 11:49:23.211
    STEP: Waiting for the pdb to be processed 04/06/23 11:49:23.231
    STEP: Patching PodDisruptionBudget status 04/06/23 11:49:23.241
    STEP: Waiting for the pdb to be processed 04/06/23 11:49:23.254
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Apr  6 11:49:23.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9790" for this suite. 04/06/23 11:49:23.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:23.276
Apr  6 11:49:23.277: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename gc 04/06/23 11:49:23.279
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:23.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:23.307
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Apr  6 11:49:23.371: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2e72cc28-e090-44fa-9ccd-14d3a7a527aa", Controller:(*bool)(0xc003517dc6), BlockOwnerDeletion:(*bool)(0xc003517dc7)}}
Apr  6 11:49:23.385: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"237d555c-dda7-48ff-b60b-ae385210c3e1", Controller:(*bool)(0xc003fd603e), BlockOwnerDeletion:(*bool)(0xc003fd603f)}}
Apr  6 11:49:23.397: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4656c3ce-4cdf-4222-8f85-00d6ff610658", Controller:(*bool)(0xc003fb014e), BlockOwnerDeletion:(*bool)(0xc003fb014f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Apr  6 11:49:28.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7045" for this suite. 04/06/23 11:49:28.429
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":334,"skipped":6113,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.162 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:23.276
    Apr  6 11:49:23.277: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename gc 04/06/23 11:49:23.279
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:23.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:23.307
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Apr  6 11:49:23.371: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2e72cc28-e090-44fa-9ccd-14d3a7a527aa", Controller:(*bool)(0xc003517dc6), BlockOwnerDeletion:(*bool)(0xc003517dc7)}}
    Apr  6 11:49:23.385: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"237d555c-dda7-48ff-b60b-ae385210c3e1", Controller:(*bool)(0xc003fd603e), BlockOwnerDeletion:(*bool)(0xc003fd603f)}}
    Apr  6 11:49:23.397: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4656c3ce-4cdf-4222-8f85-00d6ff610658", Controller:(*bool)(0xc003fb014e), BlockOwnerDeletion:(*bool)(0xc003fb014f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Apr  6 11:49:28.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7045" for this suite. 04/06/23 11:49:28.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:28.44
Apr  6 11:49:28.440: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename emptydir 04/06/23 11:49:28.441
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:28.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:28.489
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 04/06/23 11:49:28.496
Apr  6 11:49:28.513: INFO: Waiting up to 5m0s for pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731" in namespace "emptydir-1442" to be "Succeeded or Failed"
Apr  6 11:49:28.521: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731": Phase="Pending", Reason="", readiness=false. Elapsed: 7.872161ms
Apr  6 11:49:30.526: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013272364s
Apr  6 11:49:32.530: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017160463s
STEP: Saw pod success 04/06/23 11:49:32.53
Apr  6 11:49:32.531: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731" satisfied condition "Succeeded or Failed"
Apr  6 11:49:32.536: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731 container test-container: <nil>
STEP: delete the pod 04/06/23 11:49:32.553
Apr  6 11:49:32.564: INFO: Waiting for pod pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731 to disappear
Apr  6 11:49:32.570: INFO: Pod pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Apr  6 11:49:32.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1442" for this suite. 04/06/23 11:49:32.581
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":335,"skipped":6130,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:28.44
    Apr  6 11:49:28.440: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename emptydir 04/06/23 11:49:28.441
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:28.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:28.489
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 04/06/23 11:49:28.496
    Apr  6 11:49:28.513: INFO: Waiting up to 5m0s for pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731" in namespace "emptydir-1442" to be "Succeeded or Failed"
    Apr  6 11:49:28.521: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731": Phase="Pending", Reason="", readiness=false. Elapsed: 7.872161ms
    Apr  6 11:49:30.526: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013272364s
    Apr  6 11:49:32.530: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017160463s
    STEP: Saw pod success 04/06/23 11:49:32.53
    Apr  6 11:49:32.531: INFO: Pod "pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731" satisfied condition "Succeeded or Failed"
    Apr  6 11:49:32.536: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731 container test-container: <nil>
    STEP: delete the pod 04/06/23 11:49:32.553
    Apr  6 11:49:32.564: INFO: Waiting for pod pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731 to disappear
    Apr  6 11:49:32.570: INFO: Pod pod-7e3a1b3c-9387-4d45-badc-4e3fe278f731 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Apr  6 11:49:32.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1442" for this suite. 04/06/23 11:49:32.581
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:32.59
Apr  6 11:49:32.590: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename containers 04/06/23 11:49:32.591
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:32.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:32.638
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Apr  6 11:49:32.665: INFO: Waiting up to 5m0s for pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700" in namespace "containers-1141" to be "running"
Apr  6 11:49:32.671: INFO: Pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700": Phase="Pending", Reason="", readiness=false. Elapsed: 5.788424ms
Apr  6 11:49:34.678: INFO: Pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700": Phase="Running", Reason="", readiness=true. Elapsed: 2.013275435s
Apr  6 11:49:34.678: INFO: Pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Apr  6 11:49:34.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1141" for this suite. 04/06/23 11:49:34.744
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":336,"skipped":6132,"failed":0}
------------------------------
â€¢ [2.161 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:32.59
    Apr  6 11:49:32.590: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename containers 04/06/23 11:49:32.591
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:32.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:32.638
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Apr  6 11:49:32.665: INFO: Waiting up to 5m0s for pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700" in namespace "containers-1141" to be "running"
    Apr  6 11:49:32.671: INFO: Pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700": Phase="Pending", Reason="", readiness=false. Elapsed: 5.788424ms
    Apr  6 11:49:34.678: INFO: Pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700": Phase="Running", Reason="", readiness=true. Elapsed: 2.013275435s
    Apr  6 11:49:34.678: INFO: Pod "client-containers-442a625f-dac3-45c6-a56c-a9da99977700" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Apr  6 11:49:34.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1141" for this suite. 04/06/23 11:49:34.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:34.754
Apr  6 11:49:34.755: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename services 04/06/23 11:49:34.756
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:34.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:34.785
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 04/06/23 11:49:34.798
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Apr  6 11:49:34.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1879" for this suite. 04/06/23 11:49:34.816
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":337,"skipped":6159,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:34.754
    Apr  6 11:49:34.755: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename services 04/06/23 11:49:34.756
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:34.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:34.785
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 04/06/23 11:49:34.798
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Apr  6 11:49:34.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1879" for this suite. 04/06/23 11:49:34.816
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:34.825
Apr  6 11:49:34.825: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename job 04/06/23 11:49:34.826
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:34.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:34.866
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 04/06/23 11:49:34.872
STEP: Ensuring job reaches completions 04/06/23 11:49:34.878
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Apr  6 11:49:46.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7533" for this suite. 04/06/23 11:49:46.916
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":338,"skipped":6237,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.104 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:34.825
    Apr  6 11:49:34.825: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename job 04/06/23 11:49:34.826
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:34.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:34.866
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 04/06/23 11:49:34.872
    STEP: Ensuring job reaches completions 04/06/23 11:49:34.878
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Apr  6 11:49:46.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7533" for this suite. 04/06/23 11:49:46.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:46.932
Apr  6 11:49:46.932: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename controllerrevisions 04/06/23 11:49:46.934
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:46.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:46.959
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-tdlbz-daemon-set" 04/06/23 11:49:47.068
STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:49:47.08
Apr  6 11:49:47.100: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 0
Apr  6 11:49:47.100: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:49:48.122: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 1
Apr  6 11:49:48.122: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
Apr  6 11:49:49.119: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 5
Apr  6 11:49:49.119: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-tdlbz-daemon-set
STEP: Confirm DaemonSet "e2e-tdlbz-daemon-set" successfully created with "daemonset-name=e2e-tdlbz-daemon-set" label 04/06/23 11:49:49.125
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-tdlbz-daemon-set" 04/06/23 11:49:49.141
Apr  6 11:49:49.148: INFO: Located ControllerRevision: "e2e-tdlbz-daemon-set-646fd86955"
STEP: Patching ControllerRevision "e2e-tdlbz-daemon-set-646fd86955" 04/06/23 11:49:49.152
Apr  6 11:49:49.162: INFO: e2e-tdlbz-daemon-set-646fd86955 has been patched
STEP: Create a new ControllerRevision 04/06/23 11:49:49.162
Apr  6 11:49:49.168: INFO: Created ControllerRevision: e2e-tdlbz-daemon-set-6566c94579
STEP: Confirm that there are two ControllerRevisions 04/06/23 11:49:49.168
Apr  6 11:49:49.170: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr  6 11:49:49.175: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-tdlbz-daemon-set-646fd86955" 04/06/23 11:49:49.176
STEP: Confirm that there is only one ControllerRevision 04/06/23 11:49:49.183
Apr  6 11:49:49.183: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr  6 11:49:49.188: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-tdlbz-daemon-set-6566c94579" 04/06/23 11:49:49.198
Apr  6 11:49:49.212: INFO: e2e-tdlbz-daemon-set-6566c94579 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 04/06/23 11:49:49.212
W0406 11:49:49.221071      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 04/06/23 11:49:49.221
Apr  6 11:49:49.221: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr  6 11:49:50.226: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr  6 11:49:50.234: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-tdlbz-daemon-set-6566c94579=updated" 04/06/23 11:49:50.234
STEP: Confirm that there is only one ControllerRevision 04/06/23 11:49:50.241
Apr  6 11:49:50.245: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr  6 11:49:50.250: INFO: Found 1 ControllerRevisions
Apr  6 11:49:50.254: INFO: ControllerRevision "e2e-tdlbz-daemon-set-54674fc64b" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-tdlbz-daemon-set" 04/06/23 11:49:50.258
STEP: deleting DaemonSet.extensions e2e-tdlbz-daemon-set in namespace controllerrevisions-2519, will wait for the garbage collector to delete the pods 04/06/23 11:49:50.258
Apr  6 11:49:50.322: INFO: Deleting DaemonSet.extensions e2e-tdlbz-daemon-set took: 8.4754ms
Apr  6 11:49:50.423: INFO: Terminating DaemonSet.extensions e2e-tdlbz-daemon-set pods took: 100.854567ms
Apr  6 11:49:51.629: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 0
Apr  6 11:49:51.629: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-tdlbz-daemon-set
Apr  6 11:49:51.637: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43204"},"items":null}

Apr  6 11:49:51.642: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43205"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:49:51.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-2519" for this suite. 04/06/23 11:49:51.744
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":339,"skipped":6273,"failed":0}
------------------------------
â€¢ [4.823 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:46.932
    Apr  6 11:49:46.932: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename controllerrevisions 04/06/23 11:49:46.934
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:46.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:46.959
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-tdlbz-daemon-set" 04/06/23 11:49:47.068
    STEP: Check that daemon pods launch on every node of the cluster. 04/06/23 11:49:47.08
    Apr  6 11:49:47.100: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 0
    Apr  6 11:49:47.100: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:49:48.122: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 1
    Apr  6 11:49:48.122: INFO: Node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-272st is running 0 daemon pod, expected 1
    Apr  6 11:49:49.119: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 5
    Apr  6 11:49:49.119: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-tdlbz-daemon-set
    STEP: Confirm DaemonSet "e2e-tdlbz-daemon-set" successfully created with "daemonset-name=e2e-tdlbz-daemon-set" label 04/06/23 11:49:49.125
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-tdlbz-daemon-set" 04/06/23 11:49:49.141
    Apr  6 11:49:49.148: INFO: Located ControllerRevision: "e2e-tdlbz-daemon-set-646fd86955"
    STEP: Patching ControllerRevision "e2e-tdlbz-daemon-set-646fd86955" 04/06/23 11:49:49.152
    Apr  6 11:49:49.162: INFO: e2e-tdlbz-daemon-set-646fd86955 has been patched
    STEP: Create a new ControllerRevision 04/06/23 11:49:49.162
    Apr  6 11:49:49.168: INFO: Created ControllerRevision: e2e-tdlbz-daemon-set-6566c94579
    STEP: Confirm that there are two ControllerRevisions 04/06/23 11:49:49.168
    Apr  6 11:49:49.170: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr  6 11:49:49.175: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-tdlbz-daemon-set-646fd86955" 04/06/23 11:49:49.176
    STEP: Confirm that there is only one ControllerRevision 04/06/23 11:49:49.183
    Apr  6 11:49:49.183: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr  6 11:49:49.188: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-tdlbz-daemon-set-6566c94579" 04/06/23 11:49:49.198
    Apr  6 11:49:49.212: INFO: e2e-tdlbz-daemon-set-6566c94579 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 04/06/23 11:49:49.212
    W0406 11:49:49.221071      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 04/06/23 11:49:49.221
    Apr  6 11:49:49.221: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr  6 11:49:50.226: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr  6 11:49:50.234: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-tdlbz-daemon-set-6566c94579=updated" 04/06/23 11:49:50.234
    STEP: Confirm that there is only one ControllerRevision 04/06/23 11:49:50.241
    Apr  6 11:49:50.245: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr  6 11:49:50.250: INFO: Found 1 ControllerRevisions
    Apr  6 11:49:50.254: INFO: ControllerRevision "e2e-tdlbz-daemon-set-54674fc64b" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-tdlbz-daemon-set" 04/06/23 11:49:50.258
    STEP: deleting DaemonSet.extensions e2e-tdlbz-daemon-set in namespace controllerrevisions-2519, will wait for the garbage collector to delete the pods 04/06/23 11:49:50.258
    Apr  6 11:49:50.322: INFO: Deleting DaemonSet.extensions e2e-tdlbz-daemon-set took: 8.4754ms
    Apr  6 11:49:50.423: INFO: Terminating DaemonSet.extensions e2e-tdlbz-daemon-set pods took: 100.854567ms
    Apr  6 11:49:51.629: INFO: Number of nodes with available pods controlled by daemonset e2e-tdlbz-daemon-set: 0
    Apr  6 11:49:51.629: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-tdlbz-daemon-set
    Apr  6 11:49:51.637: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43204"},"items":null}

    Apr  6 11:49:51.642: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43205"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:49:51.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-2519" for this suite. 04/06/23 11:49:51.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:51.758
Apr  6 11:49:51.758: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:49:51.76
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:51.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:51.791
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 04/06/23 11:49:51.805
Apr  6 11:49:51.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2" in namespace "projected-1735" to be "Succeeded or Failed"
Apr  6 11:49:51.826: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.325837ms
Apr  6 11:49:53.837: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019560144s
Apr  6 11:49:55.835: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017840813s
STEP: Saw pod success 04/06/23 11:49:55.835
Apr  6 11:49:55.835: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2" satisfied condition "Succeeded or Failed"
Apr  6 11:49:55.840: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2 container client-container: <nil>
STEP: delete the pod 04/06/23 11:49:55.894
Apr  6 11:49:55.910: INFO: Waiting for pod downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2 to disappear
Apr  6 11:49:55.913: INFO: Pod downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 11:49:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1735" for this suite. 04/06/23 11:49:55.934
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":340,"skipped":6278,"failed":0}
------------------------------
â€¢ [4.189 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:51.758
    Apr  6 11:49:51.758: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:49:51.76
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:51.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:51.791
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 04/06/23 11:49:51.805
    Apr  6 11:49:51.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2" in namespace "projected-1735" to be "Succeeded or Failed"
    Apr  6 11:49:51.826: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.325837ms
    Apr  6 11:49:53.837: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019560144s
    Apr  6 11:49:55.835: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017840813s
    STEP: Saw pod success 04/06/23 11:49:55.835
    Apr  6 11:49:55.835: INFO: Pod "downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2" satisfied condition "Succeeded or Failed"
    Apr  6 11:49:55.840: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2 container client-container: <nil>
    STEP: delete the pod 04/06/23 11:49:55.894
    Apr  6 11:49:55.910: INFO: Waiting for pod downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2 to disappear
    Apr  6 11:49:55.913: INFO: Pod downwardapi-volume-78cac997-3a61-4d9c-a592-5cf8842a77e2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 11:49:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1735" for this suite. 04/06/23 11:49:55.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:49:55.949
Apr  6 11:49:55.949: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 11:49:55.95
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:55.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:56.007
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 04/06/23 11:49:56.031
Apr  6 11:49:56.048: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5994" to be "running and ready"
Apr  6 11:49:56.054: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334972ms
Apr  6 11:49:56.054: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:49:58.061: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013463522s
Apr  6 11:49:58.061: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr  6 11:49:58.062: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 04/06/23 11:49:58.07
Apr  6 11:49:58.081: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5994" to be "running and ready"
Apr  6 11:49:58.086: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.992291ms
Apr  6 11:49:58.086: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:50:00.093: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011801571s
Apr  6 11:50:00.093: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Apr  6 11:50:00.093: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/06/23 11:50:00.106
STEP: delete the pod with lifecycle hook 04/06/23 11:50:00.179
Apr  6 11:50:00.203: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 11:50:00.212: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 11:50:02.213: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 11:50:02.218: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 11:50:04.212: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 11:50:04.218: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Apr  6 11:50:04.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5994" for this suite. 04/06/23 11:50:04.229
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":341,"skipped":6308,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.294 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:49:55.949
    Apr  6 11:49:55.949: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/06/23 11:49:55.95
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:49:55.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:49:56.007
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 04/06/23 11:49:56.031
    Apr  6 11:49:56.048: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5994" to be "running and ready"
    Apr  6 11:49:56.054: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334972ms
    Apr  6 11:49:56.054: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:49:58.061: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013463522s
    Apr  6 11:49:58.061: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr  6 11:49:58.062: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 04/06/23 11:49:58.07
    Apr  6 11:49:58.081: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5994" to be "running and ready"
    Apr  6 11:49:58.086: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.992291ms
    Apr  6 11:49:58.086: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:50:00.093: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011801571s
    Apr  6 11:50:00.093: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Apr  6 11:50:00.093: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/06/23 11:50:00.106
    STEP: delete the pod with lifecycle hook 04/06/23 11:50:00.179
    Apr  6 11:50:00.203: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr  6 11:50:00.212: INFO: Pod pod-with-poststart-http-hook still exists
    Apr  6 11:50:02.213: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr  6 11:50:02.218: INFO: Pod pod-with-poststart-http-hook still exists
    Apr  6 11:50:04.212: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr  6 11:50:04.218: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Apr  6 11:50:04.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5994" for this suite. 04/06/23 11:50:04.229
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:50:04.243
Apr  6 11:50:04.243: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename init-container 04/06/23 11:50:04.245
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:04.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:04.285
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 04/06/23 11:50:04.292
Apr  6 11:50:04.293: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Apr  6 11:50:10.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6625" for this suite. 04/06/23 11:50:10.237
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":342,"skipped":6310,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.003 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:50:04.243
    Apr  6 11:50:04.243: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename init-container 04/06/23 11:50:04.245
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:04.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:04.285
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 04/06/23 11:50:04.292
    Apr  6 11:50:04.293: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Apr  6 11:50:10.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6625" for this suite. 04/06/23 11:50:10.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:50:10.248
Apr  6 11:50:10.248: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename subpath 04/06/23 11:50:10.249
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:10.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:10.277
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/06/23 11:50:10.282
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-hbj4 04/06/23 11:50:10.303
STEP: Creating a pod to test atomic-volume-subpath 04/06/23 11:50:10.303
Apr  6 11:50:10.313: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-hbj4" in namespace "subpath-1606" to be "Succeeded or Failed"
Apr  6 11:50:10.320: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48485ms
Apr  6 11:50:12.331: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017520073s
Apr  6 11:50:14.330: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 4.016260172s
Apr  6 11:50:16.328: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 6.014389134s
Apr  6 11:50:18.328: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 8.014089116s
Apr  6 11:50:20.326: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 10.012651986s
Apr  6 11:50:22.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 12.013665966s
Apr  6 11:50:24.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 14.013077891s
Apr  6 11:50:26.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 16.013511145s
Apr  6 11:50:28.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 18.013784882s
Apr  6 11:50:30.329: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 20.015214016s
Apr  6 11:50:32.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 22.014017931s
Apr  6 11:50:34.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=false. Elapsed: 24.01364121s
Apr  6 11:50:36.326: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012772146s
STEP: Saw pod success 04/06/23 11:50:36.326
Apr  6 11:50:36.326: INFO: Pod "pod-subpath-test-downwardapi-hbj4" satisfied condition "Succeeded or Failed"
Apr  6 11:50:36.332: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-subpath-test-downwardapi-hbj4 container test-container-subpath-downwardapi-hbj4: <nil>
STEP: delete the pod 04/06/23 11:50:36.347
Apr  6 11:50:36.357: INFO: Waiting for pod pod-subpath-test-downwardapi-hbj4 to disappear
Apr  6 11:50:36.361: INFO: Pod pod-subpath-test-downwardapi-hbj4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-hbj4 04/06/23 11:50:36.362
Apr  6 11:50:36.362: INFO: Deleting pod "pod-subpath-test-downwardapi-hbj4" in namespace "subpath-1606"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Apr  6 11:50:36.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1606" for this suite. 04/06/23 11:50:36.379
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":343,"skipped":6336,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.138 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:50:10.248
    Apr  6 11:50:10.248: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename subpath 04/06/23 11:50:10.249
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:10.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:10.277
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/06/23 11:50:10.282
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-hbj4 04/06/23 11:50:10.303
    STEP: Creating a pod to test atomic-volume-subpath 04/06/23 11:50:10.303
    Apr  6 11:50:10.313: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-hbj4" in namespace "subpath-1606" to be "Succeeded or Failed"
    Apr  6 11:50:10.320: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48485ms
    Apr  6 11:50:12.331: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017520073s
    Apr  6 11:50:14.330: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 4.016260172s
    Apr  6 11:50:16.328: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 6.014389134s
    Apr  6 11:50:18.328: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 8.014089116s
    Apr  6 11:50:20.326: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 10.012651986s
    Apr  6 11:50:22.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 12.013665966s
    Apr  6 11:50:24.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 14.013077891s
    Apr  6 11:50:26.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 16.013511145s
    Apr  6 11:50:28.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 18.013784882s
    Apr  6 11:50:30.329: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 20.015214016s
    Apr  6 11:50:32.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=true. Elapsed: 22.014017931s
    Apr  6 11:50:34.327: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Running", Reason="", readiness=false. Elapsed: 24.01364121s
    Apr  6 11:50:36.326: INFO: Pod "pod-subpath-test-downwardapi-hbj4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012772146s
    STEP: Saw pod success 04/06/23 11:50:36.326
    Apr  6 11:50:36.326: INFO: Pod "pod-subpath-test-downwardapi-hbj4" satisfied condition "Succeeded or Failed"
    Apr  6 11:50:36.332: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-subpath-test-downwardapi-hbj4 container test-container-subpath-downwardapi-hbj4: <nil>
    STEP: delete the pod 04/06/23 11:50:36.347
    Apr  6 11:50:36.357: INFO: Waiting for pod pod-subpath-test-downwardapi-hbj4 to disappear
    Apr  6 11:50:36.361: INFO: Pod pod-subpath-test-downwardapi-hbj4 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-hbj4 04/06/23 11:50:36.362
    Apr  6 11:50:36.362: INFO: Deleting pod "pod-subpath-test-downwardapi-hbj4" in namespace "subpath-1606"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Apr  6 11:50:36.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1606" for this suite. 04/06/23 11:50:36.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:50:36.388
Apr  6 11:50:36.388: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename pods 04/06/23 11:50:36.392
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:36.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:36.421
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 04/06/23 11:50:36.434
STEP: submitting the pod to kubernetes 04/06/23 11:50:36.434
STEP: verifying QOS class is set on the pod 04/06/23 11:50:36.452
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Apr  6 11:50:36.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4647" for this suite. 04/06/23 11:50:36.469
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":344,"skipped":6373,"failed":0}
------------------------------
â€¢ [0.089 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:50:36.388
    Apr  6 11:50:36.388: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename pods 04/06/23 11:50:36.392
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:36.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:36.421
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 04/06/23 11:50:36.434
    STEP: submitting the pod to kubernetes 04/06/23 11:50:36.434
    STEP: verifying QOS class is set on the pod 04/06/23 11:50:36.452
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Apr  6 11:50:36.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4647" for this suite. 04/06/23 11:50:36.469
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:50:36.478
Apr  6 11:50:36.478: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename kubectl 04/06/23 11:50:36.48
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:36.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:36.504
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 04/06/23 11:50:36.511
Apr  6 11:50:36.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 create -f -'
Apr  6 11:50:37.332: INFO: stderr: ""
Apr  6 11:50:37.332: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 11:50:37.332
Apr  6 11:50:37.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 11:50:37.568: INFO: stderr: ""
Apr  6 11:50:37.568: INFO: stdout: "update-demo-nautilus-c8rmt update-demo-nautilus-zfkb4 "
Apr  6 11:50:37.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-c8rmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 11:50:37.824: INFO: stderr: ""
Apr  6 11:50:37.824: INFO: stdout: ""
Apr  6 11:50:37.824: INFO: update-demo-nautilus-c8rmt is created but not running
Apr  6 11:50:42.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr  6 11:50:43.032: INFO: stderr: ""
Apr  6 11:50:43.032: INFO: stdout: "update-demo-nautilus-c8rmt update-demo-nautilus-zfkb4 "
Apr  6 11:50:43.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-c8rmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 11:50:43.208: INFO: stderr: ""
Apr  6 11:50:43.208: INFO: stdout: "true"
Apr  6 11:50:43.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-c8rmt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 11:50:43.369: INFO: stderr: ""
Apr  6 11:50:43.369: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Apr  6 11:50:43.369: INFO: validating pod update-demo-nautilus-c8rmt
Apr  6 11:50:43.495: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 11:50:43.495: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 11:50:43.495: INFO: update-demo-nautilus-c8rmt is verified up and running
Apr  6 11:50:43.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-zfkb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr  6 11:50:43.676: INFO: stderr: ""
Apr  6 11:50:43.676: INFO: stdout: "true"
Apr  6 11:50:43.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-zfkb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr  6 11:50:43.856: INFO: stderr: ""
Apr  6 11:50:43.856: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Apr  6 11:50:43.856: INFO: validating pod update-demo-nautilus-zfkb4
Apr  6 11:50:43.895: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 11:50:43.895: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 11:50:43.895: INFO: update-demo-nautilus-zfkb4 is verified up and running
STEP: using delete to clean up resources 04/06/23 11:50:43.895
Apr  6 11:50:43.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 delete --grace-period=0 --force -f -'
Apr  6 11:50:44.050: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 11:50:44.050: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  6 11:50:44.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get rc,svc -l name=update-demo --no-headers'
Apr  6 11:50:44.204: INFO: stderr: "No resources found in kubectl-4585 namespace.\n"
Apr  6 11:50:44.204: INFO: stdout: ""
Apr  6 11:50:44.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 11:50:44.347: INFO: stderr: ""
Apr  6 11:50:44.347: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Apr  6 11:50:44.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4585" for this suite. 04/06/23 11:50:44.357
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":345,"skipped":6373,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.886 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:50:36.478
    Apr  6 11:50:36.478: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename kubectl 04/06/23 11:50:36.48
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:36.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:36.504
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 04/06/23 11:50:36.511
    Apr  6 11:50:36.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 create -f -'
    Apr  6 11:50:37.332: INFO: stderr: ""
    Apr  6 11:50:37.332: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/06/23 11:50:37.332
    Apr  6 11:50:37.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr  6 11:50:37.568: INFO: stderr: ""
    Apr  6 11:50:37.568: INFO: stdout: "update-demo-nautilus-c8rmt update-demo-nautilus-zfkb4 "
    Apr  6 11:50:37.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-c8rmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 11:50:37.824: INFO: stderr: ""
    Apr  6 11:50:37.824: INFO: stdout: ""
    Apr  6 11:50:37.824: INFO: update-demo-nautilus-c8rmt is created but not running
    Apr  6 11:50:42.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr  6 11:50:43.032: INFO: stderr: ""
    Apr  6 11:50:43.032: INFO: stdout: "update-demo-nautilus-c8rmt update-demo-nautilus-zfkb4 "
    Apr  6 11:50:43.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-c8rmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 11:50:43.208: INFO: stderr: ""
    Apr  6 11:50:43.208: INFO: stdout: "true"
    Apr  6 11:50:43.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-c8rmt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr  6 11:50:43.369: INFO: stderr: ""
    Apr  6 11:50:43.369: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Apr  6 11:50:43.369: INFO: validating pod update-demo-nautilus-c8rmt
    Apr  6 11:50:43.495: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr  6 11:50:43.495: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr  6 11:50:43.495: INFO: update-demo-nautilus-c8rmt is verified up and running
    Apr  6 11:50:43.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-zfkb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr  6 11:50:43.676: INFO: stderr: ""
    Apr  6 11:50:43.676: INFO: stdout: "true"
    Apr  6 11:50:43.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods update-demo-nautilus-zfkb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr  6 11:50:43.856: INFO: stderr: ""
    Apr  6 11:50:43.856: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Apr  6 11:50:43.856: INFO: validating pod update-demo-nautilus-zfkb4
    Apr  6 11:50:43.895: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr  6 11:50:43.895: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr  6 11:50:43.895: INFO: update-demo-nautilus-zfkb4 is verified up and running
    STEP: using delete to clean up resources 04/06/23 11:50:43.895
    Apr  6 11:50:43.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 delete --grace-period=0 --force -f -'
    Apr  6 11:50:44.050: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr  6 11:50:44.050: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr  6 11:50:44.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get rc,svc -l name=update-demo --no-headers'
    Apr  6 11:50:44.204: INFO: stderr: "No resources found in kubectl-4585 namespace.\n"
    Apr  6 11:50:44.204: INFO: stdout: ""
    Apr  6 11:50:44.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=kubectl-4585 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr  6 11:50:44.347: INFO: stderr: ""
    Apr  6 11:50:44.347: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Apr  6 11:50:44.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4585" for this suite. 04/06/23 11:50:44.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:50:44.364
Apr  6 11:50:44.364: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:50:44.366
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:44.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:44.397
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 04/06/23 11:51:01.423
STEP: Creating a ResourceQuota 04/06/23 11:51:06.429
STEP: Ensuring resource quota status is calculated 04/06/23 11:51:06.437
STEP: Creating a ConfigMap 04/06/23 11:51:08.454
STEP: Ensuring resource quota status captures configMap creation 04/06/23 11:51:08.467
STEP: Deleting a ConfigMap 04/06/23 11:51:10.474
STEP: Ensuring resource quota status released usage 04/06/23 11:51:10.48
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:51:12.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5996" for this suite. 04/06/23 11:51:12.502
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":346,"skipped":6378,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:50:44.364
    Apr  6 11:50:44.364: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:50:44.366
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:50:44.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:50:44.397
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 04/06/23 11:51:01.423
    STEP: Creating a ResourceQuota 04/06/23 11:51:06.429
    STEP: Ensuring resource quota status is calculated 04/06/23 11:51:06.437
    STEP: Creating a ConfigMap 04/06/23 11:51:08.454
    STEP: Ensuring resource quota status captures configMap creation 04/06/23 11:51:08.467
    STEP: Deleting a ConfigMap 04/06/23 11:51:10.474
    STEP: Ensuring resource quota status released usage 04/06/23 11:51:10.48
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:51:12.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5996" for this suite. 04/06/23 11:51:12.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:51:12.517
Apr  6 11:51:12.518: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 11:51:12.519
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:12.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:12.552
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 04/06/23 11:51:12.561
Apr  6 11:51:12.581: INFO: Waiting up to 5m0s for pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492" in namespace "var-expansion-4217" to be "Succeeded or Failed"
Apr  6 11:51:12.599: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492": Phase="Pending", Reason="", readiness=false. Elapsed: 18.411326ms
Apr  6 11:51:14.621: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04059301s
Apr  6 11:51:16.607: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026634525s
STEP: Saw pod success 04/06/23 11:51:16.607
Apr  6 11:51:16.607: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492" satisfied condition "Succeeded or Failed"
Apr  6 11:51:16.631: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492 container dapi-container: <nil>
STEP: delete the pod 04/06/23 11:51:16.648
Apr  6 11:51:16.666: INFO: Waiting for pod var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492 to disappear
Apr  6 11:51:16.670: INFO: Pod var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 11:51:16.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4217" for this suite. 04/06/23 11:51:16.69
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":347,"skipped":6384,"failed":0}
------------------------------
â€¢ [4.181 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:51:12.517
    Apr  6 11:51:12.518: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 11:51:12.519
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:12.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:12.552
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 04/06/23 11:51:12.561
    Apr  6 11:51:12.581: INFO: Waiting up to 5m0s for pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492" in namespace "var-expansion-4217" to be "Succeeded or Failed"
    Apr  6 11:51:12.599: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492": Phase="Pending", Reason="", readiness=false. Elapsed: 18.411326ms
    Apr  6 11:51:14.621: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04059301s
    Apr  6 11:51:16.607: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026634525s
    STEP: Saw pod success 04/06/23 11:51:16.607
    Apr  6 11:51:16.607: INFO: Pod "var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492" satisfied condition "Succeeded or Failed"
    Apr  6 11:51:16.631: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492 container dapi-container: <nil>
    STEP: delete the pod 04/06/23 11:51:16.648
    Apr  6 11:51:16.666: INFO: Waiting for pod var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492 to disappear
    Apr  6 11:51:16.670: INFO: Pod var-expansion-eaba540f-5370-4a17-bd58-d3d2b2630492 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 11:51:16.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4217" for this suite. 04/06/23 11:51:16.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:51:16.705
Apr  6 11:51:16.706: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:51:16.707
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:16.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:16.742
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 04/06/23 11:51:16.76
STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:16.77
STEP: Creating a ResourceQuota with not best effort scope 04/06/23 11:51:18.787
STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:18.806
STEP: Creating a best-effort pod 04/06/23 11:51:20.812
STEP: Ensuring resource quota with best effort scope captures the pod usage 04/06/23 11:51:20.826
STEP: Ensuring resource quota with not best effort ignored the pod usage 04/06/23 11:51:22.833
STEP: Deleting the pod 04/06/23 11:51:24.84
STEP: Ensuring resource quota status released the pod usage 04/06/23 11:51:24.848
STEP: Creating a not best-effort pod 04/06/23 11:51:26.855
STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/06/23 11:51:26.87
STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/06/23 11:51:28.878
STEP: Deleting the pod 04/06/23 11:51:30.885
STEP: Ensuring resource quota status released the pod usage 04/06/23 11:51:30.902
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:51:32.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-265" for this suite. 04/06/23 11:51:32.925
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":348,"skipped":6404,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.227 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:51:16.705
    Apr  6 11:51:16.706: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:51:16.707
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:16.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:16.742
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 04/06/23 11:51:16.76
    STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:16.77
    STEP: Creating a ResourceQuota with not best effort scope 04/06/23 11:51:18.787
    STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:18.806
    STEP: Creating a best-effort pod 04/06/23 11:51:20.812
    STEP: Ensuring resource quota with best effort scope captures the pod usage 04/06/23 11:51:20.826
    STEP: Ensuring resource quota with not best effort ignored the pod usage 04/06/23 11:51:22.833
    STEP: Deleting the pod 04/06/23 11:51:24.84
    STEP: Ensuring resource quota status released the pod usage 04/06/23 11:51:24.848
    STEP: Creating a not best-effort pod 04/06/23 11:51:26.855
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/06/23 11:51:26.87
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/06/23 11:51:28.878
    STEP: Deleting the pod 04/06/23 11:51:30.885
    STEP: Ensuring resource quota status released the pod usage 04/06/23 11:51:30.902
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:51:32.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-265" for this suite. 04/06/23 11:51:32.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:51:32.934
Apr  6 11:51:32.934: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 11:51:32.936
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:32.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:32.963
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4686 04/06/23 11:51:32.971
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Apr  6 11:51:32.993: INFO: Found 0 stateful pods, waiting for 1
Apr  6 11:51:43.001: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 04/06/23 11:51:43.014
W0406 11:51:43.023777      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr  6 11:51:43.034: INFO: Found 1 stateful pods, waiting for 2
Apr  6 11:51:53.043: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:51:53.043: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 04/06/23 11:51:53.052
STEP: Delete all of the StatefulSets 04/06/23 11:51:53.057
STEP: Verify that StatefulSets have been deleted 04/06/23 11:51:53.064
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 11:51:53.068: INFO: Deleting all statefulset in ns statefulset-4686
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 11:51:53.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4686" for this suite. 04/06/23 11:51:53.091
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":349,"skipped":6415,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:51:32.934
    Apr  6 11:51:32.934: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 11:51:32.936
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:32.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:32.963
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4686 04/06/23 11:51:32.971
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Apr  6 11:51:32.993: INFO: Found 0 stateful pods, waiting for 1
    Apr  6 11:51:43.001: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 04/06/23 11:51:43.014
    W0406 11:51:43.023777      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr  6 11:51:43.034: INFO: Found 1 stateful pods, waiting for 2
    Apr  6 11:51:53.043: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:51:53.043: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 04/06/23 11:51:53.052
    STEP: Delete all of the StatefulSets 04/06/23 11:51:53.057
    STEP: Verify that StatefulSets have been deleted 04/06/23 11:51:53.064
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 11:51:53.068: INFO: Deleting all statefulset in ns statefulset-4686
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 11:51:53.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4686" for this suite. 04/06/23 11:51:53.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:51:53.102
Apr  6 11:51:53.102: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename ingressclass 04/06/23 11:51:53.103
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:53.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:53.128
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 04/06/23 11:51:53.136
STEP: getting /apis/networking.k8s.io 04/06/23 11:51:53.142
STEP: getting /apis/networking.k8s.iov1 04/06/23 11:51:53.144
STEP: creating 04/06/23 11:51:53.146
STEP: getting 04/06/23 11:51:53.163
STEP: listing 04/06/23 11:51:53.169
STEP: watching 04/06/23 11:51:53.174
Apr  6 11:51:53.174: INFO: starting watch
STEP: patching 04/06/23 11:51:53.177
STEP: updating 04/06/23 11:51:53.184
Apr  6 11:51:53.189: INFO: waiting for watch events with expected annotations
Apr  6 11:51:53.189: INFO: saw patched and updated annotations
STEP: deleting 04/06/23 11:51:53.189
STEP: deleting a collection 04/06/23 11:51:53.208
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Apr  6 11:51:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5822" for this suite. 04/06/23 11:51:53.23
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":350,"skipped":6499,"failed":0}
------------------------------
â€¢ [0.135 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:51:53.102
    Apr  6 11:51:53.102: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename ingressclass 04/06/23 11:51:53.103
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:53.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:53.128
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 04/06/23 11:51:53.136
    STEP: getting /apis/networking.k8s.io 04/06/23 11:51:53.142
    STEP: getting /apis/networking.k8s.iov1 04/06/23 11:51:53.144
    STEP: creating 04/06/23 11:51:53.146
    STEP: getting 04/06/23 11:51:53.163
    STEP: listing 04/06/23 11:51:53.169
    STEP: watching 04/06/23 11:51:53.174
    Apr  6 11:51:53.174: INFO: starting watch
    STEP: patching 04/06/23 11:51:53.177
    STEP: updating 04/06/23 11:51:53.184
    Apr  6 11:51:53.189: INFO: waiting for watch events with expected annotations
    Apr  6 11:51:53.189: INFO: saw patched and updated annotations
    STEP: deleting 04/06/23 11:51:53.189
    STEP: deleting a collection 04/06/23 11:51:53.208
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Apr  6 11:51:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-5822" for this suite. 04/06/23 11:51:53.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:51:53.238
Apr  6 11:51:53.238: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename resourcequota 04/06/23 11:51:53.239
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:53.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:53.28
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 04/06/23 11:51:53.287
STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:53.294
STEP: Creating a ResourceQuota with not terminating scope 04/06/23 11:51:55.306
STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:55.314
STEP: Creating a long running pod 04/06/23 11:51:57.321
STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/06/23 11:51:57.343
STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/06/23 11:51:59.353
STEP: Deleting the pod 04/06/23 11:52:01.36
STEP: Ensuring resource quota status released the pod usage 04/06/23 11:52:01.369
STEP: Creating a terminating pod 04/06/23 11:52:03.376
STEP: Ensuring resource quota with terminating scope captures the pod usage 04/06/23 11:52:03.398
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/06/23 11:52:05.405
STEP: Deleting the pod 04/06/23 11:52:07.412
STEP: Ensuring resource quota status released the pod usage 04/06/23 11:52:07.423
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Apr  6 11:52:09.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-363" for this suite. 04/06/23 11:52:09.446
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":351,"skipped":6511,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.217 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:51:53.238
    Apr  6 11:51:53.238: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename resourcequota 04/06/23 11:51:53.239
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:51:53.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:51:53.28
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 04/06/23 11:51:53.287
    STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:53.294
    STEP: Creating a ResourceQuota with not terminating scope 04/06/23 11:51:55.306
    STEP: Ensuring ResourceQuota status is calculated 04/06/23 11:51:55.314
    STEP: Creating a long running pod 04/06/23 11:51:57.321
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/06/23 11:51:57.343
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/06/23 11:51:59.353
    STEP: Deleting the pod 04/06/23 11:52:01.36
    STEP: Ensuring resource quota status released the pod usage 04/06/23 11:52:01.369
    STEP: Creating a terminating pod 04/06/23 11:52:03.376
    STEP: Ensuring resource quota with terminating scope captures the pod usage 04/06/23 11:52:03.398
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/06/23 11:52:05.405
    STEP: Deleting the pod 04/06/23 11:52:07.412
    STEP: Ensuring resource quota status released the pod usage 04/06/23 11:52:07.423
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Apr  6 11:52:09.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-363" for this suite. 04/06/23 11:52:09.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:09.455
Apr  6 11:52:09.456: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename var-expansion 04/06/23 11:52:09.457
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:09.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:09.485
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Apr  6 11:52:09.522: INFO: Waiting up to 2m0s for pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" in namespace "var-expansion-7987" to be "container 0 failed with reason CreateContainerConfigError"
Apr  6 11:52:09.529: INFO: Pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.631287ms
Apr  6 11:52:11.542: INFO: Pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019908047s
Apr  6 11:52:11.542: INFO: Pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr  6 11:52:11.542: INFO: Deleting pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" in namespace "var-expansion-7987"
Apr  6 11:52:11.550: INFO: Wait up to 5m0s for pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Apr  6 11:52:15.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7987" for this suite. 04/06/23 11:52:15.586
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":352,"skipped":6516,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.141 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:09.455
    Apr  6 11:52:09.456: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename var-expansion 04/06/23 11:52:09.457
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:09.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:09.485
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Apr  6 11:52:09.522: INFO: Waiting up to 2m0s for pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" in namespace "var-expansion-7987" to be "container 0 failed with reason CreateContainerConfigError"
    Apr  6 11:52:09.529: INFO: Pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.631287ms
    Apr  6 11:52:11.542: INFO: Pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019908047s
    Apr  6 11:52:11.542: INFO: Pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr  6 11:52:11.542: INFO: Deleting pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" in namespace "var-expansion-7987"
    Apr  6 11:52:11.550: INFO: Wait up to 5m0s for pod "var-expansion-e46e9693-b14d-4af7-ad68-9a447b644fe6" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Apr  6 11:52:15.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7987" for this suite. 04/06/23 11:52:15.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:15.599
Apr  6 11:52:15.599: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:52:15.6
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:15.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:15.629
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-29a9b8c3-d772-4f44-9dd4-4aa327cc199e 04/06/23 11:52:15.636
STEP: Creating a pod to test consume configMaps 04/06/23 11:52:15.643
Apr  6 11:52:15.667: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5" in namespace "projected-6277" to be "Succeeded or Failed"
Apr  6 11:52:15.672: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.744905ms
Apr  6 11:52:17.679: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012482787s
Apr  6 11:52:19.678: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011434702s
STEP: Saw pod success 04/06/23 11:52:19.678
Apr  6 11:52:19.679: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5" satisfied condition "Succeeded or Failed"
Apr  6 11:52:19.684: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5 container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:52:19.703
Apr  6 11:52:19.717: INFO: Waiting for pod pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5 to disappear
Apr  6 11:52:19.722: INFO: Pod pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Apr  6 11:52:19.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6277" for this suite. 04/06/23 11:52:19.731
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":353,"skipped":6561,"failed":0}
------------------------------
â€¢ [4.139 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:15.599
    Apr  6 11:52:15.599: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:52:15.6
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:15.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:15.629
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-29a9b8c3-d772-4f44-9dd4-4aa327cc199e 04/06/23 11:52:15.636
    STEP: Creating a pod to test consume configMaps 04/06/23 11:52:15.643
    Apr  6 11:52:15.667: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5" in namespace "projected-6277" to be "Succeeded or Failed"
    Apr  6 11:52:15.672: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.744905ms
    Apr  6 11:52:17.679: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012482787s
    Apr  6 11:52:19.678: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011434702s
    STEP: Saw pod success 04/06/23 11:52:19.678
    Apr  6 11:52:19.679: INFO: Pod "pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5" satisfied condition "Succeeded or Failed"
    Apr  6 11:52:19.684: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5 container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:52:19.703
    Apr  6 11:52:19.717: INFO: Waiting for pod pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5 to disappear
    Apr  6 11:52:19.722: INFO: Pod pod-projected-configmaps-d621005f-f38a-470b-aec4-5a2ccedb70a5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Apr  6 11:52:19.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6277" for this suite. 04/06/23 11:52:19.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:19.744
Apr  6 11:52:19.744: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename podtemplate 04/06/23 11:52:19.746
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:19.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:19.779
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Apr  6 11:52:19.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9815" for this suite. 04/06/23 11:52:19.835
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":354,"skipped":6573,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:19.744
    Apr  6 11:52:19.744: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename podtemplate 04/06/23 11:52:19.746
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:19.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:19.779
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Apr  6 11:52:19.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9815" for this suite. 04/06/23 11:52:19.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:19.843
Apr  6 11:52:19.844: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename namespaces 04/06/23 11:52:19.845
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:19.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:19.869
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 04/06/23 11:52:19.881
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:19.907
STEP: Creating a pod in the namespace 04/06/23 11:52:19.914
STEP: Waiting for the pod to have running status 04/06/23 11:52:19.93
Apr  6 11:52:19.930: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3995" to be "running"
Apr  6 11:52:19.941: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.814886ms
Apr  6 11:52:21.951: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021320054s
Apr  6 11:52:21.951: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 04/06/23 11:52:21.951
STEP: Waiting for the namespace to be removed. 04/06/23 11:52:21.959
STEP: Recreating the namespace 04/06/23 11:52:32.966
STEP: Verifying there are no pods in the namespace 04/06/23 11:52:32.983
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:52:32.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8378" for this suite. 04/06/23 11:52:33.002
STEP: Destroying namespace "nsdeletetest-3995" for this suite. 04/06/23 11:52:33.009
Apr  6 11:52:33.013: INFO: Namespace nsdeletetest-3995 was already deleted
STEP: Destroying namespace "nsdeletetest-3694" for this suite. 04/06/23 11:52:33.013
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":355,"skipped":6593,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.187 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:19.843
    Apr  6 11:52:19.844: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename namespaces 04/06/23 11:52:19.845
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:19.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:19.869
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 04/06/23 11:52:19.881
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:19.907
    STEP: Creating a pod in the namespace 04/06/23 11:52:19.914
    STEP: Waiting for the pod to have running status 04/06/23 11:52:19.93
    Apr  6 11:52:19.930: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3995" to be "running"
    Apr  6 11:52:19.941: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.814886ms
    Apr  6 11:52:21.951: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021320054s
    Apr  6 11:52:21.951: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 04/06/23 11:52:21.951
    STEP: Waiting for the namespace to be removed. 04/06/23 11:52:21.959
    STEP: Recreating the namespace 04/06/23 11:52:32.966
    STEP: Verifying there are no pods in the namespace 04/06/23 11:52:32.983
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:52:32.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8378" for this suite. 04/06/23 11:52:33.002
    STEP: Destroying namespace "nsdeletetest-3995" for this suite. 04/06/23 11:52:33.009
    Apr  6 11:52:33.013: INFO: Namespace nsdeletetest-3995 was already deleted
    STEP: Destroying namespace "nsdeletetest-3694" for this suite. 04/06/23 11:52:33.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:33.032
Apr  6 11:52:33.033: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename projected 04/06/23 11:52:33.035
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:33.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:33.076
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 04/06/23 11:52:33.083
Apr  6 11:52:33.103: INFO: Waiting up to 5m0s for pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48" in namespace "projected-6607" to be "running and ready"
Apr  6 11:52:33.109: INFO: Pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.769288ms
Apr  6 11:52:33.109: INFO: The phase of Pod labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 11:52:35.138: INFO: Pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48": Phase="Running", Reason="", readiness=true. Elapsed: 2.034688694s
Apr  6 11:52:35.138: INFO: The phase of Pod labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48 is Running (Ready = true)
Apr  6 11:52:35.138: INFO: Pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48" satisfied condition "running and ready"
Apr  6 11:52:35.695: INFO: Successfully updated pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Apr  6 11:52:39.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6607" for this suite. 04/06/23 11:52:39.758
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":356,"skipped":6600,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.733 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:33.032
    Apr  6 11:52:33.033: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename projected 04/06/23 11:52:33.035
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:33.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:33.076
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 04/06/23 11:52:33.083
    Apr  6 11:52:33.103: INFO: Waiting up to 5m0s for pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48" in namespace "projected-6607" to be "running and ready"
    Apr  6 11:52:33.109: INFO: Pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.769288ms
    Apr  6 11:52:33.109: INFO: The phase of Pod labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48 is Pending, waiting for it to be Running (with Ready = true)
    Apr  6 11:52:35.138: INFO: Pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48": Phase="Running", Reason="", readiness=true. Elapsed: 2.034688694s
    Apr  6 11:52:35.138: INFO: The phase of Pod labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48 is Running (Ready = true)
    Apr  6 11:52:35.138: INFO: Pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48" satisfied condition "running and ready"
    Apr  6 11:52:35.695: INFO: Successfully updated pod "labelsupdateb1eaad93-7485-4c8e-aace-a8838a30ca48"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Apr  6 11:52:39.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6607" for this suite. 04/06/23 11:52:39.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:39.767
Apr  6 11:52:39.767: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 11:52:39.768
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:39.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:39.793
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Apr  6 11:52:39.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Apr  6 11:52:40.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9328" for this suite. 04/06/23 11:52:40.843
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":357,"skipped":6621,"failed":0}
------------------------------
â€¢ [1.082 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:39.767
    Apr  6 11:52:39.767: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename custom-resource-definition 04/06/23 11:52:39.768
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:39.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:39.793
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Apr  6 11:52:39.802: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Apr  6 11:52:40.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9328" for this suite. 04/06/23 11:52:40.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:40.85
Apr  6 11:52:40.850: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename svcaccounts 04/06/23 11:52:40.851
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:40.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:40.871
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  04/06/23 11:52:40.877
Apr  6 11:52:40.897: INFO: Waiting up to 5m0s for pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba" in namespace "svcaccounts-8723" to be "Succeeded or Failed"
Apr  6 11:52:40.902: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.271623ms
Apr  6 11:52:42.909: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011252091s
Apr  6 11:52:44.908: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010131348s
STEP: Saw pod success 04/06/23 11:52:44.908
Apr  6 11:52:44.908: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba" satisfied condition "Succeeded or Failed"
Apr  6 11:52:44.911: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod test-pod-c7e331c4-a654-4016-85e3-659603949eba container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:52:44.922
Apr  6 11:52:44.932: INFO: Waiting for pod test-pod-c7e331c4-a654-4016-85e3-659603949eba to disappear
Apr  6 11:52:44.935: INFO: Pod test-pod-c7e331c4-a654-4016-85e3-659603949eba no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Apr  6 11:52:44.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8723" for this suite. 04/06/23 11:52:44.95
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":358,"skipped":6645,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:40.85
    Apr  6 11:52:40.850: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename svcaccounts 04/06/23 11:52:40.851
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:40.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:40.871
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  04/06/23 11:52:40.877
    Apr  6 11:52:40.897: INFO: Waiting up to 5m0s for pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba" in namespace "svcaccounts-8723" to be "Succeeded or Failed"
    Apr  6 11:52:40.902: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.271623ms
    Apr  6 11:52:42.909: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011252091s
    Apr  6 11:52:44.908: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010131348s
    STEP: Saw pod success 04/06/23 11:52:44.908
    Apr  6 11:52:44.908: INFO: Pod "test-pod-c7e331c4-a654-4016-85e3-659603949eba" satisfied condition "Succeeded or Failed"
    Apr  6 11:52:44.911: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod test-pod-c7e331c4-a654-4016-85e3-659603949eba container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:52:44.922
    Apr  6 11:52:44.932: INFO: Waiting for pod test-pod-c7e331c4-a654-4016-85e3-659603949eba to disappear
    Apr  6 11:52:44.935: INFO: Pod test-pod-c7e331c4-a654-4016-85e3-659603949eba no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Apr  6 11:52:44.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8723" for this suite. 04/06/23 11:52:44.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:52:44.956
Apr  6 11:52:44.956: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-preemption 04/06/23 11:52:44.958
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:44.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:44.978
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Apr  6 11:52:45.009: INFO: Waiting up to 1m0s for all nodes to be ready
Apr  6 11:53:45.164: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:53:45.17
Apr  6 11:53:45.170: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename sched-preemption-path 04/06/23 11:53:45.172
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:45.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:45.229
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Apr  6 11:53:45.281: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Apr  6 11:53:45.286: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Apr  6 11:53:45.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8902" for this suite. 04/06/23 11:53:45.331
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Apr  6 11:53:45.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9921" for this suite. 04/06/23 11:53:45.36
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":359,"skipped":6652,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.555 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:52:44.956
    Apr  6 11:52:44.956: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-preemption 04/06/23 11:52:44.958
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:52:44.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:52:44.978
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Apr  6 11:52:45.009: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr  6 11:53:45.164: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:53:45.17
    Apr  6 11:53:45.170: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename sched-preemption-path 04/06/23 11:53:45.172
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:45.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:45.229
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Apr  6 11:53:45.281: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Apr  6 11:53:45.286: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Apr  6 11:53:45.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-8902" for this suite. 04/06/23 11:53:45.331
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Apr  6 11:53:45.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9921" for this suite. 04/06/23 11:53:45.36
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:53:45.518
Apr  6 11:53:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename secrets 04/06/23 11:53:45.52
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:45.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:45.559
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-6785ffa1-deab-40c8-b1f1-a357caf57903 04/06/23 11:53:45.587
STEP: Creating a pod to test consume secrets 04/06/23 11:53:45.592
Apr  6 11:53:45.610: INFO: Waiting up to 5m0s for pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e" in namespace "secrets-5972" to be "Succeeded or Failed"
Apr  6 11:53:45.618: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894962ms
Apr  6 11:53:47.625: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014905753s
Apr  6 11:53:49.630: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019270145s
STEP: Saw pod success 04/06/23 11:53:49.631
Apr  6 11:53:49.631: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e" satisfied condition "Succeeded or Failed"
Apr  6 11:53:49.637: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e container secret-volume-test: <nil>
STEP: delete the pod 04/06/23 11:53:49.783
Apr  6 11:53:49.809: INFO: Waiting for pod pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e to disappear
Apr  6 11:53:49.814: INFO: Pod pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Apr  6 11:53:49.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5972" for this suite. 04/06/23 11:53:49.826
STEP: Destroying namespace "secret-namespace-3475" for this suite. 04/06/23 11:53:49.833
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":360,"skipped":6655,"failed":0}
------------------------------
â€¢ [4.325 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:53:45.518
    Apr  6 11:53:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename secrets 04/06/23 11:53:45.52
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:45.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:45.559
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-6785ffa1-deab-40c8-b1f1-a357caf57903 04/06/23 11:53:45.587
    STEP: Creating a pod to test consume secrets 04/06/23 11:53:45.592
    Apr  6 11:53:45.610: INFO: Waiting up to 5m0s for pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e" in namespace "secrets-5972" to be "Succeeded or Failed"
    Apr  6 11:53:45.618: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894962ms
    Apr  6 11:53:47.625: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014905753s
    Apr  6 11:53:49.630: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019270145s
    STEP: Saw pod success 04/06/23 11:53:49.631
    Apr  6 11:53:49.631: INFO: Pod "pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e" satisfied condition "Succeeded or Failed"
    Apr  6 11:53:49.637: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-8w22d pod pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e container secret-volume-test: <nil>
    STEP: delete the pod 04/06/23 11:53:49.783
    Apr  6 11:53:49.809: INFO: Waiting for pod pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e to disappear
    Apr  6 11:53:49.814: INFO: Pod pod-secrets-29aae4d1-e017-4eaa-b902-8f6ef6bcbd5e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Apr  6 11:53:49.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5972" for this suite. 04/06/23 11:53:49.826
    STEP: Destroying namespace "secret-namespace-3475" for this suite. 04/06/23 11:53:49.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:53:49.843
Apr  6 11:53:49.843: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename containers 04/06/23 11:53:49.845
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:49.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:49.873
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 04/06/23 11:53:49.881
Apr  6 11:53:49.926: INFO: Waiting up to 5m0s for pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b" in namespace "containers-5002" to be "Succeeded or Failed"
Apr  6 11:53:49.937: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.440869ms
Apr  6 11:53:51.947: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021066298s
Apr  6 11:53:53.944: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017950033s
STEP: Saw pod success 04/06/23 11:53:53.944
Apr  6 11:53:53.944: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b" satisfied condition "Succeeded or Failed"
Apr  6 11:53:53.948: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-containers-cabde13d-c788-4c2d-8a40-db09d526604b container agnhost-container: <nil>
STEP: delete the pod 04/06/23 11:53:53.961
Apr  6 11:53:53.970: INFO: Waiting for pod client-containers-cabde13d-c788-4c2d-8a40-db09d526604b to disappear
Apr  6 11:53:53.974: INFO: Pod client-containers-cabde13d-c788-4c2d-8a40-db09d526604b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Apr  6 11:53:53.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5002" for this suite. 04/06/23 11:53:53.981
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":361,"skipped":6668,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:53:49.843
    Apr  6 11:53:49.843: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename containers 04/06/23 11:53:49.845
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:49.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:49.873
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 04/06/23 11:53:49.881
    Apr  6 11:53:49.926: INFO: Waiting up to 5m0s for pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b" in namespace "containers-5002" to be "Succeeded or Failed"
    Apr  6 11:53:49.937: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.440869ms
    Apr  6 11:53:51.947: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021066298s
    Apr  6 11:53:53.944: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017950033s
    STEP: Saw pod success 04/06/23 11:53:53.944
    Apr  6 11:53:53.944: INFO: Pod "client-containers-cabde13d-c788-4c2d-8a40-db09d526604b" satisfied condition "Succeeded or Failed"
    Apr  6 11:53:53.948: INFO: Trying to get logs from node shoot--4f62e10b2e--cl-0czl78yf-pool-5srtzxdh8p-z1-86479-sjrqk pod client-containers-cabde13d-c788-4c2d-8a40-db09d526604b container agnhost-container: <nil>
    STEP: delete the pod 04/06/23 11:53:53.961
    Apr  6 11:53:53.970: INFO: Waiting for pod client-containers-cabde13d-c788-4c2d-8a40-db09d526604b to disappear
    Apr  6 11:53:53.974: INFO: Pod client-containers-cabde13d-c788-4c2d-8a40-db09d526604b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Apr  6 11:53:53.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5002" for this suite. 04/06/23 11:53:53.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 04/06/23 11:53:53.991
Apr  6 11:53:53.991: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
STEP: Building a namespace api object, basename statefulset 04/06/23 11:53:53.992
STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:54.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:54.021
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7690 04/06/23 11:53:54.028
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 04/06/23 11:53:54.034
STEP: Creating stateful set ss in namespace statefulset-7690 04/06/23 11:53:54.039
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7690 04/06/23 11:53:54.046
Apr  6 11:53:54.052: INFO: Found 0 stateful pods, waiting for 1
Apr  6 11:54:04.063: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/06/23 11:54:04.064
Apr  6 11:54:04.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:54:04.749: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:54:04.749: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:54:04.749: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:54:04.754: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  6 11:54:14.762: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:54:14.762: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:54:14.791: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999594s
Apr  6 11:54:15.807: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988222198s
Apr  6 11:54:16.819: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97739889s
Apr  6 11:54:17.827: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.967292909s
Apr  6 11:54:18.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.95944685s
Apr  6 11:54:19.847: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.946453874s
Apr  6 11:54:20.854: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.939667465s
Apr  6 11:54:21.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926432863s
Apr  6 11:54:22.876: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.917827634s
Apr  6 11:54:23.888: INFO: Verifying statefulset ss doesn't scale past 1 for another 910.067477ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7690 04/06/23 11:54:24.896
Apr  6 11:54:24.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:54:25.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 11:54:25.866: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:54:25.866: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 11:54:25.872: INFO: Found 1 stateful pods, waiting for 3
Apr  6 11:54:35.883: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:54:35.883: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 11:54:35.883: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 04/06/23 11:54:35.883
STEP: Scale down will halt with unhealthy stateful pod 04/06/23 11:54:35.883
Apr  6 11:54:35.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:54:36.497: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:54:36.497: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:54:36.497: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:54:36.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:54:37.139: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:54:37.140: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:54:37.140: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:54:37.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 11:54:37.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 11:54:37.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 11:54:37.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 11:54:37.626: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 11:54:37.633: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr  6 11:54:47.650: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:54:47.650: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:54:47.650: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 11:54:47.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999577s
Apr  6 11:54:48.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986054395s
Apr  6 11:54:49.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.966605967s
Apr  6 11:54:50.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953782541s
Apr  6 11:54:51.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945568211s
Apr  6 11:54:52.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.936816894s
Apr  6 11:54:53.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.930072025s
Apr  6 11:54:54.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.923486677s
Apr  6 11:54:55.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916498271s
Apr  6 11:54:56.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.372387ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7690 04/06/23 11:54:57.784
Apr  6 11:54:57.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:54:58.456: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 11:54:58.457: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:54:58.457: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 11:54:58.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:54:59.261: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 11:54:59.261: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 11:54:59.261: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 11:54:59.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:55:00.051: INFO: rc: 1
Apr  6 11:55:00.052: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "0107eec55c88e006aa5873ffbda83e41e4a1c7017042b4857bc09ab31920998e": OCI runtime exec failed: exec failed: unable to start container process: error executing setns process: exit status 1: unknown

error:
exit status 1
Apr  6 11:55:10.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:55:10.255: INFO: rc: 1
Apr  6 11:55:10.255: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:55:20.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:55:20.403: INFO: rc: 1
Apr  6 11:55:20.403: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:55:30.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:55:30.591: INFO: rc: 1
Apr  6 11:55:30.591: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:55:40.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:55:40.724: INFO: rc: 1
Apr  6 11:55:40.724: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:55:50.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:55:50.829: INFO: rc: 1
Apr  6 11:55:50.829: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:56:00.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:56:01.040: INFO: rc: 1
Apr  6 11:56:01.040: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:56:11.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:56:11.193: INFO: rc: 1
Apr  6 11:56:11.193: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:56:21.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:56:21.360: INFO: rc: 1
Apr  6 11:56:21.360: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:56:31.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:56:31.527: INFO: rc: 1
Apr  6 11:56:31.527: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:56:41.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:56:41.677: INFO: rc: 1
Apr  6 11:56:41.677: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:56:51.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:56:51.857: INFO: rc: 1
Apr  6 11:56:51.857: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:57:01.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:57:02.034: INFO: rc: 1
Apr  6 11:57:02.034: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:57:12.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:57:12.195: INFO: rc: 1
Apr  6 11:57:12.195: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:57:22.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:57:22.337: INFO: rc: 1
Apr  6 11:57:22.338: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:57:32.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:57:32.498: INFO: rc: 1
Apr  6 11:57:32.498: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:57:42.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:57:42.646: INFO: rc: 1
Apr  6 11:57:42.646: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:57:52.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:57:52.903: INFO: rc: 1
Apr  6 11:57:52.903: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:58:02.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:58:03.124: INFO: rc: 1
Apr  6 11:58:03.124: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:58:13.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:58:13.302: INFO: rc: 1
Apr  6 11:58:13.303: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:58:23.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:58:23.558: INFO: rc: 1
Apr  6 11:58:23.558: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:58:33.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:58:33.828: INFO: rc: 1
Apr  6 11:58:33.829: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:58:43.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:58:44.036: INFO: rc: 1
Apr  6 11:58:44.036: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:58:54.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:58:54.182: INFO: rc: 1
Apr  6 11:58:54.182: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:59:04.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:59:04.400: INFO: rc: 1
Apr  6 11:59:04.400: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:59:14.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:59:14.562: INFO: rc: 1
Apr  6 11:59:14.562: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:59:24.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:59:24.704: INFO: rc: 1
Apr  6 11:59:24.704: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:59:34.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:59:34.812: INFO: rc: 1
Apr  6 11:59:34.814: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:59:44.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:59:44.965: INFO: rc: 1
Apr  6 11:59:44.965: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 11:59:54.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 11:59:55.167: INFO: rc: 1
Apr  6 11:59:55.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr  6 12:00:05.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 12:00:05.523: INFO: rc: 1
Apr  6 12:00:05.523: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Apr  6 12:00:05.523: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 04/06/23 12:00:05.553
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Apr  6 12:00:05.553: INFO: Deleting all statefulset in ns statefulset-7690
Apr  6 12:00:05.565: INFO: Scaling statefulset ss to 0
Apr  6 12:00:05.585: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 12:00:05.599: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Apr  6 12:00:05.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7690" for this suite. 04/06/23 12:00:05.651
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":362,"skipped":6677,"failed":0}
------------------------------
â€¢ [SLOW TEST] [371.670 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 04/06/23 11:53:53.991
    Apr  6 11:53:53.991: INFO: >>> kubeConfig: /tmp/kubeconfig-550776287
    STEP: Building a namespace api object, basename statefulset 04/06/23 11:53:53.992
    STEP: Waiting for a default service account to be provisioned in namespace 04/06/23 11:53:54.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/06/23 11:53:54.021
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7690 04/06/23 11:53:54.028
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 04/06/23 11:53:54.034
    STEP: Creating stateful set ss in namespace statefulset-7690 04/06/23 11:53:54.039
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7690 04/06/23 11:53:54.046
    Apr  6 11:53:54.052: INFO: Found 0 stateful pods, waiting for 1
    Apr  6 11:54:04.063: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/06/23 11:54:04.064
    Apr  6 11:54:04.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:54:04.749: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:54:04.749: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:54:04.749: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:54:04.754: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr  6 11:54:14.762: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:54:14.762: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:54:14.791: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999594s
    Apr  6 11:54:15.807: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988222198s
    Apr  6 11:54:16.819: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97739889s
    Apr  6 11:54:17.827: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.967292909s
    Apr  6 11:54:18.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.95944685s
    Apr  6 11:54:19.847: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.946453874s
    Apr  6 11:54:20.854: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.939667465s
    Apr  6 11:54:21.868: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926432863s
    Apr  6 11:54:22.876: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.917827634s
    Apr  6 11:54:23.888: INFO: Verifying statefulset ss doesn't scale past 1 for another 910.067477ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7690 04/06/23 11:54:24.896
    Apr  6 11:54:24.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:54:25.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr  6 11:54:25.866: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:54:25.866: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr  6 11:54:25.872: INFO: Found 1 stateful pods, waiting for 3
    Apr  6 11:54:35.883: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:54:35.883: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr  6 11:54:35.883: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 04/06/23 11:54:35.883
    STEP: Scale down will halt with unhealthy stateful pod 04/06/23 11:54:35.883
    Apr  6 11:54:35.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:54:36.497: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:54:36.497: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:54:36.497: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:54:36.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:54:37.139: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:54:37.140: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:54:37.140: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:54:37.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr  6 11:54:37.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr  6 11:54:37.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr  6 11:54:37.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr  6 11:54:37.626: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 11:54:37.633: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Apr  6 11:54:47.650: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:54:47.650: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:54:47.650: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr  6 11:54:47.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999577s
    Apr  6 11:54:48.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986054395s
    Apr  6 11:54:49.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.966605967s
    Apr  6 11:54:50.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953782541s
    Apr  6 11:54:51.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945568211s
    Apr  6 11:54:52.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.936816894s
    Apr  6 11:54:53.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.930072025s
    Apr  6 11:54:54.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.923486677s
    Apr  6 11:54:55.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916498271s
    Apr  6 11:54:56.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.372387ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7690 04/06/23 11:54:57.784
    Apr  6 11:54:57.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:54:58.456: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr  6 11:54:58.457: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:54:58.457: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr  6 11:54:58.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:54:59.261: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr  6 11:54:59.261: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr  6 11:54:59.261: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr  6 11:54:59.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:55:00.051: INFO: rc: 1
    Apr  6 11:55:00.052: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "0107eec55c88e006aa5873ffbda83e41e4a1c7017042b4857bc09ab31920998e": OCI runtime exec failed: exec failed: unable to start container process: error executing setns process: exit status 1: unknown

    error:
    exit status 1
    Apr  6 11:55:10.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:55:10.255: INFO: rc: 1
    Apr  6 11:55:10.255: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:55:20.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:55:20.403: INFO: rc: 1
    Apr  6 11:55:20.403: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:55:30.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:55:30.591: INFO: rc: 1
    Apr  6 11:55:30.591: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:55:40.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:55:40.724: INFO: rc: 1
    Apr  6 11:55:40.724: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:55:50.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:55:50.829: INFO: rc: 1
    Apr  6 11:55:50.829: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:56:00.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:56:01.040: INFO: rc: 1
    Apr  6 11:56:01.040: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:56:11.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:56:11.193: INFO: rc: 1
    Apr  6 11:56:11.193: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:56:21.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:56:21.360: INFO: rc: 1
    Apr  6 11:56:21.360: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:56:31.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:56:31.527: INFO: rc: 1
    Apr  6 11:56:31.527: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:56:41.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:56:41.677: INFO: rc: 1
    Apr  6 11:56:41.677: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:56:51.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:56:51.857: INFO: rc: 1
    Apr  6 11:56:51.857: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:57:01.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:57:02.034: INFO: rc: 1
    Apr  6 11:57:02.034: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:57:12.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:57:12.195: INFO: rc: 1
    Apr  6 11:57:12.195: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:57:22.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:57:22.337: INFO: rc: 1
    Apr  6 11:57:22.338: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:57:32.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:57:32.498: INFO: rc: 1
    Apr  6 11:57:32.498: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:57:42.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:57:42.646: INFO: rc: 1
    Apr  6 11:57:42.646: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:57:52.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:57:52.903: INFO: rc: 1
    Apr  6 11:57:52.903: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:58:02.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:58:03.124: INFO: rc: 1
    Apr  6 11:58:03.124: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:58:13.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:58:13.302: INFO: rc: 1
    Apr  6 11:58:13.303: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:58:23.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:58:23.558: INFO: rc: 1
    Apr  6 11:58:23.558: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:58:33.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:58:33.828: INFO: rc: 1
    Apr  6 11:58:33.829: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:58:43.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:58:44.036: INFO: rc: 1
    Apr  6 11:58:44.036: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:58:54.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:58:54.182: INFO: rc: 1
    Apr  6 11:58:54.182: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:59:04.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:59:04.400: INFO: rc: 1
    Apr  6 11:59:04.400: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:59:14.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:59:14.562: INFO: rc: 1
    Apr  6 11:59:14.562: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:59:24.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:59:24.704: INFO: rc: 1
    Apr  6 11:59:24.704: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:59:34.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:59:34.812: INFO: rc: 1
    Apr  6 11:59:34.814: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:59:44.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:59:44.965: INFO: rc: 1
    Apr  6 11:59:44.965: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 11:59:54.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 11:59:55.167: INFO: rc: 1
    Apr  6 11:59:55.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Apr  6 12:00:05.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-550776287 --namespace=statefulset-7690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr  6 12:00:05.523: INFO: rc: 1
    Apr  6 12:00:05.523: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Apr  6 12:00:05.523: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 04/06/23 12:00:05.553
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Apr  6 12:00:05.553: INFO: Deleting all statefulset in ns statefulset-7690
    Apr  6 12:00:05.565: INFO: Scaling statefulset ss to 0
    Apr  6 12:00:05.585: INFO: Waiting for statefulset status.replicas updated to 0
    Apr  6 12:00:05.599: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Apr  6 12:00:05.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7690" for this suite. 04/06/23 12:00:05.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Apr  6 12:00:05.664: INFO: Running AfterSuite actions on all nodes
Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Apr  6 12:00:05.664: INFO: Running AfterSuite actions on node 1
Apr  6 12:00:05.664: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Apr  6 12:00:05.664: INFO: Running AfterSuite actions on all nodes
    Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Apr  6 12:00:05.664: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Apr  6 12:00:05.664: INFO: Running AfterSuite actions on node 1
    Apr  6 12:00:05.664: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.207 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6132.781 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h42m13.53207678s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

