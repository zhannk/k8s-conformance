I0309 15:46:37.467926      22 e2e.go:116] Starting e2e run "3cb7456a-7771-43f0-b012-5689e2ab8cc3" on Ginkgo node 1
Mar  9 15:46:37.486: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1678376797 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Mar  9 15:46:37.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:46:37.622: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  9 15:46:37.639: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  9 15:46:37.654: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  9 15:46:37.654: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar  9 15:46:37.654: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  9 15:46:37.657: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  9 15:46:37.657: INFO: e2e test version: v1.25.7
Mar  9 15:46:37.659: INFO: kube-apiserver version: v1.25.7+1.el8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Mar  9 15:46:37.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:46:37.668: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.047 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  9 15:46:37.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:46:37.622: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Mar  9 15:46:37.639: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar  9 15:46:37.654: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar  9 15:46:37.654: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Mar  9 15:46:37.654: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar  9 15:46:37.657: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Mar  9 15:46:37.657: INFO: e2e test version: v1.25.7
    Mar  9 15:46:37.659: INFO: kube-apiserver version: v1.25.7+1.el8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  9 15:46:37.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:46:37.668: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:46:37.689
Mar  9 15:46:37.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 15:46:37.69
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:37.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:37.708
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 03/09/23 15:46:37.711
Mar  9 15:46:37.718: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6" in namespace "emptydir-4024" to be "running"
Mar  9 15:46:37.722: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133449ms
Mar  9 15:46:39.726: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00819146s
Mar  9 15:46:41.726: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6": Phase="Running", Reason="", readiness=false. Elapsed: 4.007903046s
Mar  9 15:46:41.726: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/09/23 15:46:41.726
Mar  9 15:46:41.726: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4024 PodName:pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:46:41.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:46:41.727: INFO: ExecWithOptions: Clientset creation
Mar  9 15:46:41.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4024/pods/pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar  9 15:46:41.798: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 15:46:41.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4024" for this suite. 03/09/23 15:46:41.802
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":1,"skipped":6,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:46:37.689
    Mar  9 15:46:37.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 15:46:37.69
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:37.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:37.708
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 03/09/23 15:46:37.711
    Mar  9 15:46:37.718: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6" in namespace "emptydir-4024" to be "running"
    Mar  9 15:46:37.722: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133449ms
    Mar  9 15:46:39.726: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00819146s
    Mar  9 15:46:41.726: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6": Phase="Running", Reason="", readiness=false. Elapsed: 4.007903046s
    Mar  9 15:46:41.726: INFO: Pod "pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/09/23 15:46:41.726
    Mar  9 15:46:41.726: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4024 PodName:pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:46:41.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:46:41.727: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:46:41.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4024/pods/pod-sharedvolume-0a52c43d-12c2-4f93-a9fc-e25ed8970ea6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar  9 15:46:41.798: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 15:46:41.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4024" for this suite. 03/09/23 15:46:41.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:46:41.809
Mar  9 15:46:41.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename tables 03/09/23 15:46:41.811
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:41.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:41.824
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Mar  9 15:46:41.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7652" for this suite. 03/09/23 15:46:41.837
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":2,"skipped":32,"failed":0}
------------------------------
â€¢ [0.032 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:46:41.809
    Mar  9 15:46:41.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename tables 03/09/23 15:46:41.811
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:41.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:41.824
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Mar  9 15:46:41.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7652" for this suite. 03/09/23 15:46:41.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:46:41.843
Mar  9 15:46:41.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename hostport 03/09/23 15:46:41.844
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:41.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:41.858
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/09/23 15:46:41.865
Mar  9 15:46:41.873: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4190" to be "running and ready"
Mar  9 15:46:41.875: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476509ms
Mar  9 15:46:41.876: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:46:43.880: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007059573s
Mar  9 15:46:43.880: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  9 15:46:43.880: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 100.100.231.104 on the node which pod1 resides and expect scheduled 03/09/23 15:46:43.88
Mar  9 15:46:43.885: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4190" to be "running and ready"
Mar  9 15:46:43.888: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346685ms
Mar  9 15:46:43.888: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:46:45.894: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008028458s
Mar  9 15:46:45.894: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  9 15:46:45.894: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 100.100.231.104 but use UDP protocol on the node which pod2 resides 03/09/23 15:46:45.894
Mar  9 15:46:45.899: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4190" to be "running and ready"
Mar  9 15:46:45.901: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355419ms
Mar  9 15:46:45.901: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:46:47.906: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007049108s
Mar  9 15:46:47.906: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar  9 15:46:47.906: INFO: Pod "pod3" satisfied condition "running and ready"
Mar  9 15:46:47.911: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4190" to be "running and ready"
Mar  9 15:46:47.913: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467119ms
Mar  9 15:46:47.913: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:46:49.917: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.006156549s
Mar  9 15:46:49.917: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar  9 15:46:49.917: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/09/23 15:46:49.919
Mar  9 15:46:49.920: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 100.100.231.104 http://127.0.0.1:54323/hostname] Namespace:hostport-4190 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:46:49.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:46:49.920: INFO: ExecWithOptions: Clientset creation
Mar  9 15:46:49.921: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4190/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+100.100.231.104+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.231.104, port: 54323 03/09/23 15:46:50.005
Mar  9 15:46:50.006: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://100.100.231.104:54323/hostname] Namespace:hostport-4190 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:46:50.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:46:50.006: INFO: ExecWithOptions: Clientset creation
Mar  9 15:46:50.006: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4190/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F100.100.231.104%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.231.104, port: 54323 UDP 03/09/23 15:46:50.085
Mar  9 15:46:50.085: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 100.100.231.104 54323] Namespace:hostport-4190 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:46:50.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:46:50.086: INFO: ExecWithOptions: Clientset creation
Mar  9 15:46:50.086: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4190/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+100.100.231.104+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Mar  9 15:46:55.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4190" for this suite. 03/09/23 15:46:55.163
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":3,"skipped":44,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.327 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:46:41.843
    Mar  9 15:46:41.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename hostport 03/09/23 15:46:41.844
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:41.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:41.858
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/09/23 15:46:41.865
    Mar  9 15:46:41.873: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4190" to be "running and ready"
    Mar  9 15:46:41.875: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.476509ms
    Mar  9 15:46:41.876: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:46:43.880: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007059573s
    Mar  9 15:46:43.880: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  9 15:46:43.880: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 100.100.231.104 on the node which pod1 resides and expect scheduled 03/09/23 15:46:43.88
    Mar  9 15:46:43.885: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4190" to be "running and ready"
    Mar  9 15:46:43.888: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346685ms
    Mar  9 15:46:43.888: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:46:45.894: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008028458s
    Mar  9 15:46:45.894: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  9 15:46:45.894: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 100.100.231.104 but use UDP protocol on the node which pod2 resides 03/09/23 15:46:45.894
    Mar  9 15:46:45.899: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4190" to be "running and ready"
    Mar  9 15:46:45.901: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355419ms
    Mar  9 15:46:45.901: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:46:47.906: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007049108s
    Mar  9 15:46:47.906: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar  9 15:46:47.906: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar  9 15:46:47.911: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4190" to be "running and ready"
    Mar  9 15:46:47.913: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467119ms
    Mar  9 15:46:47.913: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:46:49.917: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.006156549s
    Mar  9 15:46:49.917: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar  9 15:46:49.917: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/09/23 15:46:49.919
    Mar  9 15:46:49.920: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 100.100.231.104 http://127.0.0.1:54323/hostname] Namespace:hostport-4190 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:46:49.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:46:49.920: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:46:49.921: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4190/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+100.100.231.104+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.231.104, port: 54323 03/09/23 15:46:50.005
    Mar  9 15:46:50.006: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://100.100.231.104:54323/hostname] Namespace:hostport-4190 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:46:50.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:46:50.006: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:46:50.006: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4190/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F100.100.231.104%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.231.104, port: 54323 UDP 03/09/23 15:46:50.085
    Mar  9 15:46:50.085: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 100.100.231.104 54323] Namespace:hostport-4190 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:46:50.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:46:50.086: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:46:50.086: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4190/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+100.100.231.104+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Mar  9 15:46:55.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-4190" for this suite. 03/09/23 15:46:55.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:46:55.173
Mar  9 15:46:55.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 15:46:55.174
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:55.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:55.19
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-12545c20-848f-4a4c-a47e-69068b2e74ea 03/09/23 15:46:55.196
STEP: Creating configMap with name cm-test-opt-upd-90e0d2d6-ddae-4d87-bee6-45b94741c47d 03/09/23 15:46:55.2
STEP: Creating the pod 03/09/23 15:46:55.204
Mar  9 15:46:55.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8" in namespace "projected-8809" to be "running and ready"
Mar  9 15:46:55.215: INFO: Pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559409ms
Mar  9 15:46:55.215: INFO: The phase of Pod pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:46:57.218: INFO: Pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006012683s
Mar  9 15:46:57.218: INFO: The phase of Pod pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8 is Running (Ready = true)
Mar  9 15:46:57.218: INFO: Pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-12545c20-848f-4a4c-a47e-69068b2e74ea 03/09/23 15:46:57.245
STEP: Updating configmap cm-test-opt-upd-90e0d2d6-ddae-4d87-bee6-45b94741c47d 03/09/23 15:46:57.25
STEP: Creating configMap with name cm-test-opt-create-207fb51d-7768-4c0d-b9fc-4408c4dbd156 03/09/23 15:46:57.254
STEP: waiting to observe update in volume 03/09/23 15:46:57.258
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 15:46:59.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8809" for this suite. 03/09/23 15:46:59.285
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":4,"skipped":69,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:46:55.173
    Mar  9 15:46:55.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 15:46:55.174
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:55.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:55.19
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-12545c20-848f-4a4c-a47e-69068b2e74ea 03/09/23 15:46:55.196
    STEP: Creating configMap with name cm-test-opt-upd-90e0d2d6-ddae-4d87-bee6-45b94741c47d 03/09/23 15:46:55.2
    STEP: Creating the pod 03/09/23 15:46:55.204
    Mar  9 15:46:55.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8" in namespace "projected-8809" to be "running and ready"
    Mar  9 15:46:55.215: INFO: Pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559409ms
    Mar  9 15:46:55.215: INFO: The phase of Pod pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:46:57.218: INFO: Pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006012683s
    Mar  9 15:46:57.218: INFO: The phase of Pod pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8 is Running (Ready = true)
    Mar  9 15:46:57.218: INFO: Pod "pod-projected-configmaps-ca4c6d21-275c-4481-877b-474cc3d2bed8" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-12545c20-848f-4a4c-a47e-69068b2e74ea 03/09/23 15:46:57.245
    STEP: Updating configmap cm-test-opt-upd-90e0d2d6-ddae-4d87-bee6-45b94741c47d 03/09/23 15:46:57.25
    STEP: Creating configMap with name cm-test-opt-create-207fb51d-7768-4c0d-b9fc-4408c4dbd156 03/09/23 15:46:57.254
    STEP: waiting to observe update in volume 03/09/23 15:46:57.258
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 15:46:59.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8809" for this suite. 03/09/23 15:46:59.285
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:46:59.29
Mar  9 15:46:59.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 15:46:59.291
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:59.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:59.303
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/09/23 15:46:59.306
Mar  9 15:46:59.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/09/23 15:47:14.612
Mar  9 15:47:14.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:47:17.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 15:47:32.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7772" for this suite. 03/09/23 15:47:32.932
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":5,"skipped":72,"failed":0}
------------------------------
â€¢ [SLOW TEST] [33.647 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:46:59.29
    Mar  9 15:46:59.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 15:46:59.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:46:59.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:46:59.303
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/09/23 15:46:59.306
    Mar  9 15:46:59.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/09/23 15:47:14.612
    Mar  9 15:47:14.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:47:17.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 15:47:32.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7772" for this suite. 03/09/23 15:47:32.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:47:32.94
Mar  9 15:47:32.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 15:47:32.942
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:47:32.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:47:32.957
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2121 03/09/23 15:47:32.96
STEP: changing the ExternalName service to type=ClusterIP 03/09/23 15:47:32.966
STEP: creating replication controller externalname-service in namespace services-2121 03/09/23 15:47:32.983
I0309 15:47:32.988875      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2121, replica count: 2
I0309 15:47:36.039949      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 15:47:36.039: INFO: Creating new exec pod
Mar  9 15:47:36.045: INFO: Waiting up to 5m0s for pod "execpodlfb54" in namespace "services-2121" to be "running"
Mar  9 15:47:36.048: INFO: Pod "execpodlfb54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.624679ms
Mar  9 15:47:38.051: INFO: Pod "execpodlfb54": Phase="Running", Reason="", readiness=true. Elapsed: 2.005948198s
Mar  9 15:47:38.051: INFO: Pod "execpodlfb54" satisfied condition "running"
Mar  9 15:47:39.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-2121 exec execpodlfb54 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  9 15:47:39.218: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  9 15:47:39.218: INFO: stdout: "externalname-service-mqpl9"
Mar  9 15:47:39.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-2121 exec execpodlfb54 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.151.81 80'
Mar  9 15:47:39.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.151.81 80\nConnection to 10.105.151.81 80 port [tcp/http] succeeded!\n"
Mar  9 15:47:39.361: INFO: stdout: "externalname-service-mqpl9"
Mar  9 15:47:39.361: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 15:47:39.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2121" for this suite. 03/09/23 15:47:39.391
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":6,"skipped":129,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.457 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:47:32.94
    Mar  9 15:47:32.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 15:47:32.942
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:47:32.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:47:32.957
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2121 03/09/23 15:47:32.96
    STEP: changing the ExternalName service to type=ClusterIP 03/09/23 15:47:32.966
    STEP: creating replication controller externalname-service in namespace services-2121 03/09/23 15:47:32.983
    I0309 15:47:32.988875      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2121, replica count: 2
    I0309 15:47:36.039949      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 15:47:36.039: INFO: Creating new exec pod
    Mar  9 15:47:36.045: INFO: Waiting up to 5m0s for pod "execpodlfb54" in namespace "services-2121" to be "running"
    Mar  9 15:47:36.048: INFO: Pod "execpodlfb54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.624679ms
    Mar  9 15:47:38.051: INFO: Pod "execpodlfb54": Phase="Running", Reason="", readiness=true. Elapsed: 2.005948198s
    Mar  9 15:47:38.051: INFO: Pod "execpodlfb54" satisfied condition "running"
    Mar  9 15:47:39.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-2121 exec execpodlfb54 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  9 15:47:39.218: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  9 15:47:39.218: INFO: stdout: "externalname-service-mqpl9"
    Mar  9 15:47:39.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-2121 exec execpodlfb54 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.151.81 80'
    Mar  9 15:47:39.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.151.81 80\nConnection to 10.105.151.81 80 port [tcp/http] succeeded!\n"
    Mar  9 15:47:39.361: INFO: stdout: "externalname-service-mqpl9"
    Mar  9 15:47:39.361: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 15:47:39.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2121" for this suite. 03/09/23 15:47:39.391
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:47:39.4
Mar  9 15:47:39.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 15:47:39.402
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:47:39.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:47:39.418
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Mar  9 15:47:39.429: INFO: Waiting up to 5m0s for pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4" in namespace "container-probe-6572" to be "running and ready"
Mar  9 15:47:39.433: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765841ms
Mar  9 15:47:39.433: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:47:41.438: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008798076s
Mar  9 15:47:41.438: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:43.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 4.008013727s
Mar  9 15:47:43.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:45.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 6.008278667s
Mar  9 15:47:45.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:47.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 8.007632688s
Mar  9 15:47:47.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:49.436: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 10.007538142s
Mar  9 15:47:49.436: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:51.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 12.007789041s
Mar  9 15:47:51.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:53.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 14.00764496s
Mar  9 15:47:53.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:55.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 16.007895586s
Mar  9 15:47:55.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:57.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 18.007753163s
Mar  9 15:47:57.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:47:59.436: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 20.007588194s
Mar  9 15:47:59.436: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
Mar  9 15:48:01.438: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=true. Elapsed: 22.008749979s
Mar  9 15:48:01.438: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = true)
Mar  9 15:48:01.438: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4" satisfied condition "running and ready"
Mar  9 15:48:01.440: INFO: Container started at 2023-03-09 15:47:40 +0000 UTC, pod became ready at 2023-03-09 15:47:59 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 15:48:01.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6572" for this suite. 03/09/23 15:48:01.444
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":7,"skipped":184,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.049 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:47:39.4
    Mar  9 15:47:39.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 15:47:39.402
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:47:39.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:47:39.418
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Mar  9 15:47:39.429: INFO: Waiting up to 5m0s for pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4" in namespace "container-probe-6572" to be "running and ready"
    Mar  9 15:47:39.433: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765841ms
    Mar  9 15:47:39.433: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:47:41.438: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008798076s
    Mar  9 15:47:41.438: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:43.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 4.008013727s
    Mar  9 15:47:43.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:45.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 6.008278667s
    Mar  9 15:47:45.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:47.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 8.007632688s
    Mar  9 15:47:47.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:49.436: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 10.007538142s
    Mar  9 15:47:49.436: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:51.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 12.007789041s
    Mar  9 15:47:51.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:53.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 14.00764496s
    Mar  9 15:47:53.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:55.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 16.007895586s
    Mar  9 15:47:55.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:57.437: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 18.007753163s
    Mar  9 15:47:57.437: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:47:59.436: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=false. Elapsed: 20.007588194s
    Mar  9 15:47:59.436: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = false)
    Mar  9 15:48:01.438: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4": Phase="Running", Reason="", readiness=true. Elapsed: 22.008749979s
    Mar  9 15:48:01.438: INFO: The phase of Pod test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4 is Running (Ready = true)
    Mar  9 15:48:01.438: INFO: Pod "test-webserver-2c6d5cfb-16f5-463f-9768-381c634d87d4" satisfied condition "running and ready"
    Mar  9 15:48:01.440: INFO: Container started at 2023-03-09 15:47:40 +0000 UTC, pod became ready at 2023-03-09 15:47:59 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 15:48:01.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6572" for this suite. 03/09/23 15:48:01.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:48:01.452
Mar  9 15:48:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 15:48:01.454
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:01.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:01.467
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 15:48:01.482
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 15:48:01.726
STEP: Deploying the webhook pod 03/09/23 15:48:01.733
STEP: Wait for the deployment to be ready 03/09/23 15:48:01.743
Mar  9 15:48:01.749: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 15:48:03.757
STEP: Verifying the service has paired with the endpoint 03/09/23 15:48:03.77
Mar  9 15:48:04.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/09/23 15:48:04.773
STEP: create a configmap that should be updated by the webhook 03/09/23 15:48:04.79
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 15:48:04.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4630" for this suite. 03/09/23 15:48:04.811
STEP: Destroying namespace "webhook-4630-markers" for this suite. 03/09/23 15:48:04.815
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":8,"skipped":216,"failed":0}
------------------------------
â€¢ [3.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:48:01.452
    Mar  9 15:48:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 15:48:01.454
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:01.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:01.467
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 15:48:01.482
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 15:48:01.726
    STEP: Deploying the webhook pod 03/09/23 15:48:01.733
    STEP: Wait for the deployment to be ready 03/09/23 15:48:01.743
    Mar  9 15:48:01.749: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 15:48:03.757
    STEP: Verifying the service has paired with the endpoint 03/09/23 15:48:03.77
    Mar  9 15:48:04.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/09/23 15:48:04.773
    STEP: create a configmap that should be updated by the webhook 03/09/23 15:48:04.79
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 15:48:04.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4630" for this suite. 03/09/23 15:48:04.811
    STEP: Destroying namespace "webhook-4630-markers" for this suite. 03/09/23 15:48:04.815
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:48:04.861
Mar  9 15:48:04.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 15:48:04.862
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:04.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:04.886
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/09/23 15:48:04.889
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/09/23 15:48:04.889
STEP: creating a pod to probe DNS 03/09/23 15:48:04.889
STEP: submitting the pod to kubernetes 03/09/23 15:48:04.889
Mar  9 15:48:04.898: INFO: Waiting up to 15m0s for pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83" in namespace "dns-6622" to be "running"
Mar  9 15:48:04.901: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081173ms
Mar  9 15:48:06.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007401572s
Mar  9 15:48:08.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007315454s
Mar  9 15:48:10.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00741187s
Mar  9 15:48:12.905: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006993489s
Mar  9 15:48:14.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Running", Reason="", readiness=true. Elapsed: 10.008004822s
Mar  9 15:48:14.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83" satisfied condition "running"
STEP: retrieving the pod 03/09/23 15:48:14.906
STEP: looking for the results for each expected name from probers 03/09/23 15:48:14.909
Mar  9 15:48:14.922: INFO: DNS probes using dns-6622/dns-test-7c04d229-be69-4f51-a1c6-298373be4b83 succeeded

STEP: deleting the pod 03/09/23 15:48:14.922
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 15:48:14.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6622" for this suite. 03/09/23 15:48:14.938
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":9,"skipped":217,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.088 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:48:04.861
    Mar  9 15:48:04.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 15:48:04.862
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:04.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:04.886
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/09/23 15:48:04.889
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/09/23 15:48:04.889
    STEP: creating a pod to probe DNS 03/09/23 15:48:04.889
    STEP: submitting the pod to kubernetes 03/09/23 15:48:04.889
    Mar  9 15:48:04.898: INFO: Waiting up to 15m0s for pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83" in namespace "dns-6622" to be "running"
    Mar  9 15:48:04.901: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081173ms
    Mar  9 15:48:06.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007401572s
    Mar  9 15:48:08.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007315454s
    Mar  9 15:48:10.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00741187s
    Mar  9 15:48:12.905: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006993489s
    Mar  9 15:48:14.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83": Phase="Running", Reason="", readiness=true. Elapsed: 10.008004822s
    Mar  9 15:48:14.906: INFO: Pod "dns-test-7c04d229-be69-4f51-a1c6-298373be4b83" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 15:48:14.906
    STEP: looking for the results for each expected name from probers 03/09/23 15:48:14.909
    Mar  9 15:48:14.922: INFO: DNS probes using dns-6622/dns-test-7c04d229-be69-4f51-a1c6-298373be4b83 succeeded

    STEP: deleting the pod 03/09/23 15:48:14.922
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 15:48:14.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6622" for this suite. 03/09/23 15:48:14.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:48:14.95
Mar  9 15:48:14.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename gc 03/09/23 15:48:14.952
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:14.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:14.967
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/09/23 15:48:14.973
STEP: create the rc2 03/09/23 15:48:14.977
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/09/23 15:48:19.991
STEP: delete the rc simpletest-rc-to-be-deleted 03/09/23 15:48:20.416
STEP: wait for the rc to be deleted 03/09/23 15:48:20.421
Mar  9 15:48:25.437: INFO: 68 pods remaining
Mar  9 15:48:25.437: INFO: 68 pods has nil DeletionTimestamp
Mar  9 15:48:25.437: INFO: 
STEP: Gathering metrics 03/09/23 15:48:30.433
Mar  9 15:48:30.452: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
Mar  9 15:48:30.455: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.583689ms
Mar  9 15:48:30.455: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
Mar  9 15:48:30.455: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
Mar  9 15:48:30.551: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  9 15:48:30.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dlgb" in namespace "gc-8725"
Mar  9 15:48:30.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rrtv" in namespace "gc-8725"
Mar  9 15:48:30.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rm75" in namespace "gc-8725"
Mar  9 15:48:30.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-4x4s4" in namespace "gc-8725"
Mar  9 15:48:30.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s2d7" in namespace "gc-8725"
Mar  9 15:48:30.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-6knkl" in namespace "gc-8725"
Mar  9 15:48:30.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n5nd" in namespace "gc-8725"
Mar  9 15:48:30.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ttp4" in namespace "gc-8725"
Mar  9 15:48:30.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-72fs2" in namespace "gc-8725"
Mar  9 15:48:30.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-75mcm" in namespace "gc-8725"
Mar  9 15:48:30.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v5l8" in namespace "gc-8725"
Mar  9 15:48:30.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vdv4" in namespace "gc-8725"
Mar  9 15:48:30.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jhtr" in namespace "gc-8725"
Mar  9 15:48:30.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ktdf" in namespace "gc-8725"
Mar  9 15:48:30.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kxgt" in namespace "gc-8725"
Mar  9 15:48:30.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wbrr" in namespace "gc-8725"
Mar  9 15:48:30.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-95vkg" in namespace "gc-8725"
Mar  9 15:48:30.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-989f4" in namespace "gc-8725"
Mar  9 15:48:30.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-98htj" in namespace "gc-8725"
Mar  9 15:48:30.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9pdd" in namespace "gc-8725"
Mar  9 15:48:30.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-bg8vc" in namespace "gc-8725"
Mar  9 15:48:30.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl7ff" in namespace "gc-8725"
Mar  9 15:48:30.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq78t" in namespace "gc-8725"
Mar  9 15:48:30.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-brf92" in namespace "gc-8725"
Mar  9 15:48:30.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2jq2" in namespace "gc-8725"
Mar  9 15:48:30.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-cff59" in namespace "gc-8725"
Mar  9 15:48:30.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-cv6mw" in namespace "gc-8725"
Mar  9 15:48:30.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcd2x" in namespace "gc-8725"
Mar  9 15:48:30.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-drxvt" in namespace "gc-8725"
Mar  9 15:48:30.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8wqb" in namespace "gc-8725"
Mar  9 15:48:30.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdfjq" in namespace "gc-8725"
Mar  9 15:48:30.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftb48" in namespace "gc-8725"
Mar  9 15:48:30.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvzjz" in namespace "gc-8725"
Mar  9 15:48:30.842: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzpjj" in namespace "gc-8725"
Mar  9 15:48:30.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-g44qn" in namespace "gc-8725"
Mar  9 15:48:30.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6hgt" in namespace "gc-8725"
Mar  9 15:48:30.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-g857r" in namespace "gc-8725"
Mar  9 15:48:30.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-gn7w7" in namespace "gc-8725"
Mar  9 15:48:30.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtd57" in namespace "gc-8725"
Mar  9 15:48:30.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxg2b" in namespace "gc-8725"
Mar  9 15:48:30.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9xpk" in namespace "gc-8725"
Mar  9 15:48:30.934: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrgcd" in namespace "gc-8725"
Mar  9 15:48:30.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvsgg" in namespace "gc-8725"
Mar  9 15:48:30.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk2d2" in namespace "gc-8725"
Mar  9 15:48:30.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbfbv" in namespace "gc-8725"
Mar  9 15:48:30.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-kqbdc" in namespace "gc-8725"
Mar  9 15:48:30.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-ktbpf" in namespace "gc-8725"
Mar  9 15:48:30.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-lkcjx" in namespace "gc-8725"
Mar  9 15:48:31.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-m6647" in namespace "gc-8725"
Mar  9 15:48:31.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-mbw99" in namespace "gc-8725"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  9 15:48:31.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8725" for this suite. 03/09/23 15:48:31.034
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":10,"skipped":233,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.090 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:48:14.95
    Mar  9 15:48:14.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename gc 03/09/23 15:48:14.952
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:14.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:14.967
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/09/23 15:48:14.973
    STEP: create the rc2 03/09/23 15:48:14.977
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/09/23 15:48:19.991
    STEP: delete the rc simpletest-rc-to-be-deleted 03/09/23 15:48:20.416
    STEP: wait for the rc to be deleted 03/09/23 15:48:20.421
    Mar  9 15:48:25.437: INFO: 68 pods remaining
    Mar  9 15:48:25.437: INFO: 68 pods has nil DeletionTimestamp
    Mar  9 15:48:25.437: INFO: 
    STEP: Gathering metrics 03/09/23 15:48:30.433
    Mar  9 15:48:30.452: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
    Mar  9 15:48:30.455: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.583689ms
    Mar  9 15:48:30.455: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
    Mar  9 15:48:30.455: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
    Mar  9 15:48:30.551: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  9 15:48:30.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dlgb" in namespace "gc-8725"
    Mar  9 15:48:30.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rrtv" in namespace "gc-8725"
    Mar  9 15:48:30.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rm75" in namespace "gc-8725"
    Mar  9 15:48:30.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-4x4s4" in namespace "gc-8725"
    Mar  9 15:48:30.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s2d7" in namespace "gc-8725"
    Mar  9 15:48:30.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-6knkl" in namespace "gc-8725"
    Mar  9 15:48:30.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n5nd" in namespace "gc-8725"
    Mar  9 15:48:30.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ttp4" in namespace "gc-8725"
    Mar  9 15:48:30.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-72fs2" in namespace "gc-8725"
    Mar  9 15:48:30.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-75mcm" in namespace "gc-8725"
    Mar  9 15:48:30.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v5l8" in namespace "gc-8725"
    Mar  9 15:48:30.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vdv4" in namespace "gc-8725"
    Mar  9 15:48:30.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jhtr" in namespace "gc-8725"
    Mar  9 15:48:30.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ktdf" in namespace "gc-8725"
    Mar  9 15:48:30.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kxgt" in namespace "gc-8725"
    Mar  9 15:48:30.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wbrr" in namespace "gc-8725"
    Mar  9 15:48:30.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-95vkg" in namespace "gc-8725"
    Mar  9 15:48:30.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-989f4" in namespace "gc-8725"
    Mar  9 15:48:30.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-98htj" in namespace "gc-8725"
    Mar  9 15:48:30.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9pdd" in namespace "gc-8725"
    Mar  9 15:48:30.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-bg8vc" in namespace "gc-8725"
    Mar  9 15:48:30.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl7ff" in namespace "gc-8725"
    Mar  9 15:48:30.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq78t" in namespace "gc-8725"
    Mar  9 15:48:30.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-brf92" in namespace "gc-8725"
    Mar  9 15:48:30.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2jq2" in namespace "gc-8725"
    Mar  9 15:48:30.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-cff59" in namespace "gc-8725"
    Mar  9 15:48:30.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-cv6mw" in namespace "gc-8725"
    Mar  9 15:48:30.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcd2x" in namespace "gc-8725"
    Mar  9 15:48:30.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-drxvt" in namespace "gc-8725"
    Mar  9 15:48:30.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8wqb" in namespace "gc-8725"
    Mar  9 15:48:30.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdfjq" in namespace "gc-8725"
    Mar  9 15:48:30.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftb48" in namespace "gc-8725"
    Mar  9 15:48:30.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvzjz" in namespace "gc-8725"
    Mar  9 15:48:30.842: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzpjj" in namespace "gc-8725"
    Mar  9 15:48:30.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-g44qn" in namespace "gc-8725"
    Mar  9 15:48:30.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6hgt" in namespace "gc-8725"
    Mar  9 15:48:30.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-g857r" in namespace "gc-8725"
    Mar  9 15:48:30.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-gn7w7" in namespace "gc-8725"
    Mar  9 15:48:30.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtd57" in namespace "gc-8725"
    Mar  9 15:48:30.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxg2b" in namespace "gc-8725"
    Mar  9 15:48:30.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9xpk" in namespace "gc-8725"
    Mar  9 15:48:30.934: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrgcd" in namespace "gc-8725"
    Mar  9 15:48:30.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvsgg" in namespace "gc-8725"
    Mar  9 15:48:30.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk2d2" in namespace "gc-8725"
    Mar  9 15:48:30.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbfbv" in namespace "gc-8725"
    Mar  9 15:48:30.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-kqbdc" in namespace "gc-8725"
    Mar  9 15:48:30.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-ktbpf" in namespace "gc-8725"
    Mar  9 15:48:30.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-lkcjx" in namespace "gc-8725"
    Mar  9 15:48:31.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-m6647" in namespace "gc-8725"
    Mar  9 15:48:31.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-mbw99" in namespace "gc-8725"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  9 15:48:31.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8725" for this suite. 03/09/23 15:48:31.034
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:48:31.043
Mar  9 15:48:31.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 15:48:31.053
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:31.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:31.067
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 03/09/23 15:48:31.072
Mar  9 15:48:31.073: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-536 proxy --unix-socket=/tmp/kubectl-proxy-unix1025670028/test'
STEP: retrieving proxy /api/ output 03/09/23 15:48:31.249
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 15:48:31.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-536" for this suite. 03/09/23 15:48:31.254
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":11,"skipped":237,"failed":0}
------------------------------
â€¢ [0.216 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:48:31.043
    Mar  9 15:48:31.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 15:48:31.053
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:31.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:31.067
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 03/09/23 15:48:31.072
    Mar  9 15:48:31.073: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-536 proxy --unix-socket=/tmp/kubectl-proxy-unix1025670028/test'
    STEP: retrieving proxy /api/ output 03/09/23 15:48:31.249
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 15:48:31.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-536" for this suite. 03/09/23 15:48:31.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:48:31.265
Mar  9 15:48:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename job 03/09/23 15:48:31.267
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:31.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:31.28
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 03/09/23 15:48:31.283
STEP: Ensuring active pods == parallelism 03/09/23 15:48:31.288
STEP: delete a job 03/09/23 15:48:43.292
STEP: deleting Job.batch foo in namespace job-1555, will wait for the garbage collector to delete the pods 03/09/23 15:48:43.292
Mar  9 15:48:43.352: INFO: Deleting Job.batch foo took: 6.844773ms
Mar  9 15:48:43.453: INFO: Terminating Job.batch foo pods took: 100.476166ms
STEP: Ensuring job was deleted 03/09/23 15:49:14.253
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  9 15:49:14.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1555" for this suite. 03/09/23 15:49:14.26
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":12,"skipped":311,"failed":0}
------------------------------
â€¢ [SLOW TEST] [43.000 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:48:31.265
    Mar  9 15:48:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename job 03/09/23 15:48:31.267
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:48:31.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:48:31.28
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 03/09/23 15:48:31.283
    STEP: Ensuring active pods == parallelism 03/09/23 15:48:31.288
    STEP: delete a job 03/09/23 15:48:43.292
    STEP: deleting Job.batch foo in namespace job-1555, will wait for the garbage collector to delete the pods 03/09/23 15:48:43.292
    Mar  9 15:48:43.352: INFO: Deleting Job.batch foo took: 6.844773ms
    Mar  9 15:48:43.453: INFO: Terminating Job.batch foo pods took: 100.476166ms
    STEP: Ensuring job was deleted 03/09/23 15:49:14.253
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  9 15:49:14.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1555" for this suite. 03/09/23 15:49:14.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:14.267
Mar  9 15:49:14.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 15:49:14.269
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:14.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:14.285
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 15:49:14.288
Mar  9 15:49:14.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2592 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  9 15:49:14.369: INFO: stderr: ""
Mar  9 15:49:14.369: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/09/23 15:49:14.37
Mar  9 15:49:14.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2592 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar  9 15:49:15.343: INFO: stderr: ""
Mar  9 15:49:15.343: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 15:49:15.343
Mar  9 15:49:15.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2592 delete pods e2e-test-httpd-pod'
Mar  9 15:49:17.931: INFO: stderr: ""
Mar  9 15:49:17.931: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 15:49:17.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2592" for this suite. 03/09/23 15:49:17.935
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":13,"skipped":332,"failed":0}
------------------------------
â€¢ [3.686 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:14.267
    Mar  9 15:49:14.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 15:49:14.269
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:14.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:14.285
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 15:49:14.288
    Mar  9 15:49:14.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2592 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  9 15:49:14.369: INFO: stderr: ""
    Mar  9 15:49:14.369: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/09/23 15:49:14.37
    Mar  9 15:49:14.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2592 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Mar  9 15:49:15.343: INFO: stderr: ""
    Mar  9 15:49:15.343: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 15:49:15.343
    Mar  9 15:49:15.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2592 delete pods e2e-test-httpd-pod'
    Mar  9 15:49:17.931: INFO: stderr: ""
    Mar  9 15:49:17.931: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 15:49:17.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2592" for this suite. 03/09/23 15:49:17.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:17.954
Mar  9 15:49:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename podtemplate 03/09/23 15:49:17.955
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:17.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:17.97
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/09/23 15:49:17.973
Mar  9 15:49:17.978: INFO: created test-podtemplate-1
Mar  9 15:49:17.982: INFO: created test-podtemplate-2
Mar  9 15:49:17.987: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/09/23 15:49:17.987
STEP: delete collection of pod templates 03/09/23 15:49:17.99
Mar  9 15:49:17.990: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/09/23 15:49:18.003
Mar  9 15:49:18.003: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  9 15:49:18.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8313" for this suite. 03/09/23 15:49:18.009
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":14,"skipped":342,"failed":0}
------------------------------
â€¢ [0.060 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:17.954
    Mar  9 15:49:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename podtemplate 03/09/23 15:49:17.955
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:17.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:17.97
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/09/23 15:49:17.973
    Mar  9 15:49:17.978: INFO: created test-podtemplate-1
    Mar  9 15:49:17.982: INFO: created test-podtemplate-2
    Mar  9 15:49:17.987: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/09/23 15:49:17.987
    STEP: delete collection of pod templates 03/09/23 15:49:17.99
    Mar  9 15:49:17.990: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/09/23 15:49:18.003
    Mar  9 15:49:18.003: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  9 15:49:18.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8313" for this suite. 03/09/23 15:49:18.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:18.014
Mar  9 15:49:18.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename init-container 03/09/23 15:49:18.016
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:18.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:18.031
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 03/09/23 15:49:18.034
Mar  9 15:49:18.034: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  9 15:49:22.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1374" for this suite. 03/09/23 15:49:22.223
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":15,"skipped":354,"failed":0}
------------------------------
â€¢ [4.213 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:18.014
    Mar  9 15:49:18.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename init-container 03/09/23 15:49:18.016
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:18.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:18.031
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 03/09/23 15:49:18.034
    Mar  9 15:49:18.034: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  9 15:49:22.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1374" for this suite. 03/09/23 15:49:22.223
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:22.229
Mar  9 15:49:22.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename disruption 03/09/23 15:49:22.23
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:22.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:22.249
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:22.252
Mar  9 15:49:22.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename disruption-2 03/09/23 15:49:22.253
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:22.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:22.267
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 03/09/23 15:49:22.274
STEP: Waiting for the pdb to be processed 03/09/23 15:49:24.286
STEP: Waiting for the pdb to be processed 03/09/23 15:49:26.295
STEP: listing a collection of PDBs across all namespaces 03/09/23 15:49:28.302
STEP: listing a collection of PDBs in namespace disruption-1685 03/09/23 15:49:28.305
STEP: deleting a collection of PDBs 03/09/23 15:49:28.307
STEP: Waiting for the PDB collection to be deleted 03/09/23 15:49:28.316
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Mar  9 15:49:28.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1067" for this suite. 03/09/23 15:49:28.322
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  9 15:49:28.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1685" for this suite. 03/09/23 15:49:28.33
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":16,"skipped":354,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.106 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:22.229
    Mar  9 15:49:22.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename disruption 03/09/23 15:49:22.23
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:22.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:22.249
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:22.252
    Mar  9 15:49:22.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename disruption-2 03/09/23 15:49:22.253
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:22.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:22.267
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 03/09/23 15:49:22.274
    STEP: Waiting for the pdb to be processed 03/09/23 15:49:24.286
    STEP: Waiting for the pdb to be processed 03/09/23 15:49:26.295
    STEP: listing a collection of PDBs across all namespaces 03/09/23 15:49:28.302
    STEP: listing a collection of PDBs in namespace disruption-1685 03/09/23 15:49:28.305
    STEP: deleting a collection of PDBs 03/09/23 15:49:28.307
    STEP: Waiting for the PDB collection to be deleted 03/09/23 15:49:28.316
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Mar  9 15:49:28.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-1067" for this suite. 03/09/23 15:49:28.322
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  9 15:49:28.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1685" for this suite. 03/09/23 15:49:28.33
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:28.335
Mar  9 15:49:28.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename security-context-test 03/09/23 15:49:28.336
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:28.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:28.35
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Mar  9 15:49:28.359: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76" in namespace "security-context-test-9078" to be "Succeeded or Failed"
Mar  9 15:49:28.362: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687414ms
Mar  9 15:49:30.366: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006914622s
Mar  9 15:49:32.365: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005975873s
Mar  9 15:49:32.366: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76" satisfied condition "Succeeded or Failed"
Mar  9 15:49:32.378: INFO: Got logs for pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  9 15:49:32.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9078" for this suite. 03/09/23 15:49:32.382
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":355,"failed":0}
------------------------------
â€¢ [4.052 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:28.335
    Mar  9 15:49:28.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename security-context-test 03/09/23 15:49:28.336
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:28.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:28.35
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Mar  9 15:49:28.359: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76" in namespace "security-context-test-9078" to be "Succeeded or Failed"
    Mar  9 15:49:28.362: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687414ms
    Mar  9 15:49:30.366: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006914622s
    Mar  9 15:49:32.365: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005975873s
    Mar  9 15:49:32.366: INFO: Pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76" satisfied condition "Succeeded or Failed"
    Mar  9 15:49:32.378: INFO: Got logs for pod "busybox-privileged-false-26fe43fe-f9bc-4bdc-90df-1f0b187e4d76": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  9 15:49:32.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9078" for this suite. 03/09/23 15:49:32.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:32.39
Mar  9 15:49:32.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename prestop 03/09/23 15:49:32.392
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:32.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:32.406
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5036 03/09/23 15:49:32.409
STEP: Waiting for pods to come up. 03/09/23 15:49:32.415
Mar  9 15:49:32.415: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5036" to be "running"
Mar  9 15:49:32.418: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83045ms
Mar  9 15:49:34.423: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007109529s
Mar  9 15:49:34.423: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5036 03/09/23 15:49:34.425
Mar  9 15:49:34.431: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5036" to be "running"
Mar  9 15:49:34.433: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222273ms
Mar  9 15:49:36.437: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006208892s
Mar  9 15:49:36.437: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/09/23 15:49:36.437
Mar  9 15:49:41.448: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/09/23 15:49:41.448
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Mar  9 15:49:41.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5036" for this suite. 03/09/23 15:49:41.464
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":18,"skipped":401,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.079 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:32.39
    Mar  9 15:49:32.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename prestop 03/09/23 15:49:32.392
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:32.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:32.406
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5036 03/09/23 15:49:32.409
    STEP: Waiting for pods to come up. 03/09/23 15:49:32.415
    Mar  9 15:49:32.415: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5036" to be "running"
    Mar  9 15:49:32.418: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83045ms
    Mar  9 15:49:34.423: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007109529s
    Mar  9 15:49:34.423: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5036 03/09/23 15:49:34.425
    Mar  9 15:49:34.431: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5036" to be "running"
    Mar  9 15:49:34.433: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222273ms
    Mar  9 15:49:36.437: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006208892s
    Mar  9 15:49:36.437: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/09/23 15:49:36.437
    Mar  9 15:49:41.448: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/09/23 15:49:41.448
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Mar  9 15:49:41.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-5036" for this suite. 03/09/23 15:49:41.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:41.47
Mar  9 15:49:41.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 15:49:41.472
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:41.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:41.487
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 03/09/23 15:49:41.489
Mar  9 15:49:41.496: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f" in namespace "projected-1174" to be "Succeeded or Failed"
Mar  9 15:49:41.499: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.508977ms
Mar  9 15:49:43.503: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006791996s
Mar  9 15:49:45.503: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007320435s
STEP: Saw pod success 03/09/23 15:49:45.503
Mar  9 15:49:45.504: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f" satisfied condition "Succeeded or Failed"
Mar  9 15:49:45.506: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f container client-container: <nil>
STEP: delete the pod 03/09/23 15:49:45.511
Mar  9 15:49:45.522: INFO: Waiting for pod downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f to disappear
Mar  9 15:49:45.524: INFO: Pod downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 15:49:45.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1174" for this suite. 03/09/23 15:49:45.527
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":19,"skipped":416,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:41.47
    Mar  9 15:49:41.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 15:49:41.472
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:41.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:41.487
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 03/09/23 15:49:41.489
    Mar  9 15:49:41.496: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f" in namespace "projected-1174" to be "Succeeded or Failed"
    Mar  9 15:49:41.499: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.508977ms
    Mar  9 15:49:43.503: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006791996s
    Mar  9 15:49:45.503: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007320435s
    STEP: Saw pod success 03/09/23 15:49:45.503
    Mar  9 15:49:45.504: INFO: Pod "downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f" satisfied condition "Succeeded or Failed"
    Mar  9 15:49:45.506: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f container client-container: <nil>
    STEP: delete the pod 03/09/23 15:49:45.511
    Mar  9 15:49:45.522: INFO: Waiting for pod downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f to disappear
    Mar  9 15:49:45.524: INFO: Pod downwardapi-volume-a7a00340-8503-4b5c-90f7-93a8d5f1473f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 15:49:45.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1174" for this suite. 03/09/23 15:49:45.527
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:45.535
Mar  9 15:49:45.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename events 03/09/23 15:49:45.536
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:45.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:45.55
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/09/23 15:49:45.553
STEP: listing events in all namespaces 03/09/23 15:49:45.56
STEP: listing events in test namespace 03/09/23 15:49:45.564
STEP: listing events with field selection filtering on source 03/09/23 15:49:45.566
STEP: listing events with field selection filtering on reportingController 03/09/23 15:49:45.568
STEP: getting the test event 03/09/23 15:49:45.571
STEP: patching the test event 03/09/23 15:49:45.573
STEP: getting the test event 03/09/23 15:49:45.581
STEP: updating the test event 03/09/23 15:49:45.584
STEP: getting the test event 03/09/23 15:49:45.588
STEP: deleting the test event 03/09/23 15:49:45.591
STEP: listing events in all namespaces 03/09/23 15:49:45.596
STEP: listing events in test namespace 03/09/23 15:49:45.6
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  9 15:49:45.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6514" for this suite. 03/09/23 15:49:45.605
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":20,"skipped":416,"failed":0}
------------------------------
â€¢ [0.075 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:45.535
    Mar  9 15:49:45.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename events 03/09/23 15:49:45.536
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:45.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:45.55
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/09/23 15:49:45.553
    STEP: listing events in all namespaces 03/09/23 15:49:45.56
    STEP: listing events in test namespace 03/09/23 15:49:45.564
    STEP: listing events with field selection filtering on source 03/09/23 15:49:45.566
    STEP: listing events with field selection filtering on reportingController 03/09/23 15:49:45.568
    STEP: getting the test event 03/09/23 15:49:45.571
    STEP: patching the test event 03/09/23 15:49:45.573
    STEP: getting the test event 03/09/23 15:49:45.581
    STEP: updating the test event 03/09/23 15:49:45.584
    STEP: getting the test event 03/09/23 15:49:45.588
    STEP: deleting the test event 03/09/23 15:49:45.591
    STEP: listing events in all namespaces 03/09/23 15:49:45.596
    STEP: listing events in test namespace 03/09/23 15:49:45.6
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  9 15:49:45.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6514" for this suite. 03/09/23 15:49:45.605
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:45.611
Mar  9 15:49:45.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 15:49:45.612
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:45.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:45.627
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/09/23 15:49:45.63
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/09/23 15:49:45.631
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/09/23 15:49:45.631
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/09/23 15:49:45.632
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/09/23 15:49:45.633
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/09/23 15:49:45.633
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/09/23 15:49:45.634
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 15:49:45.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4364" for this suite. 03/09/23 15:49:45.639
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":21,"skipped":417,"failed":0}
------------------------------
â€¢ [0.032 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:45.611
    Mar  9 15:49:45.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 15:49:45.612
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:45.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:45.627
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/09/23 15:49:45.63
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/09/23 15:49:45.631
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/09/23 15:49:45.631
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/09/23 15:49:45.632
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/09/23 15:49:45.633
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/09/23 15:49:45.633
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/09/23 15:49:45.634
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 15:49:45.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4364" for this suite. 03/09/23 15:49:45.639
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:45.644
Mar  9 15:49:45.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-webhook 03/09/23 15:49:45.645
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:45.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:45.659
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/09/23 15:49:45.663
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/09/23 15:49:46.106
STEP: Deploying the custom resource conversion webhook pod 03/09/23 15:49:46.113
STEP: Wait for the deployment to be ready 03/09/23 15:49:46.124
Mar  9 15:49:46.130: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 15:49:48.14
STEP: Verifying the service has paired with the endpoint 03/09/23 15:49:48.153
Mar  9 15:49:49.154: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar  9 15:49:49.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Creating a v1 custom resource 03/09/23 15:49:51.74
STEP: Create a v2 custom resource 03/09/23 15:49:51.757
STEP: List CRs in v1 03/09/23 15:49:51.826
STEP: List CRs in v2 03/09/23 15:49:51.831
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 15:49:52.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7488" for this suite. 03/09/23 15:49:52.354
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":22,"skipped":417,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.752 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:45.644
    Mar  9 15:49:45.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-webhook 03/09/23 15:49:45.645
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:45.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:45.659
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/09/23 15:49:45.663
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/09/23 15:49:46.106
    STEP: Deploying the custom resource conversion webhook pod 03/09/23 15:49:46.113
    STEP: Wait for the deployment to be ready 03/09/23 15:49:46.124
    Mar  9 15:49:46.130: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 15:49:48.14
    STEP: Verifying the service has paired with the endpoint 03/09/23 15:49:48.153
    Mar  9 15:49:49.154: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar  9 15:49:49.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Creating a v1 custom resource 03/09/23 15:49:51.74
    STEP: Create a v2 custom resource 03/09/23 15:49:51.757
    STEP: List CRs in v1 03/09/23 15:49:51.826
    STEP: List CRs in v2 03/09/23 15:49:51.831
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 15:49:52.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7488" for this suite. 03/09/23 15:49:52.354
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:52.396
Mar  9 15:49:52.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 15:49:52.398
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:52.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:52.419
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 03/09/23 15:49:52.423
Mar  9 15:49:52.432: INFO: Waiting up to 5m0s for pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f" in namespace "projected-9920" to be "running and ready"
Mar  9 15:49:52.448: INFO: Pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.71407ms
Mar  9 15:49:52.449: INFO: The phase of Pod labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:49:54.453: INFO: Pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f": Phase="Running", Reason="", readiness=true. Elapsed: 2.020530379s
Mar  9 15:49:54.453: INFO: The phase of Pod labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f is Running (Ready = true)
Mar  9 15:49:54.453: INFO: Pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f" satisfied condition "running and ready"
Mar  9 15:49:54.973: INFO: Successfully updated pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 15:49:56.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9920" for this suite. 03/09/23 15:49:56.99
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":23,"skipped":427,"failed":0}
------------------------------
â€¢ [4.600 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:52.396
    Mar  9 15:49:52.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 15:49:52.398
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:52.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:52.419
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 03/09/23 15:49:52.423
    Mar  9 15:49:52.432: INFO: Waiting up to 5m0s for pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f" in namespace "projected-9920" to be "running and ready"
    Mar  9 15:49:52.448: INFO: Pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.71407ms
    Mar  9 15:49:52.449: INFO: The phase of Pod labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:49:54.453: INFO: Pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f": Phase="Running", Reason="", readiness=true. Elapsed: 2.020530379s
    Mar  9 15:49:54.453: INFO: The phase of Pod labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f is Running (Ready = true)
    Mar  9 15:49:54.453: INFO: Pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f" satisfied condition "running and ready"
    Mar  9 15:49:54.973: INFO: Successfully updated pod "labelsupdate6e8d1e6f-555f-4334-bb62-2b23e572c35f"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 15:49:56.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9920" for this suite. 03/09/23 15:49:56.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:49:57
Mar  9 15:49:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename controllerrevisions 03/09/23 15:49:57.002
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:57.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:57.016
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-mc9wz-daemon-set" 03/09/23 15:49:57.031
STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 15:49:57.037
Mar  9 15:49:57.041: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 15:49:57.043: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 0
Mar  9 15:49:57.043: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 15:49:58.048: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 15:49:58.051: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 0
Mar  9 15:49:58.051: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 15:49:59.047: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 15:49:59.050: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 2
Mar  9 15:49:59.050: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-mc9wz-daemon-set
STEP: Confirm DaemonSet "e2e-mc9wz-daemon-set" successfully created with "daemonset-name=e2e-mc9wz-daemon-set" label 03/09/23 15:49:59.053
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-mc9wz-daemon-set" 03/09/23 15:49:59.058
Mar  9 15:49:59.061: INFO: Located ControllerRevision: "e2e-mc9wz-daemon-set-6c89cb6f57"
STEP: Patching ControllerRevision "e2e-mc9wz-daemon-set-6c89cb6f57" 03/09/23 15:49:59.063
Mar  9 15:49:59.070: INFO: e2e-mc9wz-daemon-set-6c89cb6f57 has been patched
STEP: Create a new ControllerRevision 03/09/23 15:49:59.07
Mar  9 15:49:59.074: INFO: Created ControllerRevision: e2e-mc9wz-daemon-set-7bcbc5656f
STEP: Confirm that there are two ControllerRevisions 03/09/23 15:49:59.074
Mar  9 15:49:59.074: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  9 15:49:59.077: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-mc9wz-daemon-set-6c89cb6f57" 03/09/23 15:49:59.077
STEP: Confirm that there is only one ControllerRevision 03/09/23 15:49:59.081
Mar  9 15:49:59.081: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  9 15:49:59.083: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-mc9wz-daemon-set-7bcbc5656f" 03/09/23 15:49:59.085
Mar  9 15:49:59.092: INFO: e2e-mc9wz-daemon-set-7bcbc5656f has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/09/23 15:49:59.092
W0309 15:49:59.102952      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/09/23 15:49:59.103
Mar  9 15:49:59.103: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  9 15:50:00.106: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  9 15:50:00.110: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-mc9wz-daemon-set-7bcbc5656f=updated" 03/09/23 15:50:00.11
STEP: Confirm that there is only one ControllerRevision 03/09/23 15:50:00.116
Mar  9 15:50:00.117: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  9 15:50:00.119: INFO: Found 1 ControllerRevisions
Mar  9 15:50:00.121: INFO: ControllerRevision "e2e-mc9wz-daemon-set-968c8cfbd" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-mc9wz-daemon-set" 03/09/23 15:50:00.124
STEP: deleting DaemonSet.extensions e2e-mc9wz-daemon-set in namespace controllerrevisions-6622, will wait for the garbage collector to delete the pods 03/09/23 15:50:00.124
Mar  9 15:50:00.182: INFO: Deleting DaemonSet.extensions e2e-mc9wz-daemon-set took: 4.542153ms
Mar  9 15:50:00.283: INFO: Terminating DaemonSet.extensions e2e-mc9wz-daemon-set pods took: 100.810019ms
Mar  9 15:50:01.186: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 0
Mar  9 15:50:01.186: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-mc9wz-daemon-set
Mar  9 15:50:01.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"90626"},"items":null}

Mar  9 15:50:01.193: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"90626"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Mar  9 15:50:01.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6622" for this suite. 03/09/23 15:50:01.205
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":24,"skipped":470,"failed":0}
------------------------------
â€¢ [4.210 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:49:57
    Mar  9 15:49:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename controllerrevisions 03/09/23 15:49:57.002
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:49:57.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:49:57.016
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-mc9wz-daemon-set" 03/09/23 15:49:57.031
    STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 15:49:57.037
    Mar  9 15:49:57.041: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 15:49:57.043: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 0
    Mar  9 15:49:57.043: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 15:49:58.048: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 15:49:58.051: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 0
    Mar  9 15:49:58.051: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 15:49:59.047: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 15:49:59.050: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 2
    Mar  9 15:49:59.050: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-mc9wz-daemon-set
    STEP: Confirm DaemonSet "e2e-mc9wz-daemon-set" successfully created with "daemonset-name=e2e-mc9wz-daemon-set" label 03/09/23 15:49:59.053
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-mc9wz-daemon-set" 03/09/23 15:49:59.058
    Mar  9 15:49:59.061: INFO: Located ControllerRevision: "e2e-mc9wz-daemon-set-6c89cb6f57"
    STEP: Patching ControllerRevision "e2e-mc9wz-daemon-set-6c89cb6f57" 03/09/23 15:49:59.063
    Mar  9 15:49:59.070: INFO: e2e-mc9wz-daemon-set-6c89cb6f57 has been patched
    STEP: Create a new ControllerRevision 03/09/23 15:49:59.07
    Mar  9 15:49:59.074: INFO: Created ControllerRevision: e2e-mc9wz-daemon-set-7bcbc5656f
    STEP: Confirm that there are two ControllerRevisions 03/09/23 15:49:59.074
    Mar  9 15:49:59.074: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  9 15:49:59.077: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-mc9wz-daemon-set-6c89cb6f57" 03/09/23 15:49:59.077
    STEP: Confirm that there is only one ControllerRevision 03/09/23 15:49:59.081
    Mar  9 15:49:59.081: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  9 15:49:59.083: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-mc9wz-daemon-set-7bcbc5656f" 03/09/23 15:49:59.085
    Mar  9 15:49:59.092: INFO: e2e-mc9wz-daemon-set-7bcbc5656f has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/09/23 15:49:59.092
    W0309 15:49:59.102952      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/09/23 15:49:59.103
    Mar  9 15:49:59.103: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  9 15:50:00.106: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  9 15:50:00.110: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-mc9wz-daemon-set-7bcbc5656f=updated" 03/09/23 15:50:00.11
    STEP: Confirm that there is only one ControllerRevision 03/09/23 15:50:00.116
    Mar  9 15:50:00.117: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  9 15:50:00.119: INFO: Found 1 ControllerRevisions
    Mar  9 15:50:00.121: INFO: ControllerRevision "e2e-mc9wz-daemon-set-968c8cfbd" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-mc9wz-daemon-set" 03/09/23 15:50:00.124
    STEP: deleting DaemonSet.extensions e2e-mc9wz-daemon-set in namespace controllerrevisions-6622, will wait for the garbage collector to delete the pods 03/09/23 15:50:00.124
    Mar  9 15:50:00.182: INFO: Deleting DaemonSet.extensions e2e-mc9wz-daemon-set took: 4.542153ms
    Mar  9 15:50:00.283: INFO: Terminating DaemonSet.extensions e2e-mc9wz-daemon-set pods took: 100.810019ms
    Mar  9 15:50:01.186: INFO: Number of nodes with available pods controlled by daemonset e2e-mc9wz-daemon-set: 0
    Mar  9 15:50:01.186: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-mc9wz-daemon-set
    Mar  9 15:50:01.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"90626"},"items":null}

    Mar  9 15:50:01.193: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"90626"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 15:50:01.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6622" for this suite. 03/09/23 15:50:01.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:01.212
Mar  9 15:50:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 15:50:01.214
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:01.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:01.228
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 03/09/23 15:50:01.231
Mar  9 15:50:01.239: INFO: Waiting up to 5m0s for pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689" in namespace "emptydir-211" to be "Succeeded or Failed"
Mar  9 15:50:01.241: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698774ms
Mar  9 15:50:03.244: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005470127s
Mar  9 15:50:05.246: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006935102s
STEP: Saw pod success 03/09/23 15:50:05.246
Mar  9 15:50:05.246: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689" satisfied condition "Succeeded or Failed"
Mar  9 15:50:05.249: INFO: Trying to get logs from node tt-test-el8-003 pod pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689 container test-container: <nil>
STEP: delete the pod 03/09/23 15:50:05.254
Mar  9 15:50:05.264: INFO: Waiting for pod pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689 to disappear
Mar  9 15:50:05.267: INFO: Pod pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 15:50:05.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-211" for this suite. 03/09/23 15:50:05.27
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":25,"skipped":498,"failed":0}
------------------------------
â€¢ [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:01.212
    Mar  9 15:50:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 15:50:01.214
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:01.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:01.228
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/09/23 15:50:01.231
    Mar  9 15:50:01.239: INFO: Waiting up to 5m0s for pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689" in namespace "emptydir-211" to be "Succeeded or Failed"
    Mar  9 15:50:01.241: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698774ms
    Mar  9 15:50:03.244: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005470127s
    Mar  9 15:50:05.246: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006935102s
    STEP: Saw pod success 03/09/23 15:50:05.246
    Mar  9 15:50:05.246: INFO: Pod "pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689" satisfied condition "Succeeded or Failed"
    Mar  9 15:50:05.249: INFO: Trying to get logs from node tt-test-el8-003 pod pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689 container test-container: <nil>
    STEP: delete the pod 03/09/23 15:50:05.254
    Mar  9 15:50:05.264: INFO: Waiting for pod pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689 to disappear
    Mar  9 15:50:05.267: INFO: Pod pod-7d499c4a-af3d-40dc-8a47-01b6c1dcd689 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 15:50:05.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-211" for this suite. 03/09/23 15:50:05.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:05.276
Mar  9 15:50:05.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 15:50:05.277
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:05.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:05.293
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-f14fdcf2-4fe9-4a3d-8498-0ad0ed25cd61 03/09/23 15:50:05.295
STEP: Creating a pod to test consume configMaps 03/09/23 15:50:05.299
Mar  9 15:50:05.307: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86" in namespace "projected-4953" to be "Succeeded or Failed"
Mar  9 15:50:05.310: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.496782ms
Mar  9 15:50:07.314: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007111629s
Mar  9 15:50:09.314: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006598655s
STEP: Saw pod success 03/09/23 15:50:09.314
Mar  9 15:50:09.314: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86" satisfied condition "Succeeded or Failed"
Mar  9 15:50:09.317: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 15:50:09.322
Mar  9 15:50:09.332: INFO: Waiting for pod pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86 to disappear
Mar  9 15:50:09.334: INFO: Pod pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 15:50:09.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4953" for this suite. 03/09/23 15:50:09.338
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":26,"skipped":506,"failed":0}
------------------------------
â€¢ [4.068 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:05.276
    Mar  9 15:50:05.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 15:50:05.277
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:05.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:05.293
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-f14fdcf2-4fe9-4a3d-8498-0ad0ed25cd61 03/09/23 15:50:05.295
    STEP: Creating a pod to test consume configMaps 03/09/23 15:50:05.299
    Mar  9 15:50:05.307: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86" in namespace "projected-4953" to be "Succeeded or Failed"
    Mar  9 15:50:05.310: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.496782ms
    Mar  9 15:50:07.314: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007111629s
    Mar  9 15:50:09.314: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006598655s
    STEP: Saw pod success 03/09/23 15:50:09.314
    Mar  9 15:50:09.314: INFO: Pod "pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86" satisfied condition "Succeeded or Failed"
    Mar  9 15:50:09.317: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 15:50:09.322
    Mar  9 15:50:09.332: INFO: Waiting for pod pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86 to disappear
    Mar  9 15:50:09.334: INFO: Pod pod-projected-configmaps-315edc6f-1f75-4e23-92d7-b00b9c593c86 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 15:50:09.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4953" for this suite. 03/09/23 15:50:09.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:09.345
Mar  9 15:50:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 15:50:09.346
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:09.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:09.361
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 15:50:09.374
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 15:50:09.617
STEP: Deploying the webhook pod 03/09/23 15:50:09.622
STEP: Wait for the deployment to be ready 03/09/23 15:50:09.633
Mar  9 15:50:09.650: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 15:50:11.659
STEP: Verifying the service has paired with the endpoint 03/09/23 15:50:11.674
Mar  9 15:50:12.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Mar  9 15:50:12.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9376-crds.webhook.example.com via the AdmissionRegistration API 03/09/23 15:50:13.188
STEP: Creating a custom resource that should be mutated by the webhook 03/09/23 15:50:13.204
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 15:50:15.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4770" for this suite. 03/09/23 15:50:15.745
STEP: Destroying namespace "webhook-4770-markers" for this suite. 03/09/23 15:50:15.749
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":27,"skipped":511,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.443 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:09.345
    Mar  9 15:50:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 15:50:09.346
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:09.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:09.361
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 15:50:09.374
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 15:50:09.617
    STEP: Deploying the webhook pod 03/09/23 15:50:09.622
    STEP: Wait for the deployment to be ready 03/09/23 15:50:09.633
    Mar  9 15:50:09.650: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 15:50:11.659
    STEP: Verifying the service has paired with the endpoint 03/09/23 15:50:11.674
    Mar  9 15:50:12.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Mar  9 15:50:12.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9376-crds.webhook.example.com via the AdmissionRegistration API 03/09/23 15:50:13.188
    STEP: Creating a custom resource that should be mutated by the webhook 03/09/23 15:50:13.204
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 15:50:15.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4770" for this suite. 03/09/23 15:50:15.745
    STEP: Destroying namespace "webhook-4770-markers" for this suite. 03/09/23 15:50:15.749
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:15.789
Mar  9 15:50:15.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename svcaccounts 03/09/23 15:50:15.79
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:15.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:15.809
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 03/09/23 15:50:15.813
STEP: watching for the ServiceAccount to be added 03/09/23 15:50:15.822
STEP: patching the ServiceAccount 03/09/23 15:50:15.824
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/09/23 15:50:15.833
STEP: deleting the ServiceAccount 03/09/23 15:50:15.837
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  9 15:50:15.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8469" for this suite. 03/09/23 15:50:15.852
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":28,"skipped":521,"failed":0}
------------------------------
â€¢ [0.069 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:15.789
    Mar  9 15:50:15.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename svcaccounts 03/09/23 15:50:15.79
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:15.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:15.809
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 03/09/23 15:50:15.813
    STEP: watching for the ServiceAccount to be added 03/09/23 15:50:15.822
    STEP: patching the ServiceAccount 03/09/23 15:50:15.824
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/09/23 15:50:15.833
    STEP: deleting the ServiceAccount 03/09/23 15:50:15.837
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  9 15:50:15.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8469" for this suite. 03/09/23 15:50:15.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:15.861
Mar  9 15:50:15.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 15:50:15.863
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:15.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:15.879
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7674 03/09/23 15:50:15.882
STEP: changing the ExternalName service to type=NodePort 03/09/23 15:50:15.893
STEP: creating replication controller externalname-service in namespace services-7674 03/09/23 15:50:15.929
I0309 15:50:15.936250      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7674, replica count: 2
I0309 15:50:18.991318      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 15:50:18.991: INFO: Creating new exec pod
Mar  9 15:50:18.997: INFO: Waiting up to 5m0s for pod "execpod2x2ft" in namespace "services-7674" to be "running"
Mar  9 15:50:19.000: INFO: Pod "execpod2x2ft": Phase="Pending", Reason="", readiness=false. Elapsed: 2.811674ms
Mar  9 15:50:21.003: INFO: Pod "execpod2x2ft": Phase="Running", Reason="", readiness=true. Elapsed: 2.006015051s
Mar  9 15:50:21.003: INFO: Pod "execpod2x2ft" satisfied condition "running"
Mar  9 15:50:22.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  9 15:50:22.203: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  9 15:50:22.203: INFO: stdout: "externalname-service-mxct7"
Mar  9 15:50:22.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.118.138 80'
Mar  9 15:50:22.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.118.138 80\nConnection to 10.96.118.138 80 port [tcp/http] succeeded!\n"
Mar  9 15:50:22.389: INFO: stdout: "externalname-service-mxct7"
Mar  9 15:50:22.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 30672'
Mar  9 15:50:22.529: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 30672\nConnection to 100.100.230.140 30672 port [tcp/*] succeeded!\n"
Mar  9 15:50:22.530: INFO: stdout: "externalname-service-dctdf"
Mar  9 15:50:22.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 30672'
Mar  9 15:50:22.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 30672\nConnection to 100.100.231.104 30672 port [tcp/*] succeeded!\n"
Mar  9 15:50:22.672: INFO: stdout: "externalname-service-mxct7"
Mar  9 15:50:22.672: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 15:50:22.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7674" for this suite. 03/09/23 15:50:22.72
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":29,"skipped":552,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.864 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:15.861
    Mar  9 15:50:15.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 15:50:15.863
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:15.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:15.879
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7674 03/09/23 15:50:15.882
    STEP: changing the ExternalName service to type=NodePort 03/09/23 15:50:15.893
    STEP: creating replication controller externalname-service in namespace services-7674 03/09/23 15:50:15.929
    I0309 15:50:15.936250      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7674, replica count: 2
    I0309 15:50:18.991318      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 15:50:18.991: INFO: Creating new exec pod
    Mar  9 15:50:18.997: INFO: Waiting up to 5m0s for pod "execpod2x2ft" in namespace "services-7674" to be "running"
    Mar  9 15:50:19.000: INFO: Pod "execpod2x2ft": Phase="Pending", Reason="", readiness=false. Elapsed: 2.811674ms
    Mar  9 15:50:21.003: INFO: Pod "execpod2x2ft": Phase="Running", Reason="", readiness=true. Elapsed: 2.006015051s
    Mar  9 15:50:21.003: INFO: Pod "execpod2x2ft" satisfied condition "running"
    Mar  9 15:50:22.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  9 15:50:22.203: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  9 15:50:22.203: INFO: stdout: "externalname-service-mxct7"
    Mar  9 15:50:22.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.118.138 80'
    Mar  9 15:50:22.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.118.138 80\nConnection to 10.96.118.138 80 port [tcp/http] succeeded!\n"
    Mar  9 15:50:22.389: INFO: stdout: "externalname-service-mxct7"
    Mar  9 15:50:22.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 30672'
    Mar  9 15:50:22.529: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 30672\nConnection to 100.100.230.140 30672 port [tcp/*] succeeded!\n"
    Mar  9 15:50:22.530: INFO: stdout: "externalname-service-dctdf"
    Mar  9 15:50:22.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7674 exec execpod2x2ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 30672'
    Mar  9 15:50:22.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 30672\nConnection to 100.100.231.104 30672 port [tcp/*] succeeded!\n"
    Mar  9 15:50:22.672: INFO: stdout: "externalname-service-mxct7"
    Mar  9 15:50:22.672: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 15:50:22.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7674" for this suite. 03/09/23 15:50:22.72
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:22.726
Mar  9 15:50:22.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename runtimeclass 03/09/23 15:50:22.727
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:22.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:22.742
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar  9 15:50:22.757: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9423 to be scheduled
Mar  9 15:50:22.760: INFO: 1 pods are not scheduled: [runtimeclass-9423/test-runtimeclass-runtimeclass-9423-preconfigured-handler-txclz(2f34c3e0-6620-4588-8140-015401fee564)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  9 15:50:24.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9423" for this suite. 03/09/23 15:50:24.773
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":30,"skipped":554,"failed":0}
------------------------------
â€¢ [2.052 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:22.726
    Mar  9 15:50:22.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename runtimeclass 03/09/23 15:50:22.727
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:22.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:22.742
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar  9 15:50:22.757: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9423 to be scheduled
    Mar  9 15:50:22.760: INFO: 1 pods are not scheduled: [runtimeclass-9423/test-runtimeclass-runtimeclass-9423-preconfigured-handler-txclz(2f34c3e0-6620-4588-8140-015401fee564)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  9 15:50:24.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9423" for this suite. 03/09/23 15:50:24.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:24.782
Mar  9 15:50:24.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-runtime 03/09/23 15:50:24.783
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:24.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:24.797
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 03/09/23 15:50:24.8
STEP: wait for the container to reach Succeeded 03/09/23 15:50:24.806
STEP: get the container status 03/09/23 15:50:28.823
STEP: the container should be terminated 03/09/23 15:50:28.826
STEP: the termination message should be set 03/09/23 15:50:28.826
Mar  9 15:50:28.826: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/09/23 15:50:28.826
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  9 15:50:28.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4837" for this suite. 03/09/23 15:50:28.842
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":31,"skipped":610,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:24.782
    Mar  9 15:50:24.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-runtime 03/09/23 15:50:24.783
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:24.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:24.797
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 03/09/23 15:50:24.8
    STEP: wait for the container to reach Succeeded 03/09/23 15:50:24.806
    STEP: get the container status 03/09/23 15:50:28.823
    STEP: the container should be terminated 03/09/23 15:50:28.826
    STEP: the termination message should be set 03/09/23 15:50:28.826
    Mar  9 15:50:28.826: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/09/23 15:50:28.826
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  9 15:50:28.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4837" for this suite. 03/09/23 15:50:28.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:28.848
Mar  9 15:50:28.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 15:50:28.85
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:28.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:28.865
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 03/09/23 15:50:28.868
STEP: fetching the ConfigMap 03/09/23 15:50:28.872
STEP: patching the ConfigMap 03/09/23 15:50:28.874
STEP: listing all ConfigMaps in all namespaces with a label selector 03/09/23 15:50:28.88
STEP: deleting the ConfigMap by collection with a label selector 03/09/23 15:50:28.883
STEP: listing all ConfigMaps in test namespace 03/09/23 15:50:28.889
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 15:50:28.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6226" for this suite. 03/09/23 15:50:28.894
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":32,"skipped":633,"failed":0}
------------------------------
â€¢ [0.051 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:28.848
    Mar  9 15:50:28.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 15:50:28.85
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:28.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:28.865
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 03/09/23 15:50:28.868
    STEP: fetching the ConfigMap 03/09/23 15:50:28.872
    STEP: patching the ConfigMap 03/09/23 15:50:28.874
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/09/23 15:50:28.88
    STEP: deleting the ConfigMap by collection with a label selector 03/09/23 15:50:28.883
    STEP: listing all ConfigMaps in test namespace 03/09/23 15:50:28.889
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 15:50:28.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6226" for this suite. 03/09/23 15:50:28.894
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:28.9
Mar  9 15:50:28.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-runtime 03/09/23 15:50:28.901
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:28.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:28.914
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 03/09/23 15:50:28.917
STEP: wait for the container to reach Succeeded 03/09/23 15:50:28.924
STEP: get the container status 03/09/23 15:50:32.943
STEP: the container should be terminated 03/09/23 15:50:32.946
STEP: the termination message should be set 03/09/23 15:50:32.946
Mar  9 15:50:32.946: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/09/23 15:50:32.946
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  9 15:50:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9710" for this suite. 03/09/23 15:50:32.963
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":33,"skipped":637,"failed":0}
------------------------------
â€¢ [4.068 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:28.9
    Mar  9 15:50:28.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-runtime 03/09/23 15:50:28.901
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:28.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:28.914
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 03/09/23 15:50:28.917
    STEP: wait for the container to reach Succeeded 03/09/23 15:50:28.924
    STEP: get the container status 03/09/23 15:50:32.943
    STEP: the container should be terminated 03/09/23 15:50:32.946
    STEP: the termination message should be set 03/09/23 15:50:32.946
    Mar  9 15:50:32.946: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/09/23 15:50:32.946
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  9 15:50:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9710" for this suite. 03/09/23 15:50:32.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:32.969
Mar  9 15:50:32.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 15:50:32.97
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:32.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:32.983
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 03/09/23 15:50:32.986
Mar  9 15:50:32.993: INFO: Waiting up to 5m0s for pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94" in namespace "downward-api-5253" to be "running and ready"
Mar  9 15:50:32.996: INFO: Pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671721ms
Mar  9 15:50:32.996: INFO: The phase of Pod labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:50:35.002: INFO: Pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94": Phase="Running", Reason="", readiness=true. Elapsed: 2.009000165s
Mar  9 15:50:35.002: INFO: The phase of Pod labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94 is Running (Ready = true)
Mar  9 15:50:35.002: INFO: Pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94" satisfied condition "running and ready"
Mar  9 15:50:35.528: INFO: Successfully updated pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 15:50:39.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5253" for this suite. 03/09/23 15:50:39.552
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":34,"skipped":658,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.588 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:32.969
    Mar  9 15:50:32.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 15:50:32.97
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:32.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:32.983
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 03/09/23 15:50:32.986
    Mar  9 15:50:32.993: INFO: Waiting up to 5m0s for pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94" in namespace "downward-api-5253" to be "running and ready"
    Mar  9 15:50:32.996: INFO: Pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671721ms
    Mar  9 15:50:32.996: INFO: The phase of Pod labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:50:35.002: INFO: Pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94": Phase="Running", Reason="", readiness=true. Elapsed: 2.009000165s
    Mar  9 15:50:35.002: INFO: The phase of Pod labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94 is Running (Ready = true)
    Mar  9 15:50:35.002: INFO: Pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94" satisfied condition "running and ready"
    Mar  9 15:50:35.528: INFO: Successfully updated pod "labelsupdate78aaf64c-38f5-468a-9bef-c74906ccbe94"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 15:50:39.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5253" for this suite. 03/09/23 15:50:39.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:39.559
Mar  9 15:50:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename gc 03/09/23 15:50:39.56
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:39.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:39.575
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/09/23 15:50:39.577
STEP: Wait for the Deployment to create new ReplicaSet 03/09/23 15:50:39.582
STEP: delete the deployment 03/09/23 15:50:40.088
STEP: wait for all rs to be garbage collected 03/09/23 15:50:40.093
STEP: expected 0 pods, got 2 pods 03/09/23 15:50:40.097
STEP: Gathering metrics 03/09/23 15:50:40.61
Mar  9 15:50:40.628: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
Mar  9 15:50:40.630: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.450644ms
Mar  9 15:50:40.630: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
Mar  9 15:50:40.630: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
Mar  9 15:50:40.703: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  9 15:50:40.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8346" for this suite. 03/09/23 15:50:40.707
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":35,"skipped":687,"failed":0}
------------------------------
â€¢ [1.153 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:39.559
    Mar  9 15:50:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename gc 03/09/23 15:50:39.56
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:39.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:39.575
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/09/23 15:50:39.577
    STEP: Wait for the Deployment to create new ReplicaSet 03/09/23 15:50:39.582
    STEP: delete the deployment 03/09/23 15:50:40.088
    STEP: wait for all rs to be garbage collected 03/09/23 15:50:40.093
    STEP: expected 0 pods, got 2 pods 03/09/23 15:50:40.097
    STEP: Gathering metrics 03/09/23 15:50:40.61
    Mar  9 15:50:40.628: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
    Mar  9 15:50:40.630: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.450644ms
    Mar  9 15:50:40.630: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
    Mar  9 15:50:40.630: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
    Mar  9 15:50:40.703: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  9 15:50:40.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8346" for this suite. 03/09/23 15:50:40.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:40.712
Mar  9 15:50:40.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 15:50:40.713
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:40.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:40.729
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 03/09/23 15:50:40.732
Mar  9 15:50:40.739: INFO: Waiting up to 5m0s for pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af" in namespace "downward-api-8079" to be "Succeeded or Failed"
Mar  9 15:50:40.741: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400209ms
Mar  9 15:50:42.745: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006633428s
Mar  9 15:50:44.744: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00582494s
STEP: Saw pod success 03/09/23 15:50:44.744
Mar  9 15:50:44.745: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af" satisfied condition "Succeeded or Failed"
Mar  9 15:50:44.747: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-7170ae74-16e4-413e-b650-b64152f984af container dapi-container: <nil>
STEP: delete the pod 03/09/23 15:50:44.754
Mar  9 15:50:44.766: INFO: Waiting for pod downward-api-7170ae74-16e4-413e-b650-b64152f984af to disappear
Mar  9 15:50:44.770: INFO: Pod downward-api-7170ae74-16e4-413e-b650-b64152f984af no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  9 15:50:44.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8079" for this suite. 03/09/23 15:50:44.773
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":36,"skipped":692,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:40.712
    Mar  9 15:50:40.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 15:50:40.713
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:40.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:40.729
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 03/09/23 15:50:40.732
    Mar  9 15:50:40.739: INFO: Waiting up to 5m0s for pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af" in namespace "downward-api-8079" to be "Succeeded or Failed"
    Mar  9 15:50:40.741: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400209ms
    Mar  9 15:50:42.745: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006633428s
    Mar  9 15:50:44.744: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00582494s
    STEP: Saw pod success 03/09/23 15:50:44.744
    Mar  9 15:50:44.745: INFO: Pod "downward-api-7170ae74-16e4-413e-b650-b64152f984af" satisfied condition "Succeeded or Failed"
    Mar  9 15:50:44.747: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-7170ae74-16e4-413e-b650-b64152f984af container dapi-container: <nil>
    STEP: delete the pod 03/09/23 15:50:44.754
    Mar  9 15:50:44.766: INFO: Waiting for pod downward-api-7170ae74-16e4-413e-b650-b64152f984af to disappear
    Mar  9 15:50:44.770: INFO: Pod downward-api-7170ae74-16e4-413e-b650-b64152f984af no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  9 15:50:44.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8079" for this suite. 03/09/23 15:50:44.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:44.781
Mar  9 15:50:44.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 15:50:44.783
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:44.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:44.796
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-c179778a-ec0a-4a32-a2c2-7dc8d8d327f8 03/09/23 15:50:44.798
STEP: Creating a pod to test consume configMaps 03/09/23 15:50:44.802
Mar  9 15:50:44.809: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c" in namespace "configmap-3644" to be "Succeeded or Failed"
Mar  9 15:50:44.811: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.323377ms
Mar  9 15:50:46.815: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005953067s
Mar  9 15:50:48.815: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005950913s
STEP: Saw pod success 03/09/23 15:50:48.815
Mar  9 15:50:48.815: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c" satisfied condition "Succeeded or Failed"
Mar  9 15:50:48.818: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c container agnhost-container: <nil>
STEP: delete the pod 03/09/23 15:50:48.824
Mar  9 15:50:48.833: INFO: Waiting for pod pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c to disappear
Mar  9 15:50:48.836: INFO: Pod pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 15:50:48.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3644" for this suite. 03/09/23 15:50:48.839
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":37,"skipped":699,"failed":0}
------------------------------
â€¢ [4.062 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:44.781
    Mar  9 15:50:44.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 15:50:44.783
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:44.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:44.796
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-c179778a-ec0a-4a32-a2c2-7dc8d8d327f8 03/09/23 15:50:44.798
    STEP: Creating a pod to test consume configMaps 03/09/23 15:50:44.802
    Mar  9 15:50:44.809: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c" in namespace "configmap-3644" to be "Succeeded or Failed"
    Mar  9 15:50:44.811: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.323377ms
    Mar  9 15:50:46.815: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005953067s
    Mar  9 15:50:48.815: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005950913s
    STEP: Saw pod success 03/09/23 15:50:48.815
    Mar  9 15:50:48.815: INFO: Pod "pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c" satisfied condition "Succeeded or Failed"
    Mar  9 15:50:48.818: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 15:50:48.824
    Mar  9 15:50:48.833: INFO: Waiting for pod pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c to disappear
    Mar  9 15:50:48.836: INFO: Pod pod-configmaps-0f3e0290-9d61-4a38-9314-80a59edb905c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 15:50:48.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3644" for this suite. 03/09/23 15:50:48.839
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:50:48.844
Mar  9 15:50:48.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 15:50:48.846
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:48.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:48.86
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 03/09/23 15:50:48.862
STEP: Creating a ResourceQuota 03/09/23 15:50:53.866
STEP: Ensuring resource quota status is calculated 03/09/23 15:50:53.871
STEP: Creating a Pod that fits quota 03/09/23 15:50:55.874
STEP: Ensuring ResourceQuota status captures the pod usage 03/09/23 15:50:55.887
STEP: Not allowing a pod to be created that exceeds remaining quota 03/09/23 15:50:57.89
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/09/23 15:50:57.893
STEP: Ensuring a pod cannot update its resource requirements 03/09/23 15:50:57.895
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/09/23 15:50:57.898
STEP: Deleting the pod 03/09/23 15:50:59.903
STEP: Ensuring resource quota status released the pod usage 03/09/23 15:50:59.91
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 15:51:01.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7752" for this suite. 03/09/23 15:51:01.917
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":38,"skipped":702,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.078 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:50:48.844
    Mar  9 15:50:48.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 15:50:48.846
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:50:48.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:50:48.86
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 03/09/23 15:50:48.862
    STEP: Creating a ResourceQuota 03/09/23 15:50:53.866
    STEP: Ensuring resource quota status is calculated 03/09/23 15:50:53.871
    STEP: Creating a Pod that fits quota 03/09/23 15:50:55.874
    STEP: Ensuring ResourceQuota status captures the pod usage 03/09/23 15:50:55.887
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/09/23 15:50:57.89
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/09/23 15:50:57.893
    STEP: Ensuring a pod cannot update its resource requirements 03/09/23 15:50:57.895
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/09/23 15:50:57.898
    STEP: Deleting the pod 03/09/23 15:50:59.903
    STEP: Ensuring resource quota status released the pod usage 03/09/23 15:50:59.91
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 15:51:01.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7752" for this suite. 03/09/23 15:51:01.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:51:01.925
Mar  9 15:51:01.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pod-network-test 03/09/23 15:51:01.926
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:51:01.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:51:01.939
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-2563 03/09/23 15:51:01.942
STEP: creating a selector 03/09/23 15:51:01.942
STEP: Creating the service pods in kubernetes 03/09/23 15:51:01.942
Mar  9 15:51:01.942: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  9 15:51:01.960: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2563" to be "running and ready"
Mar  9 15:51:01.965: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.763651ms
Mar  9 15:51:01.965: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:51:03.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010086592s
Mar  9 15:51:03.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:51:05.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010694576s
Mar  9 15:51:05.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:51:07.969: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009602213s
Mar  9 15:51:07.969: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:51:09.969: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009556902s
Mar  9 15:51:09.969: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:51:11.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010025709s
Mar  9 15:51:11.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:51:13.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.01008201s
Mar  9 15:51:13.970: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  9 15:51:13.970: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  9 15:51:13.972: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2563" to be "running and ready"
Mar  9 15:51:13.975: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.561792ms
Mar  9 15:51:13.975: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  9 15:51:13.975: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/09/23 15:51:13.977
Mar  9 15:51:13.987: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2563" to be "running"
Mar  9 15:51:13.993: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189253ms
Mar  9 15:51:15.997: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009757342s
Mar  9 15:51:15.997: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  9 15:51:16.000: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2563" to be "running"
Mar  9 15:51:16.002: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.225794ms
Mar  9 15:51:16.002: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  9 15:51:16.004: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar  9 15:51:16.004: INFO: Going to poll 10.244.42.203 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar  9 15:51:16.006: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.42.203 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2563 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:51:16.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:51:16.007: INFO: ExecWithOptions: Clientset creation
Mar  9 15:51:16.007: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2563/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.42.203+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  9 15:51:17.088: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  9 15:51:17.089: INFO: Going to poll 10.244.88.250 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar  9 15:51:17.091: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.88.250 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2563 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:51:17.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:51:17.092: INFO: ExecWithOptions: Clientset creation
Mar  9 15:51:17.092: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2563/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.88.250+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  9 15:51:18.177: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  9 15:51:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2563" for this suite. 03/09/23 15:51:18.181
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":39,"skipped":729,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.261 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:51:01.925
    Mar  9 15:51:01.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pod-network-test 03/09/23 15:51:01.926
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:51:01.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:51:01.939
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-2563 03/09/23 15:51:01.942
    STEP: creating a selector 03/09/23 15:51:01.942
    STEP: Creating the service pods in kubernetes 03/09/23 15:51:01.942
    Mar  9 15:51:01.942: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  9 15:51:01.960: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2563" to be "running and ready"
    Mar  9 15:51:01.965: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.763651ms
    Mar  9 15:51:01.965: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:51:03.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010086592s
    Mar  9 15:51:03.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:51:05.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010694576s
    Mar  9 15:51:05.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:51:07.969: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009602213s
    Mar  9 15:51:07.969: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:51:09.969: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009556902s
    Mar  9 15:51:09.969: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:51:11.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010025709s
    Mar  9 15:51:11.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:51:13.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.01008201s
    Mar  9 15:51:13.970: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  9 15:51:13.970: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  9 15:51:13.972: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2563" to be "running and ready"
    Mar  9 15:51:13.975: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.561792ms
    Mar  9 15:51:13.975: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  9 15:51:13.975: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/09/23 15:51:13.977
    Mar  9 15:51:13.987: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2563" to be "running"
    Mar  9 15:51:13.993: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189253ms
    Mar  9 15:51:15.997: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009757342s
    Mar  9 15:51:15.997: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  9 15:51:16.000: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2563" to be "running"
    Mar  9 15:51:16.002: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.225794ms
    Mar  9 15:51:16.002: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  9 15:51:16.004: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar  9 15:51:16.004: INFO: Going to poll 10.244.42.203 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Mar  9 15:51:16.006: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.42.203 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2563 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:51:16.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:51:16.007: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:51:16.007: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2563/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.42.203+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  9 15:51:17.088: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  9 15:51:17.089: INFO: Going to poll 10.244.88.250 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Mar  9 15:51:17.091: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.88.250 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2563 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:51:17.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:51:17.092: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:51:17.092: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2563/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.88.250+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  9 15:51:18.177: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  9 15:51:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2563" for this suite. 03/09/23 15:51:18.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:51:18.187
Mar  9 15:51:18.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-pred 03/09/23 15:51:18.189
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:51:18.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:51:18.203
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  9 15:51:18.206: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  9 15:51:18.212: INFO: Waiting for terminating namespaces to be deleted...
Mar  9 15:51:18.215: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-003 before test
Mar  9 15:51:18.223: INFO: calico-apiserver-7c747f5cd5-rb7jh from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container calico-apiserver ready: true, restart count 0
Mar  9 15:51:18.223: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 15:51:18.223: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 15:51:18.223: INFO: csi-node-driver-75b8t from calico-system started at 2023-03-09 03:23:36 +0000 UTC (2 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 15:51:18.223: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 15:51:18.223: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 15:51:18.223: INFO: host-test-container-pod from pod-network-test-2563 started at 2023-03-09 15:51:13 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container agnhost-container ready: true, restart count 0
Mar  9 15:51:18.223: INFO: netserver-0 from pod-network-test-2563 started at 2023-03-09 15:51:01 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container webserver ready: true, restart count 0
Mar  9 15:51:18.223: INFO: test-container-pod from pod-network-test-2563 started at 2023-03-09 15:51:13 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container webserver ready: true, restart count 0
Mar  9 15:51:18.223: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  9 15:51:18.223: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container e2e ready: true, restart count 0
Mar  9 15:51:18.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 15:51:18.223: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 15:51:18.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 15:51:18.223: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 15:51:18.223: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-004 before test
Mar  9 15:51:18.230: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container calico-apiserver ready: true, restart count 0
Mar  9 15:51:18.230: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  9 15:51:18.230: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 15:51:18.230: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 15:51:18.230: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 15:51:18.230: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 15:51:18.230: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container webhook ready: true, restart count 0
Mar  9 15:51:18.230: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container coredns ready: true, restart count 0
Mar  9 15:51:18.230: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container coredns ready: true, restart count 0
Mar  9 15:51:18.230: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 15:51:18.230: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  9 15:51:18.230: INFO: netserver-1 from pod-network-test-2563 started at 2023-03-09 15:51:01 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container webserver ready: true, restart count 0
Mar  9 15:51:18.230: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 15:51:18.230: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 15:51:18.230: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 15:51:18.230: INFO: 	Container tigera-operator ready: true, restart count 3
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/09/23 15:51:18.23
Mar  9 15:51:18.236: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7228" to be "running"
Mar  9 15:51:18.241: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815735ms
Mar  9 15:51:20.245: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009317617s
Mar  9 15:51:20.245: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/09/23 15:51:20.248
STEP: Trying to apply a random label on the found node. 03/09/23 15:51:20.259
STEP: verifying the node has the label kubernetes.io/e2e-ccd83a12-ee70-420c-baaa-a3dd71fbd8ba 95 03/09/23 15:51:20.27
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/09/23 15:51:20.274
Mar  9 15:51:20.279: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7228" to be "not pending"
Mar  9 15:51:20.283: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065632ms
Mar  9 15:51:22.287: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.007293268s
Mar  9 15:51:22.287: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 100.100.230.140 on the node which pod4 resides and expect not scheduled 03/09/23 15:51:22.287
Mar  9 15:51:22.293: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7228" to be "not pending"
Mar  9 15:51:22.298: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09953ms
Mar  9 15:51:24.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007880349s
Mar  9 15:51:26.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00768021s
Mar  9 15:51:28.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007421335s
Mar  9 15:51:30.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008854499s
Mar  9 15:51:32.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008090396s
Mar  9 15:51:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008475948s
Mar  9 15:51:36.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007673424s
Mar  9 15:51:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007675257s
Mar  9 15:51:40.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008742515s
Mar  9 15:51:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008816818s
Mar  9 15:51:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008100677s
Mar  9 15:51:46.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007722631s
Mar  9 15:51:48.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008248977s
Mar  9 15:51:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008900356s
Mar  9 15:51:52.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007910384s
Mar  9 15:51:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008974413s
Mar  9 15:51:56.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007567462s
Mar  9 15:51:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00788428s
Mar  9 15:52:00.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008147288s
Mar  9 15:52:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007619293s
Mar  9 15:52:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008879653s
Mar  9 15:52:06.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008096925s
Mar  9 15:52:08.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007863095s
Mar  9 15:52:10.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008837322s
Mar  9 15:52:12.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008463094s
Mar  9 15:52:14.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007451829s
Mar  9 15:52:16.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009036564s
Mar  9 15:52:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007543064s
Mar  9 15:52:20.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00733189s
Mar  9 15:52:22.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008049442s
Mar  9 15:52:24.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008156749s
Mar  9 15:52:26.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007013926s
Mar  9 15:52:28.300: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006859938s
Mar  9 15:52:30.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008391002s
Mar  9 15:52:32.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007465707s
Mar  9 15:52:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008539333s
Mar  9 15:52:36.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007887583s
Mar  9 15:52:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007652949s
Mar  9 15:52:40.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007885452s
Mar  9 15:52:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008385529s
Mar  9 15:52:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008120306s
Mar  9 15:52:46.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00762495s
Mar  9 15:52:48.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007787563s
Mar  9 15:52:50.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009175436s
Mar  9 15:52:52.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007286887s
Mar  9 15:52:54.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007550129s
Mar  9 15:52:56.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008087784s
Mar  9 15:52:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007886192s
Mar  9 15:53:00.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008693956s
Mar  9 15:53:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007591681s
Mar  9 15:53:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008357926s
Mar  9 15:53:06.300: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006893546s
Mar  9 15:53:08.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007863929s
Mar  9 15:53:10.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007018092s
Mar  9 15:53:12.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009106605s
Mar  9 15:53:14.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008193318s
Mar  9 15:53:16.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007441093s
Mar  9 15:53:18.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008012461s
Mar  9 15:53:20.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008744886s
Mar  9 15:53:22.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008219297s
Mar  9 15:53:24.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007921619s
Mar  9 15:53:26.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.00886971s
Mar  9 15:53:28.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008028788s
Mar  9 15:53:30.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007477533s
Mar  9 15:53:32.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009025103s
Mar  9 15:53:34.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009044158s
Mar  9 15:53:36.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008032856s
Mar  9 15:53:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.00792134s
Mar  9 15:53:40.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008626356s
Mar  9 15:53:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008544791s
Mar  9 15:53:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008530929s
Mar  9 15:53:46.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008072543s
Mar  9 15:53:48.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008268835s
Mar  9 15:53:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008148459s
Mar  9 15:53:52.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.009575456s
Mar  9 15:53:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008889891s
Mar  9 15:53:56.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008738461s
Mar  9 15:53:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007835138s
Mar  9 15:54:00.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007811673s
Mar  9 15:54:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007710029s
Mar  9 15:54:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008844145s
Mar  9 15:54:06.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009181415s
Mar  9 15:54:08.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.00805344s
Mar  9 15:54:10.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009012335s
Mar  9 15:54:12.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.007574047s
Mar  9 15:54:14.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009051543s
Mar  9 15:54:16.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007816498s
Mar  9 15:54:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007196924s
Mar  9 15:54:20.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009316981s
Mar  9 15:54:22.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009661854s
Mar  9 15:54:24.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008983005s
Mar  9 15:54:26.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009333507s
Mar  9 15:54:28.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007969315s
Mar  9 15:54:30.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00896167s
Mar  9 15:54:32.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007881232s
Mar  9 15:54:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008187647s
Mar  9 15:54:36.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009042801s
Mar  9 15:54:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007116499s
Mar  9 15:54:40.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009688243s
Mar  9 15:54:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008503803s
Mar  9 15:54:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008868397s
Mar  9 15:54:46.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008955125s
Mar  9 15:54:48.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008238781s
Mar  9 15:54:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008881782s
Mar  9 15:54:52.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007776574s
Mar  9 15:54:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008782786s
Mar  9 15:54:56.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.0077414s
Mar  9 15:54:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007557064s
Mar  9 15:55:00.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00805706s
Mar  9 15:55:02.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008842656s
Mar  9 15:55:04.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009020867s
Mar  9 15:55:06.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007816323s
Mar  9 15:55:08.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007225446s
Mar  9 15:55:10.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008995249s
Mar  9 15:55:12.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008132719s
Mar  9 15:55:14.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007381008s
Mar  9 15:55:16.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00854568s
Mar  9 15:55:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007435627s
Mar  9 15:55:20.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007950178s
Mar  9 15:55:22.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009859354s
Mar  9 15:55:24.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007711826s
Mar  9 15:55:26.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009068096s
Mar  9 15:55:28.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007545512s
Mar  9 15:55:30.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006996072s
Mar  9 15:55:32.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.00910116s
Mar  9 15:55:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008580394s
Mar  9 15:55:36.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007715098s
Mar  9 15:55:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007592333s
Mar  9 15:55:40.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007680947s
Mar  9 15:55:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00806005s
Mar  9 15:55:44.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007870919s
Mar  9 15:55:46.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009556813s
Mar  9 15:55:48.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007870661s
Mar  9 15:55:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008793109s
Mar  9 15:55:52.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008458679s
Mar  9 15:55:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008849753s
Mar  9 15:55:56.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008418973s
Mar  9 15:55:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007414863s
Mar  9 15:56:00.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007555542s
Mar  9 15:56:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007843614s
Mar  9 15:56:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008619963s
Mar  9 15:56:06.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008828405s
Mar  9 15:56:08.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008533752s
Mar  9 15:56:10.304: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010720321s
Mar  9 15:56:12.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008792935s
Mar  9 15:56:14.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009336226s
Mar  9 15:56:16.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007474824s
Mar  9 15:56:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007433628s
Mar  9 15:56:20.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008081797s
Mar  9 15:56:22.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009425657s
Mar  9 15:56:22.306: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012330214s
STEP: removing the label kubernetes.io/e2e-ccd83a12-ee70-420c-baaa-a3dd71fbd8ba off the node tt-test-el8-003 03/09/23 15:56:22.306
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ccd83a12-ee70-420c-baaa-a3dd71fbd8ba 03/09/23 15:56:22.318
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  9 15:56:22.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7228" for this suite. 03/09/23 15:56:22.328
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":40,"skipped":755,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.146 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:51:18.187
    Mar  9 15:51:18.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-pred 03/09/23 15:51:18.189
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:51:18.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:51:18.203
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  9 15:51:18.206: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  9 15:51:18.212: INFO: Waiting for terminating namespaces to be deleted...
    Mar  9 15:51:18.215: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-003 before test
    Mar  9 15:51:18.223: INFO: calico-apiserver-7c747f5cd5-rb7jh from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container calico-apiserver ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: csi-node-driver-75b8t from calico-system started at 2023-03-09 03:23:36 +0000 UTC (2 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: host-test-container-pod from pod-network-test-2563 started at 2023-03-09 15:51:13 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container agnhost-container ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: netserver-0 from pod-network-test-2563 started at 2023-03-09 15:51:01 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container webserver ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: test-container-pod from pod-network-test-2563 started at 2023-03-09 15:51:13 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container webserver ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container e2e ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 15:51:18.223: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 15:51:18.223: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-004 before test
    Mar  9 15:51:18.230: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container calico-apiserver ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container webhook ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: netserver-1 from pod-network-test-2563 started at 2023-03-09 15:51:01 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container webserver ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 15:51:18.230: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 15:51:18.230: INFO: 	Container tigera-operator ready: true, restart count 3
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/09/23 15:51:18.23
    Mar  9 15:51:18.236: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7228" to be "running"
    Mar  9 15:51:18.241: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815735ms
    Mar  9 15:51:20.245: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009317617s
    Mar  9 15:51:20.245: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/09/23 15:51:20.248
    STEP: Trying to apply a random label on the found node. 03/09/23 15:51:20.259
    STEP: verifying the node has the label kubernetes.io/e2e-ccd83a12-ee70-420c-baaa-a3dd71fbd8ba 95 03/09/23 15:51:20.27
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/09/23 15:51:20.274
    Mar  9 15:51:20.279: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7228" to be "not pending"
    Mar  9 15:51:20.283: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065632ms
    Mar  9 15:51:22.287: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.007293268s
    Mar  9 15:51:22.287: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 100.100.230.140 on the node which pod4 resides and expect not scheduled 03/09/23 15:51:22.287
    Mar  9 15:51:22.293: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7228" to be "not pending"
    Mar  9 15:51:22.298: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09953ms
    Mar  9 15:51:24.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007880349s
    Mar  9 15:51:26.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00768021s
    Mar  9 15:51:28.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007421335s
    Mar  9 15:51:30.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008854499s
    Mar  9 15:51:32.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008090396s
    Mar  9 15:51:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008475948s
    Mar  9 15:51:36.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007673424s
    Mar  9 15:51:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007675257s
    Mar  9 15:51:40.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008742515s
    Mar  9 15:51:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008816818s
    Mar  9 15:51:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008100677s
    Mar  9 15:51:46.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007722631s
    Mar  9 15:51:48.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008248977s
    Mar  9 15:51:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008900356s
    Mar  9 15:51:52.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007910384s
    Mar  9 15:51:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008974413s
    Mar  9 15:51:56.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007567462s
    Mar  9 15:51:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00788428s
    Mar  9 15:52:00.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008147288s
    Mar  9 15:52:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007619293s
    Mar  9 15:52:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008879653s
    Mar  9 15:52:06.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008096925s
    Mar  9 15:52:08.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007863095s
    Mar  9 15:52:10.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008837322s
    Mar  9 15:52:12.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008463094s
    Mar  9 15:52:14.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007451829s
    Mar  9 15:52:16.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009036564s
    Mar  9 15:52:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007543064s
    Mar  9 15:52:20.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00733189s
    Mar  9 15:52:22.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008049442s
    Mar  9 15:52:24.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008156749s
    Mar  9 15:52:26.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007013926s
    Mar  9 15:52:28.300: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006859938s
    Mar  9 15:52:30.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008391002s
    Mar  9 15:52:32.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007465707s
    Mar  9 15:52:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008539333s
    Mar  9 15:52:36.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007887583s
    Mar  9 15:52:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007652949s
    Mar  9 15:52:40.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007885452s
    Mar  9 15:52:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008385529s
    Mar  9 15:52:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008120306s
    Mar  9 15:52:46.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00762495s
    Mar  9 15:52:48.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007787563s
    Mar  9 15:52:50.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009175436s
    Mar  9 15:52:52.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007286887s
    Mar  9 15:52:54.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007550129s
    Mar  9 15:52:56.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008087784s
    Mar  9 15:52:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007886192s
    Mar  9 15:53:00.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008693956s
    Mar  9 15:53:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007591681s
    Mar  9 15:53:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008357926s
    Mar  9 15:53:06.300: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006893546s
    Mar  9 15:53:08.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007863929s
    Mar  9 15:53:10.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007018092s
    Mar  9 15:53:12.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009106605s
    Mar  9 15:53:14.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008193318s
    Mar  9 15:53:16.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007441093s
    Mar  9 15:53:18.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008012461s
    Mar  9 15:53:20.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008744886s
    Mar  9 15:53:22.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008219297s
    Mar  9 15:53:24.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007921619s
    Mar  9 15:53:26.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.00886971s
    Mar  9 15:53:28.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008028788s
    Mar  9 15:53:30.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007477533s
    Mar  9 15:53:32.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009025103s
    Mar  9 15:53:34.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009044158s
    Mar  9 15:53:36.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008032856s
    Mar  9 15:53:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.00792134s
    Mar  9 15:53:40.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008626356s
    Mar  9 15:53:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008544791s
    Mar  9 15:53:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008530929s
    Mar  9 15:53:46.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008072543s
    Mar  9 15:53:48.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008268835s
    Mar  9 15:53:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008148459s
    Mar  9 15:53:52.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.009575456s
    Mar  9 15:53:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008889891s
    Mar  9 15:53:56.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008738461s
    Mar  9 15:53:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007835138s
    Mar  9 15:54:00.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007811673s
    Mar  9 15:54:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007710029s
    Mar  9 15:54:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008844145s
    Mar  9 15:54:06.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009181415s
    Mar  9 15:54:08.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.00805344s
    Mar  9 15:54:10.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009012335s
    Mar  9 15:54:12.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.007574047s
    Mar  9 15:54:14.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009051543s
    Mar  9 15:54:16.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007816498s
    Mar  9 15:54:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007196924s
    Mar  9 15:54:20.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009316981s
    Mar  9 15:54:22.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009661854s
    Mar  9 15:54:24.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008983005s
    Mar  9 15:54:26.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009333507s
    Mar  9 15:54:28.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007969315s
    Mar  9 15:54:30.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00896167s
    Mar  9 15:54:32.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007881232s
    Mar  9 15:54:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008187647s
    Mar  9 15:54:36.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009042801s
    Mar  9 15:54:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007116499s
    Mar  9 15:54:40.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009688243s
    Mar  9 15:54:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008503803s
    Mar  9 15:54:44.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008868397s
    Mar  9 15:54:46.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008955125s
    Mar  9 15:54:48.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008238781s
    Mar  9 15:54:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008881782s
    Mar  9 15:54:52.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007776574s
    Mar  9 15:54:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008782786s
    Mar  9 15:54:56.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.0077414s
    Mar  9 15:54:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007557064s
    Mar  9 15:55:00.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00805706s
    Mar  9 15:55:02.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008842656s
    Mar  9 15:55:04.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009020867s
    Mar  9 15:55:06.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007816323s
    Mar  9 15:55:08.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007225446s
    Mar  9 15:55:10.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.008995249s
    Mar  9 15:55:12.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008132719s
    Mar  9 15:55:14.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007381008s
    Mar  9 15:55:16.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00854568s
    Mar  9 15:55:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007435627s
    Mar  9 15:55:20.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007950178s
    Mar  9 15:55:22.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009859354s
    Mar  9 15:55:24.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007711826s
    Mar  9 15:55:26.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009068096s
    Mar  9 15:55:28.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007545512s
    Mar  9 15:55:30.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006996072s
    Mar  9 15:55:32.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.00910116s
    Mar  9 15:55:34.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008580394s
    Mar  9 15:55:36.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007715098s
    Mar  9 15:55:38.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007592333s
    Mar  9 15:55:40.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007680947s
    Mar  9 15:55:42.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00806005s
    Mar  9 15:55:44.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007870919s
    Mar  9 15:55:46.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009556813s
    Mar  9 15:55:48.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007870661s
    Mar  9 15:55:50.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008793109s
    Mar  9 15:55:52.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008458679s
    Mar  9 15:55:54.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008849753s
    Mar  9 15:55:56.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008418973s
    Mar  9 15:55:58.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007414863s
    Mar  9 15:56:00.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007555542s
    Mar  9 15:56:02.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007843614s
    Mar  9 15:56:04.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008619963s
    Mar  9 15:56:06.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008828405s
    Mar  9 15:56:08.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008533752s
    Mar  9 15:56:10.304: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010720321s
    Mar  9 15:56:12.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008792935s
    Mar  9 15:56:14.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009336226s
    Mar  9 15:56:16.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007474824s
    Mar  9 15:56:18.301: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007433628s
    Mar  9 15:56:20.302: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008081797s
    Mar  9 15:56:22.303: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009425657s
    Mar  9 15:56:22.306: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012330214s
    STEP: removing the label kubernetes.io/e2e-ccd83a12-ee70-420c-baaa-a3dd71fbd8ba off the node tt-test-el8-003 03/09/23 15:56:22.306
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ccd83a12-ee70-420c-baaa-a3dd71fbd8ba 03/09/23 15:56:22.318
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 15:56:22.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7228" for this suite. 03/09/23 15:56:22.328
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:22.338
Mar  9 15:56:22.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 15:56:22.339
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:22.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:22.362
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 15:56:22.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4125" for this suite. 03/09/23 15:56:22.406
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":41,"skipped":757,"failed":0}
------------------------------
â€¢ [0.072 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:22.338
    Mar  9 15:56:22.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 15:56:22.339
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:22.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:22.362
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 15:56:22.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4125" for this suite. 03/09/23 15:56:22.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:22.415
Mar  9 15:56:22.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 15:56:22.416
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:22.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:22.445
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 03/09/23 15:56:22.448
Mar  9 15:56:22.458: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875" in namespace "downward-api-1531" to be "Succeeded or Failed"
Mar  9 15:56:22.462: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875": Phase="Pending", Reason="", readiness=false. Elapsed: 4.527156ms
Mar  9 15:56:24.466: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00809462s
Mar  9 15:56:26.466: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008828727s
STEP: Saw pod success 03/09/23 15:56:26.467
Mar  9 15:56:26.467: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875" satisfied condition "Succeeded or Failed"
Mar  9 15:56:26.469: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875 container client-container: <nil>
STEP: delete the pod 03/09/23 15:56:26.482
Mar  9 15:56:26.492: INFO: Waiting for pod downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875 to disappear
Mar  9 15:56:26.494: INFO: Pod downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 15:56:26.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1531" for this suite. 03/09/23 15:56:26.497
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":42,"skipped":779,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:22.415
    Mar  9 15:56:22.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 15:56:22.416
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:22.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:22.445
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 03/09/23 15:56:22.448
    Mar  9 15:56:22.458: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875" in namespace "downward-api-1531" to be "Succeeded or Failed"
    Mar  9 15:56:22.462: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875": Phase="Pending", Reason="", readiness=false. Elapsed: 4.527156ms
    Mar  9 15:56:24.466: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00809462s
    Mar  9 15:56:26.466: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008828727s
    STEP: Saw pod success 03/09/23 15:56:26.467
    Mar  9 15:56:26.467: INFO: Pod "downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875" satisfied condition "Succeeded or Failed"
    Mar  9 15:56:26.469: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875 container client-container: <nil>
    STEP: delete the pod 03/09/23 15:56:26.482
    Mar  9 15:56:26.492: INFO: Waiting for pod downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875 to disappear
    Mar  9 15:56:26.494: INFO: Pod downwardapi-volume-87d01c45-b336-413c-bf85-2a25956bf875 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 15:56:26.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1531" for this suite. 03/09/23 15:56:26.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:26.503
Mar  9 15:56:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 15:56:26.505
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:26.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:26.517
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 03/09/23 15:56:26.52
Mar  9 15:56:26.526: INFO: Waiting up to 5m0s for pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9" in namespace "var-expansion-2860" to be "Succeeded or Failed"
Mar  9 15:56:26.529: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404769ms
Mar  9 15:56:28.533: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006106624s
Mar  9 15:56:30.534: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007268542s
STEP: Saw pod success 03/09/23 15:56:30.534
Mar  9 15:56:30.534: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9" satisfied condition "Succeeded or Failed"
Mar  9 15:56:30.536: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9 container dapi-container: <nil>
STEP: delete the pod 03/09/23 15:56:30.543
Mar  9 15:56:30.558: INFO: Waiting for pod var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9 to disappear
Mar  9 15:56:30.560: INFO: Pod var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 15:56:30.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2860" for this suite. 03/09/23 15:56:30.564
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":43,"skipped":785,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:26.503
    Mar  9 15:56:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 15:56:26.505
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:26.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:26.517
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 03/09/23 15:56:26.52
    Mar  9 15:56:26.526: INFO: Waiting up to 5m0s for pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9" in namespace "var-expansion-2860" to be "Succeeded or Failed"
    Mar  9 15:56:26.529: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404769ms
    Mar  9 15:56:28.533: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006106624s
    Mar  9 15:56:30.534: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007268542s
    STEP: Saw pod success 03/09/23 15:56:30.534
    Mar  9 15:56:30.534: INFO: Pod "var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9" satisfied condition "Succeeded or Failed"
    Mar  9 15:56:30.536: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9 container dapi-container: <nil>
    STEP: delete the pod 03/09/23 15:56:30.543
    Mar  9 15:56:30.558: INFO: Waiting for pod var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9 to disappear
    Mar  9 15:56:30.560: INFO: Pod var-expansion-07e9fd75-5bf0-47c1-82c8-f615cb5d78f9 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 15:56:30.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2860" for this suite. 03/09/23 15:56:30.564
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:30.569
Mar  9 15:56:30.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 15:56:30.57
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:30.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:30.584
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 03/09/23 15:56:30.586
Mar  9 15:56:30.594: INFO: Waiting up to 5m0s for pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64" in namespace "emptydir-5366" to be "Succeeded or Failed"
Mar  9 15:56:30.596: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523125ms
Mar  9 15:56:32.599: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005392523s
Mar  9 15:56:34.600: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006037786s
STEP: Saw pod success 03/09/23 15:56:34.6
Mar  9 15:56:34.600: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64" satisfied condition "Succeeded or Failed"
Mar  9 15:56:34.603: INFO: Trying to get logs from node tt-test-el8-003 pod pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64 container test-container: <nil>
STEP: delete the pod 03/09/23 15:56:34.608
Mar  9 15:56:34.618: INFO: Waiting for pod pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64 to disappear
Mar  9 15:56:34.620: INFO: Pod pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 15:56:34.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5366" for this suite. 03/09/23 15:56:34.623
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":44,"skipped":785,"failed":0}
------------------------------
â€¢ [4.059 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:30.569
    Mar  9 15:56:30.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 15:56:30.57
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:30.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:30.584
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/09/23 15:56:30.586
    Mar  9 15:56:30.594: INFO: Waiting up to 5m0s for pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64" in namespace "emptydir-5366" to be "Succeeded or Failed"
    Mar  9 15:56:30.596: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523125ms
    Mar  9 15:56:32.599: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005392523s
    Mar  9 15:56:34.600: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006037786s
    STEP: Saw pod success 03/09/23 15:56:34.6
    Mar  9 15:56:34.600: INFO: Pod "pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64" satisfied condition "Succeeded or Failed"
    Mar  9 15:56:34.603: INFO: Trying to get logs from node tt-test-el8-003 pod pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64 container test-container: <nil>
    STEP: delete the pod 03/09/23 15:56:34.608
    Mar  9 15:56:34.618: INFO: Waiting for pod pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64 to disappear
    Mar  9 15:56:34.620: INFO: Pod pod-bff3df5b-9d68-4f09-aa55-9a8fb1fb0d64 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 15:56:34.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5366" for this suite. 03/09/23 15:56:34.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:34.63
Mar  9 15:56:34.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 15:56:34.631
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:34.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:34.643
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/09/23 15:56:34.649
Mar  9 15:56:34.655: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8086" to be "running and ready"
Mar  9 15:56:34.660: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.387721ms
Mar  9 15:56:34.660: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:56:36.663: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008065037s
Mar  9 15:56:36.663: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  9 15:56:36.663: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 03/09/23 15:56:36.666
Mar  9 15:56:36.670: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8086" to be "running and ready"
Mar  9 15:56:36.674: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431227ms
Mar  9 15:56:36.674: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:56:38.677: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006679045s
Mar  9 15:56:38.677: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:56:40.678: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.00763147s
Mar  9 15:56:40.678: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar  9 15:56:40.678: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/09/23 15:56:40.68
Mar  9 15:56:40.686: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  9 15:56:40.689: INFO: Pod pod-with-prestop-http-hook still exists
Mar  9 15:56:42.690: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  9 15:56:42.693: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/09/23 15:56:42.693
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  9 15:56:42.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8086" for this suite. 03/09/23 15:56:42.71
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":45,"skipped":812,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.086 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:34.63
    Mar  9 15:56:34.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 15:56:34.631
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:34.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:34.643
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/09/23 15:56:34.649
    Mar  9 15:56:34.655: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8086" to be "running and ready"
    Mar  9 15:56:34.660: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.387721ms
    Mar  9 15:56:34.660: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:56:36.663: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008065037s
    Mar  9 15:56:36.663: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  9 15:56:36.663: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 03/09/23 15:56:36.666
    Mar  9 15:56:36.670: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8086" to be "running and ready"
    Mar  9 15:56:36.674: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431227ms
    Mar  9 15:56:36.674: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:56:38.677: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006679045s
    Mar  9 15:56:38.677: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:56:40.678: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.00763147s
    Mar  9 15:56:40.678: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar  9 15:56:40.678: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/09/23 15:56:40.68
    Mar  9 15:56:40.686: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  9 15:56:40.689: INFO: Pod pod-with-prestop-http-hook still exists
    Mar  9 15:56:42.690: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  9 15:56:42.693: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/09/23 15:56:42.693
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  9 15:56:42.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8086" for this suite. 03/09/23 15:56:42.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:42.717
Mar  9 15:56:42.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 15:56:42.718
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:42.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:42.733
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 03/09/23 15:56:42.736
STEP: Creating a ResourceQuota 03/09/23 15:56:47.738
STEP: Ensuring resource quota status is calculated 03/09/23 15:56:47.742
STEP: Creating a Service 03/09/23 15:56:49.747
STEP: Creating a NodePort Service 03/09/23 15:56:49.773
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/09/23 15:56:49.796
STEP: Ensuring resource quota status captures service creation 03/09/23 15:56:49.822
STEP: Deleting Services 03/09/23 15:56:51.826
STEP: Ensuring resource quota status released usage 03/09/23 15:56:51.881
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 15:56:53.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2775" for this suite. 03/09/23 15:56:53.888
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":46,"skipped":817,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.176 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:42.717
    Mar  9 15:56:42.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 15:56:42.718
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:42.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:42.733
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 03/09/23 15:56:42.736
    STEP: Creating a ResourceQuota 03/09/23 15:56:47.738
    STEP: Ensuring resource quota status is calculated 03/09/23 15:56:47.742
    STEP: Creating a Service 03/09/23 15:56:49.747
    STEP: Creating a NodePort Service 03/09/23 15:56:49.773
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/09/23 15:56:49.796
    STEP: Ensuring resource quota status captures service creation 03/09/23 15:56:49.822
    STEP: Deleting Services 03/09/23 15:56:51.826
    STEP: Ensuring resource quota status released usage 03/09/23 15:56:51.881
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 15:56:53.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2775" for this suite. 03/09/23 15:56:53.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:53.896
Mar  9 15:56:53.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename daemonsets 03/09/23 15:56:53.897
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:53.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:53.912
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 03/09/23 15:56:53.927
STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 15:56:53.931
Mar  9 15:56:53.934: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 15:56:53.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 15:56:53.937: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 15:56:54.941: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 15:56:54.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 15:56:54.944: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 15:56:55.942: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 15:56:55.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 15:56:55.945: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 03/09/23 15:56:55.947
STEP: DeleteCollection of the DaemonSets 03/09/23 15:56:55.95
STEP: Verify that ReplicaSets have been deleted 03/09/23 15:56:55.956
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar  9 15:56:55.967: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"92364"},"items":null}

Mar  9 15:56:55.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"92364"},"items":[{"metadata":{"name":"daemon-set-7q7n7","generateName":"daemon-set-","namespace":"daemonsets-1897","uid":"cd31f8f7-4c0f-4b02-9fe7-ca876c49cf5b","resourceVersion":"92358","creationTimestamp":"2023-03-09T15:56:53Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7b36c574d08ccdccab0829b6f639a3df74c9608d2d97819acee0e640cb2870bb","cni.projectcalico.org/podIP":"10.244.42.195/32","cni.projectcalico.org/podIPs":"10.244.42.195/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"33a34eff-f6b5-431f-80be-f5a9427baa40","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33a34eff-f6b5-431f-80be-f5a9427baa40\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qm7pv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qm7pv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tt-test-el8-003","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tt-test-el8-003"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"}],"hostIP":"100.100.230.140","podIP":"10.244.42.195","podIPs":[{"ip":"10.244.42.195"}],"startTime":"2023-03-09T15:56:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-09T15:56:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://113a77e94b83760e9ecee4111ddf4d68627098845157ad07f91aa824596f9b3c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fmlgb","generateName":"daemon-set-","namespace":"daemonsets-1897","uid":"45335b2f-2905-4251-bfbb-e27581f9ce3d","resourceVersion":"92356","creationTimestamp":"2023-03-09T15:56:53Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"dce6545db25b827f98a2c6d45fbe4535a061f191fdf034a35b8529afc43d9484","cni.projectcalico.org/podIP":"10.244.88.253/32","cni.projectcalico.org/podIPs":"10.244.88.253/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"33a34eff-f6b5-431f-80be-f5a9427baa40","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33a34eff-f6b5-431f-80be-f5a9427baa40\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tqccc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tqccc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tt-test-el8-004","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tt-test-el8-004"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"}],"hostIP":"100.100.231.104","podIP":"10.244.88.253","podIPs":[{"ip":"10.244.88.253"}],"startTime":"2023-03-09T15:56:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-09T15:56:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://6dbccd334db401a711806240525982a424d6865f9195df9cef87ddee29ba011f","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  9 15:56:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1897" for this suite. 03/09/23 15:56:55.987
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":47,"skipped":836,"failed":0}
------------------------------
â€¢ [2.096 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:53.896
    Mar  9 15:56:53.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename daemonsets 03/09/23 15:56:53.897
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:53.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:53.912
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 03/09/23 15:56:53.927
    STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 15:56:53.931
    Mar  9 15:56:53.934: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 15:56:53.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 15:56:53.937: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 15:56:54.941: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 15:56:54.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 15:56:54.944: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 15:56:55.942: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 15:56:55.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 15:56:55.945: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 03/09/23 15:56:55.947
    STEP: DeleteCollection of the DaemonSets 03/09/23 15:56:55.95
    STEP: Verify that ReplicaSets have been deleted 03/09/23 15:56:55.956
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Mar  9 15:56:55.967: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"92364"},"items":null}

    Mar  9 15:56:55.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"92364"},"items":[{"metadata":{"name":"daemon-set-7q7n7","generateName":"daemon-set-","namespace":"daemonsets-1897","uid":"cd31f8f7-4c0f-4b02-9fe7-ca876c49cf5b","resourceVersion":"92358","creationTimestamp":"2023-03-09T15:56:53Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7b36c574d08ccdccab0829b6f639a3df74c9608d2d97819acee0e640cb2870bb","cni.projectcalico.org/podIP":"10.244.42.195/32","cni.projectcalico.org/podIPs":"10.244.42.195/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"33a34eff-f6b5-431f-80be-f5a9427baa40","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33a34eff-f6b5-431f-80be-f5a9427baa40\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qm7pv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qm7pv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tt-test-el8-003","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tt-test-el8-003"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"}],"hostIP":"100.100.230.140","podIP":"10.244.42.195","podIPs":[{"ip":"10.244.42.195"}],"startTime":"2023-03-09T15:56:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-09T15:56:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://113a77e94b83760e9ecee4111ddf4d68627098845157ad07f91aa824596f9b3c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fmlgb","generateName":"daemon-set-","namespace":"daemonsets-1897","uid":"45335b2f-2905-4251-bfbb-e27581f9ce3d","resourceVersion":"92356","creationTimestamp":"2023-03-09T15:56:53Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"dce6545db25b827f98a2c6d45fbe4535a061f191fdf034a35b8529afc43d9484","cni.projectcalico.org/podIP":"10.244.88.253/32","cni.projectcalico.org/podIPs":"10.244.88.253/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"33a34eff-f6b5-431f-80be-f5a9427baa40","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33a34eff-f6b5-431f-80be-f5a9427baa40\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-09T15:56:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tqccc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tqccc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"tt-test-el8-004","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["tt-test-el8-004"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:55Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-09T15:56:53Z"}],"hostIP":"100.100.231.104","podIP":"10.244.88.253","podIPs":[{"ip":"10.244.88.253"}],"startTime":"2023-03-09T15:56:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-09T15:56:54Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://6dbccd334db401a711806240525982a424d6865f9195df9cef87ddee29ba011f","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 15:56:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1897" for this suite. 03/09/23 15:56:55.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:56:55.994
Mar  9 15:56:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-preemption 03/09/23 15:56:55.996
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:56.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:56.009
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  9 15:56:56.024: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  9 15:57:56.062: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:57:56.065
Mar  9 15:57:56.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-preemption-path 03/09/23 15:57:56.066
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:57:56.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:57:56.082
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 03/09/23 15:57:56.085
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/09/23 15:57:56.085
Mar  9 15:57:56.092: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-327" to be "running"
Mar  9 15:57:56.095: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412331ms
Mar  9 15:57:58.098: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006076326s
Mar  9 15:57:58.098: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/09/23 15:57:58.101
Mar  9 15:57:58.109: INFO: found a healthy node: tt-test-el8-003
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Mar  9 15:58:10.179: INFO: pods created so far: [1 1 1]
Mar  9 15:58:10.179: INFO: length of pods created so far: 3
Mar  9 15:58:12.187: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Mar  9 15:58:19.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-327" for this suite. 03/09/23 15:58:19.192
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  9 15:58:19.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7302" for this suite. 03/09/23 15:58:19.225
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":48,"skipped":843,"failed":0}
------------------------------
â€¢ [SLOW TEST] [83.265 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:56:55.994
    Mar  9 15:56:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-preemption 03/09/23 15:56:55.996
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:56:56.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:56:56.009
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  9 15:56:56.024: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  9 15:57:56.062: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:57:56.065
    Mar  9 15:57:56.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-preemption-path 03/09/23 15:57:56.066
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:57:56.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:57:56.082
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 03/09/23 15:57:56.085
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/09/23 15:57:56.085
    Mar  9 15:57:56.092: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-327" to be "running"
    Mar  9 15:57:56.095: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412331ms
    Mar  9 15:57:58.098: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006076326s
    Mar  9 15:57:58.098: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/09/23 15:57:58.101
    Mar  9 15:57:58.109: INFO: found a healthy node: tt-test-el8-003
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Mar  9 15:58:10.179: INFO: pods created so far: [1 1 1]
    Mar  9 15:58:10.179: INFO: length of pods created so far: 3
    Mar  9 15:58:12.187: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Mar  9 15:58:19.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-327" for this suite. 03/09/23 15:58:19.192
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 15:58:19.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7302" for this suite. 03/09/23 15:58:19.225
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:58:19.26
Mar  9 15:58:19.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 15:58:19.261
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:19.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:19.276
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/09/23 15:58:19.282
Mar  9 15:58:19.289: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6516" to be "running and ready"
Mar  9 15:58:19.292: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.136577ms
Mar  9 15:58:19.292: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:58:21.297: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007614037s
Mar  9 15:58:21.297: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  9 15:58:21.297: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 03/09/23 15:58:21.299
Mar  9 15:58:21.304: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6516" to be "running and ready"
Mar  9 15:58:21.307: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926243ms
Mar  9 15:58:21.307: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:58:23.311: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006329977s
Mar  9 15:58:23.311: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar  9 15:58:23.311: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/09/23 15:58:23.313
Mar  9 15:58:23.320: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  9 15:58:23.323: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  9 15:58:25.323: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  9 15:58:25.327: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  9 15:58:27.325: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  9 15:58:27.328: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/09/23 15:58:27.328
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  9 15:58:27.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6516" for this suite. 03/09/23 15:58:27.345
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":49,"skipped":844,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.090 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:58:19.26
    Mar  9 15:58:19.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 15:58:19.261
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:19.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:19.276
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/09/23 15:58:19.282
    Mar  9 15:58:19.289: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6516" to be "running and ready"
    Mar  9 15:58:19.292: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.136577ms
    Mar  9 15:58:19.292: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:58:21.297: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007614037s
    Mar  9 15:58:21.297: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  9 15:58:21.297: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 03/09/23 15:58:21.299
    Mar  9 15:58:21.304: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6516" to be "running and ready"
    Mar  9 15:58:21.307: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926243ms
    Mar  9 15:58:21.307: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:58:23.311: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006329977s
    Mar  9 15:58:23.311: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar  9 15:58:23.311: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/09/23 15:58:23.313
    Mar  9 15:58:23.320: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  9 15:58:23.323: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  9 15:58:25.323: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  9 15:58:25.327: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  9 15:58:27.325: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  9 15:58:27.328: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/09/23 15:58:27.328
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  9 15:58:27.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6516" for this suite. 03/09/23 15:58:27.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:58:27.351
Mar  9 15:58:27.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubelet-test 03/09/23 15:58:27.353
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:27.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:27.367
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/09/23 15:58:27.376
Mar  9 15:58:27.376: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3" in namespace "kubelet-test-2944" to be "completed"
Mar  9 15:58:27.379: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298335ms
Mar  9 15:58:29.383: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006468765s
Mar  9 15:58:31.384: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007263263s
Mar  9 15:58:31.384: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  9 15:58:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2944" for this suite. 03/09/23 15:58:31.401
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":50,"skipped":858,"failed":0}
------------------------------
â€¢ [4.056 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:58:27.351
    Mar  9 15:58:27.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubelet-test 03/09/23 15:58:27.353
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:27.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:27.367
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/09/23 15:58:27.376
    Mar  9 15:58:27.376: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3" in namespace "kubelet-test-2944" to be "completed"
    Mar  9 15:58:27.379: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298335ms
    Mar  9 15:58:29.383: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006468765s
    Mar  9 15:58:31.384: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007263263s
    Mar  9 15:58:31.384: INFO: Pod "agnhost-host-aliases70aca018-41a9-4080-b91f-b98b56cb3ba3" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  9 15:58:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2944" for this suite. 03/09/23 15:58:31.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:58:31.409
Mar  9 15:58:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 15:58:31.41
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:31.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:31.423
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-90ce0430-91f7-4678-babd-db378cb26ff2 03/09/23 15:58:31.426
STEP: Creating a pod to test consume secrets 03/09/23 15:58:31.43
Mar  9 15:58:31.437: INFO: Waiting up to 5m0s for pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76" in namespace "secrets-6762" to be "Succeeded or Failed"
Mar  9 15:58:31.440: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368458ms
Mar  9 15:58:33.445: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00719363s
Mar  9 15:58:35.444: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007026395s
STEP: Saw pod success 03/09/23 15:58:35.444
Mar  9 15:58:35.445: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76" satisfied condition "Succeeded or Failed"
Mar  9 15:58:35.447: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76 container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 15:58:35.452
Mar  9 15:58:35.462: INFO: Waiting for pod pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76 to disappear
Mar  9 15:58:35.465: INFO: Pod pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 15:58:35.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6762" for this suite. 03/09/23 15:58:35.468
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":51,"skipped":881,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:58:31.409
    Mar  9 15:58:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 15:58:31.41
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:31.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:31.423
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-90ce0430-91f7-4678-babd-db378cb26ff2 03/09/23 15:58:31.426
    STEP: Creating a pod to test consume secrets 03/09/23 15:58:31.43
    Mar  9 15:58:31.437: INFO: Waiting up to 5m0s for pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76" in namespace "secrets-6762" to be "Succeeded or Failed"
    Mar  9 15:58:31.440: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368458ms
    Mar  9 15:58:33.445: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00719363s
    Mar  9 15:58:35.444: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007026395s
    STEP: Saw pod success 03/09/23 15:58:35.444
    Mar  9 15:58:35.445: INFO: Pod "pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76" satisfied condition "Succeeded or Failed"
    Mar  9 15:58:35.447: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76 container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 15:58:35.452
    Mar  9 15:58:35.462: INFO: Waiting for pod pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76 to disappear
    Mar  9 15:58:35.465: INFO: Pod pod-secrets-331c304a-ac73-4996-97d9-e2dc7bd48c76 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 15:58:35.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6762" for this suite. 03/09/23 15:58:35.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:58:35.476
Mar  9 15:58:35.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename endpointslice 03/09/23 15:58:35.477
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:35.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:35.491
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Mar  9 15:58:35.501: INFO: Endpoints addresses: [100.100.231.202] , ports: [6443]
Mar  9 15:58:35.501: INFO: EndpointSlices addresses: [100.100.231.202] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  9 15:58:35.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5653" for this suite. 03/09/23 15:58:35.505
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":52,"skipped":917,"failed":0}
------------------------------
â€¢ [0.033 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:58:35.476
    Mar  9 15:58:35.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename endpointslice 03/09/23 15:58:35.477
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:35.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:35.491
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Mar  9 15:58:35.501: INFO: Endpoints addresses: [100.100.231.202] , ports: [6443]
    Mar  9 15:58:35.501: INFO: EndpointSlices addresses: [100.100.231.202] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  9 15:58:35.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5653" for this suite. 03/09/23 15:58:35.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:58:35.51
Mar  9 15:58:35.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 15:58:35.512
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:35.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:35.527
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Mar  9 15:58:35.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/09/23 15:58:38.396
Mar  9 15:58:38.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
Mar  9 15:58:39.355: INFO: stderr: ""
Mar  9 15:58:39.355: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  9 15:58:39.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 delete e2e-test-crd-publish-openapi-5982-crds test-foo'
Mar  9 15:58:39.428: INFO: stderr: ""
Mar  9 15:58:39.428: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  9 15:58:39.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 apply -f -'
Mar  9 15:58:39.692: INFO: stderr: ""
Mar  9 15:58:39.692: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  9 15:58:39.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 delete e2e-test-crd-publish-openapi-5982-crds test-foo'
Mar  9 15:58:39.767: INFO: stderr: ""
Mar  9 15:58:39.767: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/09/23 15:58:39.767
Mar  9 15:58:39.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
Mar  9 15:58:40.016: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/09/23 15:58:40.016
Mar  9 15:58:40.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
Mar  9 15:58:40.265: INFO: rc: 1
Mar  9 15:58:40.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 apply -f -'
Mar  9 15:58:40.511: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/09/23 15:58:40.511
Mar  9 15:58:40.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
Mar  9 15:58:40.767: INFO: rc: 1
Mar  9 15:58:40.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 apply -f -'
Mar  9 15:58:41.047: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/09/23 15:58:41.047
Mar  9 15:58:41.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds'
Mar  9 15:58:41.293: INFO: stderr: ""
Mar  9 15:58:41.293: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/09/23 15:58:41.293
Mar  9 15:58:41.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.metadata'
Mar  9 15:58:41.550: INFO: stderr: ""
Mar  9 15:58:41.550: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  9 15:58:41.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.spec'
Mar  9 15:58:41.790: INFO: stderr: ""
Mar  9 15:58:41.790: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  9 15:58:41.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.spec.bars'
Mar  9 15:58:42.037: INFO: stderr: ""
Mar  9 15:58:42.037: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/09/23 15:58:42.037
Mar  9 15:58:42.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.spec.bars2'
Mar  9 15:58:42.295: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 15:58:45.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9772" for this suite. 03/09/23 15:58:45.15
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":53,"skipped":931,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.645 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:58:35.51
    Mar  9 15:58:35.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 15:58:35.512
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:35.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:35.527
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Mar  9 15:58:35.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/09/23 15:58:38.396
    Mar  9 15:58:38.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
    Mar  9 15:58:39.355: INFO: stderr: ""
    Mar  9 15:58:39.355: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  9 15:58:39.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 delete e2e-test-crd-publish-openapi-5982-crds test-foo'
    Mar  9 15:58:39.428: INFO: stderr: ""
    Mar  9 15:58:39.428: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar  9 15:58:39.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 apply -f -'
    Mar  9 15:58:39.692: INFO: stderr: ""
    Mar  9 15:58:39.692: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  9 15:58:39.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 delete e2e-test-crd-publish-openapi-5982-crds test-foo'
    Mar  9 15:58:39.767: INFO: stderr: ""
    Mar  9 15:58:39.767: INFO: stdout: "e2e-test-crd-publish-openapi-5982-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/09/23 15:58:39.767
    Mar  9 15:58:39.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
    Mar  9 15:58:40.016: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/09/23 15:58:40.016
    Mar  9 15:58:40.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
    Mar  9 15:58:40.265: INFO: rc: 1
    Mar  9 15:58:40.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 apply -f -'
    Mar  9 15:58:40.511: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/09/23 15:58:40.511
    Mar  9 15:58:40.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 create -f -'
    Mar  9 15:58:40.767: INFO: rc: 1
    Mar  9 15:58:40.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 --namespace=crd-publish-openapi-9772 apply -f -'
    Mar  9 15:58:41.047: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/09/23 15:58:41.047
    Mar  9 15:58:41.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds'
    Mar  9 15:58:41.293: INFO: stderr: ""
    Mar  9 15:58:41.293: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/09/23 15:58:41.293
    Mar  9 15:58:41.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.metadata'
    Mar  9 15:58:41.550: INFO: stderr: ""
    Mar  9 15:58:41.550: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar  9 15:58:41.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.spec'
    Mar  9 15:58:41.790: INFO: stderr: ""
    Mar  9 15:58:41.790: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar  9 15:58:41.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.spec.bars'
    Mar  9 15:58:42.037: INFO: stderr: ""
    Mar  9 15:58:42.037: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5982-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/09/23 15:58:42.037
    Mar  9 15:58:42.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9772 explain e2e-test-crd-publish-openapi-5982-crds.spec.bars2'
    Mar  9 15:58:42.295: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 15:58:45.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9772" for this suite. 03/09/23 15:58:45.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:58:45.157
Mar  9 15:58:45.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 15:58:45.158
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:45.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:45.172
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-29ffb578-1b3e-40ba-88e5-0c5cde4ddf50 03/09/23 15:58:45.22
STEP: Creating secret with name secret-projected-all-test-volume-f75c29e3-ef12-4a74-bff9-0c2ff1f58ae2 03/09/23 15:58:45.224
STEP: Creating a pod to test Check all projections for projected volume plugin 03/09/23 15:58:45.228
Mar  9 15:58:45.234: INFO: Waiting up to 5m0s for pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e" in namespace "projected-4932" to be "Succeeded or Failed"
Mar  9 15:58:45.237: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.502249ms
Mar  9 15:58:47.241: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006998387s
Mar  9 15:58:49.242: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007412997s
STEP: Saw pod success 03/09/23 15:58:49.242
Mar  9 15:58:49.242: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e" satisfied condition "Succeeded or Failed"
Mar  9 15:58:49.245: INFO: Trying to get logs from node tt-test-el8-003 pod projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e container projected-all-volume-test: <nil>
STEP: delete the pod 03/09/23 15:58:49.25
Mar  9 15:58:49.258: INFO: Waiting for pod projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e to disappear
Mar  9 15:58:49.261: INFO: Pod projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Mar  9 15:58:49.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4932" for this suite. 03/09/23 15:58:49.264
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":54,"skipped":947,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:58:45.157
    Mar  9 15:58:45.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 15:58:45.158
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:45.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:45.172
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-29ffb578-1b3e-40ba-88e5-0c5cde4ddf50 03/09/23 15:58:45.22
    STEP: Creating secret with name secret-projected-all-test-volume-f75c29e3-ef12-4a74-bff9-0c2ff1f58ae2 03/09/23 15:58:45.224
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/09/23 15:58:45.228
    Mar  9 15:58:45.234: INFO: Waiting up to 5m0s for pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e" in namespace "projected-4932" to be "Succeeded or Failed"
    Mar  9 15:58:45.237: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.502249ms
    Mar  9 15:58:47.241: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006998387s
    Mar  9 15:58:49.242: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007412997s
    STEP: Saw pod success 03/09/23 15:58:49.242
    Mar  9 15:58:49.242: INFO: Pod "projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e" satisfied condition "Succeeded or Failed"
    Mar  9 15:58:49.245: INFO: Trying to get logs from node tt-test-el8-003 pod projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e container projected-all-volume-test: <nil>
    STEP: delete the pod 03/09/23 15:58:49.25
    Mar  9 15:58:49.258: INFO: Waiting for pod projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e to disappear
    Mar  9 15:58:49.261: INFO: Pod projected-volume-a9754c5c-7a1b-4642-9877-e1caa642f04e no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Mar  9 15:58:49.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4932" for this suite. 03/09/23 15:58:49.264
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:58:49.27
Mar  9 15:58:49.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pod-network-test 03/09/23 15:58:49.271
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:49.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:49.287
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-206 03/09/23 15:58:49.289
STEP: creating a selector 03/09/23 15:58:49.29
STEP: Creating the service pods in kubernetes 03/09/23 15:58:49.29
Mar  9 15:58:49.290: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  9 15:58:49.309: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-206" to be "running and ready"
Mar  9 15:58:49.313: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355349ms
Mar  9 15:58:49.313: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 15:58:51.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008479063s
Mar  9 15:58:51.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:58:53.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007558246s
Mar  9 15:58:53.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:58:55.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007333076s
Mar  9 15:58:55.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:58:57.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007204997s
Mar  9 15:58:57.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:58:59.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007217154s
Mar  9 15:58:59.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:59:01.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007572135s
Mar  9 15:59:01.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:59:03.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007481193s
Mar  9 15:59:03.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:59:05.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.007087171s
Mar  9 15:59:05.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:59:07.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008046008s
Mar  9 15:59:07.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:59:09.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008653587s
Mar  9 15:59:09.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 15:59:11.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008300655s
Mar  9 15:59:11.318: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  9 15:59:11.318: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  9 15:59:11.320: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-206" to be "running and ready"
Mar  9 15:59:11.323: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.50725ms
Mar  9 15:59:11.323: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  9 15:59:11.323: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/09/23 15:59:11.325
Mar  9 15:59:11.329: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-206" to be "running"
Mar  9 15:59:11.331: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080321ms
Mar  9 15:59:13.335: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005973295s
Mar  9 15:59:13.335: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  9 15:59:13.338: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar  9 15:59:13.338: INFO: Breadth first check of 10.244.42.204 on host 100.100.230.140...
Mar  9 15:59:13.340: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.232:9080/dial?request=hostname&protocol=http&host=10.244.42.204&port=8083&tries=1'] Namespace:pod-network-test-206 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:59:13.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:59:13.341: INFO: ExecWithOptions: Clientset creation
Mar  9 15:59:13.341: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-206/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.42.204%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  9 15:59:13.419: INFO: Waiting for responses: map[]
Mar  9 15:59:13.419: INFO: reached 10.244.42.204 after 0/1 tries
Mar  9 15:59:13.419: INFO: Breadth first check of 10.244.88.255 on host 100.100.231.104...
Mar  9 15:59:13.422: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.232:9080/dial?request=hostname&protocol=http&host=10.244.88.255&port=8083&tries=1'] Namespace:pod-network-test-206 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 15:59:13.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 15:59:13.423: INFO: ExecWithOptions: Clientset creation
Mar  9 15:59:13.423: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-206/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.88.255%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  9 15:59:13.505: INFO: Waiting for responses: map[]
Mar  9 15:59:13.505: INFO: reached 10.244.88.255 after 0/1 tries
Mar  9 15:59:13.505: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  9 15:59:13.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-206" for this suite. 03/09/23 15:59:13.509
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":55,"skipped":947,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.244 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:58:49.27
    Mar  9 15:58:49.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pod-network-test 03/09/23 15:58:49.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:58:49.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:58:49.287
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-206 03/09/23 15:58:49.289
    STEP: creating a selector 03/09/23 15:58:49.29
    STEP: Creating the service pods in kubernetes 03/09/23 15:58:49.29
    Mar  9 15:58:49.290: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  9 15:58:49.309: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-206" to be "running and ready"
    Mar  9 15:58:49.313: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355349ms
    Mar  9 15:58:49.313: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 15:58:51.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008479063s
    Mar  9 15:58:51.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:58:53.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007558246s
    Mar  9 15:58:53.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:58:55.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007333076s
    Mar  9 15:58:55.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:58:57.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007204997s
    Mar  9 15:58:57.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:58:59.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007217154s
    Mar  9 15:58:59.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:59:01.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007572135s
    Mar  9 15:59:01.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:59:03.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007481193s
    Mar  9 15:59:03.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:59:05.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.007087171s
    Mar  9 15:59:05.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:59:07.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008046008s
    Mar  9 15:59:07.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:59:09.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008653587s
    Mar  9 15:59:09.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 15:59:11.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008300655s
    Mar  9 15:59:11.318: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  9 15:59:11.318: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  9 15:59:11.320: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-206" to be "running and ready"
    Mar  9 15:59:11.323: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.50725ms
    Mar  9 15:59:11.323: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  9 15:59:11.323: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/09/23 15:59:11.325
    Mar  9 15:59:11.329: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-206" to be "running"
    Mar  9 15:59:11.331: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080321ms
    Mar  9 15:59:13.335: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005973295s
    Mar  9 15:59:13.335: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  9 15:59:13.338: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar  9 15:59:13.338: INFO: Breadth first check of 10.244.42.204 on host 100.100.230.140...
    Mar  9 15:59:13.340: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.232:9080/dial?request=hostname&protocol=http&host=10.244.42.204&port=8083&tries=1'] Namespace:pod-network-test-206 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:59:13.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:59:13.341: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:59:13.341: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-206/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.42.204%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  9 15:59:13.419: INFO: Waiting for responses: map[]
    Mar  9 15:59:13.419: INFO: reached 10.244.42.204 after 0/1 tries
    Mar  9 15:59:13.419: INFO: Breadth first check of 10.244.88.255 on host 100.100.231.104...
    Mar  9 15:59:13.422: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.232:9080/dial?request=hostname&protocol=http&host=10.244.88.255&port=8083&tries=1'] Namespace:pod-network-test-206 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 15:59:13.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 15:59:13.423: INFO: ExecWithOptions: Clientset creation
    Mar  9 15:59:13.423: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-206/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.88.255%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  9 15:59:13.505: INFO: Waiting for responses: map[]
    Mar  9 15:59:13.505: INFO: reached 10.244.88.255 after 0/1 tries
    Mar  9 15:59:13.505: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  9 15:59:13.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-206" for this suite. 03/09/23 15:59:13.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:59:13.515
Mar  9 15:59:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 15:59:13.517
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:13.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:13.531
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 03/09/23 15:59:13.534
Mar  9 15:59:13.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce" in namespace "downward-api-7438" to be "Succeeded or Failed"
Mar  9 15:59:13.544: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698914ms
Mar  9 15:59:15.549: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007090225s
Mar  9 15:59:17.548: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006476926s
STEP: Saw pod success 03/09/23 15:59:17.548
Mar  9 15:59:17.548: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce" satisfied condition "Succeeded or Failed"
Mar  9 15:59:17.551: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce container client-container: <nil>
STEP: delete the pod 03/09/23 15:59:17.556
Mar  9 15:59:17.564: INFO: Waiting for pod downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce to disappear
Mar  9 15:59:17.566: INFO: Pod downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 15:59:17.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7438" for this suite. 03/09/23 15:59:17.57
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":56,"skipped":967,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:59:13.515
    Mar  9 15:59:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 15:59:13.517
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:13.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:13.531
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 03/09/23 15:59:13.534
    Mar  9 15:59:13.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce" in namespace "downward-api-7438" to be "Succeeded or Failed"
    Mar  9 15:59:13.544: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698914ms
    Mar  9 15:59:15.549: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007090225s
    Mar  9 15:59:17.548: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006476926s
    STEP: Saw pod success 03/09/23 15:59:17.548
    Mar  9 15:59:17.548: INFO: Pod "downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce" satisfied condition "Succeeded or Failed"
    Mar  9 15:59:17.551: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce container client-container: <nil>
    STEP: delete the pod 03/09/23 15:59:17.556
    Mar  9 15:59:17.564: INFO: Waiting for pod downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce to disappear
    Mar  9 15:59:17.566: INFO: Pod downwardapi-volume-5ace194d-1e24-443a-a69c-5c33ee0ce0ce no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 15:59:17.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7438" for this suite. 03/09/23 15:59:17.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:59:17.579
Mar  9 15:59:17.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 15:59:17.58
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:17.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:17.594
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 03/09/23 15:59:17.597
STEP: setting up watch 03/09/23 15:59:17.597
STEP: submitting the pod to kubernetes 03/09/23 15:59:17.7
STEP: verifying the pod is in kubernetes 03/09/23 15:59:17.711
STEP: verifying pod creation was observed 03/09/23 15:59:17.714
Mar  9 15:59:17.714: INFO: Waiting up to 5m0s for pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e" in namespace "pods-8000" to be "running"
Mar  9 15:59:17.717: INFO: Pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.47339ms
Mar  9 15:59:19.720: INFO: Pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005955455s
Mar  9 15:59:19.720: INFO: Pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e" satisfied condition "running"
STEP: deleting the pod gracefully 03/09/23 15:59:19.723
STEP: verifying pod deletion was observed 03/09/23 15:59:19.731
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 15:59:21.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8000" for this suite. 03/09/23 15:59:21.602
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":57,"skipped":1007,"failed":0}
------------------------------
â€¢ [4.029 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:59:17.579
    Mar  9 15:59:17.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 15:59:17.58
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:17.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:17.594
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 03/09/23 15:59:17.597
    STEP: setting up watch 03/09/23 15:59:17.597
    STEP: submitting the pod to kubernetes 03/09/23 15:59:17.7
    STEP: verifying the pod is in kubernetes 03/09/23 15:59:17.711
    STEP: verifying pod creation was observed 03/09/23 15:59:17.714
    Mar  9 15:59:17.714: INFO: Waiting up to 5m0s for pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e" in namespace "pods-8000" to be "running"
    Mar  9 15:59:17.717: INFO: Pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.47339ms
    Mar  9 15:59:19.720: INFO: Pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005955455s
    Mar  9 15:59:19.720: INFO: Pod "pod-submit-remove-cc6ba9e7-bc09-4d17-8a79-0442a5946a9e" satisfied condition "running"
    STEP: deleting the pod gracefully 03/09/23 15:59:19.723
    STEP: verifying pod deletion was observed 03/09/23 15:59:19.731
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 15:59:21.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8000" for this suite. 03/09/23 15:59:21.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:59:21.609
Mar  9 15:59:21.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 15:59:21.61
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:21.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:21.624
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/09/23 15:59:21.627
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5539.svc.cluster.local;sleep 1; done
 03/09/23 15:59:21.633
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5539.svc.cluster.local;sleep 1; done
 03/09/23 15:59:21.633
STEP: creating a pod to probe DNS 03/09/23 15:59:21.633
STEP: submitting the pod to kubernetes 03/09/23 15:59:21.633
Mar  9 15:59:21.642: INFO: Waiting up to 15m0s for pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155" in namespace "dns-5539" to be "running"
Mar  9 15:59:21.644: INFO: Pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.480744ms
Mar  9 15:59:23.649: INFO: Pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155": Phase="Running", Reason="", readiness=true. Elapsed: 2.007518801s
Mar  9 15:59:23.649: INFO: Pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155" satisfied condition "running"
STEP: retrieving the pod 03/09/23 15:59:23.65
STEP: looking for the results for each expected name from probers 03/09/23 15:59:23.652
Mar  9 15:59:23.657: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.661: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.665: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.668: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.674: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.677: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.680: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:23.680: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5539.svc.cluster.local]

Mar  9 15:59:28.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:28.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:28.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:28.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:28.704: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

Mar  9 15:59:33.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:33.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:33.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:33.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:33.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

Mar  9 15:59:38.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:38.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:38.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:38.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:38.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

Mar  9 15:59:43.685: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:43.688: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:43.697: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:43.700: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:43.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

Mar  9 15:59:48.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:48.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:48.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:48.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
Mar  9 15:59:48.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

Mar  9 15:59:53.705: INFO: DNS probes using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 succeeded

STEP: deleting the pod 03/09/23 15:59:53.705
STEP: deleting the test headless service 03/09/23 15:59:53.718
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 15:59:53.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5539" for this suite. 03/09/23 15:59:53.743
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":58,"skipped":1012,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.141 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:59:21.609
    Mar  9 15:59:21.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 15:59:21.61
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:21.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:21.624
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/09/23 15:59:21.627
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5539.svc.cluster.local;sleep 1; done
     03/09/23 15:59:21.633
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5539.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5539.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5539.svc.cluster.local;sleep 1; done
     03/09/23 15:59:21.633
    STEP: creating a pod to probe DNS 03/09/23 15:59:21.633
    STEP: submitting the pod to kubernetes 03/09/23 15:59:21.633
    Mar  9 15:59:21.642: INFO: Waiting up to 15m0s for pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155" in namespace "dns-5539" to be "running"
    Mar  9 15:59:21.644: INFO: Pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.480744ms
    Mar  9 15:59:23.649: INFO: Pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155": Phase="Running", Reason="", readiness=true. Elapsed: 2.007518801s
    Mar  9 15:59:23.649: INFO: Pod "dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 15:59:23.65
    STEP: looking for the results for each expected name from probers 03/09/23 15:59:23.652
    Mar  9 15:59:23.657: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.661: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.665: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.668: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.674: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.677: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.680: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:23.680: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5539.svc.cluster.local]

    Mar  9 15:59:28.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:28.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:28.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:28.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:28.704: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

    Mar  9 15:59:33.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:33.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:33.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:33.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:33.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

    Mar  9 15:59:38.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:38.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:38.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:38.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:38.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

    Mar  9 15:59:43.685: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:43.688: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:43.697: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:43.700: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:43.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

    Mar  9 15:59:48.684: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:48.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:48.696: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:48.699: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local from pod dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155: the server could not find the requested resource (get pods dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155)
    Mar  9 15:59:48.705: INFO: Lookups using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5539.svc.cluster.local]

    Mar  9 15:59:53.705: INFO: DNS probes using dns-5539/dns-test-51d616a2-406d-46bc-a54d-3ffde75e0155 succeeded

    STEP: deleting the pod 03/09/23 15:59:53.705
    STEP: deleting the test headless service 03/09/23 15:59:53.718
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 15:59:53.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5539" for this suite. 03/09/23 15:59:53.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 15:59:53.757
Mar  9 15:59:53.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 15:59:53.759
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:53.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:53.774
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 16:00:53.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2239" for this suite. 03/09/23 16:00:53.793
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":59,"skipped":1032,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.041 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 15:59:53.757
    Mar  9 15:59:53.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 15:59:53.759
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 15:59:53.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 15:59:53.774
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 16:00:53.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2239" for this suite. 03/09/23 16:00:53.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:00:53.802
Mar  9 16:00:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename discovery 03/09/23 16:00:53.804
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:00:53.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:00:53.819
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/09/23 16:00:53.824
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar  9 16:00:54.266: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  9 16:00:54.267: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  9 16:00:54.267: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar  9 16:00:54.267: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  9 16:00:54.267: INFO: Checking APIGroup: apps
Mar  9 16:00:54.268: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  9 16:00:54.268: INFO: Versions found [{apps/v1 v1}]
Mar  9 16:00:54.268: INFO: apps/v1 matches apps/v1
Mar  9 16:00:54.268: INFO: Checking APIGroup: events.k8s.io
Mar  9 16:00:54.269: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  9 16:00:54.269: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar  9 16:00:54.269: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  9 16:00:54.269: INFO: Checking APIGroup: authentication.k8s.io
Mar  9 16:00:54.270: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  9 16:00:54.270: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar  9 16:00:54.270: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  9 16:00:54.270: INFO: Checking APIGroup: authorization.k8s.io
Mar  9 16:00:54.271: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  9 16:00:54.271: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar  9 16:00:54.271: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  9 16:00:54.271: INFO: Checking APIGroup: autoscaling
Mar  9 16:00:54.272: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar  9 16:00:54.272: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Mar  9 16:00:54.272: INFO: autoscaling/v2 matches autoscaling/v2
Mar  9 16:00:54.272: INFO: Checking APIGroup: batch
Mar  9 16:00:54.273: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  9 16:00:54.273: INFO: Versions found [{batch/v1 v1}]
Mar  9 16:00:54.273: INFO: batch/v1 matches batch/v1
Mar  9 16:00:54.273: INFO: Checking APIGroup: certificates.k8s.io
Mar  9 16:00:54.274: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  9 16:00:54.274: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar  9 16:00:54.274: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  9 16:00:54.274: INFO: Checking APIGroup: networking.k8s.io
Mar  9 16:00:54.275: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  9 16:00:54.275: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar  9 16:00:54.275: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  9 16:00:54.275: INFO: Checking APIGroup: policy
Mar  9 16:00:54.276: INFO: PreferredVersion.GroupVersion: policy/v1
Mar  9 16:00:54.276: INFO: Versions found [{policy/v1 v1}]
Mar  9 16:00:54.276: INFO: policy/v1 matches policy/v1
Mar  9 16:00:54.276: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  9 16:00:54.277: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  9 16:00:54.277: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar  9 16:00:54.277: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  9 16:00:54.277: INFO: Checking APIGroup: storage.k8s.io
Mar  9 16:00:54.278: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  9 16:00:54.278: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  9 16:00:54.278: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  9 16:00:54.278: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  9 16:00:54.279: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  9 16:00:54.279: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar  9 16:00:54.279: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  9 16:00:54.279: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  9 16:00:54.280: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  9 16:00:54.280: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar  9 16:00:54.280: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  9 16:00:54.280: INFO: Checking APIGroup: scheduling.k8s.io
Mar  9 16:00:54.281: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  9 16:00:54.281: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar  9 16:00:54.281: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  9 16:00:54.281: INFO: Checking APIGroup: coordination.k8s.io
Mar  9 16:00:54.282: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  9 16:00:54.282: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar  9 16:00:54.282: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  9 16:00:54.282: INFO: Checking APIGroup: node.k8s.io
Mar  9 16:00:54.283: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  9 16:00:54.283: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar  9 16:00:54.283: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  9 16:00:54.283: INFO: Checking APIGroup: discovery.k8s.io
Mar  9 16:00:54.284: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar  9 16:00:54.284: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar  9 16:00:54.284: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar  9 16:00:54.284: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  9 16:00:54.285: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar  9 16:00:54.285: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  9 16:00:54.285: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar  9 16:00:54.285: INFO: Checking APIGroup: projectcalico.org
Mar  9 16:00:54.286: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
Mar  9 16:00:54.286: INFO: Versions found [{projectcalico.org/v3 v3}]
Mar  9 16:00:54.286: INFO: projectcalico.org/v3 matches projectcalico.org/v3
Mar  9 16:00:54.286: INFO: Checking APIGroup: crd.projectcalico.org
Mar  9 16:00:54.287: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  9 16:00:54.287: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  9 16:00:54.287: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  9 16:00:54.287: INFO: Checking APIGroup: operator.tigera.io
Mar  9 16:00:54.288: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Mar  9 16:00:54.288: INFO: Versions found [{operator.tigera.io/v1 v1}]
Mar  9 16:00:54.288: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Mar  9 16:00:54.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9427" for this suite. 03/09/23 16:00:54.292
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":60,"skipped":1079,"failed":0}
------------------------------
â€¢ [0.494 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:00:53.802
    Mar  9 16:00:53.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename discovery 03/09/23 16:00:53.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:00:53.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:00:53.819
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/09/23 16:00:53.824
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar  9 16:00:54.266: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar  9 16:00:54.267: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar  9 16:00:54.267: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar  9 16:00:54.267: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar  9 16:00:54.267: INFO: Checking APIGroup: apps
    Mar  9 16:00:54.268: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar  9 16:00:54.268: INFO: Versions found [{apps/v1 v1}]
    Mar  9 16:00:54.268: INFO: apps/v1 matches apps/v1
    Mar  9 16:00:54.268: INFO: Checking APIGroup: events.k8s.io
    Mar  9 16:00:54.269: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar  9 16:00:54.269: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar  9 16:00:54.269: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar  9 16:00:54.269: INFO: Checking APIGroup: authentication.k8s.io
    Mar  9 16:00:54.270: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar  9 16:00:54.270: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar  9 16:00:54.270: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar  9 16:00:54.270: INFO: Checking APIGroup: authorization.k8s.io
    Mar  9 16:00:54.271: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar  9 16:00:54.271: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar  9 16:00:54.271: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar  9 16:00:54.271: INFO: Checking APIGroup: autoscaling
    Mar  9 16:00:54.272: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar  9 16:00:54.272: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Mar  9 16:00:54.272: INFO: autoscaling/v2 matches autoscaling/v2
    Mar  9 16:00:54.272: INFO: Checking APIGroup: batch
    Mar  9 16:00:54.273: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar  9 16:00:54.273: INFO: Versions found [{batch/v1 v1}]
    Mar  9 16:00:54.273: INFO: batch/v1 matches batch/v1
    Mar  9 16:00:54.273: INFO: Checking APIGroup: certificates.k8s.io
    Mar  9 16:00:54.274: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar  9 16:00:54.274: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar  9 16:00:54.274: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar  9 16:00:54.274: INFO: Checking APIGroup: networking.k8s.io
    Mar  9 16:00:54.275: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar  9 16:00:54.275: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar  9 16:00:54.275: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar  9 16:00:54.275: INFO: Checking APIGroup: policy
    Mar  9 16:00:54.276: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar  9 16:00:54.276: INFO: Versions found [{policy/v1 v1}]
    Mar  9 16:00:54.276: INFO: policy/v1 matches policy/v1
    Mar  9 16:00:54.276: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar  9 16:00:54.277: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar  9 16:00:54.277: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar  9 16:00:54.277: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar  9 16:00:54.277: INFO: Checking APIGroup: storage.k8s.io
    Mar  9 16:00:54.278: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar  9 16:00:54.278: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar  9 16:00:54.278: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar  9 16:00:54.278: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar  9 16:00:54.279: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar  9 16:00:54.279: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar  9 16:00:54.279: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar  9 16:00:54.279: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar  9 16:00:54.280: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar  9 16:00:54.280: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar  9 16:00:54.280: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar  9 16:00:54.280: INFO: Checking APIGroup: scheduling.k8s.io
    Mar  9 16:00:54.281: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar  9 16:00:54.281: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar  9 16:00:54.281: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar  9 16:00:54.281: INFO: Checking APIGroup: coordination.k8s.io
    Mar  9 16:00:54.282: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar  9 16:00:54.282: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar  9 16:00:54.282: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar  9 16:00:54.282: INFO: Checking APIGroup: node.k8s.io
    Mar  9 16:00:54.283: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar  9 16:00:54.283: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar  9 16:00:54.283: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar  9 16:00:54.283: INFO: Checking APIGroup: discovery.k8s.io
    Mar  9 16:00:54.284: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar  9 16:00:54.284: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar  9 16:00:54.284: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar  9 16:00:54.284: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar  9 16:00:54.285: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Mar  9 16:00:54.285: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Mar  9 16:00:54.285: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Mar  9 16:00:54.285: INFO: Checking APIGroup: projectcalico.org
    Mar  9 16:00:54.286: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
    Mar  9 16:00:54.286: INFO: Versions found [{projectcalico.org/v3 v3}]
    Mar  9 16:00:54.286: INFO: projectcalico.org/v3 matches projectcalico.org/v3
    Mar  9 16:00:54.286: INFO: Checking APIGroup: crd.projectcalico.org
    Mar  9 16:00:54.287: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar  9 16:00:54.287: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar  9 16:00:54.287: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Mar  9 16:00:54.287: INFO: Checking APIGroup: operator.tigera.io
    Mar  9 16:00:54.288: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Mar  9 16:00:54.288: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Mar  9 16:00:54.288: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Mar  9 16:00:54.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-9427" for this suite. 03/09/23 16:00:54.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:00:54.298
Mar  9 16:00:54.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:00:54.299
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:00:54.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:00:54.313
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 03/09/23 16:00:54.316
Mar  9 16:00:54.323: INFO: Waiting up to 5m0s for pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11" in namespace "downward-api-2688" to be "Succeeded or Failed"
Mar  9 16:00:54.326: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.663029ms
Mar  9 16:00:56.330: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006872711s
Mar  9 16:00:58.329: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005992579s
STEP: Saw pod success 03/09/23 16:00:58.329
Mar  9 16:00:58.329: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11" satisfied condition "Succeeded or Failed"
Mar  9 16:00:58.332: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-64a06c0b-c78b-4712-962a-fb191d95da11 container dapi-container: <nil>
STEP: delete the pod 03/09/23 16:00:58.347
Mar  9 16:00:58.357: INFO: Waiting for pod downward-api-64a06c0b-c78b-4712-962a-fb191d95da11 to disappear
Mar  9 16:00:58.359: INFO: Pod downward-api-64a06c0b-c78b-4712-962a-fb191d95da11 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  9 16:00:58.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2688" for this suite. 03/09/23 16:00:58.362
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":61,"skipped":1099,"failed":0}
------------------------------
â€¢ [4.069 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:00:54.298
    Mar  9 16:00:54.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:00:54.299
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:00:54.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:00:54.313
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 03/09/23 16:00:54.316
    Mar  9 16:00:54.323: INFO: Waiting up to 5m0s for pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11" in namespace "downward-api-2688" to be "Succeeded or Failed"
    Mar  9 16:00:54.326: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.663029ms
    Mar  9 16:00:56.330: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006872711s
    Mar  9 16:00:58.329: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005992579s
    STEP: Saw pod success 03/09/23 16:00:58.329
    Mar  9 16:00:58.329: INFO: Pod "downward-api-64a06c0b-c78b-4712-962a-fb191d95da11" satisfied condition "Succeeded or Failed"
    Mar  9 16:00:58.332: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-64a06c0b-c78b-4712-962a-fb191d95da11 container dapi-container: <nil>
    STEP: delete the pod 03/09/23 16:00:58.347
    Mar  9 16:00:58.357: INFO: Waiting for pod downward-api-64a06c0b-c78b-4712-962a-fb191d95da11 to disappear
    Mar  9 16:00:58.359: INFO: Pod downward-api-64a06c0b-c78b-4712-962a-fb191d95da11 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  9 16:00:58.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2688" for this suite. 03/09/23 16:00:58.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:00:58.369
Mar  9 16:00:58.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:00:58.371
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:00:58.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:00:58.436
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4325 03/09/23 16:00:58.438
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/09/23 16:00:58.454
STEP: creating service externalsvc in namespace services-4325 03/09/23 16:00:58.454
STEP: creating replication controller externalsvc in namespace services-4325 03/09/23 16:00:58.471
I0309 16:00:58.478434      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4325, replica count: 2
I0309 16:01:01.529595      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/09/23 16:01:01.532
Mar  9 16:01:01.552: INFO: Creating new exec pod
Mar  9 16:01:01.559: INFO: Waiting up to 5m0s for pod "execpodgkrcm" in namespace "services-4325" to be "running"
Mar  9 16:01:01.561: INFO: Pod "execpodgkrcm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524217ms
Mar  9 16:01:03.565: INFO: Pod "execpodgkrcm": Phase="Running", Reason="", readiness=true. Elapsed: 2.00636532s
Mar  9 16:01:03.565: INFO: Pod "execpodgkrcm" satisfied condition "running"
Mar  9 16:01:03.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-4325 exec execpodgkrcm -- /bin/sh -x -c nslookup nodeport-service.services-4325.svc.cluster.local'
Mar  9 16:01:03.745: INFO: stderr: "+ nslookup nodeport-service.services-4325.svc.cluster.local\n"
Mar  9 16:01:03.745: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4325.svc.cluster.local\tcanonical name = externalsvc.services-4325.svc.cluster.local.\nName:\texternalsvc.services-4325.svc.cluster.local\nAddress: 10.110.40.235\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4325, will wait for the garbage collector to delete the pods 03/09/23 16:01:03.745
Mar  9 16:01:03.804: INFO: Deleting ReplicationController externalsvc took: 5.5282ms
Mar  9 16:01:03.905: INFO: Terminating ReplicationController externalsvc pods took: 100.424781ms
Mar  9 16:01:05.926: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:01:05.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4325" for this suite. 03/09/23 16:01:05.94
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":62,"skipped":1125,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.579 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:00:58.369
    Mar  9 16:00:58.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:00:58.371
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:00:58.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:00:58.436
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-4325 03/09/23 16:00:58.438
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/09/23 16:00:58.454
    STEP: creating service externalsvc in namespace services-4325 03/09/23 16:00:58.454
    STEP: creating replication controller externalsvc in namespace services-4325 03/09/23 16:00:58.471
    I0309 16:00:58.478434      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4325, replica count: 2
    I0309 16:01:01.529595      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/09/23 16:01:01.532
    Mar  9 16:01:01.552: INFO: Creating new exec pod
    Mar  9 16:01:01.559: INFO: Waiting up to 5m0s for pod "execpodgkrcm" in namespace "services-4325" to be "running"
    Mar  9 16:01:01.561: INFO: Pod "execpodgkrcm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524217ms
    Mar  9 16:01:03.565: INFO: Pod "execpodgkrcm": Phase="Running", Reason="", readiness=true. Elapsed: 2.00636532s
    Mar  9 16:01:03.565: INFO: Pod "execpodgkrcm" satisfied condition "running"
    Mar  9 16:01:03.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-4325 exec execpodgkrcm -- /bin/sh -x -c nslookup nodeport-service.services-4325.svc.cluster.local'
    Mar  9 16:01:03.745: INFO: stderr: "+ nslookup nodeport-service.services-4325.svc.cluster.local\n"
    Mar  9 16:01:03.745: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4325.svc.cluster.local\tcanonical name = externalsvc.services-4325.svc.cluster.local.\nName:\texternalsvc.services-4325.svc.cluster.local\nAddress: 10.110.40.235\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4325, will wait for the garbage collector to delete the pods 03/09/23 16:01:03.745
    Mar  9 16:01:03.804: INFO: Deleting ReplicationController externalsvc took: 5.5282ms
    Mar  9 16:01:03.905: INFO: Terminating ReplicationController externalsvc pods took: 100.424781ms
    Mar  9 16:01:05.926: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:01:05.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4325" for this suite. 03/09/23 16:01:05.94
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:01:05.95
Mar  9 16:01:05.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 16:01:05.951
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:05.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:05.967
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-35665964-10a1-4c0a-81a1-315471af6f5d 03/09/23 16:01:05.97
STEP: Creating a pod to test consume configMaps 03/09/23 16:01:05.974
Mar  9 16:01:05.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f" in namespace "configmap-5926" to be "Succeeded or Failed"
Mar  9 16:01:05.985: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732735ms
Mar  9 16:01:07.988: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005785927s
Mar  9 16:01:09.988: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005953796s
STEP: Saw pod success 03/09/23 16:01:09.988
Mar  9 16:01:09.988: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f" satisfied condition "Succeeded or Failed"
Mar  9 16:01:09.990: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:01:09.997
Mar  9 16:01:10.013: INFO: Waiting for pod pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f to disappear
Mar  9 16:01:10.016: INFO: Pod pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 16:01:10.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5926" for this suite. 03/09/23 16:01:10.019
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":63,"skipped":1154,"failed":0}
------------------------------
â€¢ [4.075 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:01:05.95
    Mar  9 16:01:05.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 16:01:05.951
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:05.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:05.967
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-35665964-10a1-4c0a-81a1-315471af6f5d 03/09/23 16:01:05.97
    STEP: Creating a pod to test consume configMaps 03/09/23 16:01:05.974
    Mar  9 16:01:05.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f" in namespace "configmap-5926" to be "Succeeded or Failed"
    Mar  9 16:01:05.985: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.732735ms
    Mar  9 16:01:07.988: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005785927s
    Mar  9 16:01:09.988: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005953796s
    STEP: Saw pod success 03/09/23 16:01:09.988
    Mar  9 16:01:09.988: INFO: Pod "pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f" satisfied condition "Succeeded or Failed"
    Mar  9 16:01:09.990: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:01:09.997
    Mar  9 16:01:10.013: INFO: Waiting for pod pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f to disappear
    Mar  9 16:01:10.016: INFO: Pod pod-configmaps-6c4a6d0d-5a32-48a2-bbd4-22a91b46257f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 16:01:10.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5926" for this suite. 03/09/23 16:01:10.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:01:10.026
Mar  9 16:01:10.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename job 03/09/23 16:01:10.027
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:10.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:10.042
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 03/09/23 16:01:10.045
STEP: Ensuring active pods == parallelism 03/09/23 16:01:10.051
STEP: Orphaning one of the Job's Pods 03/09/23 16:01:12.056
Mar  9 16:01:12.570: INFO: Successfully updated pod "adopt-release-9xgqs"
STEP: Checking that the Job readopts the Pod 03/09/23 16:01:12.57
Mar  9 16:01:12.570: INFO: Waiting up to 15m0s for pod "adopt-release-9xgqs" in namespace "job-4186" to be "adopted"
Mar  9 16:01:12.573: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.661325ms
Mar  9 16:01:14.576: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.006135703s
Mar  9 16:01:14.576: INFO: Pod "adopt-release-9xgqs" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/09/23 16:01:14.576
Mar  9 16:01:15.088: INFO: Successfully updated pod "adopt-release-9xgqs"
STEP: Checking that the Job releases the Pod 03/09/23 16:01:15.088
Mar  9 16:01:15.089: INFO: Waiting up to 15m0s for pod "adopt-release-9xgqs" in namespace "job-4186" to be "released"
Mar  9 16:01:15.091: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.676888ms
Mar  9 16:01:17.096: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.007039525s
Mar  9 16:01:17.096: INFO: Pod "adopt-release-9xgqs" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  9 16:01:17.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4186" for this suite. 03/09/23 16:01:17.1
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":64,"skipped":1172,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.079 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:01:10.026
    Mar  9 16:01:10.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename job 03/09/23 16:01:10.027
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:10.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:10.042
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 03/09/23 16:01:10.045
    STEP: Ensuring active pods == parallelism 03/09/23 16:01:10.051
    STEP: Orphaning one of the Job's Pods 03/09/23 16:01:12.056
    Mar  9 16:01:12.570: INFO: Successfully updated pod "adopt-release-9xgqs"
    STEP: Checking that the Job readopts the Pod 03/09/23 16:01:12.57
    Mar  9 16:01:12.570: INFO: Waiting up to 15m0s for pod "adopt-release-9xgqs" in namespace "job-4186" to be "adopted"
    Mar  9 16:01:12.573: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.661325ms
    Mar  9 16:01:14.576: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.006135703s
    Mar  9 16:01:14.576: INFO: Pod "adopt-release-9xgqs" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/09/23 16:01:14.576
    Mar  9 16:01:15.088: INFO: Successfully updated pod "adopt-release-9xgqs"
    STEP: Checking that the Job releases the Pod 03/09/23 16:01:15.088
    Mar  9 16:01:15.089: INFO: Waiting up to 15m0s for pod "adopt-release-9xgqs" in namespace "job-4186" to be "released"
    Mar  9 16:01:15.091: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.676888ms
    Mar  9 16:01:17.096: INFO: Pod "adopt-release-9xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.007039525s
    Mar  9 16:01:17.096: INFO: Pod "adopt-release-9xgqs" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  9 16:01:17.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4186" for this suite. 03/09/23 16:01:17.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:01:17.107
Mar  9 16:01:17.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:01:17.108
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:17.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:17.124
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-5add9beb-526c-42f1-9ff0-8d104a15c9b9 03/09/23 16:01:17.127
STEP: Creating a pod to test consume secrets 03/09/23 16:01:17.131
Mar  9 16:01:17.138: INFO: Waiting up to 5m0s for pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea" in namespace "secrets-6365" to be "Succeeded or Failed"
Mar  9 16:01:17.140: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459891ms
Mar  9 16:01:19.144: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006529989s
Mar  9 16:01:21.145: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006709089s
STEP: Saw pod success 03/09/23 16:01:21.145
Mar  9 16:01:21.145: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea" satisfied condition "Succeeded or Failed"
Mar  9 16:01:21.147: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:01:21.153
Mar  9 16:01:21.161: INFO: Waiting for pod pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea to disappear
Mar  9 16:01:21.164: INFO: Pod pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:01:21.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6365" for this suite. 03/09/23 16:01:21.167
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":65,"skipped":1199,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:01:17.107
    Mar  9 16:01:17.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:01:17.108
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:17.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:17.124
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-5add9beb-526c-42f1-9ff0-8d104a15c9b9 03/09/23 16:01:17.127
    STEP: Creating a pod to test consume secrets 03/09/23 16:01:17.131
    Mar  9 16:01:17.138: INFO: Waiting up to 5m0s for pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea" in namespace "secrets-6365" to be "Succeeded or Failed"
    Mar  9 16:01:17.140: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459891ms
    Mar  9 16:01:19.144: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006529989s
    Mar  9 16:01:21.145: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006709089s
    STEP: Saw pod success 03/09/23 16:01:21.145
    Mar  9 16:01:21.145: INFO: Pod "pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea" satisfied condition "Succeeded or Failed"
    Mar  9 16:01:21.147: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:01:21.153
    Mar  9 16:01:21.161: INFO: Waiting for pod pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea to disappear
    Mar  9 16:01:21.164: INFO: Pod pod-secrets-32deb0da-5949-4a76-885d-e3ec211956ea no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:01:21.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6365" for this suite. 03/09/23 16:01:21.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:01:21.177
Mar  9 16:01:21.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 16:01:21.178
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:21.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:21.192
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-0e48fad7-3c10-40d7-8d99-ebcfcb358f7a 03/09/23 16:01:21.195
STEP: Creating a pod to test consume configMaps 03/09/23 16:01:21.199
Mar  9 16:01:21.210: INFO: Waiting up to 5m0s for pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef" in namespace "configmap-5605" to be "Succeeded or Failed"
Mar  9 16:01:21.213: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010282ms
Mar  9 16:01:23.217: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007156032s
Mar  9 16:01:25.217: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006661084s
STEP: Saw pod success 03/09/23 16:01:25.217
Mar  9 16:01:25.217: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef" satisfied condition "Succeeded or Failed"
Mar  9 16:01:25.220: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:01:25.225
Mar  9 16:01:25.235: INFO: Waiting for pod pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef to disappear
Mar  9 16:01:25.237: INFO: Pod pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 16:01:25.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5605" for this suite. 03/09/23 16:01:25.241
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":66,"skipped":1269,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:01:21.177
    Mar  9 16:01:21.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 16:01:21.178
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:21.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:21.192
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-0e48fad7-3c10-40d7-8d99-ebcfcb358f7a 03/09/23 16:01:21.195
    STEP: Creating a pod to test consume configMaps 03/09/23 16:01:21.199
    Mar  9 16:01:21.210: INFO: Waiting up to 5m0s for pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef" in namespace "configmap-5605" to be "Succeeded or Failed"
    Mar  9 16:01:21.213: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010282ms
    Mar  9 16:01:23.217: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007156032s
    Mar  9 16:01:25.217: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006661084s
    STEP: Saw pod success 03/09/23 16:01:25.217
    Mar  9 16:01:25.217: INFO: Pod "pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef" satisfied condition "Succeeded or Failed"
    Mar  9 16:01:25.220: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:01:25.225
    Mar  9 16:01:25.235: INFO: Waiting for pod pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef to disappear
    Mar  9 16:01:25.237: INFO: Pod pod-configmaps-efea139b-bdc0-4c2c-85e5-f534d12afaef no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 16:01:25.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5605" for this suite. 03/09/23 16:01:25.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:01:25.253
Mar  9 16:01:25.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:01:25.254
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:25.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:25.267
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:01:25.27
Mar  9 16:01:25.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568" in namespace "projected-7569" to be "Succeeded or Failed"
Mar  9 16:01:25.280: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298998ms
Mar  9 16:01:27.285: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007352662s
Mar  9 16:01:29.284: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006642565s
STEP: Saw pod success 03/09/23 16:01:29.284
Mar  9 16:01:29.284: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568" satisfied condition "Succeeded or Failed"
Mar  9 16:01:29.287: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568 container client-container: <nil>
STEP: delete the pod 03/09/23 16:01:29.293
Mar  9 16:01:29.304: INFO: Waiting for pod downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568 to disappear
Mar  9 16:01:29.307: INFO: Pod downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 16:01:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7569" for this suite. 03/09/23 16:01:29.31
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":67,"skipped":1376,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:01:25.253
    Mar  9 16:01:25.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:01:25.254
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:25.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:25.267
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:01:25.27
    Mar  9 16:01:25.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568" in namespace "projected-7569" to be "Succeeded or Failed"
    Mar  9 16:01:25.280: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298998ms
    Mar  9 16:01:27.285: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007352662s
    Mar  9 16:01:29.284: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006642565s
    STEP: Saw pod success 03/09/23 16:01:29.284
    Mar  9 16:01:29.284: INFO: Pod "downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568" satisfied condition "Succeeded or Failed"
    Mar  9 16:01:29.287: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568 container client-container: <nil>
    STEP: delete the pod 03/09/23 16:01:29.293
    Mar  9 16:01:29.304: INFO: Waiting for pod downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568 to disappear
    Mar  9 16:01:29.307: INFO: Pod downwardapi-volume-2ce74ec5-6dd4-4cc8-88ad-d478c781e568 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 16:01:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7569" for this suite. 03/09/23 16:01:29.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:01:29.315
Mar  9 16:01:29.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 16:01:29.316
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:29.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:29.33
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 in namespace container-probe-8626 03/09/23 16:01:29.333
Mar  9 16:01:29.339: INFO: Waiting up to 5m0s for pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42" in namespace "container-probe-8626" to be "not pending"
Mar  9 16:01:29.341: INFO: Pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.352184ms
Mar  9 16:01:31.345: INFO: Pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42": Phase="Running", Reason="", readiness=true. Elapsed: 2.005482822s
Mar  9 16:01:31.345: INFO: Pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42" satisfied condition "not pending"
Mar  9 16:01:31.345: INFO: Started pod liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 in namespace container-probe-8626
STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:01:31.345
Mar  9 16:01:31.347: INFO: Initial restart count of pod liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is 0
Mar  9 16:01:51.391: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 1 (20.043685325s elapsed)
Mar  9 16:02:11.436: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 2 (40.088615127s elapsed)
Mar  9 16:02:31.478: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 3 (1m0.131184918s elapsed)
Mar  9 16:02:51.517: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 4 (1m20.170076823s elapsed)
Mar  9 16:04:03.663: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 5 (2m32.315407321s elapsed)
STEP: deleting the pod 03/09/23 16:04:03.663
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 16:04:03.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8626" for this suite. 03/09/23 16:04:03.678
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":68,"skipped":1383,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.370 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:01:29.315
    Mar  9 16:01:29.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 16:01:29.316
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:01:29.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:01:29.33
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 in namespace container-probe-8626 03/09/23 16:01:29.333
    Mar  9 16:01:29.339: INFO: Waiting up to 5m0s for pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42" in namespace "container-probe-8626" to be "not pending"
    Mar  9 16:01:29.341: INFO: Pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.352184ms
    Mar  9 16:01:31.345: INFO: Pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42": Phase="Running", Reason="", readiness=true. Elapsed: 2.005482822s
    Mar  9 16:01:31.345: INFO: Pod "liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42" satisfied condition "not pending"
    Mar  9 16:01:31.345: INFO: Started pod liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 in namespace container-probe-8626
    STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:01:31.345
    Mar  9 16:01:31.347: INFO: Initial restart count of pod liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is 0
    Mar  9 16:01:51.391: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 1 (20.043685325s elapsed)
    Mar  9 16:02:11.436: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 2 (40.088615127s elapsed)
    Mar  9 16:02:31.478: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 3 (1m0.131184918s elapsed)
    Mar  9 16:02:51.517: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 4 (1m20.170076823s elapsed)
    Mar  9 16:04:03.663: INFO: Restart count of pod container-probe-8626/liveness-41c9cc7c-5cbb-4962-b7a0-9b22bd48da42 is now 5 (2m32.315407321s elapsed)
    STEP: deleting the pod 03/09/23 16:04:03.663
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 16:04:03.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8626" for this suite. 03/09/23 16:04:03.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:04:03.689
Mar  9 16:04:03.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename subpath 03/09/23 16:04:03.69
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:04:03.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:04:03.704
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/09/23 16:04:03.707
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-7fr7 03/09/23 16:04:03.715
STEP: Creating a pod to test atomic-volume-subpath 03/09/23 16:04:03.715
Mar  9 16:04:03.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7fr7" in namespace "subpath-9989" to be "Succeeded or Failed"
Mar  9 16:04:03.724: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282744ms
Mar  9 16:04:05.729: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006437872s
Mar  9 16:04:07.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 4.006126389s
Mar  9 16:04:09.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 6.006312236s
Mar  9 16:04:11.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 8.006060691s
Mar  9 16:04:13.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 10.006263157s
Mar  9 16:04:15.729: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 12.006358793s
Mar  9 16:04:17.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 14.005442586s
Mar  9 16:04:19.729: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 16.007281794s
Mar  9 16:04:21.730: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 18.007541079s
Mar  9 16:04:23.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 20.00634658s
Mar  9 16:04:25.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=false. Elapsed: 22.005745788s
Mar  9 16:04:27.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00601757s
STEP: Saw pod success 03/09/23 16:04:27.728
Mar  9 16:04:27.728: INFO: Pod "pod-subpath-test-projected-7fr7" satisfied condition "Succeeded or Failed"
Mar  9 16:04:27.731: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-projected-7fr7 container test-container-subpath-projected-7fr7: <nil>
STEP: delete the pod 03/09/23 16:04:27.746
Mar  9 16:04:27.757: INFO: Waiting for pod pod-subpath-test-projected-7fr7 to disappear
Mar  9 16:04:27.759: INFO: Pod pod-subpath-test-projected-7fr7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-7fr7 03/09/23 16:04:27.759
Mar  9 16:04:27.759: INFO: Deleting pod "pod-subpath-test-projected-7fr7" in namespace "subpath-9989"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  9 16:04:27.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9989" for this suite. 03/09/23 16:04:27.765
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":69,"skipped":1411,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.081 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:04:03.689
    Mar  9 16:04:03.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename subpath 03/09/23 16:04:03.69
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:04:03.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:04:03.704
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/09/23 16:04:03.707
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-7fr7 03/09/23 16:04:03.715
    STEP: Creating a pod to test atomic-volume-subpath 03/09/23 16:04:03.715
    Mar  9 16:04:03.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7fr7" in namespace "subpath-9989" to be "Succeeded or Failed"
    Mar  9 16:04:03.724: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282744ms
    Mar  9 16:04:05.729: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006437872s
    Mar  9 16:04:07.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 4.006126389s
    Mar  9 16:04:09.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 6.006312236s
    Mar  9 16:04:11.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 8.006060691s
    Mar  9 16:04:13.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 10.006263157s
    Mar  9 16:04:15.729: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 12.006358793s
    Mar  9 16:04:17.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 14.005442586s
    Mar  9 16:04:19.729: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 16.007281794s
    Mar  9 16:04:21.730: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 18.007541079s
    Mar  9 16:04:23.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=true. Elapsed: 20.00634658s
    Mar  9 16:04:25.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Running", Reason="", readiness=false. Elapsed: 22.005745788s
    Mar  9 16:04:27.728: INFO: Pod "pod-subpath-test-projected-7fr7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00601757s
    STEP: Saw pod success 03/09/23 16:04:27.728
    Mar  9 16:04:27.728: INFO: Pod "pod-subpath-test-projected-7fr7" satisfied condition "Succeeded or Failed"
    Mar  9 16:04:27.731: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-projected-7fr7 container test-container-subpath-projected-7fr7: <nil>
    STEP: delete the pod 03/09/23 16:04:27.746
    Mar  9 16:04:27.757: INFO: Waiting for pod pod-subpath-test-projected-7fr7 to disappear
    Mar  9 16:04:27.759: INFO: Pod pod-subpath-test-projected-7fr7 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-7fr7 03/09/23 16:04:27.759
    Mar  9 16:04:27.759: INFO: Deleting pod "pod-subpath-test-projected-7fr7" in namespace "subpath-9989"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  9 16:04:27.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9989" for this suite. 03/09/23 16:04:27.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:04:27.774
Mar  9 16:04:27.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename job 03/09/23 16:04:27.775
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:04:27.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:04:27.791
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 03/09/23 16:04:27.794
STEP: Ensure pods equal to paralellism count is attached to the job 03/09/23 16:04:27.799
STEP: patching /status 03/09/23 16:04:29.804
STEP: updating /status 03/09/23 16:04:29.812
STEP: get /status 03/09/23 16:04:29.839
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  9 16:04:29.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8888" for this suite. 03/09/23 16:04:29.845
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":70,"skipped":1461,"failed":0}
------------------------------
â€¢ [2.076 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:04:27.774
    Mar  9 16:04:27.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename job 03/09/23 16:04:27.775
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:04:27.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:04:27.791
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 03/09/23 16:04:27.794
    STEP: Ensure pods equal to paralellism count is attached to the job 03/09/23 16:04:27.799
    STEP: patching /status 03/09/23 16:04:29.804
    STEP: updating /status 03/09/23 16:04:29.812
    STEP: get /status 03/09/23 16:04:29.839
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  9 16:04:29.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8888" for this suite. 03/09/23 16:04:29.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:04:29.852
Mar  9 16:04:29.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 16:04:29.853
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:04:29.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:04:29.867
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6883 03/09/23 16:04:29.87
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 03/09/23 16:04:29.877
STEP: Creating stateful set ss in namespace statefulset-6883 03/09/23 16:04:29.88
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6883 03/09/23 16:04:29.886
Mar  9 16:04:29.888: INFO: Found 0 stateful pods, waiting for 1
Mar  9 16:04:39.892: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/09/23 16:04:39.892
Mar  9 16:04:39.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 16:04:40.043: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 16:04:40.043: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 16:04:40.043: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 16:04:40.046: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  9 16:04:50.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 16:04:50.051: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 16:04:50.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999979s
Mar  9 16:04:51.067: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99736008s
Mar  9 16:04:52.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994170965s
Mar  9 16:04:53.074: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990006274s
Mar  9 16:04:54.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985881593s
Mar  9 16:04:55.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982406621s
Mar  9 16:04:56.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978475792s
Mar  9 16:04:57.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973949455s
Mar  9 16:04:58.094: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970078502s
Mar  9 16:04:59.098: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.516277ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6883 03/09/23 16:05:00.098
Mar  9 16:05:00.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 16:05:00.250: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  9 16:05:00.250: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 16:05:00.250: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  9 16:05:00.253: INFO: Found 1 stateful pods, waiting for 3
Mar  9 16:05:10.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 16:05:10.259: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 16:05:10.259: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/09/23 16:05:10.259
STEP: Scale down will halt with unhealthy stateful pod 03/09/23 16:05:10.259
Mar  9 16:05:10.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 16:05:10.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 16:05:10.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 16:05:10.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 16:05:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 16:05:10.579: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 16:05:10.579: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 16:05:10.579: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 16:05:10.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 16:05:10.723: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 16:05:10.724: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 16:05:10.724: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 16:05:10.724: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 16:05:10.726: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  9 16:05:20.735: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 16:05:20.735: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 16:05:20.735: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 16:05:20.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999756s
Mar  9 16:05:21.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996712009s
Mar  9 16:05:22.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991786853s
Mar  9 16:05:23.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987770222s
Mar  9 16:05:24.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984082529s
Mar  9 16:05:25.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9805038s
Mar  9 16:05:26.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976984342s
Mar  9 16:05:27.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970493589s
Mar  9 16:05:28.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966914621s
Mar  9 16:05:29.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.306305ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6883 03/09/23 16:05:30.784
Mar  9 16:05:30.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 16:05:30.970: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  9 16:05:30.970: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 16:05:30.970: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  9 16:05:30.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 16:05:31.121: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  9 16:05:31.121: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 16:05:31.121: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  9 16:05:31.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 16:05:31.267: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  9 16:05:31.267: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 16:05:31.267: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  9 16:05:31.267: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/09/23 16:05:41.283
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 16:05:41.283: INFO: Deleting all statefulset in ns statefulset-6883
Mar  9 16:05:41.286: INFO: Scaling statefulset ss to 0
Mar  9 16:05:41.295: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 16:05:41.297: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 16:05:41.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6883" for this suite. 03/09/23 16:05:41.311
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":71,"skipped":1476,"failed":0}
------------------------------
â€¢ [SLOW TEST] [71.464 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:04:29.852
    Mar  9 16:04:29.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 16:04:29.853
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:04:29.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:04:29.867
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6883 03/09/23 16:04:29.87
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/09/23 16:04:29.877
    STEP: Creating stateful set ss in namespace statefulset-6883 03/09/23 16:04:29.88
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6883 03/09/23 16:04:29.886
    Mar  9 16:04:29.888: INFO: Found 0 stateful pods, waiting for 1
    Mar  9 16:04:39.892: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/09/23 16:04:39.892
    Mar  9 16:04:39.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 16:04:40.043: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 16:04:40.043: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 16:04:40.043: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 16:04:40.046: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  9 16:04:50.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 16:04:50.051: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 16:04:50.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999979s
    Mar  9 16:04:51.067: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99736008s
    Mar  9 16:04:52.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994170965s
    Mar  9 16:04:53.074: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990006274s
    Mar  9 16:04:54.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985881593s
    Mar  9 16:04:55.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982406621s
    Mar  9 16:04:56.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978475792s
    Mar  9 16:04:57.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973949455s
    Mar  9 16:04:58.094: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970078502s
    Mar  9 16:04:59.098: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.516277ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6883 03/09/23 16:05:00.098
    Mar  9 16:05:00.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 16:05:00.250: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  9 16:05:00.250: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 16:05:00.250: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  9 16:05:00.253: INFO: Found 1 stateful pods, waiting for 3
    Mar  9 16:05:10.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 16:05:10.259: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 16:05:10.259: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/09/23 16:05:10.259
    STEP: Scale down will halt with unhealthy stateful pod 03/09/23 16:05:10.259
    Mar  9 16:05:10.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 16:05:10.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 16:05:10.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 16:05:10.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 16:05:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 16:05:10.579: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 16:05:10.579: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 16:05:10.579: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 16:05:10.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 16:05:10.723: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 16:05:10.724: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 16:05:10.724: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 16:05:10.724: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 16:05:10.726: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Mar  9 16:05:20.735: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 16:05:20.735: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 16:05:20.735: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 16:05:20.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999756s
    Mar  9 16:05:21.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996712009s
    Mar  9 16:05:22.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991786853s
    Mar  9 16:05:23.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987770222s
    Mar  9 16:05:24.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984082529s
    Mar  9 16:05:25.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9805038s
    Mar  9 16:05:26.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976984342s
    Mar  9 16:05:27.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970493589s
    Mar  9 16:05:28.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966914621s
    Mar  9 16:05:29.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.306305ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6883 03/09/23 16:05:30.784
    Mar  9 16:05:30.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 16:05:30.970: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  9 16:05:30.970: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 16:05:30.970: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  9 16:05:30.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 16:05:31.121: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  9 16:05:31.121: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 16:05:31.121: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  9 16:05:31.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-6883 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 16:05:31.267: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  9 16:05:31.267: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 16:05:31.267: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  9 16:05:31.267: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/09/23 16:05:41.283
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 16:05:41.283: INFO: Deleting all statefulset in ns statefulset-6883
    Mar  9 16:05:41.286: INFO: Scaling statefulset ss to 0
    Mar  9 16:05:41.295: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 16:05:41.297: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 16:05:41.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6883" for this suite. 03/09/23 16:05:41.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:05:41.319
Mar  9 16:05:41.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:05:41.32
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:41.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:41.334
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/09/23 16:05:41.337
STEP: submitting the pod to kubernetes 03/09/23 16:05:41.337
STEP: verifying QOS class is set on the pod 03/09/23 16:05:41.343
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Mar  9 16:05:41.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3652" for this suite. 03/09/23 16:05:41.35
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":72,"skipped":1505,"failed":0}
------------------------------
â€¢ [0.038 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:05:41.319
    Mar  9 16:05:41.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:05:41.32
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:41.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:41.334
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/09/23 16:05:41.337
    STEP: submitting the pod to kubernetes 03/09/23 16:05:41.337
    STEP: verifying QOS class is set on the pod 03/09/23 16:05:41.343
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Mar  9 16:05:41.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3652" for this suite. 03/09/23 16:05:41.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:05:41.357
Mar  9 16:05:41.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:05:41.358
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:41.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:41.371
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 03/09/23 16:05:41.374
Mar  9 16:05:41.381: INFO: Waiting up to 5m0s for pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63" in namespace "pods-3661" to be "running and ready"
Mar  9 16:05:41.383: INFO: Pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733191ms
Mar  9 16:05:41.383: INFO: The phase of Pod pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:05:43.387: INFO: Pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63": Phase="Running", Reason="", readiness=true. Elapsed: 2.005977851s
Mar  9 16:05:43.387: INFO: The phase of Pod pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63 is Running (Ready = true)
Mar  9 16:05:43.387: INFO: Pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63" satisfied condition "running and ready"
Mar  9 16:05:43.391: INFO: Pod pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63 has hostIP: 100.100.230.140
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:05:43.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3661" for this suite. 03/09/23 16:05:43.395
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":73,"skipped":1511,"failed":0}
------------------------------
â€¢ [2.044 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:05:41.357
    Mar  9 16:05:41.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:05:41.358
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:41.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:41.371
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 03/09/23 16:05:41.374
    Mar  9 16:05:41.381: INFO: Waiting up to 5m0s for pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63" in namespace "pods-3661" to be "running and ready"
    Mar  9 16:05:41.383: INFO: Pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733191ms
    Mar  9 16:05:41.383: INFO: The phase of Pod pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:05:43.387: INFO: Pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63": Phase="Running", Reason="", readiness=true. Elapsed: 2.005977851s
    Mar  9 16:05:43.387: INFO: The phase of Pod pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63 is Running (Ready = true)
    Mar  9 16:05:43.387: INFO: Pod "pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63" satisfied condition "running and ready"
    Mar  9 16:05:43.391: INFO: Pod pod-hostip-3ebc111b-7dfa-4c3b-9fe5-0fee838d3c63 has hostIP: 100.100.230.140
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:05:43.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3661" for this suite. 03/09/23 16:05:43.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:05:43.402
Mar  9 16:05:43.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:05:43.403
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:43.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:43.424
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 03/09/23 16:05:43.428
Mar  9 16:05:43.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9117 create -f -'
Mar  9 16:05:44.382: INFO: stderr: ""
Mar  9 16:05:44.382: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/09/23 16:05:44.382
Mar  9 16:05:44.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9117 diff -f -'
Mar  9 16:05:44.680: INFO: rc: 1
Mar  9 16:05:44.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9117 delete -f -'
Mar  9 16:05:44.753: INFO: stderr: ""
Mar  9 16:05:44.753: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:05:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9117" for this suite. 03/09/23 16:05:44.757
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":74,"skipped":1524,"failed":0}
------------------------------
â€¢ [1.360 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:05:43.402
    Mar  9 16:05:43.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:05:43.403
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:43.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:43.424
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 03/09/23 16:05:43.428
    Mar  9 16:05:43.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9117 create -f -'
    Mar  9 16:05:44.382: INFO: stderr: ""
    Mar  9 16:05:44.382: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/09/23 16:05:44.382
    Mar  9 16:05:44.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9117 diff -f -'
    Mar  9 16:05:44.680: INFO: rc: 1
    Mar  9 16:05:44.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9117 delete -f -'
    Mar  9 16:05:44.753: INFO: stderr: ""
    Mar  9 16:05:44.753: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:05:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9117" for this suite. 03/09/23 16:05:44.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:05:44.764
Mar  9 16:05:44.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:05:44.765
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:44.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:44.784
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Mar  9 16:05:44.799: INFO: created pod
Mar  9 16:05:44.799: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-752" to be "Succeeded or Failed"
Mar  9 16:05:44.801: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418748ms
Mar  9 16:05:46.805: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.006351458s
Mar  9 16:05:48.805: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005806958s
STEP: Saw pod success 03/09/23 16:05:48.805
Mar  9 16:05:48.805: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar  9 16:06:18.805: INFO: polling logs
Mar  9 16:06:18.820: INFO: Pod logs: 
I0309 16:05:45.567088       1 log.go:195] OK: Got token
I0309 16:05:45.567149       1 log.go:195] validating with in-cluster discovery
I0309 16:05:45.567611       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0309 16:05:45.567654       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-752:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678378545, NotBefore:1678377945, IssuedAt:1678377945, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-752", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e61aac3f-fe10-449a-a5b3-6dee8603cf72"}}}
I0309 16:05:45.584858       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0309 16:05:45.591067       1 log.go:195] OK: Validated signature on JWT
I0309 16:05:45.591160       1 log.go:195] OK: Got valid claims from token!
I0309 16:05:45.591180       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-752:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678378545, NotBefore:1678377945, IssuedAt:1678377945, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-752", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e61aac3f-fe10-449a-a5b3-6dee8603cf72"}}}

Mar  9 16:06:18.820: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  9 16:06:18.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-752" for this suite. 03/09/23 16:06:18.829
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":75,"skipped":1558,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.070 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:05:44.764
    Mar  9 16:05:44.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:05:44.765
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:05:44.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:05:44.784
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Mar  9 16:05:44.799: INFO: created pod
    Mar  9 16:05:44.799: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-752" to be "Succeeded or Failed"
    Mar  9 16:05:44.801: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418748ms
    Mar  9 16:05:46.805: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.006351458s
    Mar  9 16:05:48.805: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005806958s
    STEP: Saw pod success 03/09/23 16:05:48.805
    Mar  9 16:05:48.805: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar  9 16:06:18.805: INFO: polling logs
    Mar  9 16:06:18.820: INFO: Pod logs: 
    I0309 16:05:45.567088       1 log.go:195] OK: Got token
    I0309 16:05:45.567149       1 log.go:195] validating with in-cluster discovery
    I0309 16:05:45.567611       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0309 16:05:45.567654       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-752:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678378545, NotBefore:1678377945, IssuedAt:1678377945, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-752", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e61aac3f-fe10-449a-a5b3-6dee8603cf72"}}}
    I0309 16:05:45.584858       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0309 16:05:45.591067       1 log.go:195] OK: Validated signature on JWT
    I0309 16:05:45.591160       1 log.go:195] OK: Got valid claims from token!
    I0309 16:05:45.591180       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-752:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1678378545, NotBefore:1678377945, IssuedAt:1678377945, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-752", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e61aac3f-fe10-449a-a5b3-6dee8603cf72"}}}

    Mar  9 16:06:18.820: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  9 16:06:18.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-752" for this suite. 03/09/23 16:06:18.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:06:18.835
Mar  9 16:06:18.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:06:18.837
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:06:18.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:06:18.851
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/09/23 16:06:18.853
Mar  9 16:06:18.859: INFO: Waiting up to 5m0s for pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f" in namespace "emptydir-6191" to be "Succeeded or Failed"
Mar  9 16:06:18.861: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024177ms
Mar  9 16:06:20.865: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005943968s
Mar  9 16:06:22.865: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005843491s
STEP: Saw pod success 03/09/23 16:06:22.865
Mar  9 16:06:22.865: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f" satisfied condition "Succeeded or Failed"
Mar  9 16:06:22.868: INFO: Trying to get logs from node tt-test-el8-003 pod pod-fb46bbb4-adda-48d0-b964-8bb3d879722f container test-container: <nil>
STEP: delete the pod 03/09/23 16:06:22.874
Mar  9 16:06:22.896: INFO: Waiting for pod pod-fb46bbb4-adda-48d0-b964-8bb3d879722f to disappear
Mar  9 16:06:22.898: INFO: Pod pod-fb46bbb4-adda-48d0-b964-8bb3d879722f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:06:22.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6191" for this suite. 03/09/23 16:06:22.902
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":76,"skipped":1569,"failed":0}
------------------------------
â€¢ [4.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:06:18.835
    Mar  9 16:06:18.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:06:18.837
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:06:18.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:06:18.851
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/09/23 16:06:18.853
    Mar  9 16:06:18.859: INFO: Waiting up to 5m0s for pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f" in namespace "emptydir-6191" to be "Succeeded or Failed"
    Mar  9 16:06:18.861: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024177ms
    Mar  9 16:06:20.865: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005943968s
    Mar  9 16:06:22.865: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005843491s
    STEP: Saw pod success 03/09/23 16:06:22.865
    Mar  9 16:06:22.865: INFO: Pod "pod-fb46bbb4-adda-48d0-b964-8bb3d879722f" satisfied condition "Succeeded or Failed"
    Mar  9 16:06:22.868: INFO: Trying to get logs from node tt-test-el8-003 pod pod-fb46bbb4-adda-48d0-b964-8bb3d879722f container test-container: <nil>
    STEP: delete the pod 03/09/23 16:06:22.874
    Mar  9 16:06:22.896: INFO: Waiting for pod pod-fb46bbb4-adda-48d0-b964-8bb3d879722f to disappear
    Mar  9 16:06:22.898: INFO: Pod pod-fb46bbb4-adda-48d0-b964-8bb3d879722f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:06:22.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6191" for this suite. 03/09/23 16:06:22.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:06:22.91
Mar  9 16:06:22.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:06:22.912
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:06:22.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:06:22.927
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Mar  9 16:06:22.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/09/23 16:06:27.867
Mar  9 16:06:27.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 create -f -'
Mar  9 16:06:28.915: INFO: stderr: ""
Mar  9 16:06:28.915: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  9 16:06:28.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 delete e2e-test-crd-publish-openapi-7174-crds test-cr'
Mar  9 16:06:28.989: INFO: stderr: ""
Mar  9 16:06:28.989: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  9 16:06:28.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 apply -f -'
Mar  9 16:06:29.245: INFO: stderr: ""
Mar  9 16:06:29.245: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  9 16:06:29.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 delete e2e-test-crd-publish-openapi-7174-crds test-cr'
Mar  9 16:06:29.318: INFO: stderr: ""
Mar  9 16:06:29.318: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/09/23 16:06:29.318
Mar  9 16:06:29.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 explain e2e-test-crd-publish-openapi-7174-crds'
Mar  9 16:06:29.562: INFO: stderr: ""
Mar  9 16:06:29.562: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7174-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:06:32.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1813" for this suite. 03/09/23 16:06:32.501
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":77,"skipped":1593,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.596 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:06:22.91
    Mar  9 16:06:22.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:06:22.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:06:22.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:06:22.927
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Mar  9 16:06:22.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/09/23 16:06:27.867
    Mar  9 16:06:27.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 create -f -'
    Mar  9 16:06:28.915: INFO: stderr: ""
    Mar  9 16:06:28.915: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  9 16:06:28.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 delete e2e-test-crd-publish-openapi-7174-crds test-cr'
    Mar  9 16:06:28.989: INFO: stderr: ""
    Mar  9 16:06:28.989: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar  9 16:06:28.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 apply -f -'
    Mar  9 16:06:29.245: INFO: stderr: ""
    Mar  9 16:06:29.245: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  9 16:06:29.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 --namespace=crd-publish-openapi-1813 delete e2e-test-crd-publish-openapi-7174-crds test-cr'
    Mar  9 16:06:29.318: INFO: stderr: ""
    Mar  9 16:06:29.318: INFO: stdout: "e2e-test-crd-publish-openapi-7174-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/09/23 16:06:29.318
    Mar  9 16:06:29.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-1813 explain e2e-test-crd-publish-openapi-7174-crds'
    Mar  9 16:06:29.562: INFO: stderr: ""
    Mar  9 16:06:29.562: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7174-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:06:32.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1813" for this suite. 03/09/23 16:06:32.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:06:32.507
Mar  9 16:06:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:06:32.508
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:06:32.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:06:32.523
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-7347 03/09/23 16:06:32.525
STEP: creating service affinity-nodeport-transition in namespace services-7347 03/09/23 16:06:32.525
STEP: creating replication controller affinity-nodeport-transition in namespace services-7347 03/09/23 16:06:32.548
I0309 16:06:32.556863      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7347, replica count: 3
I0309 16:06:35.607505      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 16:06:35.616: INFO: Creating new exec pod
Mar  9 16:06:35.622: INFO: Waiting up to 5m0s for pod "execpod-affinityf52w6" in namespace "services-7347" to be "running"
Mar  9 16:06:35.625: INFO: Pod "execpod-affinityf52w6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269831ms
Mar  9 16:06:37.628: INFO: Pod "execpod-affinityf52w6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005878076s
Mar  9 16:06:37.628: INFO: Pod "execpod-affinityf52w6" satisfied condition "running"
Mar  9 16:06:38.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar  9 16:06:38.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  9 16:06:38.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:06:38.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.153.89 80'
Mar  9 16:06:38.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.153.89 80\nConnection to 10.96.153.89 80 port [tcp/http] succeeded!\n"
Mar  9 16:06:38.926: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:06:38.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 31462'
Mar  9 16:06:39.072: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 31462\nConnection to 100.100.230.140 31462 port [tcp/*] succeeded!\n"
Mar  9 16:06:39.072: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:06:39.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 31462'
Mar  9 16:06:39.221: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 31462\nConnection to 100.100.231.104 31462 port [tcp/*] succeeded!\n"
Mar  9 16:06:39.221: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:06:39.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
Mar  9 16:06:39.526: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
Mar  9 16:06:39.526: INFO: stdout: "\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj"
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:09.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
Mar  9 16:07:09.757: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
Mar  9 16:07:09.757: INFO: stdout: "\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd"
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-bk2q2
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-bk2q2
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-bk2q2
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:09.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
Mar  9 16:07:10.060: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
Mar  9 16:07:10.060: INFO: stdout: "\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2"
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-bk2q2
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-bk2q2
Mar  9 16:07:40.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
Mar  9 16:07:40.295: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
Mar  9 16:07:40.295: INFO: stdout: "\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd"
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
Mar  9 16:07:40.295: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7347, will wait for the garbage collector to delete the pods 03/09/23 16:07:40.308
Mar  9 16:07:40.366: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.929109ms
Mar  9 16:07:40.467: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.110432ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:07:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7347" for this suite. 03/09/23 16:07:42.998
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":78,"skipped":1612,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.496 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:06:32.507
    Mar  9 16:06:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:06:32.508
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:06:32.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:06:32.523
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-7347 03/09/23 16:06:32.525
    STEP: creating service affinity-nodeport-transition in namespace services-7347 03/09/23 16:06:32.525
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7347 03/09/23 16:06:32.548
    I0309 16:06:32.556863      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7347, replica count: 3
    I0309 16:06:35.607505      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 16:06:35.616: INFO: Creating new exec pod
    Mar  9 16:06:35.622: INFO: Waiting up to 5m0s for pod "execpod-affinityf52w6" in namespace "services-7347" to be "running"
    Mar  9 16:06:35.625: INFO: Pod "execpod-affinityf52w6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269831ms
    Mar  9 16:06:37.628: INFO: Pod "execpod-affinityf52w6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005878076s
    Mar  9 16:06:37.628: INFO: Pod "execpod-affinityf52w6" satisfied condition "running"
    Mar  9 16:06:38.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Mar  9 16:06:38.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar  9 16:06:38.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:06:38.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.153.89 80'
    Mar  9 16:06:38.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.153.89 80\nConnection to 10.96.153.89 80 port [tcp/http] succeeded!\n"
    Mar  9 16:06:38.926: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:06:38.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 31462'
    Mar  9 16:06:39.072: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 31462\nConnection to 100.100.230.140 31462 port [tcp/*] succeeded!\n"
    Mar  9 16:06:39.072: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:06:39.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 31462'
    Mar  9 16:06:39.221: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 31462\nConnection to 100.100.231.104 31462 port [tcp/*] succeeded!\n"
    Mar  9 16:06:39.221: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:06:39.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
    Mar  9 16:06:39.526: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
    Mar  9 16:06:39.526: INFO: stdout: "\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj"
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:06:39.526: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:09.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
    Mar  9 16:07:09.757: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
    Mar  9 16:07:09.757: INFO: stdout: "\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd"
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-bk2q2
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-bk2q2
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-bk2q2
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.757: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:09.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
    Mar  9 16:07:10.060: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
    Mar  9 16:07:10.060: INFO: stdout: "\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-lbczj\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-bk2q2"
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-bk2q2
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-lbczj
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:10.060: INFO: Received response from host: affinity-nodeport-transition-bk2q2
    Mar  9 16:07:40.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7347 exec execpod-affinityf52w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31462/ ; done'
    Mar  9 16:07:40.295: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31462/\n"
    Mar  9 16:07:40.295: INFO: stdout: "\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd\naffinity-nodeport-transition-nbxqd"
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Received response from host: affinity-nodeport-transition-nbxqd
    Mar  9 16:07:40.295: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7347, will wait for the garbage collector to delete the pods 03/09/23 16:07:40.308
    Mar  9 16:07:40.366: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.929109ms
    Mar  9 16:07:40.467: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.110432ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:07:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7347" for this suite. 03/09/23 16:07:42.998
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:07:43.005
Mar  9 16:07:43.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:07:43.006
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:07:43.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:07:43.02
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:07:43.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9537" for this suite. 03/09/23 16:07:43.057
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":79,"skipped":1633,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:07:43.005
    Mar  9 16:07:43.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:07:43.006
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:07:43.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:07:43.02
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:07:43.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9537" for this suite. 03/09/23 16:07:43.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:07:43.066
Mar  9 16:07:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:07:43.067
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:07:43.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:07:43.081
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-8ddd1d83-f80f-4ceb-86b4-3e6219e7c72d 03/09/23 16:07:43.1
STEP: Creating a pod to test consume secrets 03/09/23 16:07:43.103
Mar  9 16:07:43.110: INFO: Waiting up to 5m0s for pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c" in namespace "secrets-3448" to be "Succeeded or Failed"
Mar  9 16:07:43.113: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.828086ms
Mar  9 16:07:45.117: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007212273s
Mar  9 16:07:47.117: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006598136s
STEP: Saw pod success 03/09/23 16:07:47.117
Mar  9 16:07:47.117: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c" satisfied condition "Succeeded or Failed"
Mar  9 16:07:47.119: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:07:47.125
Mar  9 16:07:47.135: INFO: Waiting for pod pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c to disappear
Mar  9 16:07:47.137: INFO: Pod pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:07:47.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3448" for this suite. 03/09/23 16:07:47.141
STEP: Destroying namespace "secret-namespace-4333" for this suite. 03/09/23 16:07:47.146
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":80,"skipped":1714,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:07:43.066
    Mar  9 16:07:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:07:43.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:07:43.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:07:43.081
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-8ddd1d83-f80f-4ceb-86b4-3e6219e7c72d 03/09/23 16:07:43.1
    STEP: Creating a pod to test consume secrets 03/09/23 16:07:43.103
    Mar  9 16:07:43.110: INFO: Waiting up to 5m0s for pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c" in namespace "secrets-3448" to be "Succeeded or Failed"
    Mar  9 16:07:43.113: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.828086ms
    Mar  9 16:07:45.117: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007212273s
    Mar  9 16:07:47.117: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006598136s
    STEP: Saw pod success 03/09/23 16:07:47.117
    Mar  9 16:07:47.117: INFO: Pod "pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c" satisfied condition "Succeeded or Failed"
    Mar  9 16:07:47.119: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:07:47.125
    Mar  9 16:07:47.135: INFO: Waiting for pod pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c to disappear
    Mar  9 16:07:47.137: INFO: Pod pod-secrets-fe8db47a-7f97-4581-81fc-1693a8d3137c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:07:47.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3448" for this suite. 03/09/23 16:07:47.141
    STEP: Destroying namespace "secret-namespace-4333" for this suite. 03/09/23 16:07:47.146
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:07:47.153
Mar  9 16:07:47.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 16:07:47.154
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:07:47.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:07:47.169
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4 in namespace container-probe-957 03/09/23 16:07:47.171
Mar  9 16:07:47.177: INFO: Waiting up to 5m0s for pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4" in namespace "container-probe-957" to be "not pending"
Mar  9 16:07:47.179: INFO: Pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2076ms
Mar  9 16:07:49.183: INFO: Pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00584209s
Mar  9 16:07:49.183: INFO: Pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4" satisfied condition "not pending"
Mar  9 16:07:49.183: INFO: Started pod test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4 in namespace container-probe-957
STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:07:49.183
Mar  9 16:07:49.186: INFO: Initial restart count of pod test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4 is 0
STEP: deleting the pod 03/09/23 16:11:49.672
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 16:11:49.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-957" for this suite. 03/09/23 16:11:49.686
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":81,"skipped":1717,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.538 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:07:47.153
    Mar  9 16:07:47.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 16:07:47.154
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:07:47.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:07:47.169
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4 in namespace container-probe-957 03/09/23 16:07:47.171
    Mar  9 16:07:47.177: INFO: Waiting up to 5m0s for pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4" in namespace "container-probe-957" to be "not pending"
    Mar  9 16:07:47.179: INFO: Pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2076ms
    Mar  9 16:07:49.183: INFO: Pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00584209s
    Mar  9 16:07:49.183: INFO: Pod "test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4" satisfied condition "not pending"
    Mar  9 16:07:49.183: INFO: Started pod test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4 in namespace container-probe-957
    STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:07:49.183
    Mar  9 16:07:49.186: INFO: Initial restart count of pod test-webserver-20dc8fbc-07be-4c4b-8f67-f1f34df7a4d4 is 0
    STEP: deleting the pod 03/09/23 16:11:49.672
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 16:11:49.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-957" for this suite. 03/09/23 16:11:49.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:11:49.699
Mar  9 16:11:49.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:11:49.701
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:11:49.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:11:49.718
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/09/23 16:11:49.722
Mar  9 16:11:49.728: INFO: Waiting up to 5m0s for pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff" in namespace "emptydir-2392" to be "Succeeded or Failed"
Mar  9 16:11:49.731: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42618ms
Mar  9 16:11:51.736: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007938259s
Mar  9 16:11:53.735: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006473306s
STEP: Saw pod success 03/09/23 16:11:53.735
Mar  9 16:11:53.735: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff" satisfied condition "Succeeded or Failed"
Mar  9 16:11:53.737: INFO: Trying to get logs from node tt-test-el8-003 pod pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff container test-container: <nil>
STEP: delete the pod 03/09/23 16:11:53.743
Mar  9 16:11:53.751: INFO: Waiting for pod pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff to disappear
Mar  9 16:11:53.753: INFO: Pod pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:11:53.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2392" for this suite. 03/09/23 16:11:53.757
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":82,"skipped":1746,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:11:49.699
    Mar  9 16:11:49.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:11:49.701
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:11:49.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:11:49.718
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/09/23 16:11:49.722
    Mar  9 16:11:49.728: INFO: Waiting up to 5m0s for pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff" in namespace "emptydir-2392" to be "Succeeded or Failed"
    Mar  9 16:11:49.731: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42618ms
    Mar  9 16:11:51.736: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007938259s
    Mar  9 16:11:53.735: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006473306s
    STEP: Saw pod success 03/09/23 16:11:53.735
    Mar  9 16:11:53.735: INFO: Pod "pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff" satisfied condition "Succeeded or Failed"
    Mar  9 16:11:53.737: INFO: Trying to get logs from node tt-test-el8-003 pod pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff container test-container: <nil>
    STEP: delete the pod 03/09/23 16:11:53.743
    Mar  9 16:11:53.751: INFO: Waiting for pod pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff to disappear
    Mar  9 16:11:53.753: INFO: Pod pod-0224e5c4-bd2a-4fc2-96c4-f6bd43fb5aff no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:11:53.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2392" for this suite. 03/09/23 16:11:53.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:11:53.764
Mar  9 16:11:53.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename taint-single-pod 03/09/23 16:11:53.765
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:11:53.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:11:53.791
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar  9 16:11:53.794: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  9 16:12:53.825: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Mar  9 16:12:53.828: INFO: Starting informer...
STEP: Starting pod... 03/09/23 16:12:53.828
Mar  9 16:12:54.041: INFO: Pod is running on tt-test-el8-003. Tainting Node
STEP: Trying to apply a taint on the Node 03/09/23 16:12:54.041
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 16:12:54.054
STEP: Waiting short time to make sure Pod is queued for deletion 03/09/23 16:12:54.058
Mar  9 16:12:54.058: INFO: Pod wasn't evicted. Proceeding
Mar  9 16:12:54.058: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 16:12:54.081
STEP: Waiting some time to make sure that toleration time passed. 03/09/23 16:12:54.087
Mar  9 16:14:09.088: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:14:09.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6451" for this suite. 03/09/23 16:14:09.092
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":83,"skipped":1768,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.335 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:11:53.764
    Mar  9 16:11:53.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename taint-single-pod 03/09/23 16:11:53.765
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:11:53.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:11:53.791
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Mar  9 16:11:53.794: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  9 16:12:53.825: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Mar  9 16:12:53.828: INFO: Starting informer...
    STEP: Starting pod... 03/09/23 16:12:53.828
    Mar  9 16:12:54.041: INFO: Pod is running on tt-test-el8-003. Tainting Node
    STEP: Trying to apply a taint on the Node 03/09/23 16:12:54.041
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 16:12:54.054
    STEP: Waiting short time to make sure Pod is queued for deletion 03/09/23 16:12:54.058
    Mar  9 16:12:54.058: INFO: Pod wasn't evicted. Proceeding
    Mar  9 16:12:54.058: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 16:12:54.081
    STEP: Waiting some time to make sure that toleration time passed. 03/09/23 16:12:54.087
    Mar  9 16:14:09.088: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:14:09.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-6451" for this suite. 03/09/23 16:14:09.092
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:14:09.099
Mar  9 16:14:09.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename limitrange 03/09/23 16:14:09.1
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:09.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:09.115
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 03/09/23 16:14:09.118
STEP: Setting up watch 03/09/23 16:14:09.118
STEP: Submitting a LimitRange 03/09/23 16:14:09.221
STEP: Verifying LimitRange creation was observed 03/09/23 16:14:09.227
STEP: Fetching the LimitRange to ensure it has proper values 03/09/23 16:14:09.227
Mar  9 16:14:09.230: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  9 16:14:09.230: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/09/23 16:14:09.23
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/09/23 16:14:09.235
Mar  9 16:14:09.238: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  9 16:14:09.238: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/09/23 16:14:09.238
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/09/23 16:14:09.245
Mar  9 16:14:09.248: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  9 16:14:09.248: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/09/23 16:14:09.248
STEP: Failing to create a Pod with more than max resources 03/09/23 16:14:09.25
STEP: Updating a LimitRange 03/09/23 16:14:09.252
STEP: Verifying LimitRange updating is effective 03/09/23 16:14:09.259
STEP: Creating a Pod with less than former min resources 03/09/23 16:14:11.264
STEP: Failing to create a Pod with more than max resources 03/09/23 16:14:11.269
STEP: Deleting a LimitRange 03/09/23 16:14:11.272
STEP: Verifying the LimitRange was deleted 03/09/23 16:14:11.277
Mar  9 16:14:16.281: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/09/23 16:14:16.281
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Mar  9 16:14:16.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8385" for this suite. 03/09/23 16:14:16.291
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":84,"skipped":1771,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.200 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:14:09.099
    Mar  9 16:14:09.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename limitrange 03/09/23 16:14:09.1
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:09.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:09.115
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 03/09/23 16:14:09.118
    STEP: Setting up watch 03/09/23 16:14:09.118
    STEP: Submitting a LimitRange 03/09/23 16:14:09.221
    STEP: Verifying LimitRange creation was observed 03/09/23 16:14:09.227
    STEP: Fetching the LimitRange to ensure it has proper values 03/09/23 16:14:09.227
    Mar  9 16:14:09.230: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  9 16:14:09.230: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/09/23 16:14:09.23
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/09/23 16:14:09.235
    Mar  9 16:14:09.238: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  9 16:14:09.238: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/09/23 16:14:09.238
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/09/23 16:14:09.245
    Mar  9 16:14:09.248: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar  9 16:14:09.248: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/09/23 16:14:09.248
    STEP: Failing to create a Pod with more than max resources 03/09/23 16:14:09.25
    STEP: Updating a LimitRange 03/09/23 16:14:09.252
    STEP: Verifying LimitRange updating is effective 03/09/23 16:14:09.259
    STEP: Creating a Pod with less than former min resources 03/09/23 16:14:11.264
    STEP: Failing to create a Pod with more than max resources 03/09/23 16:14:11.269
    STEP: Deleting a LimitRange 03/09/23 16:14:11.272
    STEP: Verifying the LimitRange was deleted 03/09/23 16:14:11.277
    Mar  9 16:14:16.281: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/09/23 16:14:16.281
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Mar  9 16:14:16.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-8385" for this suite. 03/09/23 16:14:16.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:14:16.308
Mar  9 16:14:16.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 16:14:16.309
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:16.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:16.322
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/09/23 16:14:16.329
Mar  9 16:14:16.335: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6276" to be "running and ready"
Mar  9 16:14:16.338: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.675376ms
Mar  9 16:14:16.338: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:14:18.343: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007298137s
Mar  9 16:14:18.343: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  9 16:14:18.343: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 03/09/23 16:14:18.345
Mar  9 16:14:18.350: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6276" to be "running and ready"
Mar  9 16:14:18.357: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.930697ms
Mar  9 16:14:18.357: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:14:20.363: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012898712s
Mar  9 16:14:20.363: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar  9 16:14:20.363: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/09/23 16:14:20.367
STEP: delete the pod with lifecycle hook 03/09/23 16:14:20.385
Mar  9 16:14:20.391: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  9 16:14:20.395: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  9 16:14:22.396: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  9 16:14:22.400: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  9 16:14:24.397: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  9 16:14:24.400: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  9 16:14:24.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6276" for this suite. 03/09/23 16:14:24.405
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":85,"skipped":1916,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.103 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:14:16.308
    Mar  9 16:14:16.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 16:14:16.309
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:16.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:16.322
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/09/23 16:14:16.329
    Mar  9 16:14:16.335: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6276" to be "running and ready"
    Mar  9 16:14:16.338: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.675376ms
    Mar  9 16:14:16.338: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:14:18.343: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007298137s
    Mar  9 16:14:18.343: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  9 16:14:18.343: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 03/09/23 16:14:18.345
    Mar  9 16:14:18.350: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6276" to be "running and ready"
    Mar  9 16:14:18.357: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.930697ms
    Mar  9 16:14:18.357: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:14:20.363: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012898712s
    Mar  9 16:14:20.363: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar  9 16:14:20.363: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/09/23 16:14:20.367
    STEP: delete the pod with lifecycle hook 03/09/23 16:14:20.385
    Mar  9 16:14:20.391: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  9 16:14:20.395: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar  9 16:14:22.396: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  9 16:14:22.400: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar  9 16:14:24.397: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  9 16:14:24.400: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  9 16:14:24.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6276" for this suite. 03/09/23 16:14:24.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:14:24.412
Mar  9 16:14:24.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:14:24.413
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:24.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:24.427
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 03/09/23 16:14:24.429
STEP: submitting the pod to kubernetes 03/09/23 16:14:24.429
Mar  9 16:14:24.437: INFO: Waiting up to 5m0s for pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" in namespace "pods-6798" to be "running and ready"
Mar  9 16:14:24.440: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358156ms
Mar  9 16:14:24.440: INFO: The phase of Pod pod-update-b0798345-e203-4aa2-a341-dd527f88c622 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:14:26.443: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622": Phase="Running", Reason="", readiness=true. Elapsed: 2.005976474s
Mar  9 16:14:26.443: INFO: The phase of Pod pod-update-b0798345-e203-4aa2-a341-dd527f88c622 is Running (Ready = true)
Mar  9 16:14:26.443: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/09/23 16:14:26.446
STEP: updating the pod 03/09/23 16:14:26.449
Mar  9 16:14:26.959: INFO: Successfully updated pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622"
Mar  9 16:14:26.959: INFO: Waiting up to 5m0s for pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" in namespace "pods-6798" to be "running"
Mar  9 16:14:26.961: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622": Phase="Running", Reason="", readiness=true. Elapsed: 2.621027ms
Mar  9 16:14:26.961: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/09/23 16:14:26.961
Mar  9 16:14:26.964: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:14:26.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6798" for this suite. 03/09/23 16:14:26.968
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":86,"skipped":1926,"failed":0}
------------------------------
â€¢ [2.561 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:14:24.412
    Mar  9 16:14:24.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:14:24.413
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:24.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:24.427
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 03/09/23 16:14:24.429
    STEP: submitting the pod to kubernetes 03/09/23 16:14:24.429
    Mar  9 16:14:24.437: INFO: Waiting up to 5m0s for pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" in namespace "pods-6798" to be "running and ready"
    Mar  9 16:14:24.440: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358156ms
    Mar  9 16:14:24.440: INFO: The phase of Pod pod-update-b0798345-e203-4aa2-a341-dd527f88c622 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:14:26.443: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622": Phase="Running", Reason="", readiness=true. Elapsed: 2.005976474s
    Mar  9 16:14:26.443: INFO: The phase of Pod pod-update-b0798345-e203-4aa2-a341-dd527f88c622 is Running (Ready = true)
    Mar  9 16:14:26.443: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/09/23 16:14:26.446
    STEP: updating the pod 03/09/23 16:14:26.449
    Mar  9 16:14:26.959: INFO: Successfully updated pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622"
    Mar  9 16:14:26.959: INFO: Waiting up to 5m0s for pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" in namespace "pods-6798" to be "running"
    Mar  9 16:14:26.961: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622": Phase="Running", Reason="", readiness=true. Elapsed: 2.621027ms
    Mar  9 16:14:26.961: INFO: Pod "pod-update-b0798345-e203-4aa2-a341-dd527f88c622" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/09/23 16:14:26.961
    Mar  9 16:14:26.964: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:14:26.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6798" for this suite. 03/09/23 16:14:26.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:14:26.975
Mar  9 16:14:26.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:14:26.977
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:26.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:26.99
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-758f639b-e010-4dbd-a9a7-d1e632a0a7cd 03/09/23 16:14:26.993
STEP: Creating a pod to test consume secrets 03/09/23 16:14:26.997
Mar  9 16:14:27.005: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9" in namespace "projected-6699" to be "Succeeded or Failed"
Mar  9 16:14:27.008: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871678ms
Mar  9 16:14:29.014: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008443856s
Mar  9 16:14:31.013: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007668168s
STEP: Saw pod success 03/09/23 16:14:31.013
Mar  9 16:14:31.013: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9" satisfied condition "Succeeded or Failed"
Mar  9 16:14:31.016: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:14:31.03
Mar  9 16:14:31.038: INFO: Waiting for pod pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9 to disappear
Mar  9 16:14:31.041: INFO: Pod pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  9 16:14:31.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6699" for this suite. 03/09/23 16:14:31.044
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":87,"skipped":1939,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:14:26.975
    Mar  9 16:14:26.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:14:26.977
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:26.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:26.99
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-758f639b-e010-4dbd-a9a7-d1e632a0a7cd 03/09/23 16:14:26.993
    STEP: Creating a pod to test consume secrets 03/09/23 16:14:26.997
    Mar  9 16:14:27.005: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9" in namespace "projected-6699" to be "Succeeded or Failed"
    Mar  9 16:14:27.008: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871678ms
    Mar  9 16:14:29.014: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008443856s
    Mar  9 16:14:31.013: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007668168s
    STEP: Saw pod success 03/09/23 16:14:31.013
    Mar  9 16:14:31.013: INFO: Pod "pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9" satisfied condition "Succeeded or Failed"
    Mar  9 16:14:31.016: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:14:31.03
    Mar  9 16:14:31.038: INFO: Waiting for pod pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9 to disappear
    Mar  9 16:14:31.041: INFO: Pod pod-projected-secrets-e7c53205-4e01-42f1-abfd-59478a6cf0b9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  9 16:14:31.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6699" for this suite. 03/09/23 16:14:31.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:14:31.05
Mar  9 16:14:31.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename endpointslice 03/09/23 16:14:31.051
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:31.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:31.066
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 03/09/23 16:14:36.145
STEP: referencing matching pods with named port 03/09/23 16:14:41.152
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/09/23 16:14:46.16
STEP: recreating EndpointSlices after they've been deleted 03/09/23 16:14:51.168
Mar  9 16:14:51.184: INFO: EndpointSlice for Service endpointslice-6197/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  9 16:15:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6197" for this suite. 03/09/23 16:15:01.197
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":88,"skipped":1954,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.153 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:14:31.05
    Mar  9 16:14:31.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename endpointslice 03/09/23 16:14:31.051
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:14:31.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:14:31.066
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 03/09/23 16:14:36.145
    STEP: referencing matching pods with named port 03/09/23 16:14:41.152
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/09/23 16:14:46.16
    STEP: recreating EndpointSlices after they've been deleted 03/09/23 16:14:51.168
    Mar  9 16:14:51.184: INFO: EndpointSlice for Service endpointslice-6197/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  9 16:15:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6197" for this suite. 03/09/23 16:15:01.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:01.204
Mar  9 16:15:01.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:15:01.205
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:01.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:01.219
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 03/09/23 16:15:01.222
STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:15:01.227
STEP: Creating a ResourceQuota with not best effort scope 03/09/23 16:15:03.23
STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:15:03.235
STEP: Creating a best-effort pod 03/09/23 16:15:05.239
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/09/23 16:15:05.25
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/09/23 16:15:07.254
STEP: Deleting the pod 03/09/23 16:15:09.258
STEP: Ensuring resource quota status released the pod usage 03/09/23 16:15:09.266
STEP: Creating a not best-effort pod 03/09/23 16:15:11.272
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/09/23 16:15:11.282
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/09/23 16:15:13.286
STEP: Deleting the pod 03/09/23 16:15:15.29
STEP: Ensuring resource quota status released the pod usage 03/09/23 16:15:15.303
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:15:17.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2664" for this suite. 03/09/23 16:15:17.311
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":89,"skipped":1963,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.113 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:01.204
    Mar  9 16:15:01.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:15:01.205
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:01.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:01.219
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 03/09/23 16:15:01.222
    STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:15:01.227
    STEP: Creating a ResourceQuota with not best effort scope 03/09/23 16:15:03.23
    STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:15:03.235
    STEP: Creating a best-effort pod 03/09/23 16:15:05.239
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/09/23 16:15:05.25
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/09/23 16:15:07.254
    STEP: Deleting the pod 03/09/23 16:15:09.258
    STEP: Ensuring resource quota status released the pod usage 03/09/23 16:15:09.266
    STEP: Creating a not best-effort pod 03/09/23 16:15:11.272
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/09/23 16:15:11.282
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/09/23 16:15:13.286
    STEP: Deleting the pod 03/09/23 16:15:15.29
    STEP: Ensuring resource quota status released the pod usage 03/09/23 16:15:15.303
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:15:17.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2664" for this suite. 03/09/23 16:15:17.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:17.319
Mar  9 16:15:17.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-runtime 03/09/23 16:15:17.321
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:17.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:17.336
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/09/23 16:15:17.347
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/09/23 16:15:34.415
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/09/23 16:15:34.417
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/09/23 16:15:34.422
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/09/23 16:15:34.422
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/09/23 16:15:34.44
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/09/23 16:15:37.454
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/09/23 16:15:39.463
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/09/23 16:15:39.469
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/09/23 16:15:39.469
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/09/23 16:15:39.485
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/09/23 16:15:40.493
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/09/23 16:15:43.507
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/09/23 16:15:43.512
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/09/23 16:15:43.512
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  9 16:15:43.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6988" for this suite. 03/09/23 16:15:43.538
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":90,"skipped":1998,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.225 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:17.319
    Mar  9 16:15:17.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-runtime 03/09/23 16:15:17.321
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:17.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:17.336
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/09/23 16:15:17.347
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/09/23 16:15:34.415
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/09/23 16:15:34.417
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/09/23 16:15:34.422
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/09/23 16:15:34.422
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/09/23 16:15:34.44
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/09/23 16:15:37.454
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/09/23 16:15:39.463
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/09/23 16:15:39.469
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/09/23 16:15:39.469
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/09/23 16:15:39.485
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/09/23 16:15:40.493
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/09/23 16:15:43.507
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/09/23 16:15:43.512
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/09/23 16:15:43.512
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  9 16:15:43.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6988" for this suite. 03/09/23 16:15:43.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:43.545
Mar  9 16:15:43.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:15:43.547
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:43.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:43.56
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 03/09/23 16:15:43.562
STEP: Getting a ResourceQuota 03/09/23 16:15:43.566
STEP: Listing all ResourceQuotas with LabelSelector 03/09/23 16:15:43.569
STEP: Patching the ResourceQuota 03/09/23 16:15:43.571
STEP: Deleting a Collection of ResourceQuotas 03/09/23 16:15:43.578
STEP: Verifying the deleted ResourceQuota 03/09/23 16:15:43.587
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:15:43.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5255" for this suite. 03/09/23 16:15:43.592
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":91,"skipped":2005,"failed":0}
------------------------------
â€¢ [0.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:43.545
    Mar  9 16:15:43.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:15:43.547
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:43.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:43.56
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 03/09/23 16:15:43.562
    STEP: Getting a ResourceQuota 03/09/23 16:15:43.566
    STEP: Listing all ResourceQuotas with LabelSelector 03/09/23 16:15:43.569
    STEP: Patching the ResourceQuota 03/09/23 16:15:43.571
    STEP: Deleting a Collection of ResourceQuotas 03/09/23 16:15:43.578
    STEP: Verifying the deleted ResourceQuota 03/09/23 16:15:43.587
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:15:43.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5255" for this suite. 03/09/23 16:15:43.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:43.599
Mar  9 16:15:43.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename daemonsets 03/09/23 16:15:43.6
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:43.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:43.612
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 03/09/23 16:15:43.63
STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 16:15:43.634
Mar  9 16:15:43.643: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:15:43.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:15:43.645: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:15:44.650: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:15:44.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:15:44.653: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:15:45.650: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:15:45.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 16:15:45.653: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 03/09/23 16:15:45.655
Mar  9 16:15:45.658: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/09/23 16:15:45.658
Mar  9 16:15:45.668: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/09/23 16:15:45.668
Mar  9 16:15:45.670: INFO: Observed &DaemonSet event: ADDED
Mar  9 16:15:45.670: INFO: Observed &DaemonSet event: MODIFIED
Mar  9 16:15:45.671: INFO: Observed &DaemonSet event: MODIFIED
Mar  9 16:15:45.671: INFO: Observed &DaemonSet event: MODIFIED
Mar  9 16:15:45.671: INFO: Found daemon set daemon-set in namespace daemonsets-4750 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  9 16:15:45.671: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/09/23 16:15:45.671
STEP: watching for the daemon set status to be patched 03/09/23 16:15:45.676
Mar  9 16:15:45.678: INFO: Observed &DaemonSet event: ADDED
Mar  9 16:15:45.678: INFO: Observed &DaemonSet event: MODIFIED
Mar  9 16:15:45.679: INFO: Observed &DaemonSet event: MODIFIED
Mar  9 16:15:45.679: INFO: Observed &DaemonSet event: MODIFIED
Mar  9 16:15:45.679: INFO: Observed daemon set daemon-set in namespace daemonsets-4750 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  9 16:15:45.679: INFO: Observed &DaemonSet event: MODIFIED
Mar  9 16:15:45.679: INFO: Found daemon set daemon-set in namespace daemonsets-4750 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar  9 16:15:45.679: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:15:45.683
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4750, will wait for the garbage collector to delete the pods 03/09/23 16:15:45.683
Mar  9 16:15:45.741: INFO: Deleting DaemonSet.extensions daemon-set took: 5.292665ms
Mar  9 16:15:45.842: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.071141ms
Mar  9 16:15:48.246: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:15:48.246: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  9 16:15:48.248: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"96969"},"items":null}

Mar  9 16:15:48.250: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"96969"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:15:48.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4750" for this suite. 03/09/23 16:15:48.263
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":92,"skipped":2044,"failed":0}
------------------------------
â€¢ [4.670 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:43.599
    Mar  9 16:15:43.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename daemonsets 03/09/23 16:15:43.6
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:43.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:43.612
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 03/09/23 16:15:43.63
    STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 16:15:43.634
    Mar  9 16:15:43.643: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:15:43.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:15:43.645: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:15:44.650: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:15:44.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:15:44.653: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:15:45.650: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:15:45.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 16:15:45.653: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 03/09/23 16:15:45.655
    Mar  9 16:15:45.658: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/09/23 16:15:45.658
    Mar  9 16:15:45.668: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/09/23 16:15:45.668
    Mar  9 16:15:45.670: INFO: Observed &DaemonSet event: ADDED
    Mar  9 16:15:45.670: INFO: Observed &DaemonSet event: MODIFIED
    Mar  9 16:15:45.671: INFO: Observed &DaemonSet event: MODIFIED
    Mar  9 16:15:45.671: INFO: Observed &DaemonSet event: MODIFIED
    Mar  9 16:15:45.671: INFO: Found daemon set daemon-set in namespace daemonsets-4750 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  9 16:15:45.671: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/09/23 16:15:45.671
    STEP: watching for the daemon set status to be patched 03/09/23 16:15:45.676
    Mar  9 16:15:45.678: INFO: Observed &DaemonSet event: ADDED
    Mar  9 16:15:45.678: INFO: Observed &DaemonSet event: MODIFIED
    Mar  9 16:15:45.679: INFO: Observed &DaemonSet event: MODIFIED
    Mar  9 16:15:45.679: INFO: Observed &DaemonSet event: MODIFIED
    Mar  9 16:15:45.679: INFO: Observed daemon set daemon-set in namespace daemonsets-4750 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  9 16:15:45.679: INFO: Observed &DaemonSet event: MODIFIED
    Mar  9 16:15:45.679: INFO: Found daemon set daemon-set in namespace daemonsets-4750 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar  9 16:15:45.679: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:15:45.683
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4750, will wait for the garbage collector to delete the pods 03/09/23 16:15:45.683
    Mar  9 16:15:45.741: INFO: Deleting DaemonSet.extensions daemon-set took: 5.292665ms
    Mar  9 16:15:45.842: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.071141ms
    Mar  9 16:15:48.246: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:15:48.246: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  9 16:15:48.248: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"96969"},"items":null}

    Mar  9 16:15:48.250: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"96969"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:15:48.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4750" for this suite. 03/09/23 16:15:48.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:48.27
Mar  9 16:15:48.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename init-container 03/09/23 16:15:48.271
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:48.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:48.286
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 03/09/23 16:15:48.289
Mar  9 16:15:48.289: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  9 16:15:52.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5546" for this suite. 03/09/23 16:15:52.221
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":93,"skipped":2061,"failed":0}
------------------------------
â€¢ [3.956 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:48.27
    Mar  9 16:15:48.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename init-container 03/09/23 16:15:48.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:48.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:48.286
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 03/09/23 16:15:48.289
    Mar  9 16:15:48.289: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  9 16:15:52.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5546" for this suite. 03/09/23 16:15:52.221
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:52.226
Mar  9 16:15:52.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:15:52.227
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:52.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:52.241
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 03/09/23 16:15:52.244
Mar  9 16:15:52.251: INFO: Waiting up to 5m0s for pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad" in namespace "emptydir-1585" to be "Succeeded or Failed"
Mar  9 16:15:52.253: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.387892ms
Mar  9 16:15:54.256: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005119367s
Mar  9 16:15:56.258: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007251855s
STEP: Saw pod success 03/09/23 16:15:56.258
Mar  9 16:15:56.259: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad" satisfied condition "Succeeded or Failed"
Mar  9 16:15:56.262: INFO: Trying to get logs from node tt-test-el8-003 pod pod-833fb605-7580-4a43-97dc-29bc26b4c0ad container test-container: <nil>
STEP: delete the pod 03/09/23 16:15:56.268
Mar  9 16:15:56.279: INFO: Waiting for pod pod-833fb605-7580-4a43-97dc-29bc26b4c0ad to disappear
Mar  9 16:15:56.281: INFO: Pod pod-833fb605-7580-4a43-97dc-29bc26b4c0ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:15:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1585" for this suite. 03/09/23 16:15:56.284
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":94,"skipped":2061,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:52.226
    Mar  9 16:15:52.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:15:52.227
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:52.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:52.241
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/09/23 16:15:52.244
    Mar  9 16:15:52.251: INFO: Waiting up to 5m0s for pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad" in namespace "emptydir-1585" to be "Succeeded or Failed"
    Mar  9 16:15:52.253: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.387892ms
    Mar  9 16:15:54.256: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005119367s
    Mar  9 16:15:56.258: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007251855s
    STEP: Saw pod success 03/09/23 16:15:56.258
    Mar  9 16:15:56.259: INFO: Pod "pod-833fb605-7580-4a43-97dc-29bc26b4c0ad" satisfied condition "Succeeded or Failed"
    Mar  9 16:15:56.262: INFO: Trying to get logs from node tt-test-el8-003 pod pod-833fb605-7580-4a43-97dc-29bc26b4c0ad container test-container: <nil>
    STEP: delete the pod 03/09/23 16:15:56.268
    Mar  9 16:15:56.279: INFO: Waiting for pod pod-833fb605-7580-4a43-97dc-29bc26b4c0ad to disappear
    Mar  9 16:15:56.281: INFO: Pod pod-833fb605-7580-4a43-97dc-29bc26b4c0ad no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:15:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1585" for this suite. 03/09/23 16:15:56.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:56.291
Mar  9 16:15:56.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 16:15:56.292
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:56.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:56.306
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar  9 16:15:56.309: INFO: Creating deployment "test-recreate-deployment"
Mar  9 16:15:56.313: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  9 16:15:56.318: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  9 16:15:58.325: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  9 16:15:58.328: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  9 16:15:58.336: INFO: Updating deployment test-recreate-deployment
Mar  9 16:15:58.336: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 16:15:58.401: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5215  4ca71083-3ba8-4589-af1f-9e622f999449 97134 2 2023-03-09 16:15:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b62358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-09 16:15:58 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-09 16:15:58 +0000 UTC,LastTransitionTime:2023-03-09 16:15:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  9 16:15:58.404: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5215  95d40d93-56a1-4a55-9873-1df90d143436 97131 1 2023-03-09 16:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4ca71083-3ba8-4589-af1f-9e622f999449 0xc001b629a0 0xc001b629a1}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca71083-3ba8-4589-af1f-9e622f999449\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b62c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:15:58.404: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  9 16:15:58.404: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5215  68ae61b7-cc01-47ef-a900-1f57add89f83 97122 2 2023-03-09 16:15:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4ca71083-3ba8-4589-af1f-9e622f999449 0xc001b62887 0xc001b62888}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca71083-3ba8-4589-af1f-9e622f999449\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b62938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:15:58.408: INFO: Pod "test-recreate-deployment-9d58999df-sqbds" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-sqbds test-recreate-deployment-9d58999df- deployment-5215  b56b3d7d-bbc3-442c-a905-a30000c447fa 97133 0 2023-03-09 16:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 95d40d93-56a1-4a55-9873-1df90d143436 0xc001b638e0 0xc001b638e1}] [] [{kube-controller-manager Update v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95d40d93-56a1-4a55-9873-1df90d143436\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7728f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7728f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:,StartTime:2023-03-09 16:15:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 16:15:58.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5215" for this suite. 03/09/23 16:15:58.412
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":95,"skipped":2068,"failed":0}
------------------------------
â€¢ [2.126 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:56.291
    Mar  9 16:15:56.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 16:15:56.292
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:56.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:56.306
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar  9 16:15:56.309: INFO: Creating deployment "test-recreate-deployment"
    Mar  9 16:15:56.313: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar  9 16:15:56.318: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar  9 16:15:58.325: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar  9 16:15:58.328: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar  9 16:15:58.336: INFO: Updating deployment test-recreate-deployment
    Mar  9 16:15:58.336: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 16:15:58.401: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5215  4ca71083-3ba8-4589-af1f-9e622f999449 97134 2 2023-03-09 16:15:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b62358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-09 16:15:58 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-09 16:15:58 +0000 UTC,LastTransitionTime:2023-03-09 16:15:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  9 16:15:58.404: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5215  95d40d93-56a1-4a55-9873-1df90d143436 97131 1 2023-03-09 16:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4ca71083-3ba8-4589-af1f-9e622f999449 0xc001b629a0 0xc001b629a1}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca71083-3ba8-4589-af1f-9e622f999449\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b62c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:15:58.404: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar  9 16:15:58.404: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5215  68ae61b7-cc01-47ef-a900-1f57add89f83 97122 2 2023-03-09 16:15:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4ca71083-3ba8-4589-af1f-9e622f999449 0xc001b62887 0xc001b62888}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca71083-3ba8-4589-af1f-9e622f999449\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b62938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:15:58.408: INFO: Pod "test-recreate-deployment-9d58999df-sqbds" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-sqbds test-recreate-deployment-9d58999df- deployment-5215  b56b3d7d-bbc3-442c-a905-a30000c447fa 97133 0 2023-03-09 16:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 95d40d93-56a1-4a55-9873-1df90d143436 0xc001b638e0 0xc001b638e1}] [] [{kube-controller-manager Update v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95d40d93-56a1-4a55-9873-1df90d143436\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7728f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7728f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:15:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:,StartTime:2023-03-09 16:15:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 16:15:58.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5215" for this suite. 03/09/23 16:15:58.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:15:58.421
Mar  9 16:15:58.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:15:58.422
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:58.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:58.436
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 03/09/23 16:15:58.439
Mar  9 16:15:58.445: INFO: Waiting up to 5m0s for pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569" in namespace "projected-2057" to be "running and ready"
Mar  9 16:15:58.448: INFO: Pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5993ms
Mar  9 16:15:58.448: INFO: The phase of Pod annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:16:00.452: INFO: Pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569": Phase="Running", Reason="", readiness=true. Elapsed: 2.007128419s
Mar  9 16:16:00.452: INFO: The phase of Pod annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569 is Running (Ready = true)
Mar  9 16:16:00.452: INFO: Pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569" satisfied condition "running and ready"
Mar  9 16:16:00.975: INFO: Successfully updated pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 16:16:04.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2057" for this suite. 03/09/23 16:16:04.999
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":96,"skipped":2132,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.584 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:15:58.421
    Mar  9 16:15:58.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:15:58.422
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:15:58.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:15:58.436
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 03/09/23 16:15:58.439
    Mar  9 16:15:58.445: INFO: Waiting up to 5m0s for pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569" in namespace "projected-2057" to be "running and ready"
    Mar  9 16:15:58.448: INFO: Pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5993ms
    Mar  9 16:15:58.448: INFO: The phase of Pod annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:16:00.452: INFO: Pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569": Phase="Running", Reason="", readiness=true. Elapsed: 2.007128419s
    Mar  9 16:16:00.452: INFO: The phase of Pod annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569 is Running (Ready = true)
    Mar  9 16:16:00.452: INFO: Pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569" satisfied condition "running and ready"
    Mar  9 16:16:00.975: INFO: Successfully updated pod "annotationupdate76594d8e-c4cb-4e32-b61b-679dac4cc569"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 16:16:04.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2057" for this suite. 03/09/23 16:16:04.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:05.006
Mar  9 16:16:05.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 16:16:05.006
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:05.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:05.021
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 03/09/23 16:16:05.024
Mar  9 16:16:05.031: INFO: Waiting up to 5m0s for pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376" in namespace "var-expansion-2626" to be "Succeeded or Failed"
Mar  9 16:16:05.033: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155056ms
Mar  9 16:16:07.036: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005658034s
Mar  9 16:16:09.036: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005571023s
STEP: Saw pod success 03/09/23 16:16:09.036
Mar  9 16:16:09.036: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376" satisfied condition "Succeeded or Failed"
Mar  9 16:16:09.039: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-f8b4475c-cf91-48bd-a53f-204037764376 container dapi-container: <nil>
STEP: delete the pod 03/09/23 16:16:09.044
Mar  9 16:16:09.055: INFO: Waiting for pod var-expansion-f8b4475c-cf91-48bd-a53f-204037764376 to disappear
Mar  9 16:16:09.057: INFO: Pod var-expansion-f8b4475c-cf91-48bd-a53f-204037764376 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 16:16:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2626" for this suite. 03/09/23 16:16:09.061
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":97,"skipped":2152,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:05.006
    Mar  9 16:16:05.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 16:16:05.006
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:05.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:05.021
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 03/09/23 16:16:05.024
    Mar  9 16:16:05.031: INFO: Waiting up to 5m0s for pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376" in namespace "var-expansion-2626" to be "Succeeded or Failed"
    Mar  9 16:16:05.033: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155056ms
    Mar  9 16:16:07.036: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005658034s
    Mar  9 16:16:09.036: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005571023s
    STEP: Saw pod success 03/09/23 16:16:09.036
    Mar  9 16:16:09.036: INFO: Pod "var-expansion-f8b4475c-cf91-48bd-a53f-204037764376" satisfied condition "Succeeded or Failed"
    Mar  9 16:16:09.039: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-f8b4475c-cf91-48bd-a53f-204037764376 container dapi-container: <nil>
    STEP: delete the pod 03/09/23 16:16:09.044
    Mar  9 16:16:09.055: INFO: Waiting for pod var-expansion-f8b4475c-cf91-48bd-a53f-204037764376 to disappear
    Mar  9 16:16:09.057: INFO: Pod var-expansion-f8b4475c-cf91-48bd-a53f-204037764376 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 16:16:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2626" for this suite. 03/09/23 16:16:09.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:09.067
Mar  9 16:16:09.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:16:09.068
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:09.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:09.081
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 03/09/23 16:16:09.084
Mar  9 16:16:09.091: INFO: Waiting up to 5m0s for pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540" in namespace "emptydir-6500" to be "Succeeded or Failed"
Mar  9 16:16:09.094: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414902ms
Mar  9 16:16:11.097: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005957441s
Mar  9 16:16:13.100: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008430372s
STEP: Saw pod success 03/09/23 16:16:13.1
Mar  9 16:16:13.100: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540" satisfied condition "Succeeded or Failed"
Mar  9 16:16:13.102: INFO: Trying to get logs from node tt-test-el8-003 pod pod-5cc11840-30b0-4d6f-9632-966ccb0cb540 container test-container: <nil>
STEP: delete the pod 03/09/23 16:16:13.108
Mar  9 16:16:13.117: INFO: Waiting for pod pod-5cc11840-30b0-4d6f-9632-966ccb0cb540 to disappear
Mar  9 16:16:13.119: INFO: Pod pod-5cc11840-30b0-4d6f-9632-966ccb0cb540 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:16:13.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6500" for this suite. 03/09/23 16:16:13.123
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":98,"skipped":2160,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:09.067
    Mar  9 16:16:09.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:16:09.068
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:09.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:09.081
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/09/23 16:16:09.084
    Mar  9 16:16:09.091: INFO: Waiting up to 5m0s for pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540" in namespace "emptydir-6500" to be "Succeeded or Failed"
    Mar  9 16:16:09.094: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414902ms
    Mar  9 16:16:11.097: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005957441s
    Mar  9 16:16:13.100: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008430372s
    STEP: Saw pod success 03/09/23 16:16:13.1
    Mar  9 16:16:13.100: INFO: Pod "pod-5cc11840-30b0-4d6f-9632-966ccb0cb540" satisfied condition "Succeeded or Failed"
    Mar  9 16:16:13.102: INFO: Trying to get logs from node tt-test-el8-003 pod pod-5cc11840-30b0-4d6f-9632-966ccb0cb540 container test-container: <nil>
    STEP: delete the pod 03/09/23 16:16:13.108
    Mar  9 16:16:13.117: INFO: Waiting for pod pod-5cc11840-30b0-4d6f-9632-966ccb0cb540 to disappear
    Mar  9 16:16:13.119: INFO: Pod pod-5cc11840-30b0-4d6f-9632-966ccb0cb540 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:16:13.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6500" for this suite. 03/09/23 16:16:13.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:13.129
Mar  9 16:16:13.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:16:13.129
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:13.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:13.142
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 03/09/23 16:16:13.145
Mar  9 16:16:13.151: INFO: Waiting up to 5m0s for pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef" in namespace "emptydir-2692" to be "Succeeded or Failed"
Mar  9 16:16:13.154: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802995ms
Mar  9 16:16:15.158: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006331692s
Mar  9 16:16:17.158: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007032205s
STEP: Saw pod success 03/09/23 16:16:17.158
Mar  9 16:16:17.159: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef" satisfied condition "Succeeded or Failed"
Mar  9 16:16:17.161: INFO: Trying to get logs from node tt-test-el8-003 pod pod-71357367-ab7a-40cf-b061-6b89d966d9ef container test-container: <nil>
STEP: delete the pod 03/09/23 16:16:17.166
Mar  9 16:16:17.178: INFO: Waiting for pod pod-71357367-ab7a-40cf-b061-6b89d966d9ef to disappear
Mar  9 16:16:17.181: INFO: Pod pod-71357367-ab7a-40cf-b061-6b89d966d9ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:16:17.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2692" for this suite. 03/09/23 16:16:17.184
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":2178,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:13.129
    Mar  9 16:16:13.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:16:13.129
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:13.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:13.142
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 03/09/23 16:16:13.145
    Mar  9 16:16:13.151: INFO: Waiting up to 5m0s for pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef" in namespace "emptydir-2692" to be "Succeeded or Failed"
    Mar  9 16:16:13.154: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802995ms
    Mar  9 16:16:15.158: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006331692s
    Mar  9 16:16:17.158: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007032205s
    STEP: Saw pod success 03/09/23 16:16:17.158
    Mar  9 16:16:17.159: INFO: Pod "pod-71357367-ab7a-40cf-b061-6b89d966d9ef" satisfied condition "Succeeded or Failed"
    Mar  9 16:16:17.161: INFO: Trying to get logs from node tt-test-el8-003 pod pod-71357367-ab7a-40cf-b061-6b89d966d9ef container test-container: <nil>
    STEP: delete the pod 03/09/23 16:16:17.166
    Mar  9 16:16:17.178: INFO: Waiting for pod pod-71357367-ab7a-40cf-b061-6b89d966d9ef to disappear
    Mar  9 16:16:17.181: INFO: Pod pod-71357367-ab7a-40cf-b061-6b89d966d9ef no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:16:17.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2692" for this suite. 03/09/23 16:16:17.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:17.19
Mar  9 16:16:17.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 16:16:17.191
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:17.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:17.206
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-2019/configmap-test-eaee24b1-27fb-4fa9-ae7f-7fdc058c31b3 03/09/23 16:16:17.209
STEP: Creating a pod to test consume configMaps 03/09/23 16:16:17.212
Mar  9 16:16:17.219: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939" in namespace "configmap-2019" to be "Succeeded or Failed"
Mar  9 16:16:17.221: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325293ms
Mar  9 16:16:19.225: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006055507s
Mar  9 16:16:21.225: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005941354s
STEP: Saw pod success 03/09/23 16:16:21.225
Mar  9 16:16:21.225: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939" satisfied condition "Succeeded or Failed"
Mar  9 16:16:21.228: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939 container env-test: <nil>
STEP: delete the pod 03/09/23 16:16:21.234
Mar  9 16:16:21.244: INFO: Waiting for pod pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939 to disappear
Mar  9 16:16:21.247: INFO: Pod pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 16:16:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2019" for this suite. 03/09/23 16:16:21.25
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":100,"skipped":2186,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:17.19
    Mar  9 16:16:17.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 16:16:17.191
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:17.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:17.206
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-2019/configmap-test-eaee24b1-27fb-4fa9-ae7f-7fdc058c31b3 03/09/23 16:16:17.209
    STEP: Creating a pod to test consume configMaps 03/09/23 16:16:17.212
    Mar  9 16:16:17.219: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939" in namespace "configmap-2019" to be "Succeeded or Failed"
    Mar  9 16:16:17.221: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325293ms
    Mar  9 16:16:19.225: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006055507s
    Mar  9 16:16:21.225: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005941354s
    STEP: Saw pod success 03/09/23 16:16:21.225
    Mar  9 16:16:21.225: INFO: Pod "pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939" satisfied condition "Succeeded or Failed"
    Mar  9 16:16:21.228: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939 container env-test: <nil>
    STEP: delete the pod 03/09/23 16:16:21.234
    Mar  9 16:16:21.244: INFO: Waiting for pod pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939 to disappear
    Mar  9 16:16:21.247: INFO: Pod pod-configmaps-3f3c8f73-792b-4d80-b81d-b0cac152a939 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 16:16:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2019" for this suite. 03/09/23 16:16:21.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:21.257
Mar  9 16:16:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:16:21.258
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:21.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:21.271
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 03/09/23 16:16:21.273
Mar  9 16:16:21.274: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4651 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/09/23 16:16:21.326
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:16:21.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4651" for this suite. 03/09/23 16:16:21.339
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":101,"skipped":2205,"failed":0}
------------------------------
â€¢ [0.089 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:21.257
    Mar  9 16:16:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:16:21.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:21.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:21.271
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 03/09/23 16:16:21.273
    Mar  9 16:16:21.274: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4651 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/09/23 16:16:21.326
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:16:21.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4651" for this suite. 03/09/23 16:16:21.339
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:21.346
Mar  9 16:16:21.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:16:21.347
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:21.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:21.362
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:16:21.376
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:16:22.593
STEP: Deploying the webhook pod 03/09/23 16:16:22.602
STEP: Wait for the deployment to be ready 03/09/23 16:16:22.614
Mar  9 16:16:22.620: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:16:24.63
STEP: Verifying the service has paired with the endpoint 03/09/23 16:16:24.648
Mar  9 16:16:25.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 03/09/23 16:16:25.652
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/09/23 16:16:25.67
STEP: Creating a configMap that should not be mutated 03/09/23 16:16:25.676
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/09/23 16:16:25.684
STEP: Creating a configMap that should be mutated 03/09/23 16:16:25.69
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:16:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7313" for this suite. 03/09/23 16:16:25.713
STEP: Destroying namespace "webhook-7313-markers" for this suite. 03/09/23 16:16:25.718
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":102,"skipped":2206,"failed":0}
------------------------------
â€¢ [4.418 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:21.346
    Mar  9 16:16:21.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:16:21.347
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:21.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:21.362
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:16:21.376
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:16:22.593
    STEP: Deploying the webhook pod 03/09/23 16:16:22.602
    STEP: Wait for the deployment to be ready 03/09/23 16:16:22.614
    Mar  9 16:16:22.620: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:16:24.63
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:16:24.648
    Mar  9 16:16:25.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 03/09/23 16:16:25.652
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/09/23 16:16:25.67
    STEP: Creating a configMap that should not be mutated 03/09/23 16:16:25.676
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/09/23 16:16:25.684
    STEP: Creating a configMap that should be mutated 03/09/23 16:16:25.69
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:16:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7313" for this suite. 03/09/23 16:16:25.713
    STEP: Destroying namespace "webhook-7313-markers" for this suite. 03/09/23 16:16:25.718
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:25.764
Mar  9 16:16:25.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:16:25.765
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:25.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:25.787
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-fe0c18a4-e085-4762-8441-b81abd87aced 03/09/23 16:16:25.795
STEP: Creating a pod to test consume configMaps 03/09/23 16:16:25.799
Mar  9 16:16:25.807: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3" in namespace "projected-593" to be "Succeeded or Failed"
Mar  9 16:16:25.811: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477638ms
Mar  9 16:16:27.815: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007235939s
Mar  9 16:16:29.814: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006882093s
STEP: Saw pod success 03/09/23 16:16:29.814
Mar  9 16:16:29.814: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3" satisfied condition "Succeeded or Failed"
Mar  9 16:16:29.817: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:16:29.823
Mar  9 16:16:29.832: INFO: Waiting for pod pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3 to disappear
Mar  9 16:16:29.834: INFO: Pod pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 16:16:29.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-593" for this suite. 03/09/23 16:16:29.838
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":103,"skipped":2209,"failed":0}
------------------------------
â€¢ [4.078 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:25.764
    Mar  9 16:16:25.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:16:25.765
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:25.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:25.787
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-fe0c18a4-e085-4762-8441-b81abd87aced 03/09/23 16:16:25.795
    STEP: Creating a pod to test consume configMaps 03/09/23 16:16:25.799
    Mar  9 16:16:25.807: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3" in namespace "projected-593" to be "Succeeded or Failed"
    Mar  9 16:16:25.811: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477638ms
    Mar  9 16:16:27.815: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007235939s
    Mar  9 16:16:29.814: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006882093s
    STEP: Saw pod success 03/09/23 16:16:29.814
    Mar  9 16:16:29.814: INFO: Pod "pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3" satisfied condition "Succeeded or Failed"
    Mar  9 16:16:29.817: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:16:29.823
    Mar  9 16:16:29.832: INFO: Waiting for pod pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3 to disappear
    Mar  9 16:16:29.834: INFO: Pod pod-projected-configmaps-9b1321c1-e836-4cfd-b5bc-954ad9043ac3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 16:16:29.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-593" for this suite. 03/09/23 16:16:29.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:29.843
Mar  9 16:16:29.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:16:29.845
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:29.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:29.861
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Mar  9 16:16:29.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 create -f -'
Mar  9 16:16:30.827: INFO: stderr: ""
Mar  9 16:16:30.827: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  9 16:16:30.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 create -f -'
Mar  9 16:16:31.141: INFO: stderr: ""
Mar  9 16:16:31.141: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/09/23 16:16:31.141
Mar  9 16:16:32.144: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 16:16:32.145: INFO: Found 0 / 1
Mar  9 16:16:33.146: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 16:16:33.147: INFO: Found 1 / 1
Mar  9 16:16:33.147: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  9 16:16:33.149: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 16:16:33.149: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  9 16:16:33.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe pod agnhost-primary-pd929'
Mar  9 16:16:33.226: INFO: stderr: ""
Mar  9 16:16:33.226: INFO: stdout: "Name:             agnhost-primary-pd929\nNamespace:        kubectl-9405\nPriority:         0\nService Account:  default\nNode:             tt-test-el8-003/100.100.230.140\nStart Time:       Thu, 09 Mar 2023 16:16:30 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8f0d0f0c61f03962f9cc77f2dd33ba2b3048fcf4ac60bd46ece5e6361c47acf8\n                  cni.projectcalico.org/podIP: 10.244.42.200/32\n                  cni.projectcalico.org/podIPs: 10.244.42.200/32\nStatus:           Running\nIP:               10.244.42.200\nIPs:\n  IP:           10.244.42.200\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://6cc78f553a384ec00fbdbe7419eaca8959f23914249e63a25acf096024c1f6c3\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 09 Mar 2023 16:16:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xt7d2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xt7d2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-9405/agnhost-primary-pd929 to tt-test-el8-003\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Mar  9 16:16:33.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe rc agnhost-primary'
Mar  9 16:16:33.307: INFO: stderr: ""
Mar  9 16:16:33.307: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9405\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-pd929\n"
Mar  9 16:16:33.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe service agnhost-primary'
Mar  9 16:16:33.385: INFO: stderr: ""
Mar  9 16:16:33.385: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9405\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.102.225.79\nIPs:               10.102.225.79\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.42.200:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  9 16:16:33.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe node tt-test-el8-001'
Mar  9 16:16:33.484: INFO: stderr: ""
Mar  9 16:16:33.484: INFO: stdout: "Name:               tt-test-el8-001\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=tt-test-el8-001\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.tigera.io\":\"tt-test-el8-001\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 100.100.231.202/23\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.244.254.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 09 Mar 2023 03:21:10 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  tt-test-el8-001\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 09 Mar 2023 16:16:28 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 09 Mar 2023 03:23:35 +0000   Thu, 09 Mar 2023 03:23:35 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:21:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:21:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:21:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:23:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  100.100.231.202\n  Hostname:    tt-test-el8-001\nCapacity:\n  cpu:                4\n  ephemeral-storage:  37177616Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30516444Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  34262890849\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30414044Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 8448c79c798c4cbb93b4c022a583203c\n  System UUID:                8448c79c-798c-4cbb-93b4-c022a583203c\n  Boot ID:                    91cd282b-d49d-434d-a9eb-f870ee8a3107\n  Kernel Version:             5.4.17-2136.314.6.2.el8uek.x86_64\n  OS Image:                   Oracle Linux Server 8.6\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.2\n  Kubelet Version:            v1.25.7+1.el8\n  Kube-Proxy Version:         v1.25.7+1.el8\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   ocid1.instance.oc1.iad.anuwcljrgj4tlxychputoubzk4hxr7rqdgf3w56djsdcunfth5zzdamhh46q\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-apiserver            calico-apiserver-7c747f5cd5-tq2w2                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m39s\n  calico-system               calico-node-qtxfs                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  calico-system               csi-node-driver-l22sf                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 etcd-tt-test-el8-001                                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         12h\n  kube-system                 kube-apiserver-tt-test-el8-001                             250m (6%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-controller-manager-tt-test-el8-001                    200m (5%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-proxy-2xj2g                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-scheduler-tt-test-el8-001                             100m (2%)     0 (0%)      0 (0%)           0 (0%)         12h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-pxrwl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (16%)  0 (0%)\n  memory             100Mi (0%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar  9 16:16:33.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe namespace kubectl-9405'
Mar  9 16:16:33.564: INFO: stderr: ""
Mar  9 16:16:33.564: INFO: stdout: "Name:         kubectl-9405\nLabels:       e2e-framework=kubectl\n              e2e-run=3cb7456a-7771-43f0-b012-5689e2ab8cc3\n              kubernetes.io/metadata.name=kubectl-9405\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:16:33.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9405" for this suite. 03/09/23 16:16:33.567
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":104,"skipped":2220,"failed":0}
------------------------------
â€¢ [3.729 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:29.843
    Mar  9 16:16:29.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:16:29.845
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:29.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:29.861
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Mar  9 16:16:29.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 create -f -'
    Mar  9 16:16:30.827: INFO: stderr: ""
    Mar  9 16:16:30.827: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar  9 16:16:30.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 create -f -'
    Mar  9 16:16:31.141: INFO: stderr: ""
    Mar  9 16:16:31.141: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/09/23 16:16:31.141
    Mar  9 16:16:32.144: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 16:16:32.145: INFO: Found 0 / 1
    Mar  9 16:16:33.146: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 16:16:33.147: INFO: Found 1 / 1
    Mar  9 16:16:33.147: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  9 16:16:33.149: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 16:16:33.149: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  9 16:16:33.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe pod agnhost-primary-pd929'
    Mar  9 16:16:33.226: INFO: stderr: ""
    Mar  9 16:16:33.226: INFO: stdout: "Name:             agnhost-primary-pd929\nNamespace:        kubectl-9405\nPriority:         0\nService Account:  default\nNode:             tt-test-el8-003/100.100.230.140\nStart Time:       Thu, 09 Mar 2023 16:16:30 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8f0d0f0c61f03962f9cc77f2dd33ba2b3048fcf4ac60bd46ece5e6361c47acf8\n                  cni.projectcalico.org/podIP: 10.244.42.200/32\n                  cni.projectcalico.org/podIPs: 10.244.42.200/32\nStatus:           Running\nIP:               10.244.42.200\nIPs:\n  IP:           10.244.42.200\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://6cc78f553a384ec00fbdbe7419eaca8959f23914249e63a25acf096024c1f6c3\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 09 Mar 2023 16:16:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xt7d2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xt7d2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-9405/agnhost-primary-pd929 to tt-test-el8-003\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Mar  9 16:16:33.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe rc agnhost-primary'
    Mar  9 16:16:33.307: INFO: stderr: ""
    Mar  9 16:16:33.307: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9405\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-pd929\n"
    Mar  9 16:16:33.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe service agnhost-primary'
    Mar  9 16:16:33.385: INFO: stderr: ""
    Mar  9 16:16:33.385: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9405\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.102.225.79\nIPs:               10.102.225.79\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.42.200:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar  9 16:16:33.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe node tt-test-el8-001'
    Mar  9 16:16:33.484: INFO: stderr: ""
    Mar  9 16:16:33.484: INFO: stdout: "Name:               tt-test-el8-001\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=tt-test-el8-001\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.tigera.io\":\"tt-test-el8-001\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 100.100.231.202/23\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.244.254.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 09 Mar 2023 03:21:10 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  tt-test-el8-001\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 09 Mar 2023 16:16:28 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 09 Mar 2023 03:23:35 +0000   Thu, 09 Mar 2023 03:23:35 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:21:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:21:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:21:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 09 Mar 2023 16:13:24 +0000   Thu, 09 Mar 2023 03:23:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  100.100.231.202\n  Hostname:    tt-test-el8-001\nCapacity:\n  cpu:                4\n  ephemeral-storage:  37177616Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30516444Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  34262890849\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30414044Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 8448c79c798c4cbb93b4c022a583203c\n  System UUID:                8448c79c-798c-4cbb-93b4-c022a583203c\n  Boot ID:                    91cd282b-d49d-434d-a9eb-f870ee8a3107\n  Kernel Version:             5.4.17-2136.314.6.2.el8uek.x86_64\n  OS Image:                   Oracle Linux Server 8.6\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.2\n  Kubelet Version:            v1.25.7+1.el8\n  Kube-Proxy Version:         v1.25.7+1.el8\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   ocid1.instance.oc1.iad.anuwcljrgj4tlxychputoubzk4hxr7rqdgf3w56djsdcunfth5zzdamhh46q\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-apiserver            calico-apiserver-7c747f5cd5-tq2w2                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m39s\n  calico-system               calico-node-qtxfs                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  calico-system               csi-node-driver-l22sf                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 etcd-tt-test-el8-001                                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         12h\n  kube-system                 kube-apiserver-tt-test-el8-001                             250m (6%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-controller-manager-tt-test-el8-001                    200m (5%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-proxy-2xj2g                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-scheduler-tt-test-el8-001                             100m (2%)     0 (0%)      0 (0%)           0 (0%)         12h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-pxrwl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (16%)  0 (0%)\n  memory             100Mi (0%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Mar  9 16:16:33.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9405 describe namespace kubectl-9405'
    Mar  9 16:16:33.564: INFO: stderr: ""
    Mar  9 16:16:33.564: INFO: stdout: "Name:         kubectl-9405\nLabels:       e2e-framework=kubectl\n              e2e-run=3cb7456a-7771-43f0-b012-5689e2ab8cc3\n              kubernetes.io/metadata.name=kubectl-9405\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:16:33.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9405" for this suite. 03/09/23 16:16:33.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:33.573
Mar  9 16:16:33.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 16:16:33.574
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:33.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:33.59
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6680.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6680.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/09/23 16:16:33.593
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6680.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6680.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/09/23 16:16:33.593
STEP: creating a pod to probe /etc/hosts 03/09/23 16:16:33.593
STEP: submitting the pod to kubernetes 03/09/23 16:16:33.593
Mar  9 16:16:33.600: INFO: Waiting up to 15m0s for pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169" in namespace "dns-6680" to be "running"
Mar  9 16:16:33.603: INFO: Pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.316862ms
Mar  9 16:16:35.607: INFO: Pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169": Phase="Running", Reason="", readiness=true. Elapsed: 2.006279668s
Mar  9 16:16:35.607: INFO: Pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169" satisfied condition "running"
STEP: retrieving the pod 03/09/23 16:16:35.607
STEP: looking for the results for each expected name from probers 03/09/23 16:16:35.61
Mar  9 16:16:35.623: INFO: DNS probes using dns-6680/dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169 succeeded

STEP: deleting the pod 03/09/23 16:16:35.623
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 16:16:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6680" for this suite. 03/09/23 16:16:35.638
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":105,"skipped":2230,"failed":0}
------------------------------
â€¢ [2.070 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:33.573
    Mar  9 16:16:33.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 16:16:33.574
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:33.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:33.59
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6680.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6680.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/09/23 16:16:33.593
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6680.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6680.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/09/23 16:16:33.593
    STEP: creating a pod to probe /etc/hosts 03/09/23 16:16:33.593
    STEP: submitting the pod to kubernetes 03/09/23 16:16:33.593
    Mar  9 16:16:33.600: INFO: Waiting up to 15m0s for pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169" in namespace "dns-6680" to be "running"
    Mar  9 16:16:33.603: INFO: Pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.316862ms
    Mar  9 16:16:35.607: INFO: Pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169": Phase="Running", Reason="", readiness=true. Elapsed: 2.006279668s
    Mar  9 16:16:35.607: INFO: Pod "dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 16:16:35.607
    STEP: looking for the results for each expected name from probers 03/09/23 16:16:35.61
    Mar  9 16:16:35.623: INFO: DNS probes using dns-6680/dns-test-2b1e8e2b-dee2-4408-9adf-443f8c65f169 succeeded

    STEP: deleting the pod 03/09/23 16:16:35.623
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 16:16:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6680" for this suite. 03/09/23 16:16:35.638
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:35.643
Mar  9 16:16:35.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:16:35.644
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:35.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:35.658
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 03/09/23 16:16:35.661
Mar  9 16:16:35.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4838 cluster-info'
Mar  9 16:16:35.729: INFO: stderr: ""
Mar  9 16:16:35.729: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:16:35.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4838" for this suite. 03/09/23 16:16:35.733
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":106,"skipped":2232,"failed":0}
------------------------------
â€¢ [0.094 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:35.643
    Mar  9 16:16:35.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:16:35.644
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:35.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:35.658
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 03/09/23 16:16:35.661
    Mar  9 16:16:35.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4838 cluster-info'
    Mar  9 16:16:35.729: INFO: stderr: ""
    Mar  9 16:16:35.729: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:16:35.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4838" for this suite. 03/09/23 16:16:35.733
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:35.738
Mar  9 16:16:35.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:16:35.739
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:35.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:35.753
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:16:35.756
Mar  9 16:16:35.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b" in namespace "projected-7919" to be "Succeeded or Failed"
Mar  9 16:16:35.764: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429924ms
Mar  9 16:16:37.768: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006640224s
Mar  9 16:16:39.767: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005686215s
STEP: Saw pod success 03/09/23 16:16:39.767
Mar  9 16:16:39.767: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b" satisfied condition "Succeeded or Failed"
Mar  9 16:16:39.770: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b container client-container: <nil>
STEP: delete the pod 03/09/23 16:16:39.775
Mar  9 16:16:39.785: INFO: Waiting for pod downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b to disappear
Mar  9 16:16:39.787: INFO: Pod downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 16:16:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7919" for this suite. 03/09/23 16:16:39.791
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":107,"skipped":2233,"failed":0}
------------------------------
â€¢ [4.057 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:35.738
    Mar  9 16:16:35.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:16:35.739
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:35.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:35.753
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:16:35.756
    Mar  9 16:16:35.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b" in namespace "projected-7919" to be "Succeeded or Failed"
    Mar  9 16:16:35.764: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429924ms
    Mar  9 16:16:37.768: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006640224s
    Mar  9 16:16:39.767: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005686215s
    STEP: Saw pod success 03/09/23 16:16:39.767
    Mar  9 16:16:39.767: INFO: Pod "downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b" satisfied condition "Succeeded or Failed"
    Mar  9 16:16:39.770: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b container client-container: <nil>
    STEP: delete the pod 03/09/23 16:16:39.775
    Mar  9 16:16:39.785: INFO: Waiting for pod downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b to disappear
    Mar  9 16:16:39.787: INFO: Pod downwardapi-volume-9410d197-7141-42fe-b9ab-3b89feb1c17b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 16:16:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7919" for this suite. 03/09/23 16:16:39.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:39.796
Mar  9 16:16:39.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replication-controller 03/09/23 16:16:39.797
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:39.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:39.81
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948 03/09/23 16:16:39.813
Mar  9 16:16:39.820: INFO: Pod name my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948: Found 0 pods out of 1
Mar  9 16:16:44.824: INFO: Pod name my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948: Found 1 pods out of 1
Mar  9 16:16:44.824: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948" are running
Mar  9 16:16:44.824: INFO: Waiting up to 5m0s for pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md" in namespace "replication-controller-885" to be "running"
Mar  9 16:16:44.827: INFO: Pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md": Phase="Running", Reason="", readiness=true. Elapsed: 2.455705ms
Mar  9 16:16:44.827: INFO: Pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md" satisfied condition "running"
Mar  9 16:16:44.827: INFO: Pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:39 +0000 UTC Reason: Message:}])
Mar  9 16:16:44.827: INFO: Trying to dial the pod
Mar  9 16:16:49.837: INFO: Controller my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948: Got expected result from replica 1 [my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md]: "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  9 16:16:49.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-885" for this suite. 03/09/23 16:16:49.841
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":108,"skipped":2245,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.051 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:39.796
    Mar  9 16:16:39.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replication-controller 03/09/23 16:16:39.797
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:39.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:39.81
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948 03/09/23 16:16:39.813
    Mar  9 16:16:39.820: INFO: Pod name my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948: Found 0 pods out of 1
    Mar  9 16:16:44.824: INFO: Pod name my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948: Found 1 pods out of 1
    Mar  9 16:16:44.824: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948" are running
    Mar  9 16:16:44.824: INFO: Waiting up to 5m0s for pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md" in namespace "replication-controller-885" to be "running"
    Mar  9 16:16:44.827: INFO: Pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md": Phase="Running", Reason="", readiness=true. Elapsed: 2.455705ms
    Mar  9 16:16:44.827: INFO: Pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md" satisfied condition "running"
    Mar  9 16:16:44.827: INFO: Pod "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 16:16:39 +0000 UTC Reason: Message:}])
    Mar  9 16:16:44.827: INFO: Trying to dial the pod
    Mar  9 16:16:49.837: INFO: Controller my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948: Got expected result from replica 1 [my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md]: "my-hostname-basic-da4d131c-5ffc-4814-8bd1-1db4585ab948-bt6md", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  9 16:16:49.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-885" for this suite. 03/09/23 16:16:49.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:16:49.849
Mar  9 16:16:49.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-watch 03/09/23 16:16:49.85
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:49.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:49.865
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar  9 16:16:49.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Creating first CR  03/09/23 16:16:52.416
Mar  9 16:16:52.421: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:16:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:16:52Z]] name:name1 resourceVersion:97705 uid:3372c577-05a6-426f-a786-bef3e052ba83] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/09/23 16:17:02.423
Mar  9 16:17:02.428: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:17:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:02Z]] name:name2 resourceVersion:97740 uid:26af9cdb-156f-454b-8c15-72fc79026686] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/09/23 16:17:12.429
Mar  9 16:17:12.435: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:16:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:12Z]] name:name1 resourceVersion:97760 uid:3372c577-05a6-426f-a786-bef3e052ba83] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/09/23 16:17:22.435
Mar  9 16:17:22.443: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:17:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:22Z]] name:name2 resourceVersion:97781 uid:26af9cdb-156f-454b-8c15-72fc79026686] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/09/23 16:17:32.444
Mar  9 16:17:32.451: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:16:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:12Z]] name:name1 resourceVersion:97801 uid:3372c577-05a6-426f-a786-bef3e052ba83] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/09/23 16:17:42.455
Mar  9 16:17:42.461: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:17:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:22Z]] name:name2 resourceVersion:97821 uid:26af9cdb-156f-454b-8c15-72fc79026686] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:17:52.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9248" for this suite. 03/09/23 16:17:52.98
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":109,"skipped":2296,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.138 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:16:49.849
    Mar  9 16:16:49.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-watch 03/09/23 16:16:49.85
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:16:49.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:16:49.865
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar  9 16:16:49.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Creating first CR  03/09/23 16:16:52.416
    Mar  9 16:16:52.421: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:16:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:16:52Z]] name:name1 resourceVersion:97705 uid:3372c577-05a6-426f-a786-bef3e052ba83] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/09/23 16:17:02.423
    Mar  9 16:17:02.428: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:17:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:02Z]] name:name2 resourceVersion:97740 uid:26af9cdb-156f-454b-8c15-72fc79026686] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/09/23 16:17:12.429
    Mar  9 16:17:12.435: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:16:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:12Z]] name:name1 resourceVersion:97760 uid:3372c577-05a6-426f-a786-bef3e052ba83] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/09/23 16:17:22.435
    Mar  9 16:17:22.443: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:17:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:22Z]] name:name2 resourceVersion:97781 uid:26af9cdb-156f-454b-8c15-72fc79026686] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/09/23 16:17:32.444
    Mar  9 16:17:32.451: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:16:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:12Z]] name:name1 resourceVersion:97801 uid:3372c577-05a6-426f-a786-bef3e052ba83] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/09/23 16:17:42.455
    Mar  9 16:17:42.461: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-09T16:17:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-09T16:17:22Z]] name:name2 resourceVersion:97821 uid:26af9cdb-156f-454b-8c15-72fc79026686] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:17:52.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-9248" for this suite. 03/09/23 16:17:52.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:17:52.989
Mar  9 16:17:52.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:17:52.99
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:17:53.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:17:53.006
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 16:17:53.009
Mar  9 16:17:53.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  9 16:17:53.082: INFO: stderr: ""
Mar  9 16:17:53.082: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/09/23 16:17:53.082
STEP: verifying the pod e2e-test-httpd-pod was created 03/09/23 16:17:58.133
Mar  9 16:17:58.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 get pod e2e-test-httpd-pod -o json'
Mar  9 16:17:58.202: INFO: stderr: ""
Mar  9 16:17:58.202: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5724a8277189121b696344d208bfb04d7bb508924f36f6455b4104d5ca671ecd\",\n            \"cni.projectcalico.org/podIP\": \"10.244.42.226/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.42.226/32\"\n        },\n        \"creationTimestamp\": \"2023-03-09T16:17:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3677\",\n        \"resourceVersion\": \"97864\",\n        \"uid\": \"ccd49673-10b2-46f3-bc10-6b4b12a694c1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-sscsw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"tt-test-el8-003\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-sscsw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://a0f289693bfebf611ce18c1fe891aa1840f6be404a53ea3570b29ca6945fd856\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-09T16:17:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"100.100.230.140\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.42.226\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.42.226\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-09T16:17:53Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/09/23 16:17:58.203
Mar  9 16:17:58.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 replace -f -'
Mar  9 16:17:59.504: INFO: stderr: ""
Mar  9 16:17:59.504: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/09/23 16:17:59.504
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Mar  9 16:17:59.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 delete pods e2e-test-httpd-pod'
Mar  9 16:18:01.654: INFO: stderr: ""
Mar  9 16:18:01.654: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:18:01.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3677" for this suite. 03/09/23 16:18:01.658
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":110,"skipped":2315,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.675 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:17:52.989
    Mar  9 16:17:52.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:17:52.99
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:17:53.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:17:53.006
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 16:17:53.009
    Mar  9 16:17:53.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  9 16:17:53.082: INFO: stderr: ""
    Mar  9 16:17:53.082: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/09/23 16:17:53.082
    STEP: verifying the pod e2e-test-httpd-pod was created 03/09/23 16:17:58.133
    Mar  9 16:17:58.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 get pod e2e-test-httpd-pod -o json'
    Mar  9 16:17:58.202: INFO: stderr: ""
    Mar  9 16:17:58.202: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5724a8277189121b696344d208bfb04d7bb508924f36f6455b4104d5ca671ecd\",\n            \"cni.projectcalico.org/podIP\": \"10.244.42.226/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.42.226/32\"\n        },\n        \"creationTimestamp\": \"2023-03-09T16:17:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3677\",\n        \"resourceVersion\": \"97864\",\n        \"uid\": \"ccd49673-10b2-46f3-bc10-6b4b12a694c1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-sscsw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"tt-test-el8-003\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-sscsw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-09T16:17:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://a0f289693bfebf611ce18c1fe891aa1840f6be404a53ea3570b29ca6945fd856\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-09T16:17:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"100.100.230.140\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.42.226\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.42.226\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-09T16:17:53Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/09/23 16:17:58.203
    Mar  9 16:17:58.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 replace -f -'
    Mar  9 16:17:59.504: INFO: stderr: ""
    Mar  9 16:17:59.504: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/09/23 16:17:59.504
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Mar  9 16:17:59.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3677 delete pods e2e-test-httpd-pod'
    Mar  9 16:18:01.654: INFO: stderr: ""
    Mar  9 16:18:01.654: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:18:01.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3677" for this suite. 03/09/23 16:18:01.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:01.665
Mar  9 16:18:01.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:18:01.666
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:01.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:01.679
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 03/09/23 16:18:01.682
STEP: submitting the pod to kubernetes 03/09/23 16:18:01.682
Mar  9 16:18:01.689: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" in namespace "pods-7432" to be "running and ready"
Mar  9 16:18:01.692: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6365ms
Mar  9 16:18:01.692: INFO: The phase of Pod pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:18:03.695: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Running", Reason="", readiness=true. Elapsed: 2.006199791s
Mar  9 16:18:03.695: INFO: The phase of Pod pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87 is Running (Ready = true)
Mar  9 16:18:03.695: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/09/23 16:18:03.698
STEP: updating the pod 03/09/23 16:18:03.7
Mar  9 16:18:04.211: INFO: Successfully updated pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87"
Mar  9 16:18:04.211: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" in namespace "pods-7432" to be "terminated with reason DeadlineExceeded"
Mar  9 16:18:04.213: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Running", Reason="", readiness=true. Elapsed: 2.3689ms
Mar  9 16:18:06.217: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Running", Reason="", readiness=false. Elapsed: 2.006158386s
Mar  9 16:18:08.217: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.006342548s
Mar  9 16:18:08.217: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:18:08.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7432" for this suite. 03/09/23 16:18:08.221
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":111,"skipped":2348,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.561 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:01.665
    Mar  9 16:18:01.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:18:01.666
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:01.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:01.679
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 03/09/23 16:18:01.682
    STEP: submitting the pod to kubernetes 03/09/23 16:18:01.682
    Mar  9 16:18:01.689: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" in namespace "pods-7432" to be "running and ready"
    Mar  9 16:18:01.692: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6365ms
    Mar  9 16:18:01.692: INFO: The phase of Pod pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:18:03.695: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Running", Reason="", readiness=true. Elapsed: 2.006199791s
    Mar  9 16:18:03.695: INFO: The phase of Pod pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87 is Running (Ready = true)
    Mar  9 16:18:03.695: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/09/23 16:18:03.698
    STEP: updating the pod 03/09/23 16:18:03.7
    Mar  9 16:18:04.211: INFO: Successfully updated pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87"
    Mar  9 16:18:04.211: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" in namespace "pods-7432" to be "terminated with reason DeadlineExceeded"
    Mar  9 16:18:04.213: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Running", Reason="", readiness=true. Elapsed: 2.3689ms
    Mar  9 16:18:06.217: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Running", Reason="", readiness=false. Elapsed: 2.006158386s
    Mar  9 16:18:08.217: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.006342548s
    Mar  9 16:18:08.217: INFO: Pod "pod-update-activedeadlineseconds-fd70f8b5-30f5-461e-b6b4-1b2480460a87" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:18:08.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7432" for this suite. 03/09/23 16:18:08.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:08.227
Mar  9 16:18:08.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:18:08.228
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:08.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:08.242
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:18:08.245
Mar  9 16:18:08.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4" in namespace "downward-api-6320" to be "Succeeded or Failed"
Mar  9 16:18:08.254: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424894ms
Mar  9 16:18:10.258: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006133842s
Mar  9 16:18:12.258: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006250756s
STEP: Saw pod success 03/09/23 16:18:12.258
Mar  9 16:18:12.258: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4" satisfied condition "Succeeded or Failed"
Mar  9 16:18:12.261: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4 container client-container: <nil>
STEP: delete the pod 03/09/23 16:18:12.275
Mar  9 16:18:12.285: INFO: Waiting for pod downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4 to disappear
Mar  9 16:18:12.288: INFO: Pod downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 16:18:12.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6320" for this suite. 03/09/23 16:18:12.291
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":112,"skipped":2355,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:08.227
    Mar  9 16:18:08.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:18:08.228
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:08.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:08.242
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:18:08.245
    Mar  9 16:18:08.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4" in namespace "downward-api-6320" to be "Succeeded or Failed"
    Mar  9 16:18:08.254: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424894ms
    Mar  9 16:18:10.258: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006133842s
    Mar  9 16:18:12.258: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006250756s
    STEP: Saw pod success 03/09/23 16:18:12.258
    Mar  9 16:18:12.258: INFO: Pod "downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4" satisfied condition "Succeeded or Failed"
    Mar  9 16:18:12.261: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4 container client-container: <nil>
    STEP: delete the pod 03/09/23 16:18:12.275
    Mar  9 16:18:12.285: INFO: Waiting for pod downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4 to disappear
    Mar  9 16:18:12.288: INFO: Pod downwardapi-volume-83250802-3cd4-413b-b093-539a975999a4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 16:18:12.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6320" for this suite. 03/09/23 16:18:12.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:12.3
Mar  9 16:18:12.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:18:12.301
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:12.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:12.317
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:18:12.32
Mar  9 16:18:12.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af" in namespace "downward-api-5503" to be "Succeeded or Failed"
Mar  9 16:18:12.329: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018256ms
Mar  9 16:18:14.334: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00670132s
Mar  9 16:18:16.334: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006828207s
STEP: Saw pod success 03/09/23 16:18:16.334
Mar  9 16:18:16.334: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af" satisfied condition "Succeeded or Failed"
Mar  9 16:18:16.337: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af container client-container: <nil>
STEP: delete the pod 03/09/23 16:18:16.342
Mar  9 16:18:16.353: INFO: Waiting for pod downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af to disappear
Mar  9 16:18:16.355: INFO: Pod downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 16:18:16.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5503" for this suite. 03/09/23 16:18:16.359
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":113,"skipped":2362,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:12.3
    Mar  9 16:18:12.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:18:12.301
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:12.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:12.317
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:18:12.32
    Mar  9 16:18:12.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af" in namespace "downward-api-5503" to be "Succeeded or Failed"
    Mar  9 16:18:12.329: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018256ms
    Mar  9 16:18:14.334: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00670132s
    Mar  9 16:18:16.334: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006828207s
    STEP: Saw pod success 03/09/23 16:18:16.334
    Mar  9 16:18:16.334: INFO: Pod "downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af" satisfied condition "Succeeded or Failed"
    Mar  9 16:18:16.337: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af container client-container: <nil>
    STEP: delete the pod 03/09/23 16:18:16.342
    Mar  9 16:18:16.353: INFO: Waiting for pod downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af to disappear
    Mar  9 16:18:16.355: INFO: Pod downwardapi-volume-9e261337-159d-4d40-9bef-5ab3080eb0af no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 16:18:16.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5503" for this suite. 03/09/23 16:18:16.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:16.364
Mar  9 16:18:16.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-pred 03/09/23 16:18:16.365
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:16.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:16.379
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  9 16:18:16.382: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  9 16:18:16.388: INFO: Waiting for terminating namespaces to be deleted...
Mar  9 16:18:16.391: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-003 before test
Mar  9 16:18:16.397: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.397: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 16:18:16.397: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.397: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 16:18:16.397: INFO: csi-node-driver-8kqcp from calico-system started at 2023-03-09 16:12:55 +0000 UTC (2 container statuses recorded)
Mar  9 16:18:16.397: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 16:18:16.397: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 16:18:16.397: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.397: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 16:18:16.397: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.397: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  9 16:18:16.397: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 16:18:16.397: INFO: 	Container e2e ready: true, restart count 0
Mar  9 16:18:16.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 16:18:16.397: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 16:18:16.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 16:18:16.397: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 16:18:16.397: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-004 before test
Mar  9 16:18:16.404: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container calico-apiserver ready: true, restart count 0
Mar  9 16:18:16.404: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  9 16:18:16.404: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 16:18:16.404: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 16:18:16.404: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 16:18:16.404: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 16:18:16.404: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container webhook ready: true, restart count 0
Mar  9 16:18:16.404: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container coredns ready: true, restart count 0
Mar  9 16:18:16.404: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container coredns ready: true, restart count 0
Mar  9 16:18:16.404: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 16:18:16.404: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  9 16:18:16.404: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 16:18:16.404: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 16:18:16.404: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 16:18:16.404: INFO: 	Container tigera-operator ready: true, restart count 3
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/09/23 16:18:16.404
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.174acc882f148c98], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/09/23 16:18:16.429
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:18:17.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4901" for this suite. 03/09/23 16:18:17.433
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":114,"skipped":2368,"failed":0}
------------------------------
â€¢ [1.074 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:16.364
    Mar  9 16:18:16.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-pred 03/09/23 16:18:16.365
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:16.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:16.379
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  9 16:18:16.382: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  9 16:18:16.388: INFO: Waiting for terminating namespaces to be deleted...
    Mar  9 16:18:16.391: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-003 before test
    Mar  9 16:18:16.397: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.397: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.397: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: csi-node-driver-8kqcp from calico-system started at 2023-03-09 16:12:55 +0000 UTC (2 container statuses recorded)
    Mar  9 16:18:16.397: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.397: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.397: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 16:18:16.397: INFO: 	Container e2e ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 16:18:16.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 16:18:16.397: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-004 before test
    Mar  9 16:18:16.404: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container calico-apiserver ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container webhook ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 16:18:16.404: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 16:18:16.404: INFO: 	Container tigera-operator ready: true, restart count 3
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/09/23 16:18:16.404
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.174acc882f148c98], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/09/23 16:18:16.429
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:18:17.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4901" for this suite. 03/09/23 16:18:17.433
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:17.439
Mar  9 16:18:17.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:18:17.44
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:17.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:17.454
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-1178/secret-test-963f52c5-9f5a-40c3-87f4-489045169d93 03/09/23 16:18:17.457
STEP: Creating a pod to test consume secrets 03/09/23 16:18:17.461
Mar  9 16:18:17.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a" in namespace "secrets-1178" to be "Succeeded or Failed"
Mar  9 16:18:17.471: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.927965ms
Mar  9 16:18:19.474: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006707108s
Mar  9 16:18:21.475: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007267174s
STEP: Saw pod success 03/09/23 16:18:21.475
Mar  9 16:18:21.475: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a" satisfied condition "Succeeded or Failed"
Mar  9 16:18:21.478: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a container env-test: <nil>
STEP: delete the pod 03/09/23 16:18:21.485
Mar  9 16:18:21.493: INFO: Waiting for pod pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a to disappear
Mar  9 16:18:21.496: INFO: Pod pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:18:21.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1178" for this suite. 03/09/23 16:18:21.499
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":115,"skipped":2378,"failed":0}
------------------------------
â€¢ [4.064 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:17.439
    Mar  9 16:18:17.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:18:17.44
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:17.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:17.454
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-1178/secret-test-963f52c5-9f5a-40c3-87f4-489045169d93 03/09/23 16:18:17.457
    STEP: Creating a pod to test consume secrets 03/09/23 16:18:17.461
    Mar  9 16:18:17.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a" in namespace "secrets-1178" to be "Succeeded or Failed"
    Mar  9 16:18:17.471: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.927965ms
    Mar  9 16:18:19.474: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006707108s
    Mar  9 16:18:21.475: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007267174s
    STEP: Saw pod success 03/09/23 16:18:21.475
    Mar  9 16:18:21.475: INFO: Pod "pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a" satisfied condition "Succeeded or Failed"
    Mar  9 16:18:21.478: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a container env-test: <nil>
    STEP: delete the pod 03/09/23 16:18:21.485
    Mar  9 16:18:21.493: INFO: Waiting for pod pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a to disappear
    Mar  9 16:18:21.496: INFO: Pod pod-configmaps-995c2a60-518a-4648-b55f-7631fa4e4a8a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:18:21.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1178" for this suite. 03/09/23 16:18:21.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:21.505
Mar  9 16:18:21.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:18:21.507
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:21.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:21.524
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 03/09/23 16:18:21.528
Mar  9 16:18:21.540: INFO: Waiting up to 5m0s for pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b" in namespace "downward-api-7261" to be "Succeeded or Failed"
Mar  9 16:18:21.543: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632863ms
Mar  9 16:18:23.547: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007014236s
Mar  9 16:18:25.548: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007669947s
STEP: Saw pod success 03/09/23 16:18:25.548
Mar  9 16:18:25.548: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b" satisfied condition "Succeeded or Failed"
Mar  9 16:18:25.551: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b container dapi-container: <nil>
STEP: delete the pod 03/09/23 16:18:25.559
Mar  9 16:18:25.569: INFO: Waiting for pod downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b to disappear
Mar  9 16:18:25.571: INFO: Pod downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  9 16:18:25.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7261" for this suite. 03/09/23 16:18:25.574
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":116,"skipped":2385,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:21.505
    Mar  9 16:18:21.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:18:21.507
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:21.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:21.524
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 03/09/23 16:18:21.528
    Mar  9 16:18:21.540: INFO: Waiting up to 5m0s for pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b" in namespace "downward-api-7261" to be "Succeeded or Failed"
    Mar  9 16:18:21.543: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632863ms
    Mar  9 16:18:23.547: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007014236s
    Mar  9 16:18:25.548: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007669947s
    STEP: Saw pod success 03/09/23 16:18:25.548
    Mar  9 16:18:25.548: INFO: Pod "downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b" satisfied condition "Succeeded or Failed"
    Mar  9 16:18:25.551: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b container dapi-container: <nil>
    STEP: delete the pod 03/09/23 16:18:25.559
    Mar  9 16:18:25.569: INFO: Waiting for pod downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b to disappear
    Mar  9 16:18:25.571: INFO: Pod downward-api-b693073c-b1b8-42df-925a-c55cd0fe807b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  9 16:18:25.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7261" for this suite. 03/09/23 16:18:25.574
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:25.58
Mar  9 16:18:25.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename proxy 03/09/23 16:18:25.581
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:25.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:25.595
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar  9 16:18:25.598: INFO: Creating pod...
Mar  9 16:18:25.606: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-606" to be "running"
Mar  9 16:18:25.608: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173248ms
Mar  9 16:18:27.611: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005173384s
Mar  9 16:18:27.611: INFO: Pod "agnhost" satisfied condition "running"
Mar  9 16:18:27.611: INFO: Creating service...
Mar  9 16:18:27.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/DELETE
Mar  9 16:18:27.634: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  9 16:18:27.634: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/GET
Mar  9 16:18:27.639: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  9 16:18:27.639: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/HEAD
Mar  9 16:18:27.642: INFO: http.Client request:HEAD | StatusCode:200
Mar  9 16:18:27.642: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/OPTIONS
Mar  9 16:18:27.645: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  9 16:18:27.645: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/PATCH
Mar  9 16:18:27.648: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  9 16:18:27.648: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/POST
Mar  9 16:18:27.651: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  9 16:18:27.651: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/PUT
Mar  9 16:18:27.654: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  9 16:18:27.654: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/DELETE
Mar  9 16:18:27.663: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  9 16:18:27.663: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/GET
Mar  9 16:18:27.667: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  9 16:18:27.667: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/HEAD
Mar  9 16:18:27.671: INFO: http.Client request:HEAD | StatusCode:200
Mar  9 16:18:27.671: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/OPTIONS
Mar  9 16:18:27.675: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  9 16:18:27.675: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/PATCH
Mar  9 16:18:27.679: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  9 16:18:27.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/POST
Mar  9 16:18:27.683: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  9 16:18:27.683: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/PUT
Mar  9 16:18:27.687: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  9 16:18:27.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-606" for this suite. 03/09/23 16:18:27.691
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":117,"skipped":2388,"failed":0}
------------------------------
â€¢ [2.117 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:25.58
    Mar  9 16:18:25.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename proxy 03/09/23 16:18:25.581
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:25.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:25.595
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar  9 16:18:25.598: INFO: Creating pod...
    Mar  9 16:18:25.606: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-606" to be "running"
    Mar  9 16:18:25.608: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173248ms
    Mar  9 16:18:27.611: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005173384s
    Mar  9 16:18:27.611: INFO: Pod "agnhost" satisfied condition "running"
    Mar  9 16:18:27.611: INFO: Creating service...
    Mar  9 16:18:27.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/DELETE
    Mar  9 16:18:27.634: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  9 16:18:27.634: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/GET
    Mar  9 16:18:27.639: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  9 16:18:27.639: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/HEAD
    Mar  9 16:18:27.642: INFO: http.Client request:HEAD | StatusCode:200
    Mar  9 16:18:27.642: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar  9 16:18:27.645: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  9 16:18:27.645: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/PATCH
    Mar  9 16:18:27.648: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  9 16:18:27.648: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/POST
    Mar  9 16:18:27.651: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  9 16:18:27.651: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/pods/agnhost/proxy/some/path/with/PUT
    Mar  9 16:18:27.654: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  9 16:18:27.654: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/DELETE
    Mar  9 16:18:27.663: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  9 16:18:27.663: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/GET
    Mar  9 16:18:27.667: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  9 16:18:27.667: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/HEAD
    Mar  9 16:18:27.671: INFO: http.Client request:HEAD | StatusCode:200
    Mar  9 16:18:27.671: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/OPTIONS
    Mar  9 16:18:27.675: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  9 16:18:27.675: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/PATCH
    Mar  9 16:18:27.679: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  9 16:18:27.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/POST
    Mar  9 16:18:27.683: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  9 16:18:27.683: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-606/services/test-service/proxy/some/path/with/PUT
    Mar  9 16:18:27.687: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  9 16:18:27.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-606" for this suite. 03/09/23 16:18:27.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:27.698
Mar  9 16:18:27.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename endpointslice 03/09/23 16:18:27.699
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:27.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:27.718
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 03/09/23 16:18:27.721
STEP: getting /apis/discovery.k8s.io 03/09/23 16:18:27.724
STEP: getting /apis/discovery.k8s.iov1 03/09/23 16:18:27.725
STEP: creating 03/09/23 16:18:27.726
STEP: getting 03/09/23 16:18:27.742
STEP: listing 03/09/23 16:18:27.744
STEP: watching 03/09/23 16:18:27.747
Mar  9 16:18:27.747: INFO: starting watch
STEP: cluster-wide listing 03/09/23 16:18:27.749
STEP: cluster-wide watching 03/09/23 16:18:27.752
Mar  9 16:18:27.752: INFO: starting watch
STEP: patching 03/09/23 16:18:27.753
STEP: updating 03/09/23 16:18:27.76
Mar  9 16:18:27.770: INFO: waiting for watch events with expected annotations
Mar  9 16:18:27.770: INFO: saw patched and updated annotations
STEP: deleting 03/09/23 16:18:27.77
STEP: deleting a collection 03/09/23 16:18:27.78
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  9 16:18:27.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-16" for this suite. 03/09/23 16:18:27.796
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":118,"skipped":2394,"failed":0}
------------------------------
â€¢ [0.103 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:27.698
    Mar  9 16:18:27.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename endpointslice 03/09/23 16:18:27.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:27.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:27.718
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 03/09/23 16:18:27.721
    STEP: getting /apis/discovery.k8s.io 03/09/23 16:18:27.724
    STEP: getting /apis/discovery.k8s.iov1 03/09/23 16:18:27.725
    STEP: creating 03/09/23 16:18:27.726
    STEP: getting 03/09/23 16:18:27.742
    STEP: listing 03/09/23 16:18:27.744
    STEP: watching 03/09/23 16:18:27.747
    Mar  9 16:18:27.747: INFO: starting watch
    STEP: cluster-wide listing 03/09/23 16:18:27.749
    STEP: cluster-wide watching 03/09/23 16:18:27.752
    Mar  9 16:18:27.752: INFO: starting watch
    STEP: patching 03/09/23 16:18:27.753
    STEP: updating 03/09/23 16:18:27.76
    Mar  9 16:18:27.770: INFO: waiting for watch events with expected annotations
    Mar  9 16:18:27.770: INFO: saw patched and updated annotations
    STEP: deleting 03/09/23 16:18:27.77
    STEP: deleting a collection 03/09/23 16:18:27.78
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  9 16:18:27.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-16" for this suite. 03/09/23 16:18:27.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:27.803
Mar  9 16:18:27.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:18:27.804
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:27.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:27.819
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-c0db55f4-96d0-42a0-8a9f-a31f357d61f7 03/09/23 16:18:27.827
STEP: Creating secret with name s-test-opt-upd-6a1220f1-aae5-40e2-9184-cb45d93b27ce 03/09/23 16:18:27.831
STEP: Creating the pod 03/09/23 16:18:27.835
Mar  9 16:18:27.844: INFO: Waiting up to 5m0s for pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029" in namespace "secrets-3720" to be "running and ready"
Mar  9 16:18:27.847: INFO: Pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461981ms
Mar  9 16:18:27.847: INFO: The phase of Pod pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:18:29.850: INFO: Pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029": Phase="Running", Reason="", readiness=true. Elapsed: 2.005958372s
Mar  9 16:18:29.850: INFO: The phase of Pod pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029 is Running (Ready = true)
Mar  9 16:18:29.850: INFO: Pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c0db55f4-96d0-42a0-8a9f-a31f357d61f7 03/09/23 16:18:29.868
STEP: Updating secret s-test-opt-upd-6a1220f1-aae5-40e2-9184-cb45d93b27ce 03/09/23 16:18:29.873
STEP: Creating secret with name s-test-opt-create-c802fafe-9b4f-486b-8fd5-368549c577af 03/09/23 16:18:29.877
STEP: waiting to observe update in volume 03/09/23 16:18:29.881
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:18:31.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3720" for this suite. 03/09/23 16:18:31.907
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":119,"skipped":2420,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:27.803
    Mar  9 16:18:27.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:18:27.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:27.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:27.819
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-c0db55f4-96d0-42a0-8a9f-a31f357d61f7 03/09/23 16:18:27.827
    STEP: Creating secret with name s-test-opt-upd-6a1220f1-aae5-40e2-9184-cb45d93b27ce 03/09/23 16:18:27.831
    STEP: Creating the pod 03/09/23 16:18:27.835
    Mar  9 16:18:27.844: INFO: Waiting up to 5m0s for pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029" in namespace "secrets-3720" to be "running and ready"
    Mar  9 16:18:27.847: INFO: Pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461981ms
    Mar  9 16:18:27.847: INFO: The phase of Pod pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:18:29.850: INFO: Pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029": Phase="Running", Reason="", readiness=true. Elapsed: 2.005958372s
    Mar  9 16:18:29.850: INFO: The phase of Pod pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029 is Running (Ready = true)
    Mar  9 16:18:29.850: INFO: Pod "pod-secrets-aebe597d-9353-4d95-b2ca-5411ff231029" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c0db55f4-96d0-42a0-8a9f-a31f357d61f7 03/09/23 16:18:29.868
    STEP: Updating secret s-test-opt-upd-6a1220f1-aae5-40e2-9184-cb45d93b27ce 03/09/23 16:18:29.873
    STEP: Creating secret with name s-test-opt-create-c802fafe-9b4f-486b-8fd5-368549c577af 03/09/23 16:18:29.877
    STEP: waiting to observe update in volume 03/09/23 16:18:29.881
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:18:31.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3720" for this suite. 03/09/23 16:18:31.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:31.915
Mar  9 16:18:31.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:18:31.916
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:31.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:31.929
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 03/09/23 16:18:31.934
STEP: watching for the Service to be added 03/09/23 16:18:31.948
Mar  9 16:18:31.950: INFO: Found Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar  9 16:18:31.950: INFO: Service test-service-mfp9z created
STEP: Getting /status 03/09/23 16:18:31.95
Mar  9 16:18:31.953: INFO: Service test-service-mfp9z has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/09/23 16:18:31.953
STEP: watching for the Service to be patched 03/09/23 16:18:31.961
Mar  9 16:18:31.963: INFO: observed Service test-service-mfp9z in namespace services-6456 with annotations: map[] & LoadBalancer: {[]}
Mar  9 16:18:31.963: INFO: Found Service test-service-mfp9z in namespace services-6456 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar  9 16:18:31.963: INFO: Service test-service-mfp9z has service status patched
STEP: updating the ServiceStatus 03/09/23 16:18:31.963
Mar  9 16:18:31.972: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/09/23 16:18:31.972
Mar  9 16:18:31.974: INFO: Observed Service test-service-mfp9z in namespace services-6456 with annotations: map[] & Conditions: {[]}
Mar  9 16:18:31.974: INFO: Observed event: &Service{ObjectMeta:{test-service-mfp9z  services-6456  348e6b78-f3a4-465a-8d2d-0d855d031f0a 98206 0 2023-03-09 16:18:31 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-09 16:18:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-09 16:18:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.102.177.76,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.102.177.76],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar  9 16:18:31.974: INFO: Found Service test-service-mfp9z in namespace services-6456 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  9 16:18:31.974: INFO: Service test-service-mfp9z has service status updated
STEP: patching the service 03/09/23 16:18:31.974
STEP: watching for the Service to be patched 03/09/23 16:18:31.995
Mar  9 16:18:31.997: INFO: observed Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true]
Mar  9 16:18:31.997: INFO: observed Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true]
Mar  9 16:18:31.997: INFO: observed Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true]
Mar  9 16:18:31.997: INFO: Found Service test-service-mfp9z in namespace services-6456 with labels: map[test-service:patched test-service-static:true]
Mar  9 16:18:31.997: INFO: Service test-service-mfp9z patched
STEP: deleting the service 03/09/23 16:18:31.997
STEP: watching for the Service to be deleted 03/09/23 16:18:32.017
Mar  9 16:18:32.019: INFO: Observed event: ADDED
Mar  9 16:18:32.019: INFO: Observed event: MODIFIED
Mar  9 16:18:32.019: INFO: Observed event: MODIFIED
Mar  9 16:18:32.020: INFO: Observed event: MODIFIED
Mar  9 16:18:32.020: INFO: Found Service test-service-mfp9z in namespace services-6456 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar  9 16:18:32.020: INFO: Service test-service-mfp9z deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:18:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6456" for this suite. 03/09/23 16:18:32.024
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":120,"skipped":2428,"failed":0}
------------------------------
â€¢ [0.116 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:31.915
    Mar  9 16:18:31.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:18:31.916
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:31.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:31.929
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 03/09/23 16:18:31.934
    STEP: watching for the Service to be added 03/09/23 16:18:31.948
    Mar  9 16:18:31.950: INFO: Found Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar  9 16:18:31.950: INFO: Service test-service-mfp9z created
    STEP: Getting /status 03/09/23 16:18:31.95
    Mar  9 16:18:31.953: INFO: Service test-service-mfp9z has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/09/23 16:18:31.953
    STEP: watching for the Service to be patched 03/09/23 16:18:31.961
    Mar  9 16:18:31.963: INFO: observed Service test-service-mfp9z in namespace services-6456 with annotations: map[] & LoadBalancer: {[]}
    Mar  9 16:18:31.963: INFO: Found Service test-service-mfp9z in namespace services-6456 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar  9 16:18:31.963: INFO: Service test-service-mfp9z has service status patched
    STEP: updating the ServiceStatus 03/09/23 16:18:31.963
    Mar  9 16:18:31.972: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/09/23 16:18:31.972
    Mar  9 16:18:31.974: INFO: Observed Service test-service-mfp9z in namespace services-6456 with annotations: map[] & Conditions: {[]}
    Mar  9 16:18:31.974: INFO: Observed event: &Service{ObjectMeta:{test-service-mfp9z  services-6456  348e6b78-f3a4-465a-8d2d-0d855d031f0a 98206 0 2023-03-09 16:18:31 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-09 16:18:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-09 16:18:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.102.177.76,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.102.177.76],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar  9 16:18:31.974: INFO: Found Service test-service-mfp9z in namespace services-6456 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  9 16:18:31.974: INFO: Service test-service-mfp9z has service status updated
    STEP: patching the service 03/09/23 16:18:31.974
    STEP: watching for the Service to be patched 03/09/23 16:18:31.995
    Mar  9 16:18:31.997: INFO: observed Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true]
    Mar  9 16:18:31.997: INFO: observed Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true]
    Mar  9 16:18:31.997: INFO: observed Service test-service-mfp9z in namespace services-6456 with labels: map[test-service-static:true]
    Mar  9 16:18:31.997: INFO: Found Service test-service-mfp9z in namespace services-6456 with labels: map[test-service:patched test-service-static:true]
    Mar  9 16:18:31.997: INFO: Service test-service-mfp9z patched
    STEP: deleting the service 03/09/23 16:18:31.997
    STEP: watching for the Service to be deleted 03/09/23 16:18:32.017
    Mar  9 16:18:32.019: INFO: Observed event: ADDED
    Mar  9 16:18:32.019: INFO: Observed event: MODIFIED
    Mar  9 16:18:32.019: INFO: Observed event: MODIFIED
    Mar  9 16:18:32.020: INFO: Observed event: MODIFIED
    Mar  9 16:18:32.020: INFO: Found Service test-service-mfp9z in namespace services-6456 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar  9 16:18:32.020: INFO: Service test-service-mfp9z deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:18:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6456" for this suite. 03/09/23 16:18:32.024
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:32.031
Mar  9 16:18:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:18:32.032
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:32.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:32.048
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-982 03/09/23 16:18:32.051
STEP: creating service affinity-clusterip in namespace services-982 03/09/23 16:18:32.051
STEP: creating replication controller affinity-clusterip in namespace services-982 03/09/23 16:18:32.077
I0309 16:18:32.086156      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-982, replica count: 3
I0309 16:18:35.136737      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 16:18:35.142: INFO: Creating new exec pod
Mar  9 16:18:35.147: INFO: Waiting up to 5m0s for pod "execpod-affinity9cxjc" in namespace "services-982" to be "running"
Mar  9 16:18:35.151: INFO: Pod "execpod-affinity9cxjc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639437ms
Mar  9 16:18:37.155: INFO: Pod "execpod-affinity9cxjc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007860168s
Mar  9 16:18:37.155: INFO: Pod "execpod-affinity9cxjc" satisfied condition "running"
Mar  9 16:18:38.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-982 exec execpod-affinity9cxjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar  9 16:18:38.305: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  9 16:18:38.305: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:18:38.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-982 exec execpod-affinity9cxjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.98.118 80'
Mar  9 16:18:38.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.98.118 80\nConnection to 10.111.98.118 80 port [tcp/http] succeeded!\n"
Mar  9 16:18:38.444: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:18:38.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-982 exec execpod-affinity9cxjc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.98.118:80/ ; done'
Mar  9 16:18:38.699: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n"
Mar  9 16:18:38.699: INFO: stdout: "\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr"
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
Mar  9 16:18:38.699: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-982, will wait for the garbage collector to delete the pods 03/09/23 16:18:38.71
Mar  9 16:18:38.768: INFO: Deleting ReplicationController affinity-clusterip took: 4.878123ms
Mar  9 16:18:38.869: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.685388ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:18:40.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-982" for this suite. 03/09/23 16:18:40.904
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":121,"skipped":2429,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.880 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:32.031
    Mar  9 16:18:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:18:32.032
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:32.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:32.048
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-982 03/09/23 16:18:32.051
    STEP: creating service affinity-clusterip in namespace services-982 03/09/23 16:18:32.051
    STEP: creating replication controller affinity-clusterip in namespace services-982 03/09/23 16:18:32.077
    I0309 16:18:32.086156      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-982, replica count: 3
    I0309 16:18:35.136737      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 16:18:35.142: INFO: Creating new exec pod
    Mar  9 16:18:35.147: INFO: Waiting up to 5m0s for pod "execpod-affinity9cxjc" in namespace "services-982" to be "running"
    Mar  9 16:18:35.151: INFO: Pod "execpod-affinity9cxjc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639437ms
    Mar  9 16:18:37.155: INFO: Pod "execpod-affinity9cxjc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007860168s
    Mar  9 16:18:37.155: INFO: Pod "execpod-affinity9cxjc" satisfied condition "running"
    Mar  9 16:18:38.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-982 exec execpod-affinity9cxjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Mar  9 16:18:38.305: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar  9 16:18:38.305: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:18:38.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-982 exec execpod-affinity9cxjc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.98.118 80'
    Mar  9 16:18:38.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.98.118 80\nConnection to 10.111.98.118 80 port [tcp/http] succeeded!\n"
    Mar  9 16:18:38.444: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:18:38.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-982 exec execpod-affinity9cxjc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.98.118:80/ ; done'
    Mar  9 16:18:38.699: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.98.118:80/\n"
    Mar  9 16:18:38.699: INFO: stdout: "\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr\naffinity-clusterip-kn9lr"
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Received response from host: affinity-clusterip-kn9lr
    Mar  9 16:18:38.699: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-982, will wait for the garbage collector to delete the pods 03/09/23 16:18:38.71
    Mar  9 16:18:38.768: INFO: Deleting ReplicationController affinity-clusterip took: 4.878123ms
    Mar  9 16:18:38.869: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.685388ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:18:40.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-982" for this suite. 03/09/23 16:18:40.904
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:40.911
Mar  9 16:18:40.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename daemonsets 03/09/23 16:18:40.913
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:40.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:40.935
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Mar  9 16:18:40.964: INFO: Create a RollingUpdate DaemonSet
Mar  9 16:18:40.969: INFO: Check that daemon pods launch on every node of the cluster
Mar  9 16:18:40.973: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:18:40.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:18:40.977: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:18:41.981: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:18:41.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:18:41.984: INFO: Node tt-test-el8-004 is running 0 daemon pod, expected 1
Mar  9 16:18:42.981: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:18:42.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 16:18:42.984: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Mar  9 16:18:42.984: INFO: Update the DaemonSet to trigger a rollout
Mar  9 16:18:42.992: INFO: Updating DaemonSet daemon-set
Mar  9 16:18:45.005: INFO: Roll back the DaemonSet before rollout is complete
Mar  9 16:18:45.012: INFO: Updating DaemonSet daemon-set
Mar  9 16:18:45.012: INFO: Make sure DaemonSet rollback is complete
Mar  9 16:18:45.015: INFO: Wrong image for pod: daemon-set-n8h28. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar  9 16:18:45.015: INFO: Pod daemon-set-n8h28 is not available
Mar  9 16:18:45.020: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:18:46.028: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:18:47.026: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:18:48.024: INFO: Pod daemon-set-pqt58 is not available
Mar  9 16:18:48.027: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:18:48.033
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4434, will wait for the garbage collector to delete the pods 03/09/23 16:18:48.033
Mar  9 16:18:48.091: INFO: Deleting DaemonSet.extensions daemon-set took: 4.697795ms
Mar  9 16:18:48.192: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.202081ms
Mar  9 16:18:49.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:18:49.895: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  9 16:18:49.897: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98553"},"items":null}

Mar  9 16:18:49.899: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98553"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:18:49.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4434" for this suite. 03/09/23 16:18:49.911
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":122,"skipped":2436,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.004 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:40.911
    Mar  9 16:18:40.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename daemonsets 03/09/23 16:18:40.913
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:40.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:40.935
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Mar  9 16:18:40.964: INFO: Create a RollingUpdate DaemonSet
    Mar  9 16:18:40.969: INFO: Check that daemon pods launch on every node of the cluster
    Mar  9 16:18:40.973: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:18:40.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:18:40.977: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:18:41.981: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:18:41.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:18:41.984: INFO: Node tt-test-el8-004 is running 0 daemon pod, expected 1
    Mar  9 16:18:42.981: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:18:42.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 16:18:42.984: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Mar  9 16:18:42.984: INFO: Update the DaemonSet to trigger a rollout
    Mar  9 16:18:42.992: INFO: Updating DaemonSet daemon-set
    Mar  9 16:18:45.005: INFO: Roll back the DaemonSet before rollout is complete
    Mar  9 16:18:45.012: INFO: Updating DaemonSet daemon-set
    Mar  9 16:18:45.012: INFO: Make sure DaemonSet rollback is complete
    Mar  9 16:18:45.015: INFO: Wrong image for pod: daemon-set-n8h28. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Mar  9 16:18:45.015: INFO: Pod daemon-set-n8h28 is not available
    Mar  9 16:18:45.020: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:18:46.028: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:18:47.026: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:18:48.024: INFO: Pod daemon-set-pqt58 is not available
    Mar  9 16:18:48.027: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:18:48.033
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4434, will wait for the garbage collector to delete the pods 03/09/23 16:18:48.033
    Mar  9 16:18:48.091: INFO: Deleting DaemonSet.extensions daemon-set took: 4.697795ms
    Mar  9 16:18:48.192: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.202081ms
    Mar  9 16:18:49.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:18:49.895: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  9 16:18:49.897: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98553"},"items":null}

    Mar  9 16:18:49.899: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98553"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:18:49.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4434" for this suite. 03/09/23 16:18:49.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:49.917
Mar  9 16:18:49.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 16:18:49.918
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:49.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:49.931
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar  9 16:18:49.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:18:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2504" for this suite. 03/09/23 16:18:56.198
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":123,"skipped":2458,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.285 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:49.917
    Mar  9 16:18:49.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 16:18:49.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:49.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:49.931
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar  9 16:18:49.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:18:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2504" for this suite. 03/09/23 16:18:56.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:18:56.207
Mar  9 16:18:56.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename subpath 03/09/23 16:18:56.208
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:56.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:56.222
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/09/23 16:18:56.225
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-fjzq 03/09/23 16:18:56.232
STEP: Creating a pod to test atomic-volume-subpath 03/09/23 16:18:56.232
Mar  9 16:18:56.239: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fjzq" in namespace "subpath-549" to be "Succeeded or Failed"
Mar  9 16:18:56.242: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.475285ms
Mar  9 16:18:58.247: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007369194s
Mar  9 16:19:00.245: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 4.006009141s
Mar  9 16:19:02.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 6.006857362s
Mar  9 16:19:04.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 8.00687036s
Mar  9 16:19:06.245: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 10.005799673s
Mar  9 16:19:08.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 12.006511013s
Mar  9 16:19:10.247: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 14.007841053s
Mar  9 16:19:12.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 16.006849305s
Mar  9 16:19:14.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 18.007150873s
Mar  9 16:19:16.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 20.007144825s
Mar  9 16:19:18.245: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=false. Elapsed: 22.005917324s
Mar  9 16:19:20.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00683048s
STEP: Saw pod success 03/09/23 16:19:20.246
Mar  9 16:19:20.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq" satisfied condition "Succeeded or Failed"
Mar  9 16:19:20.249: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-downwardapi-fjzq container test-container-subpath-downwardapi-fjzq: <nil>
STEP: delete the pod 03/09/23 16:19:20.256
Mar  9 16:19:20.265: INFO: Waiting for pod pod-subpath-test-downwardapi-fjzq to disappear
Mar  9 16:19:20.267: INFO: Pod pod-subpath-test-downwardapi-fjzq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fjzq 03/09/23 16:19:20.267
Mar  9 16:19:20.267: INFO: Deleting pod "pod-subpath-test-downwardapi-fjzq" in namespace "subpath-549"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  9 16:19:20.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-549" for this suite. 03/09/23 16:19:20.273
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":124,"skipped":2500,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.071 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:18:56.207
    Mar  9 16:18:56.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename subpath 03/09/23 16:18:56.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:18:56.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:18:56.222
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/09/23 16:18:56.225
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-fjzq 03/09/23 16:18:56.232
    STEP: Creating a pod to test atomic-volume-subpath 03/09/23 16:18:56.232
    Mar  9 16:18:56.239: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fjzq" in namespace "subpath-549" to be "Succeeded or Failed"
    Mar  9 16:18:56.242: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.475285ms
    Mar  9 16:18:58.247: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007369194s
    Mar  9 16:19:00.245: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 4.006009141s
    Mar  9 16:19:02.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 6.006857362s
    Mar  9 16:19:04.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 8.00687036s
    Mar  9 16:19:06.245: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 10.005799673s
    Mar  9 16:19:08.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 12.006511013s
    Mar  9 16:19:10.247: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 14.007841053s
    Mar  9 16:19:12.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 16.006849305s
    Mar  9 16:19:14.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 18.007150873s
    Mar  9 16:19:16.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=true. Elapsed: 20.007144825s
    Mar  9 16:19:18.245: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Running", Reason="", readiness=false. Elapsed: 22.005917324s
    Mar  9 16:19:20.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00683048s
    STEP: Saw pod success 03/09/23 16:19:20.246
    Mar  9 16:19:20.246: INFO: Pod "pod-subpath-test-downwardapi-fjzq" satisfied condition "Succeeded or Failed"
    Mar  9 16:19:20.249: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-downwardapi-fjzq container test-container-subpath-downwardapi-fjzq: <nil>
    STEP: delete the pod 03/09/23 16:19:20.256
    Mar  9 16:19:20.265: INFO: Waiting for pod pod-subpath-test-downwardapi-fjzq to disappear
    Mar  9 16:19:20.267: INFO: Pod pod-subpath-test-downwardapi-fjzq no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-fjzq 03/09/23 16:19:20.267
    Mar  9 16:19:20.267: INFO: Deleting pod "pod-subpath-test-downwardapi-fjzq" in namespace "subpath-549"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  9 16:19:20.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-549" for this suite. 03/09/23 16:19:20.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:19:20.279
Mar  9 16:19:20.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:19:20.28
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:20.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:20.296
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-276 03/09/23 16:19:20.298
STEP: creating replication controller nodeport-test in namespace services-276 03/09/23 16:19:20.315
I0309 16:19:20.320509      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-276, replica count: 2
I0309 16:19:23.372083      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 16:19:23.372: INFO: Creating new exec pod
Mar  9 16:19:23.378: INFO: Waiting up to 5m0s for pod "execpodjrkf9" in namespace "services-276" to be "running"
Mar  9 16:19:23.380: INFO: Pod "execpodjrkf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.576598ms
Mar  9 16:19:25.384: INFO: Pod "execpodjrkf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00661386s
Mar  9 16:19:25.384: INFO: Pod "execpodjrkf9" satisfied condition "running"
Mar  9 16:19:26.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar  9 16:19:26.592: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  9 16:19:26.592: INFO: stdout: "nodeport-test-h4hv9"
Mar  9 16:19:26.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.90.251 80'
Mar  9 16:19:26.753: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.90.251 80\nConnection to 10.100.90.251 80 port [tcp/http] succeeded!\n"
Mar  9 16:19:26.753: INFO: stdout: "nodeport-test-h4hv9"
Mar  9 16:19:26.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 30814'
Mar  9 16:19:26.902: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 30814\nConnection to 100.100.230.140 30814 port [tcp/*] succeeded!\n"
Mar  9 16:19:26.903: INFO: stdout: "nodeport-test-j6mpb"
Mar  9 16:19:26.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 30814'
Mar  9 16:19:27.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 30814\nConnection to 100.100.231.104 30814 port [tcp/*] succeeded!\n"
Mar  9 16:19:27.042: INFO: stdout: "nodeport-test-j6mpb"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:19:27.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-276" for this suite. 03/09/23 16:19:27.046
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":125,"skipped":2518,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.772 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:19:20.279
    Mar  9 16:19:20.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:19:20.28
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:20.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:20.296
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-276 03/09/23 16:19:20.298
    STEP: creating replication controller nodeport-test in namespace services-276 03/09/23 16:19:20.315
    I0309 16:19:20.320509      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-276, replica count: 2
    I0309 16:19:23.372083      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 16:19:23.372: INFO: Creating new exec pod
    Mar  9 16:19:23.378: INFO: Waiting up to 5m0s for pod "execpodjrkf9" in namespace "services-276" to be "running"
    Mar  9 16:19:23.380: INFO: Pod "execpodjrkf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.576598ms
    Mar  9 16:19:25.384: INFO: Pod "execpodjrkf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00661386s
    Mar  9 16:19:25.384: INFO: Pod "execpodjrkf9" satisfied condition "running"
    Mar  9 16:19:26.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar  9 16:19:26.592: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar  9 16:19:26.592: INFO: stdout: "nodeport-test-h4hv9"
    Mar  9 16:19:26.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.90.251 80'
    Mar  9 16:19:26.753: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.90.251 80\nConnection to 10.100.90.251 80 port [tcp/http] succeeded!\n"
    Mar  9 16:19:26.753: INFO: stdout: "nodeport-test-h4hv9"
    Mar  9 16:19:26.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 30814'
    Mar  9 16:19:26.902: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 30814\nConnection to 100.100.230.140 30814 port [tcp/*] succeeded!\n"
    Mar  9 16:19:26.903: INFO: stdout: "nodeport-test-j6mpb"
    Mar  9 16:19:26.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-276 exec execpodjrkf9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 30814'
    Mar  9 16:19:27.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 30814\nConnection to 100.100.231.104 30814 port [tcp/*] succeeded!\n"
    Mar  9 16:19:27.042: INFO: stdout: "nodeport-test-j6mpb"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:19:27.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-276" for this suite. 03/09/23 16:19:27.046
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:19:27.051
Mar  9 16:19:27.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:19:27.052
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:27.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:27.068
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-1e8b90d9-3901-438e-b4c5-f17a5a338610 03/09/23 16:19:27.071
STEP: Creating a pod to test consume secrets 03/09/23 16:19:27.074
Mar  9 16:19:27.081: INFO: Waiting up to 5m0s for pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b" in namespace "secrets-5116" to be "Succeeded or Failed"
Mar  9 16:19:27.084: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901723ms
Mar  9 16:19:29.088: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006658104s
Mar  9 16:19:31.089: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007312223s
STEP: Saw pod success 03/09/23 16:19:31.089
Mar  9 16:19:31.089: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b" satisfied condition "Succeeded or Failed"
Mar  9 16:19:31.091: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:19:31.097
Mar  9 16:19:31.108: INFO: Waiting for pod pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b to disappear
Mar  9 16:19:31.110: INFO: Pod pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:19:31.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5116" for this suite. 03/09/23 16:19:31.114
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":126,"skipped":2518,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:19:27.051
    Mar  9 16:19:27.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:19:27.052
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:27.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:27.068
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-1e8b90d9-3901-438e-b4c5-f17a5a338610 03/09/23 16:19:27.071
    STEP: Creating a pod to test consume secrets 03/09/23 16:19:27.074
    Mar  9 16:19:27.081: INFO: Waiting up to 5m0s for pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b" in namespace "secrets-5116" to be "Succeeded or Failed"
    Mar  9 16:19:27.084: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901723ms
    Mar  9 16:19:29.088: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006658104s
    Mar  9 16:19:31.089: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007312223s
    STEP: Saw pod success 03/09/23 16:19:31.089
    Mar  9 16:19:31.089: INFO: Pod "pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b" satisfied condition "Succeeded or Failed"
    Mar  9 16:19:31.091: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:19:31.097
    Mar  9 16:19:31.108: INFO: Waiting for pod pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b to disappear
    Mar  9 16:19:31.110: INFO: Pod pod-secrets-7c5fdad9-5ef9-4cd8-80b7-bd01d62c5e8b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:19:31.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5116" for this suite. 03/09/23 16:19:31.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:19:31.122
Mar  9 16:19:31.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:19:31.123
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:31.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:31.137
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-71b3a2ac-f7b2-4d42-a72e-8c3183e7d810 03/09/23 16:19:31.139
STEP: Creating a pod to test consume configMaps 03/09/23 16:19:31.143
Mar  9 16:19:31.151: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573" in namespace "projected-8302" to be "Succeeded or Failed"
Mar  9 16:19:31.153: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321718ms
Mar  9 16:19:33.157: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005820134s
Mar  9 16:19:35.157: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006171748s
STEP: Saw pod success 03/09/23 16:19:35.157
Mar  9 16:19:35.157: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573" satisfied condition "Succeeded or Failed"
Mar  9 16:19:35.160: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:19:35.165
Mar  9 16:19:35.173: INFO: Waiting for pod pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573 to disappear
Mar  9 16:19:35.175: INFO: Pod pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 16:19:35.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8302" for this suite. 03/09/23 16:19:35.178
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":127,"skipped":2575,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:19:31.122
    Mar  9 16:19:31.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:19:31.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:31.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:31.137
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-71b3a2ac-f7b2-4d42-a72e-8c3183e7d810 03/09/23 16:19:31.139
    STEP: Creating a pod to test consume configMaps 03/09/23 16:19:31.143
    Mar  9 16:19:31.151: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573" in namespace "projected-8302" to be "Succeeded or Failed"
    Mar  9 16:19:31.153: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321718ms
    Mar  9 16:19:33.157: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005820134s
    Mar  9 16:19:35.157: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006171748s
    STEP: Saw pod success 03/09/23 16:19:35.157
    Mar  9 16:19:35.157: INFO: Pod "pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573" satisfied condition "Succeeded or Failed"
    Mar  9 16:19:35.160: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:19:35.165
    Mar  9 16:19:35.173: INFO: Waiting for pod pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573 to disappear
    Mar  9 16:19:35.175: INFO: Pod pod-projected-configmaps-c8571f96-5ce4-4fcc-b9bd-ac7451058573 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 16:19:35.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8302" for this suite. 03/09/23 16:19:35.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:19:35.184
Mar  9 16:19:35.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:19:35.185
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:35.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:35.197
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-bce02c9a-a844-4804-86b9-c03f4ac7a714 03/09/23 16:19:35.2
STEP: Creating a pod to test consume secrets 03/09/23 16:19:35.204
Mar  9 16:19:35.211: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c" in namespace "projected-6205" to be "Succeeded or Failed"
Mar  9 16:19:35.213: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698653ms
Mar  9 16:19:37.216: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005425526s
Mar  9 16:19:39.217: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006794637s
STEP: Saw pod success 03/09/23 16:19:39.217
Mar  9 16:19:39.217: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c" satisfied condition "Succeeded or Failed"
Mar  9 16:19:39.220: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:19:39.226
Mar  9 16:19:39.236: INFO: Waiting for pod pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c to disappear
Mar  9 16:19:39.239: INFO: Pod pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  9 16:19:39.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6205" for this suite. 03/09/23 16:19:39.243
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":128,"skipped":2595,"failed":0}
------------------------------
â€¢ [4.064 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:19:35.184
    Mar  9 16:19:35.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:19:35.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:35.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:35.197
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-bce02c9a-a844-4804-86b9-c03f4ac7a714 03/09/23 16:19:35.2
    STEP: Creating a pod to test consume secrets 03/09/23 16:19:35.204
    Mar  9 16:19:35.211: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c" in namespace "projected-6205" to be "Succeeded or Failed"
    Mar  9 16:19:35.213: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698653ms
    Mar  9 16:19:37.216: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005425526s
    Mar  9 16:19:39.217: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006794637s
    STEP: Saw pod success 03/09/23 16:19:39.217
    Mar  9 16:19:39.217: INFO: Pod "pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c" satisfied condition "Succeeded or Failed"
    Mar  9 16:19:39.220: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:19:39.226
    Mar  9 16:19:39.236: INFO: Waiting for pod pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c to disappear
    Mar  9 16:19:39.239: INFO: Pod pod-projected-secrets-d12ee333-67f9-49a9-b016-9518f37f519c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  9 16:19:39.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6205" for this suite. 03/09/23 16:19:39.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:19:39.248
Mar  9 16:19:39.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:19:39.249
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:39.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:39.264
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 03/09/23 16:19:39.268
Mar  9 16:19:39.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 create -f -'
Mar  9 16:19:40.582: INFO: stderr: ""
Mar  9 16:19:40.582: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:19:40.582
Mar  9 16:19:40.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:19:40.663: INFO: stderr: ""
Mar  9 16:19:40.663: INFO: stdout: "update-demo-nautilus-7j6z6 update-demo-nautilus-w55hv "
Mar  9 16:19:40.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-7j6z6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:19:40.728: INFO: stderr: ""
Mar  9 16:19:40.728: INFO: stdout: ""
Mar  9 16:19:40.728: INFO: update-demo-nautilus-7j6z6 is created but not running
Mar  9 16:19:45.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:19:45.811: INFO: stderr: ""
Mar  9 16:19:45.811: INFO: stdout: "update-demo-nautilus-7j6z6 update-demo-nautilus-w55hv "
Mar  9 16:19:45.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-7j6z6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:19:45.880: INFO: stderr: ""
Mar  9 16:19:45.880: INFO: stdout: "true"
Mar  9 16:19:45.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-7j6z6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  9 16:19:45.948: INFO: stderr: ""
Mar  9 16:19:45.948: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  9 16:19:45.948: INFO: validating pod update-demo-nautilus-7j6z6
Mar  9 16:19:45.952: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  9 16:19:45.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  9 16:19:45.952: INFO: update-demo-nautilus-7j6z6 is verified up and running
Mar  9 16:19:45.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:19:46.022: INFO: stderr: ""
Mar  9 16:19:46.022: INFO: stdout: "true"
Mar  9 16:19:46.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  9 16:19:46.089: INFO: stderr: ""
Mar  9 16:19:46.089: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  9 16:19:46.090: INFO: validating pod update-demo-nautilus-w55hv
Mar  9 16:19:46.094: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  9 16:19:46.094: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  9 16:19:46.094: INFO: update-demo-nautilus-w55hv is verified up and running
STEP: scaling down the replication controller 03/09/23 16:19:46.094
Mar  9 16:19:46.096: INFO: scanned /root for discovery docs: <nil>
Mar  9 16:19:46.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  9 16:19:47.177: INFO: stderr: ""
Mar  9 16:19:47.178: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:19:47.178
Mar  9 16:19:47.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:19:47.264: INFO: stderr: ""
Mar  9 16:19:47.264: INFO: stdout: "update-demo-nautilus-7j6z6 update-demo-nautilus-w55hv "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/09/23 16:19:47.264
Mar  9 16:19:52.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:19:52.333: INFO: stderr: ""
Mar  9 16:19:52.333: INFO: stdout: "update-demo-nautilus-w55hv "
Mar  9 16:19:52.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:19:52.401: INFO: stderr: ""
Mar  9 16:19:52.401: INFO: stdout: "true"
Mar  9 16:19:52.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  9 16:19:52.474: INFO: stderr: ""
Mar  9 16:19:52.474: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  9 16:19:52.474: INFO: validating pod update-demo-nautilus-w55hv
Mar  9 16:19:52.478: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  9 16:19:52.478: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  9 16:19:52.478: INFO: update-demo-nautilus-w55hv is verified up and running
STEP: scaling up the replication controller 03/09/23 16:19:52.478
Mar  9 16:19:52.479: INFO: scanned /root for discovery docs: <nil>
Mar  9 16:19:52.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  9 16:19:53.564: INFO: stderr: ""
Mar  9 16:19:53.564: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:19:53.564
Mar  9 16:19:53.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:19:53.636: INFO: stderr: ""
Mar  9 16:19:53.636: INFO: stdout: "update-demo-nautilus-km5k9 update-demo-nautilus-w55hv "
Mar  9 16:19:53.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-km5k9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:19:53.705: INFO: stderr: ""
Mar  9 16:19:53.705: INFO: stdout: ""
Mar  9 16:19:53.705: INFO: update-demo-nautilus-km5k9 is created but not running
Mar  9 16:19:58.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:19:58.783: INFO: stderr: ""
Mar  9 16:19:58.783: INFO: stdout: "update-demo-nautilus-km5k9 update-demo-nautilus-w55hv "
Mar  9 16:19:58.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-km5k9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:19:58.852: INFO: stderr: ""
Mar  9 16:19:58.852: INFO: stdout: "true"
Mar  9 16:19:58.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-km5k9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  9 16:19:58.923: INFO: stderr: ""
Mar  9 16:19:58.923: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  9 16:19:58.923: INFO: validating pod update-demo-nautilus-km5k9
Mar  9 16:19:58.927: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  9 16:19:58.927: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  9 16:19:58.927: INFO: update-demo-nautilus-km5k9 is verified up and running
Mar  9 16:19:58.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:19:58.998: INFO: stderr: ""
Mar  9 16:19:58.998: INFO: stdout: "true"
Mar  9 16:19:58.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  9 16:19:59.066: INFO: stderr: ""
Mar  9 16:19:59.066: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  9 16:19:59.066: INFO: validating pod update-demo-nautilus-w55hv
Mar  9 16:19:59.069: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  9 16:19:59.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  9 16:19:59.069: INFO: update-demo-nautilus-w55hv is verified up and running
STEP: using delete to clean up resources 03/09/23 16:19:59.069
Mar  9 16:19:59.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 delete --grace-period=0 --force -f -'
Mar  9 16:19:59.139: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:19:59.139: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  9 16:19:59.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get rc,svc -l name=update-demo --no-headers'
Mar  9 16:19:59.229: INFO: stderr: "No resources found in kubectl-493 namespace.\n"
Mar  9 16:19:59.229: INFO: stdout: ""
Mar  9 16:19:59.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  9 16:19:59.308: INFO: stderr: ""
Mar  9 16:19:59.309: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:19:59.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-493" for this suite. 03/09/23 16:19:59.312
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":129,"skipped":2600,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.071 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:19:39.248
    Mar  9 16:19:39.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:19:39.249
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:39.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:39.264
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 03/09/23 16:19:39.268
    Mar  9 16:19:39.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 create -f -'
    Mar  9 16:19:40.582: INFO: stderr: ""
    Mar  9 16:19:40.582: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:19:40.582
    Mar  9 16:19:40.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:19:40.663: INFO: stderr: ""
    Mar  9 16:19:40.663: INFO: stdout: "update-demo-nautilus-7j6z6 update-demo-nautilus-w55hv "
    Mar  9 16:19:40.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-7j6z6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:19:40.728: INFO: stderr: ""
    Mar  9 16:19:40.728: INFO: stdout: ""
    Mar  9 16:19:40.728: INFO: update-demo-nautilus-7j6z6 is created but not running
    Mar  9 16:19:45.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:19:45.811: INFO: stderr: ""
    Mar  9 16:19:45.811: INFO: stdout: "update-demo-nautilus-7j6z6 update-demo-nautilus-w55hv "
    Mar  9 16:19:45.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-7j6z6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:19:45.880: INFO: stderr: ""
    Mar  9 16:19:45.880: INFO: stdout: "true"
    Mar  9 16:19:45.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-7j6z6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  9 16:19:45.948: INFO: stderr: ""
    Mar  9 16:19:45.948: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  9 16:19:45.948: INFO: validating pod update-demo-nautilus-7j6z6
    Mar  9 16:19:45.952: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  9 16:19:45.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  9 16:19:45.952: INFO: update-demo-nautilus-7j6z6 is verified up and running
    Mar  9 16:19:45.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:19:46.022: INFO: stderr: ""
    Mar  9 16:19:46.022: INFO: stdout: "true"
    Mar  9 16:19:46.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  9 16:19:46.089: INFO: stderr: ""
    Mar  9 16:19:46.089: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  9 16:19:46.090: INFO: validating pod update-demo-nautilus-w55hv
    Mar  9 16:19:46.094: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  9 16:19:46.094: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  9 16:19:46.094: INFO: update-demo-nautilus-w55hv is verified up and running
    STEP: scaling down the replication controller 03/09/23 16:19:46.094
    Mar  9 16:19:46.096: INFO: scanned /root for discovery docs: <nil>
    Mar  9 16:19:46.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar  9 16:19:47.177: INFO: stderr: ""
    Mar  9 16:19:47.178: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:19:47.178
    Mar  9 16:19:47.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:19:47.264: INFO: stderr: ""
    Mar  9 16:19:47.264: INFO: stdout: "update-demo-nautilus-7j6z6 update-demo-nautilus-w55hv "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/09/23 16:19:47.264
    Mar  9 16:19:52.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:19:52.333: INFO: stderr: ""
    Mar  9 16:19:52.333: INFO: stdout: "update-demo-nautilus-w55hv "
    Mar  9 16:19:52.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:19:52.401: INFO: stderr: ""
    Mar  9 16:19:52.401: INFO: stdout: "true"
    Mar  9 16:19:52.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  9 16:19:52.474: INFO: stderr: ""
    Mar  9 16:19:52.474: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  9 16:19:52.474: INFO: validating pod update-demo-nautilus-w55hv
    Mar  9 16:19:52.478: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  9 16:19:52.478: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  9 16:19:52.478: INFO: update-demo-nautilus-w55hv is verified up and running
    STEP: scaling up the replication controller 03/09/23 16:19:52.478
    Mar  9 16:19:52.479: INFO: scanned /root for discovery docs: <nil>
    Mar  9 16:19:52.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar  9 16:19:53.564: INFO: stderr: ""
    Mar  9 16:19:53.564: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:19:53.564
    Mar  9 16:19:53.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:19:53.636: INFO: stderr: ""
    Mar  9 16:19:53.636: INFO: stdout: "update-demo-nautilus-km5k9 update-demo-nautilus-w55hv "
    Mar  9 16:19:53.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-km5k9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:19:53.705: INFO: stderr: ""
    Mar  9 16:19:53.705: INFO: stdout: ""
    Mar  9 16:19:53.705: INFO: update-demo-nautilus-km5k9 is created but not running
    Mar  9 16:19:58.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:19:58.783: INFO: stderr: ""
    Mar  9 16:19:58.783: INFO: stdout: "update-demo-nautilus-km5k9 update-demo-nautilus-w55hv "
    Mar  9 16:19:58.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-km5k9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:19:58.852: INFO: stderr: ""
    Mar  9 16:19:58.852: INFO: stdout: "true"
    Mar  9 16:19:58.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-km5k9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  9 16:19:58.923: INFO: stderr: ""
    Mar  9 16:19:58.923: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  9 16:19:58.923: INFO: validating pod update-demo-nautilus-km5k9
    Mar  9 16:19:58.927: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  9 16:19:58.927: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  9 16:19:58.927: INFO: update-demo-nautilus-km5k9 is verified up and running
    Mar  9 16:19:58.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:19:58.998: INFO: stderr: ""
    Mar  9 16:19:58.998: INFO: stdout: "true"
    Mar  9 16:19:58.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods update-demo-nautilus-w55hv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  9 16:19:59.066: INFO: stderr: ""
    Mar  9 16:19:59.066: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  9 16:19:59.066: INFO: validating pod update-demo-nautilus-w55hv
    Mar  9 16:19:59.069: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  9 16:19:59.069: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  9 16:19:59.069: INFO: update-demo-nautilus-w55hv is verified up and running
    STEP: using delete to clean up resources 03/09/23 16:19:59.069
    Mar  9 16:19:59.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 delete --grace-period=0 --force -f -'
    Mar  9 16:19:59.139: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:19:59.139: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  9 16:19:59.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get rc,svc -l name=update-demo --no-headers'
    Mar  9 16:19:59.229: INFO: stderr: "No resources found in kubectl-493 namespace.\n"
    Mar  9 16:19:59.229: INFO: stdout: ""
    Mar  9 16:19:59.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-493 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  9 16:19:59.308: INFO: stderr: ""
    Mar  9 16:19:59.309: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:19:59.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-493" for this suite. 03/09/23 16:19:59.312
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:19:59.319
Mar  9 16:19:59.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename daemonsets 03/09/23 16:19:59.32
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:59.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:59.335
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Mar  9 16:19:59.351: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/09/23 16:19:59.355
Mar  9 16:19:59.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:19:59.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/09/23 16:19:59.358
Mar  9 16:19:59.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:19:59.378: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:20:00.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:20:00.382: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:20:01.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:20:01.382: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/09/23 16:20:01.385
Mar  9 16:20:01.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:20:01.405: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar  9 16:20:02.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:20:02.410: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/09/23 16:20:02.41
Mar  9 16:20:02.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:20:02.420: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:20:03.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:20:03.425: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:20:04.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:20:04.423: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:20:05.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:20:05.424: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:20:05.429
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-377, will wait for the garbage collector to delete the pods 03/09/23 16:20:05.429
Mar  9 16:20:05.488: INFO: Deleting DaemonSet.extensions daemon-set took: 5.003723ms
Mar  9 16:20:05.588: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.614659ms
Mar  9 16:20:08.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:20:08.192: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  9 16:20:08.194: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"99201"},"items":null}

Mar  9 16:20:08.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"99201"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:20:08.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-377" for this suite. 03/09/23 16:20:08.222
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":130,"skipped":2601,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.908 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:19:59.319
    Mar  9 16:19:59.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename daemonsets 03/09/23 16:19:59.32
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:19:59.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:19:59.335
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Mar  9 16:19:59.351: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/09/23 16:19:59.355
    Mar  9 16:19:59.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:19:59.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/09/23 16:19:59.358
    Mar  9 16:19:59.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:19:59.378: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:20:00.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:20:00.382: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:20:01.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:20:01.382: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/09/23 16:20:01.385
    Mar  9 16:20:01.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:20:01.405: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Mar  9 16:20:02.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:20:02.410: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/09/23 16:20:02.41
    Mar  9 16:20:02.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:20:02.420: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:20:03.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:20:03.425: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:20:04.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:20:04.423: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:20:05.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:20:05.424: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:20:05.429
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-377, will wait for the garbage collector to delete the pods 03/09/23 16:20:05.429
    Mar  9 16:20:05.488: INFO: Deleting DaemonSet.extensions daemon-set took: 5.003723ms
    Mar  9 16:20:05.588: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.614659ms
    Mar  9 16:20:08.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:20:08.192: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  9 16:20:08.194: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"99201"},"items":null}

    Mar  9 16:20:08.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"99201"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:20:08.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-377" for this suite. 03/09/23 16:20:08.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:20:08.229
Mar  9 16:20:08.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:20:08.231
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:20:08.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:20:08.244
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:20:08.248
Mar  9 16:20:08.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385" in namespace "projected-8991" to be "Succeeded or Failed"
Mar  9 16:20:08.258: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.653266ms
Mar  9 16:20:10.261: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006229158s
Mar  9 16:20:12.263: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007445083s
STEP: Saw pod success 03/09/23 16:20:12.263
Mar  9 16:20:12.263: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385" satisfied condition "Succeeded or Failed"
Mar  9 16:20:12.266: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385 container client-container: <nil>
STEP: delete the pod 03/09/23 16:20:12.271
Mar  9 16:20:12.280: INFO: Waiting for pod downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385 to disappear
Mar  9 16:20:12.283: INFO: Pod downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 16:20:12.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8991" for this suite. 03/09/23 16:20:12.287
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":131,"skipped":2639,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:20:08.229
    Mar  9 16:20:08.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:20:08.231
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:20:08.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:20:08.244
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:20:08.248
    Mar  9 16:20:08.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385" in namespace "projected-8991" to be "Succeeded or Failed"
    Mar  9 16:20:08.258: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.653266ms
    Mar  9 16:20:10.261: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006229158s
    Mar  9 16:20:12.263: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007445083s
    STEP: Saw pod success 03/09/23 16:20:12.263
    Mar  9 16:20:12.263: INFO: Pod "downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385" satisfied condition "Succeeded or Failed"
    Mar  9 16:20:12.266: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385 container client-container: <nil>
    STEP: delete the pod 03/09/23 16:20:12.271
    Mar  9 16:20:12.280: INFO: Waiting for pod downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385 to disappear
    Mar  9 16:20:12.283: INFO: Pod downwardapi-volume-74de8e17-baf3-42ea-9c62-d091ccfd4385 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 16:20:12.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8991" for this suite. 03/09/23 16:20:12.287
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:20:12.295
Mar  9 16:20:12.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:20:12.297
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:20:12.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:20:12.311
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:20:12.324
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:20:13.146
STEP: Deploying the webhook pod 03/09/23 16:20:13.153
STEP: Wait for the deployment to be ready 03/09/23 16:20:13.165
Mar  9 16:20:13.172: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:20:15.181
STEP: Verifying the service has paired with the endpoint 03/09/23 16:20:15.194
Mar  9 16:20:16.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 03/09/23 16:20:16.198
STEP: create a pod that should be denied by the webhook 03/09/23 16:20:16.213
STEP: create a pod that causes the webhook to hang 03/09/23 16:20:16.223
STEP: create a configmap that should be denied by the webhook 03/09/23 16:20:26.229
STEP: create a configmap that should be admitted by the webhook 03/09/23 16:20:26.259
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/09/23 16:20:26.267
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/09/23 16:20:26.275
STEP: create a namespace that bypass the webhook 03/09/23 16:20:26.28
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/09/23 16:20:26.286
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:20:26.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-302" for this suite. 03/09/23 16:20:26.312
STEP: Destroying namespace "webhook-302-markers" for this suite. 03/09/23 16:20:26.317
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":132,"skipped":2640,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.098 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:20:12.295
    Mar  9 16:20:12.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:20:12.297
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:20:12.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:20:12.311
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:20:12.324
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:20:13.146
    STEP: Deploying the webhook pod 03/09/23 16:20:13.153
    STEP: Wait for the deployment to be ready 03/09/23 16:20:13.165
    Mar  9 16:20:13.172: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:20:15.181
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:20:15.194
    Mar  9 16:20:16.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 03/09/23 16:20:16.198
    STEP: create a pod that should be denied by the webhook 03/09/23 16:20:16.213
    STEP: create a pod that causes the webhook to hang 03/09/23 16:20:16.223
    STEP: create a configmap that should be denied by the webhook 03/09/23 16:20:26.229
    STEP: create a configmap that should be admitted by the webhook 03/09/23 16:20:26.259
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/09/23 16:20:26.267
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/09/23 16:20:26.275
    STEP: create a namespace that bypass the webhook 03/09/23 16:20:26.28
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/09/23 16:20:26.286
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:20:26.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-302" for this suite. 03/09/23 16:20:26.312
    STEP: Destroying namespace "webhook-302-markers" for this suite. 03/09/23 16:20:26.317
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:20:26.395
Mar  9 16:20:26.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 16:20:26.396
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:20:26.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:20:26.416
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f in namespace container-probe-4523 03/09/23 16:20:26.428
Mar  9 16:20:26.450: INFO: Waiting up to 5m0s for pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f" in namespace "container-probe-4523" to be "not pending"
Mar  9 16:20:26.455: INFO: Pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.468322ms
Mar  9 16:20:28.458: INFO: Pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008014103s
Mar  9 16:20:28.458: INFO: Pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f" satisfied condition "not pending"
Mar  9 16:20:28.458: INFO: Started pod busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f in namespace container-probe-4523
STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:20:28.458
Mar  9 16:20:28.461: INFO: Initial restart count of pod busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f is 0
STEP: deleting the pod 03/09/23 16:24:28.955
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 16:24:28.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4523" for this suite. 03/09/23 16:24:28.972
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":133,"skipped":2655,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.588 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:20:26.395
    Mar  9 16:20:26.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 16:20:26.396
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:20:26.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:20:26.416
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f in namespace container-probe-4523 03/09/23 16:20:26.428
    Mar  9 16:20:26.450: INFO: Waiting up to 5m0s for pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f" in namespace "container-probe-4523" to be "not pending"
    Mar  9 16:20:26.455: INFO: Pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.468322ms
    Mar  9 16:20:28.458: INFO: Pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008014103s
    Mar  9 16:20:28.458: INFO: Pod "busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f" satisfied condition "not pending"
    Mar  9 16:20:28.458: INFO: Started pod busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f in namespace container-probe-4523
    STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:20:28.458
    Mar  9 16:20:28.461: INFO: Initial restart count of pod busybox-dfd5494c-4c9a-4be0-8cf3-08028706be5f is 0
    STEP: deleting the pod 03/09/23 16:24:28.955
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 16:24:28.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4523" for this suite. 03/09/23 16:24:28.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:24:28.984
Mar  9 16:24:28.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename cronjob 03/09/23 16:24:28.985
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:24:29.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:24:29.007
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/09/23 16:24:29.01
STEP: Ensuring no jobs are scheduled 03/09/23 16:24:29.015
STEP: Ensuring no job exists by listing jobs explicitly 03/09/23 16:29:29.021
STEP: Removing cronjob 03/09/23 16:29:29.024
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  9 16:29:29.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6631" for this suite. 03/09/23 16:29:29.032
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":134,"skipped":2680,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.055 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:24:28.984
    Mar  9 16:24:28.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename cronjob 03/09/23 16:24:28.985
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:24:29.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:24:29.007
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/09/23 16:24:29.01
    STEP: Ensuring no jobs are scheduled 03/09/23 16:24:29.015
    STEP: Ensuring no job exists by listing jobs explicitly 03/09/23 16:29:29.021
    STEP: Removing cronjob 03/09/23 16:29:29.024
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  9 16:29:29.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6631" for this suite. 03/09/23 16:29:29.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:29:29.041
Mar  9 16:29:29.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:29:29.042
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:29.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:29.056
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Mar  9 16:29:29.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: creating the pod 03/09/23 16:29:29.06
STEP: submitting the pod to kubernetes 03/09/23 16:29:29.06
Mar  9 16:29:29.067: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c" in namespace "pods-4917" to be "running and ready"
Mar  9 16:29:29.069: INFO: Pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.55677ms
Mar  9 16:29:29.069: INFO: The phase of Pod pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:29:31.073: INFO: Pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006744791s
Mar  9 16:29:31.074: INFO: The phase of Pod pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c is Running (Ready = true)
Mar  9 16:29:31.074: INFO: Pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:29:31.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4917" for this suite. 03/09/23 16:29:31.097
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":135,"skipped":2716,"failed":0}
------------------------------
â€¢ [2.060 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:29:29.041
    Mar  9 16:29:29.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:29:29.042
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:29.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:29.056
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Mar  9 16:29:29.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: creating the pod 03/09/23 16:29:29.06
    STEP: submitting the pod to kubernetes 03/09/23 16:29:29.06
    Mar  9 16:29:29.067: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c" in namespace "pods-4917" to be "running and ready"
    Mar  9 16:29:29.069: INFO: Pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.55677ms
    Mar  9 16:29:29.069: INFO: The phase of Pod pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:29:31.073: INFO: Pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006744791s
    Mar  9 16:29:31.074: INFO: The phase of Pod pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c is Running (Ready = true)
    Mar  9 16:29:31.074: INFO: Pod "pod-logs-websocket-236b129c-f780-4495-b1d8-af69c3dc534c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:29:31.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4917" for this suite. 03/09/23 16:29:31.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:29:31.102
Mar  9 16:29:31.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubelet-test 03/09/23 16:29:31.103
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:31.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:31.119
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  9 16:29:31.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4889" for this suite. 03/09/23 16:29:31.146
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":136,"skipped":2733,"failed":0}
------------------------------
â€¢ [0.048 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:29:31.102
    Mar  9 16:29:31.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubelet-test 03/09/23 16:29:31.103
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:31.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:31.119
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  9 16:29:31.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4889" for this suite. 03/09/23 16:29:31.146
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:29:31.151
Mar  9 16:29:31.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:29:31.152
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:31.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:31.164
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Mar  9 16:29:31.180: INFO: created pod pod-service-account-defaultsa
Mar  9 16:29:31.180: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  9 16:29:31.185: INFO: created pod pod-service-account-mountsa
Mar  9 16:29:31.185: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  9 16:29:31.190: INFO: created pod pod-service-account-nomountsa
Mar  9 16:29:31.190: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  9 16:29:31.197: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  9 16:29:31.197: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  9 16:29:31.210: INFO: created pod pod-service-account-mountsa-mountspec
Mar  9 16:29:31.210: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  9 16:29:31.218: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  9 16:29:31.218: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  9 16:29:31.224: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  9 16:29:31.224: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  9 16:29:31.230: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  9 16:29:31.230: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  9 16:29:31.237: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  9 16:29:31.237: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  9 16:29:31.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8061" for this suite. 03/09/23 16:29:31.243
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":137,"skipped":2733,"failed":0}
------------------------------
â€¢ [0.100 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:29:31.151
    Mar  9 16:29:31.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:29:31.152
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:31.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:31.164
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Mar  9 16:29:31.180: INFO: created pod pod-service-account-defaultsa
    Mar  9 16:29:31.180: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar  9 16:29:31.185: INFO: created pod pod-service-account-mountsa
    Mar  9 16:29:31.185: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar  9 16:29:31.190: INFO: created pod pod-service-account-nomountsa
    Mar  9 16:29:31.190: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar  9 16:29:31.197: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar  9 16:29:31.197: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar  9 16:29:31.210: INFO: created pod pod-service-account-mountsa-mountspec
    Mar  9 16:29:31.210: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar  9 16:29:31.218: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar  9 16:29:31.218: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar  9 16:29:31.224: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar  9 16:29:31.224: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar  9 16:29:31.230: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar  9 16:29:31.230: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar  9 16:29:31.237: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar  9 16:29:31.237: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  9 16:29:31.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8061" for this suite. 03/09/23 16:29:31.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:29:31.253
Mar  9 16:29:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 16:29:31.254
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:31.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:31.268
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/09/23 16:29:31.274
STEP: waiting for Deployment to be created 03/09/23 16:29:31.278
STEP: waiting for all Replicas to be Ready 03/09/23 16:29:31.28
Mar  9 16:29:31.282: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:31.282: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:31.297: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:31.297: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:31.308: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:31.308: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:31.332: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:31.332: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  9 16:29:33.014: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  9 16:29:33.014: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  9 16:29:36.357: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/09/23 16:29:36.357
W0309 16:29:36.366845      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  9 16:29:36.368: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/09/23 16:29:36.368
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.379: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.379: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.401: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.401: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:36.407: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:36.408: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:36.424: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:36.425: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:43.561: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:43.561: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:43.576: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
STEP: listing Deployments 03/09/23 16:29:43.576
Mar  9 16:29:43.582: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/09/23 16:29:43.582
Mar  9 16:29:43.593: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/09/23 16:29:43.593
Mar  9 16:29:43.600: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:43.605: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:43.618: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:43.635: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:43.640: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:45.047: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:45.478: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:45.509: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:45.523: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  9 16:29:47.061: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/09/23 16:29:47.079
STEP: fetching the DeploymentStatus 03/09/23 16:29:47.086
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
Mar  9 16:29:47.092: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3
STEP: deleting the Deployment 03/09/23 16:29:47.092
Mar  9 16:29:47.099: INFO: observed event type MODIFIED
Mar  9 16:29:47.099: INFO: observed event type MODIFIED
Mar  9 16:29:47.099: INFO: observed event type MODIFIED
Mar  9 16:29:47.099: INFO: observed event type MODIFIED
Mar  9 16:29:47.099: INFO: observed event type MODIFIED
Mar  9 16:29:47.099: INFO: observed event type MODIFIED
Mar  9 16:29:47.099: INFO: observed event type MODIFIED
Mar  9 16:29:47.100: INFO: observed event type MODIFIED
Mar  9 16:29:47.100: INFO: observed event type MODIFIED
Mar  9 16:29:47.100: INFO: observed event type MODIFIED
Mar  9 16:29:47.100: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 16:29:47.105: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  9 16:29:47.108: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-761  926b5082-1dbe-4edd-9c32-d1ed5c9b47a3 100791 4 2023-03-09 16:29:36 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 12292a5a-2acc-4470-a841-1adaedf7f1c0 0xc005167167 0xc005167168}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12292a5a-2acc-4470-a841-1adaedf7f1c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051671f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar  9 16:29:47.111: INFO: pod: "test-deployment-54cc775c4b-s9qmh":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-s9qmh test-deployment-54cc775c4b- deployment-761  7df9884a-8b98-414f-baeb-818bdd58f7bf 100786 0 2023-03-09 16:29:43 +0000 UTC 2023-03-09 16:29:48 +0000 UTC 0xc005167668 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:be3b48efa3f970f1ebd295fbef2a2535d7dc7f727138a24f2ea9e0d5b04ebd1b cni.projectcalico.org/podIP:10.244.88.227/32 cni.projectcalico.org/podIPs:10.244.88.227/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 926b5082-1dbe-4edd-9c32-d1ed5c9b47a3 0xc0051676b7 0xc0051676b8}] [] [{kube-controller-manager Update v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"926b5082-1dbe-4edd-9c32-d1ed5c9b47a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vdnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vdnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.227,StartTime:2023-03-09 16:29:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:29:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://f4d8e32d851a32ff269e37b688fc34bee0ff158a26bab64c8e122aaa874b4f23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  9 16:29:47.111: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-761  b2610d81-3f25-4681-9706-6810fcb8c2eb 100783 2 2023-03-09 16:29:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 12292a5a-2acc-4470-a841-1adaedf7f1c0 0xc005167257 0xc005167258}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12292a5a-2acc-4470-a841-1adaedf7f1c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051672e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar  9 16:29:47.117: INFO: pod: "test-deployment-7c7d8d58c8-2dr69":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2dr69 test-deployment-7c7d8d58c8- deployment-761  2cfdc9d8-3874-4afa-b866-0f4fe7966ea4 100781 0 2023-03-09 16:29:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:120de315ecbd749e3b234f2e211582c3967c1abc2b3831b8d97efd37a0c196fd cni.projectcalico.org/podIP:10.244.88.215/32 cni.projectcalico.org/podIPs:10.244.88.215/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b2610d81-3f25-4681-9706-6810fcb8c2eb 0xc00059b1e7 0xc00059b1e8}] [] [{kube-controller-manager Update v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2610d81-3f25-4681-9706-6810fcb8c2eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5br4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5br4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.215,StartTime:2023-03-09 16:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4cb73b80fef6c0694b5116a39ad7a878e6b17e4188ad9e988e3c3c8c65cc075a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  9 16:29:47.117: INFO: pod: "test-deployment-7c7d8d58c8-rh5xq":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-rh5xq test-deployment-7c7d8d58c8- deployment-761  0ea0caaf-a992-47ca-aeb9-9fbad8a244e6 100745 0 2023-03-09 16:29:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:dc5610bdbb87d188dc9e2138dfc7445d95937467db97c53f9874740de87c4044 cni.projectcalico.org/podIP:10.244.42.217/32 cni.projectcalico.org/podIPs:10.244.42.217/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b2610d81-3f25-4681-9706-6810fcb8c2eb 0xc00059ba87 0xc00059ba88}] [] [{kube-controller-manager Update v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2610d81-3f25-4681-9706-6810fcb8c2eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzh27,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzh27,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.217,StartTime:2023-03-09 16:29:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:29:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4da8961e6243873e17a12389f1150da7085673e0f2e6ee450ad0bf489b9ce91d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  9 16:29:47.117: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-761  10dfe8ef-62ae-4978-910b-cee396ea54d8 100695 3 2023-03-09 16:29:31 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 12292a5a-2acc-4470-a841-1adaedf7f1c0 0xc005167347 0xc005167348}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12292a5a-2acc-4470-a841-1adaedf7f1c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051673d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 16:29:47.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-761" for this suite. 03/09/23 16:29:47.124
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":138,"skipped":2761,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.875 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:29:31.253
    Mar  9 16:29:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 16:29:31.254
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:31.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:31.268
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/09/23 16:29:31.274
    STEP: waiting for Deployment to be created 03/09/23 16:29:31.278
    STEP: waiting for all Replicas to be Ready 03/09/23 16:29:31.28
    Mar  9 16:29:31.282: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:31.282: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:31.297: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:31.297: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:31.308: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:31.308: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:31.332: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:31.332: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  9 16:29:33.014: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  9 16:29:33.014: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  9 16:29:36.357: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/09/23 16:29:36.357
    W0309 16:29:36.366845      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  9 16:29:36.368: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/09/23 16:29:36.368
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 0
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:36.370: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.371: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.379: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.379: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.401: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.401: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:36.407: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:36.408: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:36.424: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:36.425: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:43.561: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:43.561: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:43.576: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    STEP: listing Deployments 03/09/23 16:29:43.576
    Mar  9 16:29:43.582: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/09/23 16:29:43.582
    Mar  9 16:29:43.593: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/09/23 16:29:43.593
    Mar  9 16:29:43.600: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:43.605: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:43.618: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:43.635: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:43.640: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:45.047: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:45.478: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:45.509: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:45.523: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  9 16:29:47.061: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/09/23 16:29:47.079
    STEP: fetching the DeploymentStatus 03/09/23 16:29:47.086
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 1
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:47.091: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 2
    Mar  9 16:29:47.092: INFO: observed Deployment test-deployment in namespace deployment-761 with ReadyReplicas 3
    STEP: deleting the Deployment 03/09/23 16:29:47.092
    Mar  9 16:29:47.099: INFO: observed event type MODIFIED
    Mar  9 16:29:47.099: INFO: observed event type MODIFIED
    Mar  9 16:29:47.099: INFO: observed event type MODIFIED
    Mar  9 16:29:47.099: INFO: observed event type MODIFIED
    Mar  9 16:29:47.099: INFO: observed event type MODIFIED
    Mar  9 16:29:47.099: INFO: observed event type MODIFIED
    Mar  9 16:29:47.099: INFO: observed event type MODIFIED
    Mar  9 16:29:47.100: INFO: observed event type MODIFIED
    Mar  9 16:29:47.100: INFO: observed event type MODIFIED
    Mar  9 16:29:47.100: INFO: observed event type MODIFIED
    Mar  9 16:29:47.100: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 16:29:47.105: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar  9 16:29:47.108: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-761  926b5082-1dbe-4edd-9c32-d1ed5c9b47a3 100791 4 2023-03-09 16:29:36 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 12292a5a-2acc-4470-a841-1adaedf7f1c0 0xc005167167 0xc005167168}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12292a5a-2acc-4470-a841-1adaedf7f1c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051671f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar  9 16:29:47.111: INFO: pod: "test-deployment-54cc775c4b-s9qmh":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-s9qmh test-deployment-54cc775c4b- deployment-761  7df9884a-8b98-414f-baeb-818bdd58f7bf 100786 0 2023-03-09 16:29:43 +0000 UTC 2023-03-09 16:29:48 +0000 UTC 0xc005167668 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:be3b48efa3f970f1ebd295fbef2a2535d7dc7f727138a24f2ea9e0d5b04ebd1b cni.projectcalico.org/podIP:10.244.88.227/32 cni.projectcalico.org/podIPs:10.244.88.227/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 926b5082-1dbe-4edd-9c32-d1ed5c9b47a3 0xc0051676b7 0xc0051676b8}] [] [{kube-controller-manager Update v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"926b5082-1dbe-4edd-9c32-d1ed5c9b47a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vdnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vdnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.227,StartTime:2023-03-09 16:29:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:29:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://f4d8e32d851a32ff269e37b688fc34bee0ff158a26bab64c8e122aaa874b4f23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  9 16:29:47.111: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-761  b2610d81-3f25-4681-9706-6810fcb8c2eb 100783 2 2023-03-09 16:29:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 12292a5a-2acc-4470-a841-1adaedf7f1c0 0xc005167257 0xc005167258}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12292a5a-2acc-4470-a841-1adaedf7f1c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051672e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar  9 16:29:47.117: INFO: pod: "test-deployment-7c7d8d58c8-2dr69":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2dr69 test-deployment-7c7d8d58c8- deployment-761  2cfdc9d8-3874-4afa-b866-0f4fe7966ea4 100781 0 2023-03-09 16:29:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:120de315ecbd749e3b234f2e211582c3967c1abc2b3831b8d97efd37a0c196fd cni.projectcalico.org/podIP:10.244.88.215/32 cni.projectcalico.org/podIPs:10.244.88.215/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b2610d81-3f25-4681-9706-6810fcb8c2eb 0xc00059b1e7 0xc00059b1e8}] [] [{kube-controller-manager Update v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2610d81-3f25-4681-9706-6810fcb8c2eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:29:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5br4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5br4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.215,StartTime:2023-03-09 16:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4cb73b80fef6c0694b5116a39ad7a878e6b17e4188ad9e988e3c3c8c65cc075a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  9 16:29:47.117: INFO: pod: "test-deployment-7c7d8d58c8-rh5xq":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-rh5xq test-deployment-7c7d8d58c8- deployment-761  0ea0caaf-a992-47ca-aeb9-9fbad8a244e6 100745 0 2023-03-09 16:29:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:dc5610bdbb87d188dc9e2138dfc7445d95937467db97c53f9874740de87c4044 cni.projectcalico.org/podIP:10.244.42.217/32 cni.projectcalico.org/podIPs:10.244.42.217/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b2610d81-3f25-4681-9706-6810fcb8c2eb 0xc00059ba87 0xc00059ba88}] [] [{kube-controller-manager Update v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2610d81-3f25-4681-9706-6810fcb8c2eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:29:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzh27,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzh27,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:29:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.217,StartTime:2023-03-09 16:29:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:29:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4da8961e6243873e17a12389f1150da7085673e0f2e6ee450ad0bf489b9ce91d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  9 16:29:47.117: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-761  10dfe8ef-62ae-4978-910b-cee396ea54d8 100695 3 2023-03-09 16:29:31 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 12292a5a-2acc-4470-a841-1adaedf7f1c0 0xc005167347 0xc005167348}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12292a5a-2acc-4470-a841-1adaedf7f1c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:29:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051673d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 16:29:47.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-761" for this suite. 03/09/23 16:29:47.124
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:29:47.129
Mar  9 16:29:47.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 16:29:47.131
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:47.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:47.145
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 03/09/23 16:29:47.147
Mar  9 16:29:47.153: INFO: Waiting up to 5m0s for pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7" in namespace "var-expansion-8431" to be "Succeeded or Failed"
Mar  9 16:29:47.156: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.011646ms
Mar  9 16:29:49.160: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006280465s
Mar  9 16:29:51.160: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006405687s
STEP: Saw pod success 03/09/23 16:29:51.16
Mar  9 16:29:51.160: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7" satisfied condition "Succeeded or Failed"
Mar  9 16:29:51.162: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7 container dapi-container: <nil>
STEP: delete the pod 03/09/23 16:29:51.168
Mar  9 16:29:51.177: INFO: Waiting for pod var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7 to disappear
Mar  9 16:29:51.180: INFO: Pod var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 16:29:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8431" for this suite. 03/09/23 16:29:51.183
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":139,"skipped":2764,"failed":0}
------------------------------
â€¢ [4.058 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:29:47.129
    Mar  9 16:29:47.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 16:29:47.131
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:47.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:47.145
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 03/09/23 16:29:47.147
    Mar  9 16:29:47.153: INFO: Waiting up to 5m0s for pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7" in namespace "var-expansion-8431" to be "Succeeded or Failed"
    Mar  9 16:29:47.156: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.011646ms
    Mar  9 16:29:49.160: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006280465s
    Mar  9 16:29:51.160: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006405687s
    STEP: Saw pod success 03/09/23 16:29:51.16
    Mar  9 16:29:51.160: INFO: Pod "var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7" satisfied condition "Succeeded or Failed"
    Mar  9 16:29:51.162: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7 container dapi-container: <nil>
    STEP: delete the pod 03/09/23 16:29:51.168
    Mar  9 16:29:51.177: INFO: Waiting for pod var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7 to disappear
    Mar  9 16:29:51.180: INFO: Pod var-expansion-3ff7e5ca-1095-4119-bcfd-635f6a905ca7 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 16:29:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8431" for this suite. 03/09/23 16:29:51.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:29:51.19
Mar  9 16:29:51.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:29:51.191
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:51.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:51.206
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 03/09/23 16:30:08.212
STEP: Creating a ResourceQuota 03/09/23 16:30:13.215
STEP: Ensuring resource quota status is calculated 03/09/23 16:30:13.22
STEP: Creating a ConfigMap 03/09/23 16:30:15.223
STEP: Ensuring resource quota status captures configMap creation 03/09/23 16:30:15.234
STEP: Deleting a ConfigMap 03/09/23 16:30:17.238
STEP: Ensuring resource quota status released usage 03/09/23 16:30:17.243
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:30:19.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5552" for this suite. 03/09/23 16:30:19.252
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":140,"skipped":2804,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:29:51.19
    Mar  9 16:29:51.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:29:51.191
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:29:51.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:29:51.206
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 03/09/23 16:30:08.212
    STEP: Creating a ResourceQuota 03/09/23 16:30:13.215
    STEP: Ensuring resource quota status is calculated 03/09/23 16:30:13.22
    STEP: Creating a ConfigMap 03/09/23 16:30:15.223
    STEP: Ensuring resource quota status captures configMap creation 03/09/23 16:30:15.234
    STEP: Deleting a ConfigMap 03/09/23 16:30:17.238
    STEP: Ensuring resource quota status released usage 03/09/23 16:30:17.243
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:30:19.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5552" for this suite. 03/09/23 16:30:19.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:30:19.259
Mar  9 16:30:19.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 16:30:19.261
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:30:19.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:30:19.276
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7076 03/09/23 16:30:19.279
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 03/09/23 16:30:19.287
Mar  9 16:30:19.294: INFO: Found 0 stateful pods, waiting for 3
Mar  9 16:30:29.299: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 16:30:29.299: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 16:30:29.299: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 16:30:29.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 16:30:29.462: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 16:30:29.462: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 16:30:29.462: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/09/23 16:30:39.475
Mar  9 16:30:39.493: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/09/23 16:30:39.493
STEP: Updating Pods in reverse ordinal order 03/09/23 16:30:49.506
Mar  9 16:30:49.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 16:30:49.653: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  9 16:30:49.653: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 16:30:49.653: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 03/09/23 16:31:09.676
Mar  9 16:31:09.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 16:31:09.816: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 16:31:09.816: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 16:31:09.816: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 16:31:19.848: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/09/23 16:31:29.864
Mar  9 16:31:29.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 16:31:30.012: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  9 16:31:30.012: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 16:31:30.012: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 16:31:40.033: INFO: Deleting all statefulset in ns statefulset-7076
Mar  9 16:31:40.035: INFO: Scaling statefulset ss2 to 0
Mar  9 16:31:50.049: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 16:31:50.051: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 16:31:50.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7076" for this suite. 03/09/23 16:31:50.067
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":141,"skipped":2838,"failed":0}
------------------------------
â€¢ [SLOW TEST] [90.813 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:30:19.259
    Mar  9 16:30:19.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 16:30:19.261
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:30:19.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:30:19.276
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7076 03/09/23 16:30:19.279
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 03/09/23 16:30:19.287
    Mar  9 16:30:19.294: INFO: Found 0 stateful pods, waiting for 3
    Mar  9 16:30:29.299: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 16:30:29.299: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 16:30:29.299: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 16:30:29.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 16:30:29.462: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 16:30:29.462: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 16:30:29.462: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/09/23 16:30:39.475
    Mar  9 16:30:39.493: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/09/23 16:30:39.493
    STEP: Updating Pods in reverse ordinal order 03/09/23 16:30:49.506
    Mar  9 16:30:49.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 16:30:49.653: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  9 16:30:49.653: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 16:30:49.653: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 03/09/23 16:31:09.676
    Mar  9 16:31:09.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 16:31:09.816: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 16:31:09.816: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 16:31:09.816: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 16:31:19.848: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/09/23 16:31:29.864
    Mar  9 16:31:29.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-7076 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 16:31:30.012: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  9 16:31:30.012: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 16:31:30.012: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 16:31:40.033: INFO: Deleting all statefulset in ns statefulset-7076
    Mar  9 16:31:40.035: INFO: Scaling statefulset ss2 to 0
    Mar  9 16:31:50.049: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 16:31:50.051: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 16:31:50.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7076" for this suite. 03/09/23 16:31:50.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:31:50.074
Mar  9 16:31:50.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replicaset 03/09/23 16:31:50.075
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:31:50.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:31:50.091
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/09/23 16:31:50.094
Mar  9 16:31:50.100: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  9 16:31:55.105: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/09/23 16:31:55.105
STEP: getting scale subresource 03/09/23 16:31:55.106
STEP: updating a scale subresource 03/09/23 16:31:55.109
STEP: verifying the replicaset Spec.Replicas was modified 03/09/23 16:31:55.114
STEP: Patch a scale subresource 03/09/23 16:31:55.117
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  9 16:31:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5474" for this suite. 03/09/23 16:31:55.139
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":142,"skipped":2850,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.075 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:31:50.074
    Mar  9 16:31:50.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replicaset 03/09/23 16:31:50.075
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:31:50.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:31:50.091
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/09/23 16:31:50.094
    Mar  9 16:31:50.100: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  9 16:31:55.105: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/09/23 16:31:55.105
    STEP: getting scale subresource 03/09/23 16:31:55.106
    STEP: updating a scale subresource 03/09/23 16:31:55.109
    STEP: verifying the replicaset Spec.Replicas was modified 03/09/23 16:31:55.114
    STEP: Patch a scale subresource 03/09/23 16:31:55.117
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  9 16:31:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5474" for this suite. 03/09/23 16:31:55.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:31:55.15
Mar  9 16:31:55.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:31:55.151
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:31:55.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:31:55.17
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-542c8256-82f8-4aa5-a044-2bd51d56ce80 03/09/23 16:31:55.174
STEP: Creating a pod to test consume secrets 03/09/23 16:31:55.18
Mar  9 16:31:55.190: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45" in namespace "projected-6265" to be "Succeeded or Failed"
Mar  9 16:31:55.193: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855768ms
Mar  9 16:31:57.197: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006673665s
Mar  9 16:31:59.196: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006204535s
STEP: Saw pod success 03/09/23 16:31:59.196
Mar  9 16:31:59.196: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45" satisfied condition "Succeeded or Failed"
Mar  9 16:31:59.199: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:31:59.213
Mar  9 16:31:59.224: INFO: Waiting for pod pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45 to disappear
Mar  9 16:31:59.226: INFO: Pod pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  9 16:31:59.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6265" for this suite. 03/09/23 16:31:59.23
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":143,"skipped":2868,"failed":0}
------------------------------
â€¢ [4.084 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:31:55.15
    Mar  9 16:31:55.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:31:55.151
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:31:55.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:31:55.17
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-542c8256-82f8-4aa5-a044-2bd51d56ce80 03/09/23 16:31:55.174
    STEP: Creating a pod to test consume secrets 03/09/23 16:31:55.18
    Mar  9 16:31:55.190: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45" in namespace "projected-6265" to be "Succeeded or Failed"
    Mar  9 16:31:55.193: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855768ms
    Mar  9 16:31:57.197: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006673665s
    Mar  9 16:31:59.196: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006204535s
    STEP: Saw pod success 03/09/23 16:31:59.196
    Mar  9 16:31:59.196: INFO: Pod "pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45" satisfied condition "Succeeded or Failed"
    Mar  9 16:31:59.199: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:31:59.213
    Mar  9 16:31:59.224: INFO: Waiting for pod pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45 to disappear
    Mar  9 16:31:59.226: INFO: Pod pod-projected-secrets-eaad56b0-f3b6-40de-81b8-bf529f6aef45 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  9 16:31:59.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6265" for this suite. 03/09/23 16:31:59.23
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:31:59.234
Mar  9 16:31:59.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubelet-test 03/09/23 16:31:59.236
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:31:59.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:31:59.249
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar  9 16:31:59.259: INFO: Waiting up to 5m0s for pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe" in namespace "kubelet-test-7456" to be "running and ready"
Mar  9 16:31:59.261: INFO: Pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409596ms
Mar  9 16:31:59.261: INFO: The phase of Pod busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:32:01.266: INFO: Pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.007385003s
Mar  9 16:32:01.266: INFO: The phase of Pod busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe is Running (Ready = true)
Mar  9 16:32:01.266: INFO: Pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  9 16:32:01.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7456" for this suite. 03/09/23 16:32:01.277
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":144,"skipped":2869,"failed":0}
------------------------------
â€¢ [2.048 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:31:59.234
    Mar  9 16:31:59.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubelet-test 03/09/23 16:31:59.236
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:31:59.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:31:59.249
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar  9 16:31:59.259: INFO: Waiting up to 5m0s for pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe" in namespace "kubelet-test-7456" to be "running and ready"
    Mar  9 16:31:59.261: INFO: Pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409596ms
    Mar  9 16:31:59.261: INFO: The phase of Pod busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:32:01.266: INFO: Pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.007385003s
    Mar  9 16:32:01.266: INFO: The phase of Pod busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe is Running (Ready = true)
    Mar  9 16:32:01.266: INFO: Pod "busybox-scheduling-038fc33a-994a-4066-a8ce-3ef985bfe5fe" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  9 16:32:01.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7456" for this suite. 03/09/23 16:32:01.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:32:01.283
Mar  9 16:32:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:32:01.285
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:01.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:01.299
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-4a494e6a-6085-455f-9b43-8fb1f48a6b5d 03/09/23 16:32:01.302
STEP: Creating a pod to test consume secrets 03/09/23 16:32:01.305
Mar  9 16:32:01.312: INFO: Waiting up to 5m0s for pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42" in namespace "secrets-3545" to be "Succeeded or Failed"
Mar  9 16:32:01.314: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385598ms
Mar  9 16:32:03.317: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005583043s
Mar  9 16:32:05.318: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006501303s
STEP: Saw pod success 03/09/23 16:32:05.318
Mar  9 16:32:05.319: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42" satisfied condition "Succeeded or Failed"
Mar  9 16:32:05.321: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42 container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:32:05.327
Mar  9 16:32:05.336: INFO: Waiting for pod pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42 to disappear
Mar  9 16:32:05.338: INFO: Pod pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:32:05.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3545" for this suite. 03/09/23 16:32:05.342
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":145,"skipped":2876,"failed":0}
------------------------------
â€¢ [4.064 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:32:01.283
    Mar  9 16:32:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:32:01.285
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:01.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:01.299
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-4a494e6a-6085-455f-9b43-8fb1f48a6b5d 03/09/23 16:32:01.302
    STEP: Creating a pod to test consume secrets 03/09/23 16:32:01.305
    Mar  9 16:32:01.312: INFO: Waiting up to 5m0s for pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42" in namespace "secrets-3545" to be "Succeeded or Failed"
    Mar  9 16:32:01.314: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385598ms
    Mar  9 16:32:03.317: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005583043s
    Mar  9 16:32:05.318: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006501303s
    STEP: Saw pod success 03/09/23 16:32:05.318
    Mar  9 16:32:05.319: INFO: Pod "pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42" satisfied condition "Succeeded or Failed"
    Mar  9 16:32:05.321: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42 container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:32:05.327
    Mar  9 16:32:05.336: INFO: Waiting for pod pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42 to disappear
    Mar  9 16:32:05.338: INFO: Pod pod-secrets-cf277e2d-ad12-410f-8ebf-84f894c31c42 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:32:05.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3545" for this suite. 03/09/23 16:32:05.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:32:05.35
Mar  9 16:32:05.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:32:05.351
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:05.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:05.366
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-8012 03/09/23 16:32:05.368
Mar  9 16:32:05.378: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8012" to be "running and ready"
Mar  9 16:32:05.381: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.027712ms
Mar  9 16:32:05.381: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:32:07.386: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007622685s
Mar  9 16:32:07.386: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  9 16:32:07.386: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  9 16:32:07.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  9 16:32:07.554: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  9 16:32:07.554: INFO: stdout: "iptables"
Mar  9 16:32:07.554: INFO: proxyMode: iptables
Mar  9 16:32:07.565: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  9 16:32:07.568: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-8012 03/09/23 16:32:07.568
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8012 03/09/23 16:32:07.588
I0309 16:32:07.592993      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8012, replica count: 3
I0309 16:32:10.643845      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 16:32:10.649: INFO: Creating new exec pod
Mar  9 16:32:10.653: INFO: Waiting up to 5m0s for pod "execpod-affinityvstzq" in namespace "services-8012" to be "running"
Mar  9 16:32:10.655: INFO: Pod "execpod-affinityvstzq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428091ms
Mar  9 16:32:12.659: INFO: Pod "execpod-affinityvstzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.006271853s
Mar  9 16:32:12.659: INFO: Pod "execpod-affinityvstzq" satisfied condition "running"
Mar  9 16:32:13.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar  9 16:32:13.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  9 16:32:13.806: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:32:13.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.52.91 80'
Mar  9 16:32:13.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.52.91 80\nConnection to 10.106.52.91 80 port [tcp/http] succeeded!\n"
Mar  9 16:32:13.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:32:13.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.52.91:80/ ; done'
Mar  9 16:32:14.180: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n"
Mar  9 16:32:14.180: INFO: stdout: "\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv"
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
Mar  9 16:32:14.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.52.91:80/'
Mar  9 16:32:14.324: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n"
Mar  9 16:32:14.325: INFO: stdout: "affinity-clusterip-timeout-k6zrv"
Mar  9 16:32:34.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.52.91:80/'
Mar  9 16:32:34.476: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n"
Mar  9 16:32:34.476: INFO: stdout: "affinity-clusterip-timeout-9bcw5"
Mar  9 16:32:34.476: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8012, will wait for the garbage collector to delete the pods 03/09/23 16:32:34.486
Mar  9 16:32:34.545: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.28317ms
Mar  9 16:32:34.646: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.057369ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:32:37.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8012" for this suite. 03/09/23 16:32:37.071
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":146,"skipped":2924,"failed":0}
------------------------------
â€¢ [SLOW TEST] [31.726 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:32:05.35
    Mar  9 16:32:05.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:32:05.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:05.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:05.366
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-8012 03/09/23 16:32:05.368
    Mar  9 16:32:05.378: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8012" to be "running and ready"
    Mar  9 16:32:05.381: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.027712ms
    Mar  9 16:32:05.381: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:32:07.386: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007622685s
    Mar  9 16:32:07.386: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  9 16:32:07.386: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  9 16:32:07.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  9 16:32:07.554: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar  9 16:32:07.554: INFO: stdout: "iptables"
    Mar  9 16:32:07.554: INFO: proxyMode: iptables
    Mar  9 16:32:07.565: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  9 16:32:07.568: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-8012 03/09/23 16:32:07.568
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-8012 03/09/23 16:32:07.588
    I0309 16:32:07.592993      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8012, replica count: 3
    I0309 16:32:10.643845      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 16:32:10.649: INFO: Creating new exec pod
    Mar  9 16:32:10.653: INFO: Waiting up to 5m0s for pod "execpod-affinityvstzq" in namespace "services-8012" to be "running"
    Mar  9 16:32:10.655: INFO: Pod "execpod-affinityvstzq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428091ms
    Mar  9 16:32:12.659: INFO: Pod "execpod-affinityvstzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.006271853s
    Mar  9 16:32:12.659: INFO: Pod "execpod-affinityvstzq" satisfied condition "running"
    Mar  9 16:32:13.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Mar  9 16:32:13.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Mar  9 16:32:13.806: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:32:13.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.52.91 80'
    Mar  9 16:32:13.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.52.91 80\nConnection to 10.106.52.91 80 port [tcp/http] succeeded!\n"
    Mar  9 16:32:13.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:32:13.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.52.91:80/ ; done'
    Mar  9 16:32:14.180: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n"
    Mar  9 16:32:14.180: INFO: stdout: "\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv\naffinity-clusterip-timeout-k6zrv"
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Received response from host: affinity-clusterip-timeout-k6zrv
    Mar  9 16:32:14.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.52.91:80/'
    Mar  9 16:32:14.324: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n"
    Mar  9 16:32:14.325: INFO: stdout: "affinity-clusterip-timeout-k6zrv"
    Mar  9 16:32:34.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8012 exec execpod-affinityvstzq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.52.91:80/'
    Mar  9 16:32:34.476: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.52.91:80/\n"
    Mar  9 16:32:34.476: INFO: stdout: "affinity-clusterip-timeout-9bcw5"
    Mar  9 16:32:34.476: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8012, will wait for the garbage collector to delete the pods 03/09/23 16:32:34.486
    Mar  9 16:32:34.545: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.28317ms
    Mar  9 16:32:34.646: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.057369ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:32:37.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8012" for this suite. 03/09/23 16:32:37.071
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:32:37.078
Mar  9 16:32:37.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 16:32:37.08
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:37.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:37.095
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/09/23 16:32:37.098
Mar  9 16:32:37.106: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8254  a8850169-0161-43a6-b17d-1a3cd54a6ba5 102011 0 2023-03-09 16:32:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-09 16:32:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bj4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bj4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:32:37.107: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8254" to be "running and ready"
Mar  9 16:32:37.111: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.432796ms
Mar  9 16:32:37.111: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:32:39.116: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009261504s
Mar  9 16:32:39.116: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar  9 16:32:39.116: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/09/23 16:32:39.116
Mar  9 16:32:39.116: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8254 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:32:39.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:32:39.117: INFO: ExecWithOptions: Clientset creation
Mar  9 16:32:39.117: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8254/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/09/23 16:32:39.205
Mar  9 16:32:39.205: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8254 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:32:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:32:39.206: INFO: ExecWithOptions: Clientset creation
Mar  9 16:32:39.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8254/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  9 16:32:39.301: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 16:32:39.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8254" for this suite. 03/09/23 16:32:39.317
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":147,"skipped":2946,"failed":0}
------------------------------
â€¢ [2.243 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:32:37.078
    Mar  9 16:32:37.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 16:32:37.08
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:37.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:37.095
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/09/23 16:32:37.098
    Mar  9 16:32:37.106: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8254  a8850169-0161-43a6-b17d-1a3cd54a6ba5 102011 0 2023-03-09 16:32:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-09 16:32:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bj4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bj4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:32:37.107: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8254" to be "running and ready"
    Mar  9 16:32:37.111: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.432796ms
    Mar  9 16:32:37.111: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:32:39.116: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009261504s
    Mar  9 16:32:39.116: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar  9 16:32:39.116: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/09/23 16:32:39.116
    Mar  9 16:32:39.116: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8254 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:32:39.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:32:39.117: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:32:39.117: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8254/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/09/23 16:32:39.205
    Mar  9 16:32:39.205: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8254 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:32:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:32:39.206: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:32:39.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8254/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  9 16:32:39.301: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 16:32:39.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8254" for this suite. 03/09/23 16:32:39.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:32:39.323
Mar  9 16:32:39.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename cronjob 03/09/23 16:32:39.327
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:39.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:39.343
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/09/23 16:32:39.346
STEP: Ensuring a job is scheduled 03/09/23 16:32:39.351
STEP: Ensuring exactly one is scheduled 03/09/23 16:33:01.356
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/09/23 16:33:01.358
STEP: Ensuring no more jobs are scheduled 03/09/23 16:33:01.361
STEP: Removing cronjob 03/09/23 16:38:01.369
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  9 16:38:01.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9940" for this suite. 03/09/23 16:38:01.378
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":148,"skipped":2970,"failed":0}
------------------------------
â€¢ [SLOW TEST] [322.065 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:32:39.323
    Mar  9 16:32:39.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename cronjob 03/09/23 16:32:39.327
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:32:39.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:32:39.343
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/09/23 16:32:39.346
    STEP: Ensuring a job is scheduled 03/09/23 16:32:39.351
    STEP: Ensuring exactly one is scheduled 03/09/23 16:33:01.356
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/09/23 16:33:01.358
    STEP: Ensuring no more jobs are scheduled 03/09/23 16:33:01.361
    STEP: Removing cronjob 03/09/23 16:38:01.369
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  9 16:38:01.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9940" for this suite. 03/09/23 16:38:01.378
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:38:01.388
Mar  9 16:38:01.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:38:01.389
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:01.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:01.408
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-5e5379bd-7300-4edb-a535-fa9fd6234908 03/09/23 16:38:01.412
STEP: Creating a pod to test consume secrets 03/09/23 16:38:01.416
Mar  9 16:38:01.426: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c" in namespace "projected-176" to be "Succeeded or Failed"
Mar  9 16:38:01.428: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530001ms
Mar  9 16:38:03.433: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007202472s
Mar  9 16:38:05.432: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006396224s
STEP: Saw pod success 03/09/23 16:38:05.432
Mar  9 16:38:05.432: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c" satisfied condition "Succeeded or Failed"
Mar  9 16:38:05.435: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c container projected-secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:38:05.448
Mar  9 16:38:05.456: INFO: Waiting for pod pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c to disappear
Mar  9 16:38:05.459: INFO: Pod pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  9 16:38:05.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-176" for this suite. 03/09/23 16:38:05.462
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":149,"skipped":2973,"failed":0}
------------------------------
â€¢ [4.079 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:38:01.388
    Mar  9 16:38:01.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:38:01.389
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:01.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:01.408
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-5e5379bd-7300-4edb-a535-fa9fd6234908 03/09/23 16:38:01.412
    STEP: Creating a pod to test consume secrets 03/09/23 16:38:01.416
    Mar  9 16:38:01.426: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c" in namespace "projected-176" to be "Succeeded or Failed"
    Mar  9 16:38:01.428: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530001ms
    Mar  9 16:38:03.433: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007202472s
    Mar  9 16:38:05.432: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006396224s
    STEP: Saw pod success 03/09/23 16:38:05.432
    Mar  9 16:38:05.432: INFO: Pod "pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c" satisfied condition "Succeeded or Failed"
    Mar  9 16:38:05.435: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:38:05.448
    Mar  9 16:38:05.456: INFO: Waiting for pod pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c to disappear
    Mar  9 16:38:05.459: INFO: Pod pod-projected-secrets-d3c8bdf0-a67f-4bb5-8a86-649a7456a87c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  9 16:38:05.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-176" for this suite. 03/09/23 16:38:05.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:38:05.468
Mar  9 16:38:05.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename namespaces 03/09/23 16:38:05.469
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:05.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:05.483
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 03/09/23 16:38:05.486
STEP: patching the Namespace 03/09/23 16:38:05.495
STEP: get the Namespace and ensuring it has the label 03/09/23 16:38:05.5
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:38:05.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5526" for this suite. 03/09/23 16:38:05.507
STEP: Destroying namespace "nspatchtest-8ddd51d4-c655-41a7-9491-e924cc0a3086-4848" for this suite. 03/09/23 16:38:05.513
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":150,"skipped":2989,"failed":0}
------------------------------
â€¢ [0.049 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:38:05.468
    Mar  9 16:38:05.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename namespaces 03/09/23 16:38:05.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:05.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:05.483
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 03/09/23 16:38:05.486
    STEP: patching the Namespace 03/09/23 16:38:05.495
    STEP: get the Namespace and ensuring it has the label 03/09/23 16:38:05.5
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:38:05.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5526" for this suite. 03/09/23 16:38:05.507
    STEP: Destroying namespace "nspatchtest-8ddd51d4-c655-41a7-9491-e924cc0a3086-4848" for this suite. 03/09/23 16:38:05.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:38:05.518
Mar  9 16:38:05.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:38:05.519
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:05.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:05.532
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-4dfefb79-6361-47ce-bc7b-c415bbbca73b 03/09/23 16:38:05.534
STEP: Creating a pod to test consume secrets 03/09/23 16:38:05.538
Mar  9 16:38:05.546: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af" in namespace "projected-2749" to be "Succeeded or Failed"
Mar  9 16:38:05.548: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165091ms
Mar  9 16:38:07.551: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005332229s
Mar  9 16:38:09.553: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007105554s
STEP: Saw pod success 03/09/23 16:38:09.553
Mar  9 16:38:09.553: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af" satisfied condition "Succeeded or Failed"
Mar  9 16:38:09.556: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af container projected-secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:38:09.561
Mar  9 16:38:09.571: INFO: Waiting for pod pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af to disappear
Mar  9 16:38:09.573: INFO: Pod pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  9 16:38:09.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2749" for this suite. 03/09/23 16:38:09.577
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":151,"skipped":2996,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:38:05.518
    Mar  9 16:38:05.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:38:05.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:05.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:05.532
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-4dfefb79-6361-47ce-bc7b-c415bbbca73b 03/09/23 16:38:05.534
    STEP: Creating a pod to test consume secrets 03/09/23 16:38:05.538
    Mar  9 16:38:05.546: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af" in namespace "projected-2749" to be "Succeeded or Failed"
    Mar  9 16:38:05.548: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165091ms
    Mar  9 16:38:07.551: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005332229s
    Mar  9 16:38:09.553: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007105554s
    STEP: Saw pod success 03/09/23 16:38:09.553
    Mar  9 16:38:09.553: INFO: Pod "pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af" satisfied condition "Succeeded or Failed"
    Mar  9 16:38:09.556: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:38:09.561
    Mar  9 16:38:09.571: INFO: Waiting for pod pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af to disappear
    Mar  9 16:38:09.573: INFO: Pod pod-projected-secrets-a64c21e5-3e64-414c-8bda-c558aca7d1af no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  9 16:38:09.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2749" for this suite. 03/09/23 16:38:09.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:38:09.582
Mar  9 16:38:09.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename daemonsets 03/09/23 16:38:09.583
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:09.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:09.647
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 03/09/23 16:38:09.662
STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 16:38:09.667
Mar  9 16:38:09.671: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:09.673: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:38:09.673: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:38:10.678: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:10.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:38:10.684: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:38:11.678: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:11.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 16:38:11.681: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/09/23 16:38:11.684
Mar  9 16:38:11.695: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:11.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:38:11.698: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:38:12.702: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:12.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:38:12.705: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:38:13.703: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:13.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:38:13.706: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:38:14.707: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:14.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 16:38:14.711: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 16:38:15.702: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 16:38:15.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 16:38:15.705: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:38:15.708
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8345, will wait for the garbage collector to delete the pods 03/09/23 16:38:15.708
Mar  9 16:38:15.766: INFO: Deleting DaemonSet.extensions daemon-set took: 4.439182ms
Mar  9 16:38:15.868: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.232933ms
Mar  9 16:38:17.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 16:38:17.870: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  9 16:38:17.873: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"102929"},"items":null}

Mar  9 16:38:17.875: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"102929"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:38:17.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8345" for this suite. 03/09/23 16:38:17.887
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":152,"skipped":3004,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.309 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:38:09.582
    Mar  9 16:38:09.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename daemonsets 03/09/23 16:38:09.583
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:09.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:09.647
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 03/09/23 16:38:09.662
    STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 16:38:09.667
    Mar  9 16:38:09.671: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:09.673: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:38:09.673: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:38:10.678: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:10.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:38:10.684: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:38:11.678: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:11.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 16:38:11.681: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/09/23 16:38:11.684
    Mar  9 16:38:11.695: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:11.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:38:11.698: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:38:12.702: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:12.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:38:12.705: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:38:13.703: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:13.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:38:13.706: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:38:14.707: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:14.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 16:38:14.711: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 16:38:15.702: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 16:38:15.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 16:38:15.705: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/09/23 16:38:15.708
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8345, will wait for the garbage collector to delete the pods 03/09/23 16:38:15.708
    Mar  9 16:38:15.766: INFO: Deleting DaemonSet.extensions daemon-set took: 4.439182ms
    Mar  9 16:38:15.868: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.232933ms
    Mar  9 16:38:17.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 16:38:17.870: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  9 16:38:17.873: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"102929"},"items":null}

    Mar  9 16:38:17.875: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"102929"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:38:17.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8345" for this suite. 03/09/23 16:38:17.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:38:17.892
Mar  9 16:38:17.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename runtimeclass 03/09/23 16:38:17.893
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:17.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:17.908
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  9 16:38:17.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4988" for this suite. 03/09/23 16:38:17.919
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":153,"skipped":3016,"failed":0}
------------------------------
â€¢ [0.031 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:38:17.892
    Mar  9 16:38:17.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename runtimeclass 03/09/23 16:38:17.893
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:17.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:17.908
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  9 16:38:17.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4988" for this suite. 03/09/23 16:38:17.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:38:17.924
Mar  9 16:38:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:38:17.925
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:17.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:17.942
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 03/09/23 16:38:17.948
STEP: waiting for available Endpoint 03/09/23 16:38:17.951
STEP: listing all Endpoints 03/09/23 16:38:17.953
STEP: updating the Endpoint 03/09/23 16:38:17.956
STEP: fetching the Endpoint 03/09/23 16:38:17.961
STEP: patching the Endpoint 03/09/23 16:38:17.963
STEP: fetching the Endpoint 03/09/23 16:38:17.97
STEP: deleting the Endpoint by Collection 03/09/23 16:38:17.972
STEP: waiting for Endpoint deletion 03/09/23 16:38:17.977
STEP: fetching the Endpoint 03/09/23 16:38:17.979
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:38:17.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1957" for this suite. 03/09/23 16:38:17.984
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":154,"skipped":3027,"failed":0}
------------------------------
â€¢ [0.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:38:17.924
    Mar  9 16:38:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:38:17.925
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:17.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:17.942
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 03/09/23 16:38:17.948
    STEP: waiting for available Endpoint 03/09/23 16:38:17.951
    STEP: listing all Endpoints 03/09/23 16:38:17.953
    STEP: updating the Endpoint 03/09/23 16:38:17.956
    STEP: fetching the Endpoint 03/09/23 16:38:17.961
    STEP: patching the Endpoint 03/09/23 16:38:17.963
    STEP: fetching the Endpoint 03/09/23 16:38:17.97
    STEP: deleting the Endpoint by Collection 03/09/23 16:38:17.972
    STEP: waiting for Endpoint deletion 03/09/23 16:38:17.977
    STEP: fetching the Endpoint 03/09/23 16:38:17.979
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:38:17.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1957" for this suite. 03/09/23 16:38:17.984
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:38:17.994
Mar  9 16:38:17.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 16:38:17.995
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:18.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:18.011
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 03/09/23 16:38:18.014
Mar  9 16:38:18.021: INFO: Waiting up to 2m0s for pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" in namespace "var-expansion-3382" to be "running"
Mar  9 16:38:18.024: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.427622ms
Mar  9 16:38:20.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00583682s
Mar  9 16:38:22.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007293141s
Mar  9 16:38:24.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00669504s
Mar  9 16:38:26.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006383454s
Mar  9 16:38:28.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006102399s
Mar  9 16:38:30.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006608869s
Mar  9 16:38:32.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007712165s
Mar  9 16:38:34.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005454319s
Mar  9 16:38:36.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006054557s
Mar  9 16:38:38.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005882239s
Mar  9 16:38:40.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005945426s
Mar  9 16:38:42.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006379594s
Mar  9 16:38:44.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006349078s
Mar  9 16:38:46.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007227952s
Mar  9 16:38:48.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006439172s
Mar  9 16:38:50.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006278604s
Mar  9 16:38:52.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006482369s
Mar  9 16:38:54.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006888771s
Mar  9 16:38:56.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006055959s
Mar  9 16:38:58.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006043063s
Mar  9 16:39:00.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006502365s
Mar  9 16:39:02.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00610933s
Mar  9 16:39:04.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007637271s
Mar  9 16:39:06.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006539003s
Mar  9 16:39:08.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006497892s
Mar  9 16:39:10.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006814357s
Mar  9 16:39:12.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006953749s
Mar  9 16:39:14.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006159852s
Mar  9 16:39:16.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007100189s
Mar  9 16:39:18.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006647866s
Mar  9 16:39:20.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007301209s
Mar  9 16:39:22.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007636634s
Mar  9 16:39:24.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006536604s
Mar  9 16:39:26.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006489713s
Mar  9 16:39:28.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005838568s
Mar  9 16:39:30.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007467169s
Mar  9 16:39:32.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007676279s
Mar  9 16:39:34.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005943535s
Mar  9 16:39:36.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005491312s
Mar  9 16:39:38.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006234026s
Mar  9 16:39:40.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007499328s
Mar  9 16:39:42.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007800406s
Mar  9 16:39:44.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007443408s
Mar  9 16:39:46.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007030959s
Mar  9 16:39:48.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007048469s
Mar  9 16:39:50.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007182139s
Mar  9 16:39:52.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006447022s
Mar  9 16:39:54.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.00628912s
Mar  9 16:39:56.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006188636s
Mar  9 16:39:58.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006070638s
Mar  9 16:40:00.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00643184s
Mar  9 16:40:02.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006320324s
Mar  9 16:40:04.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005786228s
Mar  9 16:40:06.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006015015s
Mar  9 16:40:08.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006272554s
Mar  9 16:40:10.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006059851s
Mar  9 16:40:12.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00711944s
Mar  9 16:40:14.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006690421s
Mar  9 16:40:16.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006945824s
Mar  9 16:40:18.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006272447s
Mar  9 16:40:18.030: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009049481s
STEP: updating the pod 03/09/23 16:40:18.03
Mar  9 16:40:18.548: INFO: Successfully updated pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650"
STEP: waiting for pod running 03/09/23 16:40:18.548
Mar  9 16:40:18.548: INFO: Waiting up to 2m0s for pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" in namespace "var-expansion-3382" to be "running"
Mar  9 16:40:18.551: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 3.159481ms
Mar  9 16:40:20.555: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Running", Reason="", readiness=true. Elapsed: 2.00712956s
Mar  9 16:40:20.555: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" satisfied condition "running"
STEP: deleting the pod gracefully 03/09/23 16:40:20.555
Mar  9 16:40:20.556: INFO: Deleting pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" in namespace "var-expansion-3382"
Mar  9 16:40:20.562: INFO: Wait up to 5m0s for pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 16:40:52.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3382" for this suite. 03/09/23 16:40:52.573
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":155,"skipped":3085,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.587 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:38:17.994
    Mar  9 16:38:17.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 16:38:17.995
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:38:18.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:38:18.011
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 03/09/23 16:38:18.014
    Mar  9 16:38:18.021: INFO: Waiting up to 2m0s for pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" in namespace "var-expansion-3382" to be "running"
    Mar  9 16:38:18.024: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.427622ms
    Mar  9 16:38:20.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00583682s
    Mar  9 16:38:22.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007293141s
    Mar  9 16:38:24.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00669504s
    Mar  9 16:38:26.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006383454s
    Mar  9 16:38:28.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006102399s
    Mar  9 16:38:30.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006608869s
    Mar  9 16:38:32.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007712165s
    Mar  9 16:38:34.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005454319s
    Mar  9 16:38:36.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006054557s
    Mar  9 16:38:38.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005882239s
    Mar  9 16:38:40.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005945426s
    Mar  9 16:38:42.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006379594s
    Mar  9 16:38:44.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006349078s
    Mar  9 16:38:46.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007227952s
    Mar  9 16:38:48.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006439172s
    Mar  9 16:38:50.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006278604s
    Mar  9 16:38:52.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006482369s
    Mar  9 16:38:54.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006888771s
    Mar  9 16:38:56.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006055959s
    Mar  9 16:38:58.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006043063s
    Mar  9 16:39:00.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006502365s
    Mar  9 16:39:02.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00610933s
    Mar  9 16:39:04.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007637271s
    Mar  9 16:39:06.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006539003s
    Mar  9 16:39:08.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006497892s
    Mar  9 16:39:10.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006814357s
    Mar  9 16:39:12.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006953749s
    Mar  9 16:39:14.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006159852s
    Mar  9 16:39:16.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007100189s
    Mar  9 16:39:18.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006647866s
    Mar  9 16:39:20.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007301209s
    Mar  9 16:39:22.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007636634s
    Mar  9 16:39:24.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006536604s
    Mar  9 16:39:26.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006489713s
    Mar  9 16:39:28.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005838568s
    Mar  9 16:39:30.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007467169s
    Mar  9 16:39:32.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007676279s
    Mar  9 16:39:34.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005943535s
    Mar  9 16:39:36.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005491312s
    Mar  9 16:39:38.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006234026s
    Mar  9 16:39:40.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007499328s
    Mar  9 16:39:42.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007800406s
    Mar  9 16:39:44.029: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007443408s
    Mar  9 16:39:46.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007030959s
    Mar  9 16:39:48.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007048469s
    Mar  9 16:39:50.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007182139s
    Mar  9 16:39:52.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006447022s
    Mar  9 16:39:54.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.00628912s
    Mar  9 16:39:56.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006188636s
    Mar  9 16:39:58.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006070638s
    Mar  9 16:40:00.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00643184s
    Mar  9 16:40:02.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006320324s
    Mar  9 16:40:04.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005786228s
    Mar  9 16:40:06.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006015015s
    Mar  9 16:40:08.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006272554s
    Mar  9 16:40:10.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006059851s
    Mar  9 16:40:12.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00711944s
    Mar  9 16:40:14.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006690421s
    Mar  9 16:40:16.028: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006945824s
    Mar  9 16:40:18.027: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006272447s
    Mar  9 16:40:18.030: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009049481s
    STEP: updating the pod 03/09/23 16:40:18.03
    Mar  9 16:40:18.548: INFO: Successfully updated pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650"
    STEP: waiting for pod running 03/09/23 16:40:18.548
    Mar  9 16:40:18.548: INFO: Waiting up to 2m0s for pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" in namespace "var-expansion-3382" to be "running"
    Mar  9 16:40:18.551: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Pending", Reason="", readiness=false. Elapsed: 3.159481ms
    Mar  9 16:40:20.555: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650": Phase="Running", Reason="", readiness=true. Elapsed: 2.00712956s
    Mar  9 16:40:20.555: INFO: Pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" satisfied condition "running"
    STEP: deleting the pod gracefully 03/09/23 16:40:20.555
    Mar  9 16:40:20.556: INFO: Deleting pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" in namespace "var-expansion-3382"
    Mar  9 16:40:20.562: INFO: Wait up to 5m0s for pod "var-expansion-c9490cd1-72d4-47c5-8bbc-fb7670a84650" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 16:40:52.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3382" for this suite. 03/09/23 16:40:52.573
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:40:52.581
Mar  9 16:40:52.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename runtimeclass 03/09/23 16:40:52.584
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:52.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:52.6
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/09/23 16:40:52.604
STEP: getting /apis/node.k8s.io 03/09/23 16:40:52.607
STEP: getting /apis/node.k8s.io/v1 03/09/23 16:40:52.608
STEP: creating 03/09/23 16:40:52.61
STEP: watching 03/09/23 16:40:52.631
Mar  9 16:40:52.631: INFO: starting watch
STEP: getting 03/09/23 16:40:52.637
STEP: listing 03/09/23 16:40:52.64
STEP: patching 03/09/23 16:40:52.644
STEP: updating 03/09/23 16:40:52.651
Mar  9 16:40:52.657: INFO: waiting for watch events with expected annotations
STEP: deleting 03/09/23 16:40:52.657
STEP: deleting a collection 03/09/23 16:40:52.67
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  9 16:40:52.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4911" for this suite. 03/09/23 16:40:52.691
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":156,"skipped":3087,"failed":0}
------------------------------
â€¢ [0.116 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:40:52.581
    Mar  9 16:40:52.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename runtimeclass 03/09/23 16:40:52.584
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:52.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:52.6
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/09/23 16:40:52.604
    STEP: getting /apis/node.k8s.io 03/09/23 16:40:52.607
    STEP: getting /apis/node.k8s.io/v1 03/09/23 16:40:52.608
    STEP: creating 03/09/23 16:40:52.61
    STEP: watching 03/09/23 16:40:52.631
    Mar  9 16:40:52.631: INFO: starting watch
    STEP: getting 03/09/23 16:40:52.637
    STEP: listing 03/09/23 16:40:52.64
    STEP: patching 03/09/23 16:40:52.644
    STEP: updating 03/09/23 16:40:52.651
    Mar  9 16:40:52.657: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/09/23 16:40:52.657
    STEP: deleting a collection 03/09/23 16:40:52.67
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  9 16:40:52.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4911" for this suite. 03/09/23 16:40:52.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:40:52.705
Mar  9 16:40:52.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:40:52.706
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:52.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:52.726
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c8c12ce8-f845-43a5-a354-24ee34707943 03/09/23 16:40:52.733
STEP: Creating the pod 03/09/23 16:40:52.738
Mar  9 16:40:52.745: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85" in namespace "projected-7807" to be "running and ready"
Mar  9 16:40:52.748: INFO: Pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698449ms
Mar  9 16:40:52.748: INFO: The phase of Pod pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:40:54.752: INFO: Pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85": Phase="Running", Reason="", readiness=true. Elapsed: 2.006865726s
Mar  9 16:40:54.752: INFO: The phase of Pod pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85 is Running (Ready = true)
Mar  9 16:40:54.752: INFO: Pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-c8c12ce8-f845-43a5-a354-24ee34707943 03/09/23 16:40:54.759
STEP: waiting to observe update in volume 03/09/23 16:40:54.764
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 16:40:56.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7807" for this suite. 03/09/23 16:40:56.781
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":157,"skipped":3134,"failed":0}
------------------------------
â€¢ [4.083 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:40:52.705
    Mar  9 16:40:52.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:40:52.706
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:52.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:52.726
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-c8c12ce8-f845-43a5-a354-24ee34707943 03/09/23 16:40:52.733
    STEP: Creating the pod 03/09/23 16:40:52.738
    Mar  9 16:40:52.745: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85" in namespace "projected-7807" to be "running and ready"
    Mar  9 16:40:52.748: INFO: Pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698449ms
    Mar  9 16:40:52.748: INFO: The phase of Pod pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:40:54.752: INFO: Pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85": Phase="Running", Reason="", readiness=true. Elapsed: 2.006865726s
    Mar  9 16:40:54.752: INFO: The phase of Pod pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85 is Running (Ready = true)
    Mar  9 16:40:54.752: INFO: Pod "pod-projected-configmaps-e84081f2-67ce-4cf0-8cd5-844cd9a33b85" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-c8c12ce8-f845-43a5-a354-24ee34707943 03/09/23 16:40:54.759
    STEP: waiting to observe update in volume 03/09/23 16:40:54.764
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 16:40:56.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7807" for this suite. 03/09/23 16:40:56.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:40:56.789
Mar  9 16:40:56.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir-wrapper 03/09/23 16:40:56.791
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:56.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:56.805
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar  9 16:40:56.823: INFO: Waiting up to 5m0s for pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1" in namespace "emptydir-wrapper-3103" to be "running and ready"
Mar  9 16:40:56.826: INFO: Pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09811ms
Mar  9 16:40:56.826: INFO: The phase of Pod pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:40:58.831: INFO: Pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007327019s
Mar  9 16:40:58.831: INFO: The phase of Pod pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1 is Running (Ready = true)
Mar  9 16:40:58.831: INFO: Pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/09/23 16:40:58.833
STEP: Cleaning up the configmap 03/09/23 16:40:58.838
STEP: Cleaning up the pod 03/09/23 16:40:58.842
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  9 16:40:58.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3103" for this suite. 03/09/23 16:40:58.855
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":158,"skipped":3146,"failed":0}
------------------------------
â€¢ [2.075 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:40:56.789
    Mar  9 16:40:56.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir-wrapper 03/09/23 16:40:56.791
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:56.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:56.805
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar  9 16:40:56.823: INFO: Waiting up to 5m0s for pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1" in namespace "emptydir-wrapper-3103" to be "running and ready"
    Mar  9 16:40:56.826: INFO: Pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09811ms
    Mar  9 16:40:56.826: INFO: The phase of Pod pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:40:58.831: INFO: Pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007327019s
    Mar  9 16:40:58.831: INFO: The phase of Pod pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1 is Running (Ready = true)
    Mar  9 16:40:58.831: INFO: Pod "pod-secrets-eec58803-fc3b-4722-9905-3947f38309d1" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/09/23 16:40:58.833
    STEP: Cleaning up the configmap 03/09/23 16:40:58.838
    STEP: Cleaning up the pod 03/09/23 16:40:58.842
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:40:58.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3103" for this suite. 03/09/23 16:40:58.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:40:58.866
Mar  9 16:40:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:40:58.868
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:58.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:58.882
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 03/09/23 16:40:58.885
Mar  9 16:40:58.891: INFO: Waiting up to 5m0s for pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225" in namespace "emptydir-1828" to be "Succeeded or Failed"
Mar  9 16:40:58.893: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18073ms
Mar  9 16:41:00.898: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006813453s
Mar  9 16:41:02.897: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006249371s
STEP: Saw pod success 03/09/23 16:41:02.897
Mar  9 16:41:02.898: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225" satisfied condition "Succeeded or Failed"
Mar  9 16:41:02.900: INFO: Trying to get logs from node tt-test-el8-003 pod pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225 container test-container: <nil>
STEP: delete the pod 03/09/23 16:41:02.906
Mar  9 16:41:02.913: INFO: Waiting for pod pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225 to disappear
Mar  9 16:41:02.915: INFO: Pod pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:41:02.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1828" for this suite. 03/09/23 16:41:02.919
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":159,"skipped":3154,"failed":0}
------------------------------
â€¢ [4.058 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:40:58.866
    Mar  9 16:40:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:40:58.868
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:40:58.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:40:58.882
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/09/23 16:40:58.885
    Mar  9 16:40:58.891: INFO: Waiting up to 5m0s for pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225" in namespace "emptydir-1828" to be "Succeeded or Failed"
    Mar  9 16:40:58.893: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18073ms
    Mar  9 16:41:00.898: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006813453s
    Mar  9 16:41:02.897: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006249371s
    STEP: Saw pod success 03/09/23 16:41:02.897
    Mar  9 16:41:02.898: INFO: Pod "pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225" satisfied condition "Succeeded or Failed"
    Mar  9 16:41:02.900: INFO: Trying to get logs from node tt-test-el8-003 pod pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225 container test-container: <nil>
    STEP: delete the pod 03/09/23 16:41:02.906
    Mar  9 16:41:02.913: INFO: Waiting for pod pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225 to disappear
    Mar  9 16:41:02.915: INFO: Pod pod-a1e1e59c-65fe-4ea3-91d9-c72f8e5df225 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:41:02.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1828" for this suite. 03/09/23 16:41:02.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:02.925
Mar  9 16:41:02.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sysctl 03/09/23 16:41:02.926
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:02.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:02.94
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/09/23 16:41:02.943
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  9 16:41:02.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1193" for this suite. 03/09/23 16:41:02.95
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":160,"skipped":3161,"failed":0}
------------------------------
â€¢ [0.030 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:02.925
    Mar  9 16:41:02.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sysctl 03/09/23 16:41:02.926
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:02.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:02.94
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/09/23 16:41:02.943
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  9 16:41:02.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-1193" for this suite. 03/09/23 16:41:02.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:02.956
Mar  9 16:41:02.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:41:02.957
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:02.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:02.97
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:41:02.972
Mar  9 16:41:02.979: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080" in namespace "downward-api-6790" to be "Succeeded or Failed"
Mar  9 16:41:02.982: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.618347ms
Mar  9 16:41:04.986: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007013778s
Mar  9 16:41:06.986: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006347s
STEP: Saw pod success 03/09/23 16:41:06.986
Mar  9 16:41:06.986: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080" satisfied condition "Succeeded or Failed"
Mar  9 16:41:06.988: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080 container client-container: <nil>
STEP: delete the pod 03/09/23 16:41:06.994
Mar  9 16:41:07.003: INFO: Waiting for pod downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080 to disappear
Mar  9 16:41:07.005: INFO: Pod downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 16:41:07.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6790" for this suite. 03/09/23 16:41:07.009
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":161,"skipped":3173,"failed":0}
------------------------------
â€¢ [4.058 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:02.956
    Mar  9 16:41:02.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:41:02.957
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:02.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:02.97
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:41:02.972
    Mar  9 16:41:02.979: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080" in namespace "downward-api-6790" to be "Succeeded or Failed"
    Mar  9 16:41:02.982: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.618347ms
    Mar  9 16:41:04.986: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007013778s
    Mar  9 16:41:06.986: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006347s
    STEP: Saw pod success 03/09/23 16:41:06.986
    Mar  9 16:41:06.986: INFO: Pod "downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080" satisfied condition "Succeeded or Failed"
    Mar  9 16:41:06.988: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080 container client-container: <nil>
    STEP: delete the pod 03/09/23 16:41:06.994
    Mar  9 16:41:07.003: INFO: Waiting for pod downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080 to disappear
    Mar  9 16:41:07.005: INFO: Pod downwardapi-volume-0e689965-8d32-4022-93a1-f33e89e86080 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 16:41:07.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6790" for this suite. 03/09/23 16:41:07.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:07.017
Mar  9 16:41:07.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename watch 03/09/23 16:41:07.018
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:07.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:07.03
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/09/23 16:41:07.033
STEP: modifying the configmap once 03/09/23 16:41:07.037
STEP: modifying the configmap a second time 03/09/23 16:41:07.043
STEP: deleting the configmap 03/09/23 16:41:07.049
STEP: creating a watch on configmaps from the resource version returned by the first update 03/09/23 16:41:07.053
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/09/23 16:41:07.055
Mar  9 16:41:07.055: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1468  9e153e0b-1fc8-4546-af66-d6b309a84bb8 103482 0 2023-03-09 16:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-09 16:41:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 16:41:07.055: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1468  9e153e0b-1fc8-4546-af66-d6b309a84bb8 103483 0 2023-03-09 16:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-09 16:41:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  9 16:41:07.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1468" for this suite. 03/09/23 16:41:07.059
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":162,"skipped":3231,"failed":0}
------------------------------
â€¢ [0.046 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:07.017
    Mar  9 16:41:07.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename watch 03/09/23 16:41:07.018
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:07.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:07.03
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/09/23 16:41:07.033
    STEP: modifying the configmap once 03/09/23 16:41:07.037
    STEP: modifying the configmap a second time 03/09/23 16:41:07.043
    STEP: deleting the configmap 03/09/23 16:41:07.049
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/09/23 16:41:07.053
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/09/23 16:41:07.055
    Mar  9 16:41:07.055: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1468  9e153e0b-1fc8-4546-af66-d6b309a84bb8 103482 0 2023-03-09 16:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-09 16:41:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 16:41:07.055: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1468  9e153e0b-1fc8-4546-af66-d6b309a84bb8 103483 0 2023-03-09 16:41:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-09 16:41:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  9 16:41:07.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1468" for this suite. 03/09/23 16:41:07.059
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:07.063
Mar  9 16:41:07.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-runtime 03/09/23 16:41:07.065
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:07.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:07.079
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 03/09/23 16:41:07.082
STEP: wait for the container to reach Succeeded 03/09/23 16:41:07.088
STEP: get the container status 03/09/23 16:41:11.106
STEP: the container should be terminated 03/09/23 16:41:11.109
STEP: the termination message should be set 03/09/23 16:41:11.109
Mar  9 16:41:11.109: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/09/23 16:41:11.109
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  9 16:41:11.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8423" for this suite. 03/09/23 16:41:11.124
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":163,"skipped":3235,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:07.063
    Mar  9 16:41:07.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-runtime 03/09/23 16:41:07.065
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:07.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:07.079
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 03/09/23 16:41:07.082
    STEP: wait for the container to reach Succeeded 03/09/23 16:41:07.088
    STEP: get the container status 03/09/23 16:41:11.106
    STEP: the container should be terminated 03/09/23 16:41:11.109
    STEP: the termination message should be set 03/09/23 16:41:11.109
    Mar  9 16:41:11.109: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/09/23 16:41:11.109
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  9 16:41:11.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8423" for this suite. 03/09/23 16:41:11.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:11.131
Mar  9 16:41:11.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename lease-test 03/09/23 16:41:11.132
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:11.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:11.148
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Mar  9 16:41:11.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8319" for this suite. 03/09/23 16:41:11.196
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":164,"skipped":3266,"failed":0}
------------------------------
â€¢ [0.070 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:11.131
    Mar  9 16:41:11.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename lease-test 03/09/23 16:41:11.132
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:11.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:11.148
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Mar  9 16:41:11.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-8319" for this suite. 03/09/23 16:41:11.196
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:11.201
Mar  9 16:41:11.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:41:11.202
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:11.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:11.216
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 03/09/23 16:41:11.219
Mar  9 16:41:11.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 create -f -'
Mar  9 16:41:11.469: INFO: stderr: ""
Mar  9 16:41:11.469: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:41:11.469
Mar  9 16:41:11.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:41:11.553: INFO: stderr: ""
Mar  9 16:41:11.553: INFO: stdout: "update-demo-nautilus-4lg8m update-demo-nautilus-stbw4 "
Mar  9 16:41:11.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-4lg8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:41:11.621: INFO: stderr: ""
Mar  9 16:41:11.621: INFO: stdout: ""
Mar  9 16:41:11.621: INFO: update-demo-nautilus-4lg8m is created but not running
Mar  9 16:41:16.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  9 16:41:16.694: INFO: stderr: ""
Mar  9 16:41:16.694: INFO: stdout: "update-demo-nautilus-4lg8m update-demo-nautilus-stbw4 "
Mar  9 16:41:16.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-4lg8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:41:16.762: INFO: stderr: ""
Mar  9 16:41:16.762: INFO: stdout: "true"
Mar  9 16:41:16.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-4lg8m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  9 16:41:16.829: INFO: stderr: ""
Mar  9 16:41:16.829: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  9 16:41:16.829: INFO: validating pod update-demo-nautilus-4lg8m
Mar  9 16:41:16.833: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  9 16:41:16.833: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  9 16:41:16.833: INFO: update-demo-nautilus-4lg8m is verified up and running
Mar  9 16:41:16.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-stbw4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  9 16:41:16.902: INFO: stderr: ""
Mar  9 16:41:16.902: INFO: stdout: "true"
Mar  9 16:41:16.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-stbw4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  9 16:41:16.970: INFO: stderr: ""
Mar  9 16:41:16.970: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  9 16:41:16.970: INFO: validating pod update-demo-nautilus-stbw4
Mar  9 16:41:16.975: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  9 16:41:16.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  9 16:41:16.975: INFO: update-demo-nautilus-stbw4 is verified up and running
STEP: using delete to clean up resources 03/09/23 16:41:16.975
Mar  9 16:41:16.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 delete --grace-period=0 --force -f -'
Mar  9 16:41:17.045: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:41:17.045: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  9 16:41:17.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get rc,svc -l name=update-demo --no-headers'
Mar  9 16:41:17.130: INFO: stderr: "No resources found in kubectl-5257 namespace.\n"
Mar  9 16:41:17.130: INFO: stdout: ""
Mar  9 16:41:17.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  9 16:41:17.207: INFO: stderr: ""
Mar  9 16:41:17.207: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:41:17.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5257" for this suite. 03/09/23 16:41:17.212
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":165,"skipped":3270,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.015 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:11.201
    Mar  9 16:41:11.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:41:11.202
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:11.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:11.216
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 03/09/23 16:41:11.219
    Mar  9 16:41:11.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 create -f -'
    Mar  9 16:41:11.469: INFO: stderr: ""
    Mar  9 16:41:11.469: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/09/23 16:41:11.469
    Mar  9 16:41:11.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:41:11.553: INFO: stderr: ""
    Mar  9 16:41:11.553: INFO: stdout: "update-demo-nautilus-4lg8m update-demo-nautilus-stbw4 "
    Mar  9 16:41:11.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-4lg8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:41:11.621: INFO: stderr: ""
    Mar  9 16:41:11.621: INFO: stdout: ""
    Mar  9 16:41:11.621: INFO: update-demo-nautilus-4lg8m is created but not running
    Mar  9 16:41:16.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  9 16:41:16.694: INFO: stderr: ""
    Mar  9 16:41:16.694: INFO: stdout: "update-demo-nautilus-4lg8m update-demo-nautilus-stbw4 "
    Mar  9 16:41:16.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-4lg8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:41:16.762: INFO: stderr: ""
    Mar  9 16:41:16.762: INFO: stdout: "true"
    Mar  9 16:41:16.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-4lg8m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  9 16:41:16.829: INFO: stderr: ""
    Mar  9 16:41:16.829: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  9 16:41:16.829: INFO: validating pod update-demo-nautilus-4lg8m
    Mar  9 16:41:16.833: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  9 16:41:16.833: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  9 16:41:16.833: INFO: update-demo-nautilus-4lg8m is verified up and running
    Mar  9 16:41:16.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-stbw4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  9 16:41:16.902: INFO: stderr: ""
    Mar  9 16:41:16.902: INFO: stdout: "true"
    Mar  9 16:41:16.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods update-demo-nautilus-stbw4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  9 16:41:16.970: INFO: stderr: ""
    Mar  9 16:41:16.970: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  9 16:41:16.970: INFO: validating pod update-demo-nautilus-stbw4
    Mar  9 16:41:16.975: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  9 16:41:16.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  9 16:41:16.975: INFO: update-demo-nautilus-stbw4 is verified up and running
    STEP: using delete to clean up resources 03/09/23 16:41:16.975
    Mar  9 16:41:16.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 delete --grace-period=0 --force -f -'
    Mar  9 16:41:17.045: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:41:17.045: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  9 16:41:17.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get rc,svc -l name=update-demo --no-headers'
    Mar  9 16:41:17.130: INFO: stderr: "No resources found in kubectl-5257 namespace.\n"
    Mar  9 16:41:17.130: INFO: stdout: ""
    Mar  9 16:41:17.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5257 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  9 16:41:17.207: INFO: stderr: ""
    Mar  9 16:41:17.207: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:41:17.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5257" for this suite. 03/09/23 16:41:17.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:17.217
Mar  9 16:41:17.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename svc-latency 03/09/23 16:41:17.219
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:17.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:17.237
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar  9 16:41:17.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5686 03/09/23 16:41:17.241
I0309 16:41:17.247193      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5686, replica count: 1
I0309 16:41:18.298921      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0309 16:41:19.299739      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 16:41:19.421: INFO: Created: latency-svc-vzw7q
Mar  9 16:41:19.426: INFO: Got endpoints: latency-svc-vzw7q [26.253032ms]
Mar  9 16:41:19.450: INFO: Created: latency-svc-5mqff
Mar  9 16:41:19.454: INFO: Got endpoints: latency-svc-5mqff [27.640179ms]
Mar  9 16:41:19.460: INFO: Created: latency-svc-d2dtc
Mar  9 16:41:19.462: INFO: Got endpoints: latency-svc-d2dtc [34.837742ms]
Mar  9 16:41:19.472: INFO: Created: latency-svc-hl9jc
Mar  9 16:41:19.477: INFO: Got endpoints: latency-svc-hl9jc [49.608156ms]
Mar  9 16:41:19.488: INFO: Created: latency-svc-d92qd
Mar  9 16:41:19.493: INFO: Got endpoints: latency-svc-d92qd [66.309697ms]
Mar  9 16:41:19.499: INFO: Created: latency-svc-txpzg
Mar  9 16:41:19.508: INFO: Got endpoints: latency-svc-txpzg [81.000349ms]
Mar  9 16:41:19.515: INFO: Created: latency-svc-z5tlc
Mar  9 16:41:19.521: INFO: Got endpoints: latency-svc-z5tlc [93.680157ms]
Mar  9 16:41:19.530: INFO: Created: latency-svc-gbhgp
Mar  9 16:41:19.536: INFO: Got endpoints: latency-svc-gbhgp [109.689836ms]
Mar  9 16:41:19.542: INFO: Created: latency-svc-kxpqb
Mar  9 16:41:19.548: INFO: Got endpoints: latency-svc-kxpqb [120.056988ms]
Mar  9 16:41:19.552: INFO: Created: latency-svc-ggkql
Mar  9 16:41:19.560: INFO: Got endpoints: latency-svc-ggkql [132.016316ms]
Mar  9 16:41:19.566: INFO: Created: latency-svc-zk7ql
Mar  9 16:41:19.573: INFO: Got endpoints: latency-svc-zk7ql [145.06548ms]
Mar  9 16:41:19.576: INFO: Created: latency-svc-sd4nj
Mar  9 16:41:19.583: INFO: Got endpoints: latency-svc-sd4nj [154.680045ms]
Mar  9 16:41:19.587: INFO: Created: latency-svc-cd8xk
Mar  9 16:41:19.595: INFO: Got endpoints: latency-svc-cd8xk [166.257238ms]
Mar  9 16:41:19.599: INFO: Created: latency-svc-vs5wg
Mar  9 16:41:19.607: INFO: Got endpoints: latency-svc-vs5wg [178.978519ms]
Mar  9 16:41:19.624: INFO: Created: latency-svc-m7fcl
Mar  9 16:41:19.631: INFO: Created: latency-svc-6fs8d
Mar  9 16:41:19.633: INFO: Got endpoints: latency-svc-m7fcl [205.546211ms]
Mar  9 16:41:19.638: INFO: Got endpoints: latency-svc-6fs8d [211.079703ms]
Mar  9 16:41:19.647: INFO: Created: latency-svc-qvwd2
Mar  9 16:41:19.652: INFO: Got endpoints: latency-svc-qvwd2 [197.869926ms]
Mar  9 16:41:19.658: INFO: Created: latency-svc-cd87f
Mar  9 16:41:19.664: INFO: Got endpoints: latency-svc-cd87f [201.876447ms]
Mar  9 16:41:19.666: INFO: Created: latency-svc-m9vrj
Mar  9 16:41:19.672: INFO: Got endpoints: latency-svc-m9vrj [195.773068ms]
Mar  9 16:41:19.679: INFO: Created: latency-svc-kzm8m
Mar  9 16:41:19.684: INFO: Got endpoints: latency-svc-kzm8m [190.883859ms]
Mar  9 16:41:19.689: INFO: Created: latency-svc-lzstb
Mar  9 16:41:19.693: INFO: Got endpoints: latency-svc-lzstb [185.24278ms]
Mar  9 16:41:19.700: INFO: Created: latency-svc-d9hs4
Mar  9 16:41:19.708: INFO: Got endpoints: latency-svc-d9hs4 [186.106974ms]
Mar  9 16:41:19.710: INFO: Created: latency-svc-hmzcc
Mar  9 16:41:19.717: INFO: Got endpoints: latency-svc-hmzcc [180.965664ms]
Mar  9 16:41:19.723: INFO: Created: latency-svc-shkdc
Mar  9 16:41:19.737: INFO: Got endpoints: latency-svc-shkdc [188.844177ms]
Mar  9 16:41:19.743: INFO: Created: latency-svc-wj2zl
Mar  9 16:41:19.748: INFO: Got endpoints: latency-svc-wj2zl [187.448124ms]
Mar  9 16:41:19.752: INFO: Created: latency-svc-5hsx2
Mar  9 16:41:19.759: INFO: Got endpoints: latency-svc-5hsx2 [185.861785ms]
Mar  9 16:41:19.763: INFO: Created: latency-svc-lckkp
Mar  9 16:41:19.771: INFO: Got endpoints: latency-svc-lckkp [188.231382ms]
Mar  9 16:41:19.774: INFO: Created: latency-svc-5v8t9
Mar  9 16:41:19.780: INFO: Got endpoints: latency-svc-5v8t9 [185.233521ms]
Mar  9 16:41:19.789: INFO: Created: latency-svc-6zt94
Mar  9 16:41:19.794: INFO: Got endpoints: latency-svc-6zt94 [186.565022ms]
Mar  9 16:41:19.799: INFO: Created: latency-svc-gq2hq
Mar  9 16:41:19.803: INFO: Got endpoints: latency-svc-gq2hq [170.767291ms]
Mar  9 16:41:19.811: INFO: Created: latency-svc-t8xvj
Mar  9 16:41:19.818: INFO: Got endpoints: latency-svc-t8xvj [179.709005ms]
Mar  9 16:41:19.821: INFO: Created: latency-svc-5ff56
Mar  9 16:41:19.828: INFO: Got endpoints: latency-svc-5ff56 [175.729517ms]
Mar  9 16:41:19.833: INFO: Created: latency-svc-vw9wp
Mar  9 16:41:19.847: INFO: Got endpoints: latency-svc-vw9wp [183.651573ms]
Mar  9 16:41:19.856: INFO: Created: latency-svc-75nh8
Mar  9 16:41:19.867: INFO: Got endpoints: latency-svc-75nh8 [194.076664ms]
Mar  9 16:41:19.872: INFO: Created: latency-svc-5jbqv
Mar  9 16:41:19.889: INFO: Got endpoints: latency-svc-5jbqv [205.116034ms]
Mar  9 16:41:19.900: INFO: Created: latency-svc-tr82b
Mar  9 16:41:19.903: INFO: Got endpoints: latency-svc-tr82b [210.015785ms]
Mar  9 16:41:19.915: INFO: Created: latency-svc-cftx9
Mar  9 16:41:19.928: INFO: Got endpoints: latency-svc-cftx9 [220.870355ms]
Mar  9 16:41:19.944: INFO: Created: latency-svc-mctgr
Mar  9 16:41:19.963: INFO: Got endpoints: latency-svc-mctgr [245.943819ms]
Mar  9 16:41:19.971: INFO: Created: latency-svc-8vj45
Mar  9 16:41:19.986: INFO: Got endpoints: latency-svc-8vj45 [249.590196ms]
Mar  9 16:41:19.996: INFO: Created: latency-svc-hplwk
Mar  9 16:41:20.004: INFO: Got endpoints: latency-svc-hplwk [256.098657ms]
Mar  9 16:41:20.006: INFO: Created: latency-svc-hr9xz
Mar  9 16:41:20.016: INFO: Got endpoints: latency-svc-hr9xz [256.492452ms]
Mar  9 16:41:20.018: INFO: Created: latency-svc-qqsw8
Mar  9 16:41:20.029: INFO: Got endpoints: latency-svc-qqsw8 [257.651484ms]
Mar  9 16:41:20.040: INFO: Created: latency-svc-q2l2g
Mar  9 16:41:20.050: INFO: Created: latency-svc-hwd6x
Mar  9 16:41:20.065: INFO: Created: latency-svc-9fjzv
Mar  9 16:41:20.076: INFO: Created: latency-svc-gg6zs
Mar  9 16:41:20.079: INFO: Got endpoints: latency-svc-q2l2g [298.767081ms]
Mar  9 16:41:20.087: INFO: Created: latency-svc-ttjns
Mar  9 16:41:20.093: INFO: Created: latency-svc-7l9w7
Mar  9 16:41:20.101: INFO: Created: latency-svc-9wqhz
Mar  9 16:41:20.109: INFO: Created: latency-svc-wktzw
Mar  9 16:41:20.120: INFO: Created: latency-svc-4fspt
Mar  9 16:41:20.128: INFO: Got endpoints: latency-svc-hwd6x [333.215965ms]
Mar  9 16:41:20.134: INFO: Created: latency-svc-zrlfd
Mar  9 16:41:20.147: INFO: Created: latency-svc-gvw69
Mar  9 16:41:20.153: INFO: Created: latency-svc-z9lml
Mar  9 16:41:20.177: INFO: Got endpoints: latency-svc-9fjzv [373.569698ms]
Mar  9 16:41:20.182: INFO: Created: latency-svc-4wkfc
Mar  9 16:41:20.191: INFO: Created: latency-svc-vqkfd
Mar  9 16:41:20.199: INFO: Created: latency-svc-qzggl
Mar  9 16:41:20.208: INFO: Created: latency-svc-vvsw4
Mar  9 16:41:20.219: INFO: Created: latency-svc-nd2zx
Mar  9 16:41:20.230: INFO: Created: latency-svc-zqfxw
Mar  9 16:41:20.231: INFO: Got endpoints: latency-svc-gg6zs [412.491876ms]
Mar  9 16:41:20.246: INFO: Created: latency-svc-8cch4
Mar  9 16:41:20.285: INFO: Got endpoints: latency-svc-ttjns [456.264704ms]
Mar  9 16:41:20.299: INFO: Created: latency-svc-vpbw9
Mar  9 16:41:20.327: INFO: Got endpoints: latency-svc-7l9w7 [479.765628ms]
Mar  9 16:41:20.344: INFO: Created: latency-svc-rjxvl
Mar  9 16:41:20.378: INFO: Got endpoints: latency-svc-9wqhz [511.170193ms]
Mar  9 16:41:20.395: INFO: Created: latency-svc-bbkzc
Mar  9 16:41:20.427: INFO: Got endpoints: latency-svc-wktzw [537.841294ms]
Mar  9 16:41:20.443: INFO: Created: latency-svc-nvrn9
Mar  9 16:41:20.479: INFO: Got endpoints: latency-svc-4fspt [575.242417ms]
Mar  9 16:41:20.496: INFO: Created: latency-svc-dvpx5
Mar  9 16:41:20.527: INFO: Got endpoints: latency-svc-zrlfd [598.085902ms]
Mar  9 16:41:20.544: INFO: Created: latency-svc-m87j5
Mar  9 16:41:20.577: INFO: Got endpoints: latency-svc-gvw69 [613.449371ms]
Mar  9 16:41:20.593: INFO: Created: latency-svc-fclh4
Mar  9 16:41:20.627: INFO: Got endpoints: latency-svc-z9lml [640.720057ms]
Mar  9 16:41:20.644: INFO: Created: latency-svc-x5h88
Mar  9 16:41:20.679: INFO: Got endpoints: latency-svc-4wkfc [675.277373ms]
Mar  9 16:41:20.701: INFO: Created: latency-svc-j7596
Mar  9 16:41:20.729: INFO: Got endpoints: latency-svc-vqkfd [713.189252ms]
Mar  9 16:41:20.744: INFO: Created: latency-svc-j8wm5
Mar  9 16:41:20.776: INFO: Got endpoints: latency-svc-qzggl [747.394901ms]
Mar  9 16:41:20.790: INFO: Created: latency-svc-8q8f5
Mar  9 16:41:20.827: INFO: Got endpoints: latency-svc-vvsw4 [748.42029ms]
Mar  9 16:41:20.842: INFO: Created: latency-svc-k29r9
Mar  9 16:41:20.884: INFO: Got endpoints: latency-svc-nd2zx [756.580264ms]
Mar  9 16:41:20.910: INFO: Created: latency-svc-jh7b2
Mar  9 16:41:20.928: INFO: Got endpoints: latency-svc-zqfxw [750.703235ms]
Mar  9 16:41:20.948: INFO: Created: latency-svc-zhg25
Mar  9 16:41:20.976: INFO: Got endpoints: latency-svc-8cch4 [745.72039ms]
Mar  9 16:41:20.992: INFO: Created: latency-svc-lcgmv
Mar  9 16:41:21.028: INFO: Got endpoints: latency-svc-vpbw9 [743.362361ms]
Mar  9 16:41:21.043: INFO: Created: latency-svc-5kmks
Mar  9 16:41:21.077: INFO: Got endpoints: latency-svc-rjxvl [749.734565ms]
Mar  9 16:41:21.092: INFO: Created: latency-svc-cs2cd
Mar  9 16:41:21.127: INFO: Got endpoints: latency-svc-bbkzc [749.098489ms]
Mar  9 16:41:21.143: INFO: Created: latency-svc-4h5cj
Mar  9 16:41:21.177: INFO: Got endpoints: latency-svc-nvrn9 [749.120819ms]
Mar  9 16:41:21.192: INFO: Created: latency-svc-qgn96
Mar  9 16:41:21.232: INFO: Got endpoints: latency-svc-dvpx5 [752.693019ms]
Mar  9 16:41:21.246: INFO: Created: latency-svc-8gfd8
Mar  9 16:41:21.278: INFO: Got endpoints: latency-svc-m87j5 [751.233502ms]
Mar  9 16:41:21.294: INFO: Created: latency-svc-fkw24
Mar  9 16:41:21.326: INFO: Got endpoints: latency-svc-fclh4 [749.467733ms]
Mar  9 16:41:21.341: INFO: Created: latency-svc-77rb5
Mar  9 16:41:21.378: INFO: Got endpoints: latency-svc-x5h88 [750.394379ms]
Mar  9 16:41:21.393: INFO: Created: latency-svc-zv8sq
Mar  9 16:41:21.426: INFO: Got endpoints: latency-svc-j7596 [747.248144ms]
Mar  9 16:41:21.444: INFO: Created: latency-svc-b76p9
Mar  9 16:41:21.477: INFO: Got endpoints: latency-svc-j8wm5 [748.051696ms]
Mar  9 16:41:21.494: INFO: Created: latency-svc-q7zjq
Mar  9 16:41:21.527: INFO: Got endpoints: latency-svc-8q8f5 [750.548154ms]
Mar  9 16:41:21.544: INFO: Created: latency-svc-8wv9x
Mar  9 16:41:21.577: INFO: Got endpoints: latency-svc-k29r9 [750.010441ms]
Mar  9 16:41:21.596: INFO: Created: latency-svc-gqqgb
Mar  9 16:41:21.628: INFO: Got endpoints: latency-svc-jh7b2 [743.675176ms]
Mar  9 16:41:21.642: INFO: Created: latency-svc-9jwbr
Mar  9 16:41:21.678: INFO: Got endpoints: latency-svc-zhg25 [749.410154ms]
Mar  9 16:41:21.695: INFO: Created: latency-svc-w2mvl
Mar  9 16:41:21.728: INFO: Got endpoints: latency-svc-lcgmv [751.635346ms]
Mar  9 16:41:21.746: INFO: Created: latency-svc-s7r54
Mar  9 16:41:21.779: INFO: Got endpoints: latency-svc-5kmks [751.116857ms]
Mar  9 16:41:21.798: INFO: Created: latency-svc-fls9q
Mar  9 16:41:21.827: INFO: Got endpoints: latency-svc-cs2cd [750.214544ms]
Mar  9 16:41:21.846: INFO: Created: latency-svc-4j486
Mar  9 16:41:21.879: INFO: Got endpoints: latency-svc-4h5cj [751.516965ms]
Mar  9 16:41:21.893: INFO: Created: latency-svc-c5mwq
Mar  9 16:41:21.926: INFO: Got endpoints: latency-svc-qgn96 [749.711675ms]
Mar  9 16:41:21.947: INFO: Created: latency-svc-9mqjz
Mar  9 16:41:21.979: INFO: Got endpoints: latency-svc-8gfd8 [747.380642ms]
Mar  9 16:41:21.993: INFO: Created: latency-svc-tbhpm
Mar  9 16:41:22.029: INFO: Got endpoints: latency-svc-fkw24 [751.104587ms]
Mar  9 16:41:22.042: INFO: Created: latency-svc-9x8zf
Mar  9 16:41:22.077: INFO: Got endpoints: latency-svc-77rb5 [750.253713ms]
Mar  9 16:41:22.093: INFO: Created: latency-svc-wv8m7
Mar  9 16:41:22.127: INFO: Got endpoints: latency-svc-zv8sq [748.843682ms]
Mar  9 16:41:22.141: INFO: Created: latency-svc-sxzp2
Mar  9 16:41:22.177: INFO: Got endpoints: latency-svc-b76p9 [750.249808ms]
Mar  9 16:41:22.195: INFO: Created: latency-svc-5k699
Mar  9 16:41:22.234: INFO: Got endpoints: latency-svc-q7zjq [756.804058ms]
Mar  9 16:41:22.250: INFO: Created: latency-svc-m8xcl
Mar  9 16:41:22.275: INFO: Got endpoints: latency-svc-8wv9x [748.011978ms]
Mar  9 16:41:22.293: INFO: Created: latency-svc-qtpvn
Mar  9 16:41:22.328: INFO: Got endpoints: latency-svc-gqqgb [750.94679ms]
Mar  9 16:41:22.345: INFO: Created: latency-svc-mqfqj
Mar  9 16:41:22.379: INFO: Got endpoints: latency-svc-9jwbr [750.7597ms]
Mar  9 16:41:22.398: INFO: Created: latency-svc-25qsb
Mar  9 16:41:22.428: INFO: Got endpoints: latency-svc-w2mvl [750.034485ms]
Mar  9 16:41:22.444: INFO: Created: latency-svc-bq98g
Mar  9 16:41:22.480: INFO: Got endpoints: latency-svc-s7r54 [751.8153ms]
Mar  9 16:41:22.504: INFO: Created: latency-svc-m4gpn
Mar  9 16:41:22.529: INFO: Got endpoints: latency-svc-fls9q [749.59284ms]
Mar  9 16:41:22.545: INFO: Created: latency-svc-kwb94
Mar  9 16:41:22.579: INFO: Got endpoints: latency-svc-4j486 [751.962823ms]
Mar  9 16:41:22.595: INFO: Created: latency-svc-h4thx
Mar  9 16:41:22.628: INFO: Got endpoints: latency-svc-c5mwq [748.960947ms]
Mar  9 16:41:22.652: INFO: Created: latency-svc-sdqkb
Mar  9 16:41:22.677: INFO: Got endpoints: latency-svc-9mqjz [750.993173ms]
Mar  9 16:41:22.727: INFO: Got endpoints: latency-svc-tbhpm [747.51961ms]
Mar  9 16:41:22.728: INFO: Created: latency-svc-4ztlq
Mar  9 16:41:22.748: INFO: Created: latency-svc-6x2fr
Mar  9 16:41:22.777: INFO: Got endpoints: latency-svc-9x8zf [748.133108ms]
Mar  9 16:41:22.792: INFO: Created: latency-svc-bdxbd
Mar  9 16:41:22.827: INFO: Got endpoints: latency-svc-wv8m7 [750.309964ms]
Mar  9 16:41:22.841: INFO: Created: latency-svc-8jtpd
Mar  9 16:41:22.877: INFO: Got endpoints: latency-svc-sxzp2 [750.288208ms]
Mar  9 16:41:22.898: INFO: Created: latency-svc-ng2b8
Mar  9 16:41:22.929: INFO: Got endpoints: latency-svc-5k699 [751.85603ms]
Mar  9 16:41:22.951: INFO: Created: latency-svc-sq6xz
Mar  9 16:41:22.977: INFO: Got endpoints: latency-svc-m8xcl [742.445829ms]
Mar  9 16:41:22.993: INFO: Created: latency-svc-lr27z
Mar  9 16:41:23.031: INFO: Got endpoints: latency-svc-qtpvn [755.763735ms]
Mar  9 16:41:23.047: INFO: Created: latency-svc-mrhfs
Mar  9 16:41:23.077: INFO: Got endpoints: latency-svc-mqfqj [749.034236ms]
Mar  9 16:41:23.091: INFO: Created: latency-svc-5fpc9
Mar  9 16:41:23.128: INFO: Got endpoints: latency-svc-25qsb [749.253404ms]
Mar  9 16:41:23.148: INFO: Created: latency-svc-2twb8
Mar  9 16:41:23.176: INFO: Got endpoints: latency-svc-bq98g [748.003677ms]
Mar  9 16:41:23.193: INFO: Created: latency-svc-2td4b
Mar  9 16:41:23.227: INFO: Got endpoints: latency-svc-m4gpn [746.569136ms]
Mar  9 16:41:23.245: INFO: Created: latency-svc-pkc46
Mar  9 16:41:23.278: INFO: Got endpoints: latency-svc-kwb94 [748.882452ms]
Mar  9 16:41:23.293: INFO: Created: latency-svc-g568f
Mar  9 16:41:23.326: INFO: Got endpoints: latency-svc-h4thx [746.684942ms]
Mar  9 16:41:23.343: INFO: Created: latency-svc-wx7jb
Mar  9 16:41:23.377: INFO: Got endpoints: latency-svc-sdqkb [749.727462ms]
Mar  9 16:41:23.395: INFO: Created: latency-svc-4b2fv
Mar  9 16:41:23.428: INFO: Got endpoints: latency-svc-4ztlq [750.368526ms]
Mar  9 16:41:23.445: INFO: Created: latency-svc-d6bv7
Mar  9 16:41:23.477: INFO: Got endpoints: latency-svc-6x2fr [750.222552ms]
Mar  9 16:41:23.496: INFO: Created: latency-svc-xn8vf
Mar  9 16:41:23.528: INFO: Got endpoints: latency-svc-bdxbd [750.563091ms]
Mar  9 16:41:23.547: INFO: Created: latency-svc-mmhjq
Mar  9 16:41:23.582: INFO: Got endpoints: latency-svc-8jtpd [755.190006ms]
Mar  9 16:41:23.600: INFO: Created: latency-svc-t7b8v
Mar  9 16:41:23.630: INFO: Got endpoints: latency-svc-ng2b8 [753.097944ms]
Mar  9 16:41:23.653: INFO: Created: latency-svc-lvnrl
Mar  9 16:41:23.678: INFO: Got endpoints: latency-svc-sq6xz [749.138712ms]
Mar  9 16:41:23.694: INFO: Created: latency-svc-tvfjt
Mar  9 16:41:23.729: INFO: Got endpoints: latency-svc-lr27z [752.420753ms]
Mar  9 16:41:23.748: INFO: Created: latency-svc-s89wp
Mar  9 16:41:23.778: INFO: Got endpoints: latency-svc-mrhfs [746.939723ms]
Mar  9 16:41:23.794: INFO: Created: latency-svc-dm5z4
Mar  9 16:41:23.828: INFO: Got endpoints: latency-svc-5fpc9 [750.693138ms]
Mar  9 16:41:23.851: INFO: Created: latency-svc-njtlw
Mar  9 16:41:23.882: INFO: Got endpoints: latency-svc-2twb8 [753.83292ms]
Mar  9 16:41:23.896: INFO: Created: latency-svc-66sxl
Mar  9 16:41:23.930: INFO: Got endpoints: latency-svc-2td4b [753.910573ms]
Mar  9 16:41:23.945: INFO: Created: latency-svc-qwnhz
Mar  9 16:41:23.978: INFO: Got endpoints: latency-svc-pkc46 [750.875233ms]
Mar  9 16:41:23.996: INFO: Created: latency-svc-qslth
Mar  9 16:41:24.027: INFO: Got endpoints: latency-svc-g568f [748.6426ms]
Mar  9 16:41:24.043: INFO: Created: latency-svc-7tmvh
Mar  9 16:41:24.077: INFO: Got endpoints: latency-svc-wx7jb [750.679047ms]
Mar  9 16:41:24.136: INFO: Got endpoints: latency-svc-4b2fv [758.745506ms]
Mar  9 16:41:24.138: INFO: Created: latency-svc-dwfmb
Mar  9 16:41:24.222: INFO: Got endpoints: latency-svc-d6bv7 [794.670123ms]
Mar  9 16:41:24.225: INFO: Created: latency-svc-qpjm6
Mar  9 16:41:24.230: INFO: Got endpoints: latency-svc-xn8vf [752.845359ms]
Mar  9 16:41:24.246: INFO: Created: latency-svc-wrbsq
Mar  9 16:41:24.254: INFO: Created: latency-svc-jbv4w
Mar  9 16:41:24.277: INFO: Got endpoints: latency-svc-mmhjq [749.003595ms]
Mar  9 16:41:24.291: INFO: Created: latency-svc-b8r2v
Mar  9 16:41:24.328: INFO: Got endpoints: latency-svc-t7b8v [745.782743ms]
Mar  9 16:41:24.341: INFO: Created: latency-svc-vtrdd
Mar  9 16:41:24.376: INFO: Got endpoints: latency-svc-lvnrl [746.113393ms]
Mar  9 16:41:24.391: INFO: Created: latency-svc-zndcd
Mar  9 16:41:24.427: INFO: Got endpoints: latency-svc-tvfjt [749.015696ms]
Mar  9 16:41:24.449: INFO: Created: latency-svc-g52d6
Mar  9 16:41:24.479: INFO: Got endpoints: latency-svc-s89wp [750.297482ms]
Mar  9 16:41:24.499: INFO: Created: latency-svc-f9gjk
Mar  9 16:41:24.529: INFO: Got endpoints: latency-svc-dm5z4 [750.30714ms]
Mar  9 16:41:24.548: INFO: Created: latency-svc-wdxfk
Mar  9 16:41:24.582: INFO: Got endpoints: latency-svc-njtlw [753.253607ms]
Mar  9 16:41:24.598: INFO: Created: latency-svc-j4g8z
Mar  9 16:41:24.628: INFO: Got endpoints: latency-svc-66sxl [746.123035ms]
Mar  9 16:41:24.653: INFO: Created: latency-svc-t842f
Mar  9 16:41:24.678: INFO: Got endpoints: latency-svc-qwnhz [748.15109ms]
Mar  9 16:41:24.694: INFO: Created: latency-svc-t6gr8
Mar  9 16:41:24.731: INFO: Got endpoints: latency-svc-qslth [753.091534ms]
Mar  9 16:41:24.748: INFO: Created: latency-svc-fdrpb
Mar  9 16:41:24.777: INFO: Got endpoints: latency-svc-7tmvh [750.682483ms]
Mar  9 16:41:24.793: INFO: Created: latency-svc-5bmkr
Mar  9 16:41:24.830: INFO: Got endpoints: latency-svc-dwfmb [753.890733ms]
Mar  9 16:41:24.847: INFO: Created: latency-svc-67b52
Mar  9 16:41:24.878: INFO: Got endpoints: latency-svc-qpjm6 [741.851638ms]
Mar  9 16:41:24.894: INFO: Created: latency-svc-cf884
Mar  9 16:41:24.929: INFO: Got endpoints: latency-svc-wrbsq [706.541616ms]
Mar  9 16:41:24.944: INFO: Created: latency-svc-thrkb
Mar  9 16:41:24.976: INFO: Got endpoints: latency-svc-jbv4w [746.686362ms]
Mar  9 16:41:24.994: INFO: Created: latency-svc-cpgdv
Mar  9 16:41:25.026: INFO: Got endpoints: latency-svc-b8r2v [749.160648ms]
Mar  9 16:41:25.041: INFO: Created: latency-svc-nhj9n
Mar  9 16:41:25.078: INFO: Got endpoints: latency-svc-vtrdd [749.234701ms]
Mar  9 16:41:25.094: INFO: Created: latency-svc-h8x67
Mar  9 16:41:25.127: INFO: Got endpoints: latency-svc-zndcd [750.466476ms]
Mar  9 16:41:25.142: INFO: Created: latency-svc-4566f
Mar  9 16:41:25.183: INFO: Got endpoints: latency-svc-g52d6 [755.413131ms]
Mar  9 16:41:25.199: INFO: Created: latency-svc-544wv
Mar  9 16:41:25.226: INFO: Got endpoints: latency-svc-f9gjk [746.903251ms]
Mar  9 16:41:25.242: INFO: Created: latency-svc-52d7r
Mar  9 16:41:25.277: INFO: Got endpoints: latency-svc-wdxfk [747.838673ms]
Mar  9 16:41:25.292: INFO: Created: latency-svc-wclds
Mar  9 16:41:25.327: INFO: Got endpoints: latency-svc-j4g8z [745.738845ms]
Mar  9 16:41:25.342: INFO: Created: latency-svc-t7l25
Mar  9 16:41:25.378: INFO: Got endpoints: latency-svc-t842f [749.493683ms]
Mar  9 16:41:25.397: INFO: Created: latency-svc-ccz5x
Mar  9 16:41:25.427: INFO: Got endpoints: latency-svc-t6gr8 [749.001973ms]
Mar  9 16:41:25.450: INFO: Created: latency-svc-lx6zv
Mar  9 16:41:25.477: INFO: Got endpoints: latency-svc-fdrpb [745.621874ms]
Mar  9 16:41:25.492: INFO: Created: latency-svc-7tsqc
Mar  9 16:41:25.526: INFO: Got endpoints: latency-svc-5bmkr [748.243066ms]
Mar  9 16:41:25.540: INFO: Created: latency-svc-npdcw
Mar  9 16:41:25.578: INFO: Got endpoints: latency-svc-67b52 [747.296054ms]
Mar  9 16:41:25.592: INFO: Created: latency-svc-ppp44
Mar  9 16:41:25.631: INFO: Got endpoints: latency-svc-cf884 [753.236095ms]
Mar  9 16:41:25.648: INFO: Created: latency-svc-g7pkx
Mar  9 16:41:25.676: INFO: Got endpoints: latency-svc-thrkb [746.595201ms]
Mar  9 16:41:25.689: INFO: Created: latency-svc-2xpl4
Mar  9 16:41:25.727: INFO: Got endpoints: latency-svc-cpgdv [750.392235ms]
Mar  9 16:41:25.742: INFO: Created: latency-svc-5mcln
Mar  9 16:41:25.777: INFO: Got endpoints: latency-svc-nhj9n [750.167795ms]
Mar  9 16:41:25.792: INFO: Created: latency-svc-sgqnl
Mar  9 16:41:25.827: INFO: Got endpoints: latency-svc-h8x67 [749.716271ms]
Mar  9 16:41:25.841: INFO: Created: latency-svc-lsrhs
Mar  9 16:41:25.878: INFO: Got endpoints: latency-svc-4566f [751.340012ms]
Mar  9 16:41:25.893: INFO: Created: latency-svc-cgcws
Mar  9 16:41:25.927: INFO: Got endpoints: latency-svc-544wv [744.574871ms]
Mar  9 16:41:25.941: INFO: Created: latency-svc-d6ftv
Mar  9 16:41:25.976: INFO: Got endpoints: latency-svc-52d7r [749.71143ms]
Mar  9 16:41:25.990: INFO: Created: latency-svc-jpj8x
Mar  9 16:41:26.026: INFO: Got endpoints: latency-svc-wclds [749.083761ms]
Mar  9 16:41:26.039: INFO: Created: latency-svc-pqkbr
Mar  9 16:41:26.077: INFO: Got endpoints: latency-svc-t7l25 [749.313343ms]
Mar  9 16:41:26.090: INFO: Created: latency-svc-cb9qg
Mar  9 16:41:26.129: INFO: Got endpoints: latency-svc-ccz5x [751.106025ms]
Mar  9 16:41:26.143: INFO: Created: latency-svc-6tgmr
Mar  9 16:41:26.176: INFO: Got endpoints: latency-svc-lx6zv [749.378844ms]
Mar  9 16:41:26.190: INFO: Created: latency-svc-w9m6r
Mar  9 16:41:26.226: INFO: Got endpoints: latency-svc-7tsqc [749.765646ms]
Mar  9 16:41:26.243: INFO: Created: latency-svc-hp2b5
Mar  9 16:41:26.277: INFO: Got endpoints: latency-svc-npdcw [751.618225ms]
Mar  9 16:41:26.290: INFO: Created: latency-svc-ck5gp
Mar  9 16:41:26.326: INFO: Got endpoints: latency-svc-ppp44 [748.337862ms]
Mar  9 16:41:26.342: INFO: Created: latency-svc-n9qml
Mar  9 16:41:26.377: INFO: Got endpoints: latency-svc-g7pkx [745.532261ms]
Mar  9 16:41:26.390: INFO: Created: latency-svc-g98rn
Mar  9 16:41:26.427: INFO: Got endpoints: latency-svc-2xpl4 [750.856478ms]
Mar  9 16:41:26.446: INFO: Created: latency-svc-9rq9l
Mar  9 16:41:26.476: INFO: Got endpoints: latency-svc-5mcln [749.142877ms]
Mar  9 16:41:26.491: INFO: Created: latency-svc-8z58k
Mar  9 16:41:26.526: INFO: Got endpoints: latency-svc-sgqnl [749.697231ms]
Mar  9 16:41:26.542: INFO: Created: latency-svc-rh9l2
Mar  9 16:41:26.580: INFO: Got endpoints: latency-svc-lsrhs [752.377804ms]
Mar  9 16:41:26.595: INFO: Created: latency-svc-zfnrk
Mar  9 16:41:26.628: INFO: Got endpoints: latency-svc-cgcws [750.026943ms]
Mar  9 16:41:26.647: INFO: Created: latency-svc-tsskk
Mar  9 16:41:26.680: INFO: Got endpoints: latency-svc-d6ftv [752.325116ms]
Mar  9 16:41:26.697: INFO: Created: latency-svc-68x9c
Mar  9 16:41:26.728: INFO: Got endpoints: latency-svc-jpj8x [752.222815ms]
Mar  9 16:41:26.764: INFO: Created: latency-svc-88wm8
Mar  9 16:41:26.778: INFO: Got endpoints: latency-svc-pqkbr [751.857471ms]
Mar  9 16:41:26.791: INFO: Created: latency-svc-g5cf2
Mar  9 16:41:26.827: INFO: Got endpoints: latency-svc-cb9qg [750.425437ms]
Mar  9 16:41:26.844: INFO: Created: latency-svc-v568w
Mar  9 16:41:26.879: INFO: Got endpoints: latency-svc-6tgmr [749.8524ms]
Mar  9 16:41:26.898: INFO: Created: latency-svc-bkwxx
Mar  9 16:41:26.927: INFO: Got endpoints: latency-svc-w9m6r [750.361922ms]
Mar  9 16:41:26.943: INFO: Created: latency-svc-jzxwf
Mar  9 16:41:26.977: INFO: Got endpoints: latency-svc-hp2b5 [750.402164ms]
Mar  9 16:41:26.991: INFO: Created: latency-svc-xflzf
Mar  9 16:41:27.027: INFO: Got endpoints: latency-svc-ck5gp [749.267261ms]
Mar  9 16:41:27.041: INFO: Created: latency-svc-cxsl6
Mar  9 16:41:27.081: INFO: Got endpoints: latency-svc-n9qml [754.156117ms]
Mar  9 16:41:27.094: INFO: Created: latency-svc-fng8r
Mar  9 16:41:27.126: INFO: Got endpoints: latency-svc-g98rn [748.506292ms]
Mar  9 16:41:27.140: INFO: Created: latency-svc-dtqpl
Mar  9 16:41:27.176: INFO: Got endpoints: latency-svc-9rq9l [749.754871ms]
Mar  9 16:41:27.194: INFO: Created: latency-svc-n67wj
Mar  9 16:41:27.228: INFO: Got endpoints: latency-svc-8z58k [752.079637ms]
Mar  9 16:41:27.241: INFO: Created: latency-svc-j6rz7
Mar  9 16:41:27.276: INFO: Got endpoints: latency-svc-rh9l2 [749.265199ms]
Mar  9 16:41:27.326: INFO: Got endpoints: latency-svc-zfnrk [746.668326ms]
Mar  9 16:41:27.377: INFO: Got endpoints: latency-svc-tsskk [748.129188ms]
Mar  9 16:41:27.426: INFO: Got endpoints: latency-svc-68x9c [746.173738ms]
Mar  9 16:41:27.478: INFO: Got endpoints: latency-svc-88wm8 [749.149827ms]
Mar  9 16:41:27.527: INFO: Got endpoints: latency-svc-g5cf2 [749.112183ms]
Mar  9 16:41:27.577: INFO: Got endpoints: latency-svc-v568w [749.868639ms]
Mar  9 16:41:27.627: INFO: Got endpoints: latency-svc-bkwxx [747.756271ms]
Mar  9 16:41:27.677: INFO: Got endpoints: latency-svc-jzxwf [749.962419ms]
Mar  9 16:41:27.727: INFO: Got endpoints: latency-svc-xflzf [750.144284ms]
Mar  9 16:41:27.779: INFO: Got endpoints: latency-svc-cxsl6 [751.672815ms]
Mar  9 16:41:27.826: INFO: Got endpoints: latency-svc-fng8r [745.717228ms]
Mar  9 16:41:27.879: INFO: Got endpoints: latency-svc-dtqpl [752.821285ms]
Mar  9 16:41:27.927: INFO: Got endpoints: latency-svc-n67wj [750.15815ms]
Mar  9 16:41:27.977: INFO: Got endpoints: latency-svc-j6rz7 [748.253078ms]
Mar  9 16:41:27.977: INFO: Latencies: [27.640179ms 34.837742ms 49.608156ms 66.309697ms 81.000349ms 93.680157ms 109.689836ms 120.056988ms 132.016316ms 145.06548ms 154.680045ms 166.257238ms 170.767291ms 175.729517ms 178.978519ms 179.709005ms 180.965664ms 183.651573ms 185.233521ms 185.24278ms 185.861785ms 186.106974ms 186.565022ms 187.448124ms 188.231382ms 188.844177ms 190.883859ms 194.076664ms 195.773068ms 197.869926ms 201.876447ms 205.116034ms 205.546211ms 210.015785ms 211.079703ms 220.870355ms 245.943819ms 249.590196ms 256.098657ms 256.492452ms 257.651484ms 298.767081ms 333.215965ms 373.569698ms 412.491876ms 456.264704ms 479.765628ms 511.170193ms 537.841294ms 575.242417ms 598.085902ms 613.449371ms 640.720057ms 675.277373ms 706.541616ms 713.189252ms 741.851638ms 742.445829ms 743.362361ms 743.675176ms 744.574871ms 745.532261ms 745.621874ms 745.717228ms 745.72039ms 745.738845ms 745.782743ms 746.113393ms 746.123035ms 746.173738ms 746.569136ms 746.595201ms 746.668326ms 746.684942ms 746.686362ms 746.903251ms 746.939723ms 747.248144ms 747.296054ms 747.380642ms 747.394901ms 747.51961ms 747.756271ms 747.838673ms 748.003677ms 748.011978ms 748.051696ms 748.129188ms 748.133108ms 748.15109ms 748.243066ms 748.253078ms 748.337862ms 748.42029ms 748.506292ms 748.6426ms 748.843682ms 748.882452ms 748.960947ms 749.001973ms 749.003595ms 749.015696ms 749.034236ms 749.083761ms 749.098489ms 749.112183ms 749.120819ms 749.138712ms 749.142877ms 749.149827ms 749.160648ms 749.234701ms 749.253404ms 749.265199ms 749.267261ms 749.313343ms 749.378844ms 749.410154ms 749.467733ms 749.493683ms 749.59284ms 749.697231ms 749.71143ms 749.711675ms 749.716271ms 749.727462ms 749.734565ms 749.754871ms 749.765646ms 749.8524ms 749.868639ms 749.962419ms 750.010441ms 750.026943ms 750.034485ms 750.144284ms 750.15815ms 750.167795ms 750.214544ms 750.222552ms 750.249808ms 750.253713ms 750.288208ms 750.297482ms 750.30714ms 750.309964ms 750.361922ms 750.368526ms 750.392235ms 750.394379ms 750.402164ms 750.425437ms 750.466476ms 750.548154ms 750.563091ms 750.679047ms 750.682483ms 750.693138ms 750.703235ms 750.7597ms 750.856478ms 750.875233ms 750.94679ms 750.993173ms 751.104587ms 751.106025ms 751.116857ms 751.233502ms 751.340012ms 751.516965ms 751.618225ms 751.635346ms 751.672815ms 751.8153ms 751.85603ms 751.857471ms 751.962823ms 752.079637ms 752.222815ms 752.325116ms 752.377804ms 752.420753ms 752.693019ms 752.821285ms 752.845359ms 753.091534ms 753.097944ms 753.236095ms 753.253607ms 753.83292ms 753.890733ms 753.910573ms 754.156117ms 755.190006ms 755.413131ms 755.763735ms 756.580264ms 756.804058ms 758.745506ms 794.670123ms]
Mar  9 16:41:27.977: INFO: 50 %ile: 749.003595ms
Mar  9 16:41:27.977: INFO: 90 %ile: 752.377804ms
Mar  9 16:41:27.977: INFO: 99 %ile: 758.745506ms
Mar  9 16:41:27.977: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Mar  9 16:41:27.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5686" for this suite. 03/09/23 16:41:27.984
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":166,"skipped":3286,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.772 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:17.217
    Mar  9 16:41:17.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename svc-latency 03/09/23 16:41:17.219
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:17.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:17.237
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar  9 16:41:17.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5686 03/09/23 16:41:17.241
    I0309 16:41:17.247193      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5686, replica count: 1
    I0309 16:41:18.298921      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0309 16:41:19.299739      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 16:41:19.421: INFO: Created: latency-svc-vzw7q
    Mar  9 16:41:19.426: INFO: Got endpoints: latency-svc-vzw7q [26.253032ms]
    Mar  9 16:41:19.450: INFO: Created: latency-svc-5mqff
    Mar  9 16:41:19.454: INFO: Got endpoints: latency-svc-5mqff [27.640179ms]
    Mar  9 16:41:19.460: INFO: Created: latency-svc-d2dtc
    Mar  9 16:41:19.462: INFO: Got endpoints: latency-svc-d2dtc [34.837742ms]
    Mar  9 16:41:19.472: INFO: Created: latency-svc-hl9jc
    Mar  9 16:41:19.477: INFO: Got endpoints: latency-svc-hl9jc [49.608156ms]
    Mar  9 16:41:19.488: INFO: Created: latency-svc-d92qd
    Mar  9 16:41:19.493: INFO: Got endpoints: latency-svc-d92qd [66.309697ms]
    Mar  9 16:41:19.499: INFO: Created: latency-svc-txpzg
    Mar  9 16:41:19.508: INFO: Got endpoints: latency-svc-txpzg [81.000349ms]
    Mar  9 16:41:19.515: INFO: Created: latency-svc-z5tlc
    Mar  9 16:41:19.521: INFO: Got endpoints: latency-svc-z5tlc [93.680157ms]
    Mar  9 16:41:19.530: INFO: Created: latency-svc-gbhgp
    Mar  9 16:41:19.536: INFO: Got endpoints: latency-svc-gbhgp [109.689836ms]
    Mar  9 16:41:19.542: INFO: Created: latency-svc-kxpqb
    Mar  9 16:41:19.548: INFO: Got endpoints: latency-svc-kxpqb [120.056988ms]
    Mar  9 16:41:19.552: INFO: Created: latency-svc-ggkql
    Mar  9 16:41:19.560: INFO: Got endpoints: latency-svc-ggkql [132.016316ms]
    Mar  9 16:41:19.566: INFO: Created: latency-svc-zk7ql
    Mar  9 16:41:19.573: INFO: Got endpoints: latency-svc-zk7ql [145.06548ms]
    Mar  9 16:41:19.576: INFO: Created: latency-svc-sd4nj
    Mar  9 16:41:19.583: INFO: Got endpoints: latency-svc-sd4nj [154.680045ms]
    Mar  9 16:41:19.587: INFO: Created: latency-svc-cd8xk
    Mar  9 16:41:19.595: INFO: Got endpoints: latency-svc-cd8xk [166.257238ms]
    Mar  9 16:41:19.599: INFO: Created: latency-svc-vs5wg
    Mar  9 16:41:19.607: INFO: Got endpoints: latency-svc-vs5wg [178.978519ms]
    Mar  9 16:41:19.624: INFO: Created: latency-svc-m7fcl
    Mar  9 16:41:19.631: INFO: Created: latency-svc-6fs8d
    Mar  9 16:41:19.633: INFO: Got endpoints: latency-svc-m7fcl [205.546211ms]
    Mar  9 16:41:19.638: INFO: Got endpoints: latency-svc-6fs8d [211.079703ms]
    Mar  9 16:41:19.647: INFO: Created: latency-svc-qvwd2
    Mar  9 16:41:19.652: INFO: Got endpoints: latency-svc-qvwd2 [197.869926ms]
    Mar  9 16:41:19.658: INFO: Created: latency-svc-cd87f
    Mar  9 16:41:19.664: INFO: Got endpoints: latency-svc-cd87f [201.876447ms]
    Mar  9 16:41:19.666: INFO: Created: latency-svc-m9vrj
    Mar  9 16:41:19.672: INFO: Got endpoints: latency-svc-m9vrj [195.773068ms]
    Mar  9 16:41:19.679: INFO: Created: latency-svc-kzm8m
    Mar  9 16:41:19.684: INFO: Got endpoints: latency-svc-kzm8m [190.883859ms]
    Mar  9 16:41:19.689: INFO: Created: latency-svc-lzstb
    Mar  9 16:41:19.693: INFO: Got endpoints: latency-svc-lzstb [185.24278ms]
    Mar  9 16:41:19.700: INFO: Created: latency-svc-d9hs4
    Mar  9 16:41:19.708: INFO: Got endpoints: latency-svc-d9hs4 [186.106974ms]
    Mar  9 16:41:19.710: INFO: Created: latency-svc-hmzcc
    Mar  9 16:41:19.717: INFO: Got endpoints: latency-svc-hmzcc [180.965664ms]
    Mar  9 16:41:19.723: INFO: Created: latency-svc-shkdc
    Mar  9 16:41:19.737: INFO: Got endpoints: latency-svc-shkdc [188.844177ms]
    Mar  9 16:41:19.743: INFO: Created: latency-svc-wj2zl
    Mar  9 16:41:19.748: INFO: Got endpoints: latency-svc-wj2zl [187.448124ms]
    Mar  9 16:41:19.752: INFO: Created: latency-svc-5hsx2
    Mar  9 16:41:19.759: INFO: Got endpoints: latency-svc-5hsx2 [185.861785ms]
    Mar  9 16:41:19.763: INFO: Created: latency-svc-lckkp
    Mar  9 16:41:19.771: INFO: Got endpoints: latency-svc-lckkp [188.231382ms]
    Mar  9 16:41:19.774: INFO: Created: latency-svc-5v8t9
    Mar  9 16:41:19.780: INFO: Got endpoints: latency-svc-5v8t9 [185.233521ms]
    Mar  9 16:41:19.789: INFO: Created: latency-svc-6zt94
    Mar  9 16:41:19.794: INFO: Got endpoints: latency-svc-6zt94 [186.565022ms]
    Mar  9 16:41:19.799: INFO: Created: latency-svc-gq2hq
    Mar  9 16:41:19.803: INFO: Got endpoints: latency-svc-gq2hq [170.767291ms]
    Mar  9 16:41:19.811: INFO: Created: latency-svc-t8xvj
    Mar  9 16:41:19.818: INFO: Got endpoints: latency-svc-t8xvj [179.709005ms]
    Mar  9 16:41:19.821: INFO: Created: latency-svc-5ff56
    Mar  9 16:41:19.828: INFO: Got endpoints: latency-svc-5ff56 [175.729517ms]
    Mar  9 16:41:19.833: INFO: Created: latency-svc-vw9wp
    Mar  9 16:41:19.847: INFO: Got endpoints: latency-svc-vw9wp [183.651573ms]
    Mar  9 16:41:19.856: INFO: Created: latency-svc-75nh8
    Mar  9 16:41:19.867: INFO: Got endpoints: latency-svc-75nh8 [194.076664ms]
    Mar  9 16:41:19.872: INFO: Created: latency-svc-5jbqv
    Mar  9 16:41:19.889: INFO: Got endpoints: latency-svc-5jbqv [205.116034ms]
    Mar  9 16:41:19.900: INFO: Created: latency-svc-tr82b
    Mar  9 16:41:19.903: INFO: Got endpoints: latency-svc-tr82b [210.015785ms]
    Mar  9 16:41:19.915: INFO: Created: latency-svc-cftx9
    Mar  9 16:41:19.928: INFO: Got endpoints: latency-svc-cftx9 [220.870355ms]
    Mar  9 16:41:19.944: INFO: Created: latency-svc-mctgr
    Mar  9 16:41:19.963: INFO: Got endpoints: latency-svc-mctgr [245.943819ms]
    Mar  9 16:41:19.971: INFO: Created: latency-svc-8vj45
    Mar  9 16:41:19.986: INFO: Got endpoints: latency-svc-8vj45 [249.590196ms]
    Mar  9 16:41:19.996: INFO: Created: latency-svc-hplwk
    Mar  9 16:41:20.004: INFO: Got endpoints: latency-svc-hplwk [256.098657ms]
    Mar  9 16:41:20.006: INFO: Created: latency-svc-hr9xz
    Mar  9 16:41:20.016: INFO: Got endpoints: latency-svc-hr9xz [256.492452ms]
    Mar  9 16:41:20.018: INFO: Created: latency-svc-qqsw8
    Mar  9 16:41:20.029: INFO: Got endpoints: latency-svc-qqsw8 [257.651484ms]
    Mar  9 16:41:20.040: INFO: Created: latency-svc-q2l2g
    Mar  9 16:41:20.050: INFO: Created: latency-svc-hwd6x
    Mar  9 16:41:20.065: INFO: Created: latency-svc-9fjzv
    Mar  9 16:41:20.076: INFO: Created: latency-svc-gg6zs
    Mar  9 16:41:20.079: INFO: Got endpoints: latency-svc-q2l2g [298.767081ms]
    Mar  9 16:41:20.087: INFO: Created: latency-svc-ttjns
    Mar  9 16:41:20.093: INFO: Created: latency-svc-7l9w7
    Mar  9 16:41:20.101: INFO: Created: latency-svc-9wqhz
    Mar  9 16:41:20.109: INFO: Created: latency-svc-wktzw
    Mar  9 16:41:20.120: INFO: Created: latency-svc-4fspt
    Mar  9 16:41:20.128: INFO: Got endpoints: latency-svc-hwd6x [333.215965ms]
    Mar  9 16:41:20.134: INFO: Created: latency-svc-zrlfd
    Mar  9 16:41:20.147: INFO: Created: latency-svc-gvw69
    Mar  9 16:41:20.153: INFO: Created: latency-svc-z9lml
    Mar  9 16:41:20.177: INFO: Got endpoints: latency-svc-9fjzv [373.569698ms]
    Mar  9 16:41:20.182: INFO: Created: latency-svc-4wkfc
    Mar  9 16:41:20.191: INFO: Created: latency-svc-vqkfd
    Mar  9 16:41:20.199: INFO: Created: latency-svc-qzggl
    Mar  9 16:41:20.208: INFO: Created: latency-svc-vvsw4
    Mar  9 16:41:20.219: INFO: Created: latency-svc-nd2zx
    Mar  9 16:41:20.230: INFO: Created: latency-svc-zqfxw
    Mar  9 16:41:20.231: INFO: Got endpoints: latency-svc-gg6zs [412.491876ms]
    Mar  9 16:41:20.246: INFO: Created: latency-svc-8cch4
    Mar  9 16:41:20.285: INFO: Got endpoints: latency-svc-ttjns [456.264704ms]
    Mar  9 16:41:20.299: INFO: Created: latency-svc-vpbw9
    Mar  9 16:41:20.327: INFO: Got endpoints: latency-svc-7l9w7 [479.765628ms]
    Mar  9 16:41:20.344: INFO: Created: latency-svc-rjxvl
    Mar  9 16:41:20.378: INFO: Got endpoints: latency-svc-9wqhz [511.170193ms]
    Mar  9 16:41:20.395: INFO: Created: latency-svc-bbkzc
    Mar  9 16:41:20.427: INFO: Got endpoints: latency-svc-wktzw [537.841294ms]
    Mar  9 16:41:20.443: INFO: Created: latency-svc-nvrn9
    Mar  9 16:41:20.479: INFO: Got endpoints: latency-svc-4fspt [575.242417ms]
    Mar  9 16:41:20.496: INFO: Created: latency-svc-dvpx5
    Mar  9 16:41:20.527: INFO: Got endpoints: latency-svc-zrlfd [598.085902ms]
    Mar  9 16:41:20.544: INFO: Created: latency-svc-m87j5
    Mar  9 16:41:20.577: INFO: Got endpoints: latency-svc-gvw69 [613.449371ms]
    Mar  9 16:41:20.593: INFO: Created: latency-svc-fclh4
    Mar  9 16:41:20.627: INFO: Got endpoints: latency-svc-z9lml [640.720057ms]
    Mar  9 16:41:20.644: INFO: Created: latency-svc-x5h88
    Mar  9 16:41:20.679: INFO: Got endpoints: latency-svc-4wkfc [675.277373ms]
    Mar  9 16:41:20.701: INFO: Created: latency-svc-j7596
    Mar  9 16:41:20.729: INFO: Got endpoints: latency-svc-vqkfd [713.189252ms]
    Mar  9 16:41:20.744: INFO: Created: latency-svc-j8wm5
    Mar  9 16:41:20.776: INFO: Got endpoints: latency-svc-qzggl [747.394901ms]
    Mar  9 16:41:20.790: INFO: Created: latency-svc-8q8f5
    Mar  9 16:41:20.827: INFO: Got endpoints: latency-svc-vvsw4 [748.42029ms]
    Mar  9 16:41:20.842: INFO: Created: latency-svc-k29r9
    Mar  9 16:41:20.884: INFO: Got endpoints: latency-svc-nd2zx [756.580264ms]
    Mar  9 16:41:20.910: INFO: Created: latency-svc-jh7b2
    Mar  9 16:41:20.928: INFO: Got endpoints: latency-svc-zqfxw [750.703235ms]
    Mar  9 16:41:20.948: INFO: Created: latency-svc-zhg25
    Mar  9 16:41:20.976: INFO: Got endpoints: latency-svc-8cch4 [745.72039ms]
    Mar  9 16:41:20.992: INFO: Created: latency-svc-lcgmv
    Mar  9 16:41:21.028: INFO: Got endpoints: latency-svc-vpbw9 [743.362361ms]
    Mar  9 16:41:21.043: INFO: Created: latency-svc-5kmks
    Mar  9 16:41:21.077: INFO: Got endpoints: latency-svc-rjxvl [749.734565ms]
    Mar  9 16:41:21.092: INFO: Created: latency-svc-cs2cd
    Mar  9 16:41:21.127: INFO: Got endpoints: latency-svc-bbkzc [749.098489ms]
    Mar  9 16:41:21.143: INFO: Created: latency-svc-4h5cj
    Mar  9 16:41:21.177: INFO: Got endpoints: latency-svc-nvrn9 [749.120819ms]
    Mar  9 16:41:21.192: INFO: Created: latency-svc-qgn96
    Mar  9 16:41:21.232: INFO: Got endpoints: latency-svc-dvpx5 [752.693019ms]
    Mar  9 16:41:21.246: INFO: Created: latency-svc-8gfd8
    Mar  9 16:41:21.278: INFO: Got endpoints: latency-svc-m87j5 [751.233502ms]
    Mar  9 16:41:21.294: INFO: Created: latency-svc-fkw24
    Mar  9 16:41:21.326: INFO: Got endpoints: latency-svc-fclh4 [749.467733ms]
    Mar  9 16:41:21.341: INFO: Created: latency-svc-77rb5
    Mar  9 16:41:21.378: INFO: Got endpoints: latency-svc-x5h88 [750.394379ms]
    Mar  9 16:41:21.393: INFO: Created: latency-svc-zv8sq
    Mar  9 16:41:21.426: INFO: Got endpoints: latency-svc-j7596 [747.248144ms]
    Mar  9 16:41:21.444: INFO: Created: latency-svc-b76p9
    Mar  9 16:41:21.477: INFO: Got endpoints: latency-svc-j8wm5 [748.051696ms]
    Mar  9 16:41:21.494: INFO: Created: latency-svc-q7zjq
    Mar  9 16:41:21.527: INFO: Got endpoints: latency-svc-8q8f5 [750.548154ms]
    Mar  9 16:41:21.544: INFO: Created: latency-svc-8wv9x
    Mar  9 16:41:21.577: INFO: Got endpoints: latency-svc-k29r9 [750.010441ms]
    Mar  9 16:41:21.596: INFO: Created: latency-svc-gqqgb
    Mar  9 16:41:21.628: INFO: Got endpoints: latency-svc-jh7b2 [743.675176ms]
    Mar  9 16:41:21.642: INFO: Created: latency-svc-9jwbr
    Mar  9 16:41:21.678: INFO: Got endpoints: latency-svc-zhg25 [749.410154ms]
    Mar  9 16:41:21.695: INFO: Created: latency-svc-w2mvl
    Mar  9 16:41:21.728: INFO: Got endpoints: latency-svc-lcgmv [751.635346ms]
    Mar  9 16:41:21.746: INFO: Created: latency-svc-s7r54
    Mar  9 16:41:21.779: INFO: Got endpoints: latency-svc-5kmks [751.116857ms]
    Mar  9 16:41:21.798: INFO: Created: latency-svc-fls9q
    Mar  9 16:41:21.827: INFO: Got endpoints: latency-svc-cs2cd [750.214544ms]
    Mar  9 16:41:21.846: INFO: Created: latency-svc-4j486
    Mar  9 16:41:21.879: INFO: Got endpoints: latency-svc-4h5cj [751.516965ms]
    Mar  9 16:41:21.893: INFO: Created: latency-svc-c5mwq
    Mar  9 16:41:21.926: INFO: Got endpoints: latency-svc-qgn96 [749.711675ms]
    Mar  9 16:41:21.947: INFO: Created: latency-svc-9mqjz
    Mar  9 16:41:21.979: INFO: Got endpoints: latency-svc-8gfd8 [747.380642ms]
    Mar  9 16:41:21.993: INFO: Created: latency-svc-tbhpm
    Mar  9 16:41:22.029: INFO: Got endpoints: latency-svc-fkw24 [751.104587ms]
    Mar  9 16:41:22.042: INFO: Created: latency-svc-9x8zf
    Mar  9 16:41:22.077: INFO: Got endpoints: latency-svc-77rb5 [750.253713ms]
    Mar  9 16:41:22.093: INFO: Created: latency-svc-wv8m7
    Mar  9 16:41:22.127: INFO: Got endpoints: latency-svc-zv8sq [748.843682ms]
    Mar  9 16:41:22.141: INFO: Created: latency-svc-sxzp2
    Mar  9 16:41:22.177: INFO: Got endpoints: latency-svc-b76p9 [750.249808ms]
    Mar  9 16:41:22.195: INFO: Created: latency-svc-5k699
    Mar  9 16:41:22.234: INFO: Got endpoints: latency-svc-q7zjq [756.804058ms]
    Mar  9 16:41:22.250: INFO: Created: latency-svc-m8xcl
    Mar  9 16:41:22.275: INFO: Got endpoints: latency-svc-8wv9x [748.011978ms]
    Mar  9 16:41:22.293: INFO: Created: latency-svc-qtpvn
    Mar  9 16:41:22.328: INFO: Got endpoints: latency-svc-gqqgb [750.94679ms]
    Mar  9 16:41:22.345: INFO: Created: latency-svc-mqfqj
    Mar  9 16:41:22.379: INFO: Got endpoints: latency-svc-9jwbr [750.7597ms]
    Mar  9 16:41:22.398: INFO: Created: latency-svc-25qsb
    Mar  9 16:41:22.428: INFO: Got endpoints: latency-svc-w2mvl [750.034485ms]
    Mar  9 16:41:22.444: INFO: Created: latency-svc-bq98g
    Mar  9 16:41:22.480: INFO: Got endpoints: latency-svc-s7r54 [751.8153ms]
    Mar  9 16:41:22.504: INFO: Created: latency-svc-m4gpn
    Mar  9 16:41:22.529: INFO: Got endpoints: latency-svc-fls9q [749.59284ms]
    Mar  9 16:41:22.545: INFO: Created: latency-svc-kwb94
    Mar  9 16:41:22.579: INFO: Got endpoints: latency-svc-4j486 [751.962823ms]
    Mar  9 16:41:22.595: INFO: Created: latency-svc-h4thx
    Mar  9 16:41:22.628: INFO: Got endpoints: latency-svc-c5mwq [748.960947ms]
    Mar  9 16:41:22.652: INFO: Created: latency-svc-sdqkb
    Mar  9 16:41:22.677: INFO: Got endpoints: latency-svc-9mqjz [750.993173ms]
    Mar  9 16:41:22.727: INFO: Got endpoints: latency-svc-tbhpm [747.51961ms]
    Mar  9 16:41:22.728: INFO: Created: latency-svc-4ztlq
    Mar  9 16:41:22.748: INFO: Created: latency-svc-6x2fr
    Mar  9 16:41:22.777: INFO: Got endpoints: latency-svc-9x8zf [748.133108ms]
    Mar  9 16:41:22.792: INFO: Created: latency-svc-bdxbd
    Mar  9 16:41:22.827: INFO: Got endpoints: latency-svc-wv8m7 [750.309964ms]
    Mar  9 16:41:22.841: INFO: Created: latency-svc-8jtpd
    Mar  9 16:41:22.877: INFO: Got endpoints: latency-svc-sxzp2 [750.288208ms]
    Mar  9 16:41:22.898: INFO: Created: latency-svc-ng2b8
    Mar  9 16:41:22.929: INFO: Got endpoints: latency-svc-5k699 [751.85603ms]
    Mar  9 16:41:22.951: INFO: Created: latency-svc-sq6xz
    Mar  9 16:41:22.977: INFO: Got endpoints: latency-svc-m8xcl [742.445829ms]
    Mar  9 16:41:22.993: INFO: Created: latency-svc-lr27z
    Mar  9 16:41:23.031: INFO: Got endpoints: latency-svc-qtpvn [755.763735ms]
    Mar  9 16:41:23.047: INFO: Created: latency-svc-mrhfs
    Mar  9 16:41:23.077: INFO: Got endpoints: latency-svc-mqfqj [749.034236ms]
    Mar  9 16:41:23.091: INFO: Created: latency-svc-5fpc9
    Mar  9 16:41:23.128: INFO: Got endpoints: latency-svc-25qsb [749.253404ms]
    Mar  9 16:41:23.148: INFO: Created: latency-svc-2twb8
    Mar  9 16:41:23.176: INFO: Got endpoints: latency-svc-bq98g [748.003677ms]
    Mar  9 16:41:23.193: INFO: Created: latency-svc-2td4b
    Mar  9 16:41:23.227: INFO: Got endpoints: latency-svc-m4gpn [746.569136ms]
    Mar  9 16:41:23.245: INFO: Created: latency-svc-pkc46
    Mar  9 16:41:23.278: INFO: Got endpoints: latency-svc-kwb94 [748.882452ms]
    Mar  9 16:41:23.293: INFO: Created: latency-svc-g568f
    Mar  9 16:41:23.326: INFO: Got endpoints: latency-svc-h4thx [746.684942ms]
    Mar  9 16:41:23.343: INFO: Created: latency-svc-wx7jb
    Mar  9 16:41:23.377: INFO: Got endpoints: latency-svc-sdqkb [749.727462ms]
    Mar  9 16:41:23.395: INFO: Created: latency-svc-4b2fv
    Mar  9 16:41:23.428: INFO: Got endpoints: latency-svc-4ztlq [750.368526ms]
    Mar  9 16:41:23.445: INFO: Created: latency-svc-d6bv7
    Mar  9 16:41:23.477: INFO: Got endpoints: latency-svc-6x2fr [750.222552ms]
    Mar  9 16:41:23.496: INFO: Created: latency-svc-xn8vf
    Mar  9 16:41:23.528: INFO: Got endpoints: latency-svc-bdxbd [750.563091ms]
    Mar  9 16:41:23.547: INFO: Created: latency-svc-mmhjq
    Mar  9 16:41:23.582: INFO: Got endpoints: latency-svc-8jtpd [755.190006ms]
    Mar  9 16:41:23.600: INFO: Created: latency-svc-t7b8v
    Mar  9 16:41:23.630: INFO: Got endpoints: latency-svc-ng2b8 [753.097944ms]
    Mar  9 16:41:23.653: INFO: Created: latency-svc-lvnrl
    Mar  9 16:41:23.678: INFO: Got endpoints: latency-svc-sq6xz [749.138712ms]
    Mar  9 16:41:23.694: INFO: Created: latency-svc-tvfjt
    Mar  9 16:41:23.729: INFO: Got endpoints: latency-svc-lr27z [752.420753ms]
    Mar  9 16:41:23.748: INFO: Created: latency-svc-s89wp
    Mar  9 16:41:23.778: INFO: Got endpoints: latency-svc-mrhfs [746.939723ms]
    Mar  9 16:41:23.794: INFO: Created: latency-svc-dm5z4
    Mar  9 16:41:23.828: INFO: Got endpoints: latency-svc-5fpc9 [750.693138ms]
    Mar  9 16:41:23.851: INFO: Created: latency-svc-njtlw
    Mar  9 16:41:23.882: INFO: Got endpoints: latency-svc-2twb8 [753.83292ms]
    Mar  9 16:41:23.896: INFO: Created: latency-svc-66sxl
    Mar  9 16:41:23.930: INFO: Got endpoints: latency-svc-2td4b [753.910573ms]
    Mar  9 16:41:23.945: INFO: Created: latency-svc-qwnhz
    Mar  9 16:41:23.978: INFO: Got endpoints: latency-svc-pkc46 [750.875233ms]
    Mar  9 16:41:23.996: INFO: Created: latency-svc-qslth
    Mar  9 16:41:24.027: INFO: Got endpoints: latency-svc-g568f [748.6426ms]
    Mar  9 16:41:24.043: INFO: Created: latency-svc-7tmvh
    Mar  9 16:41:24.077: INFO: Got endpoints: latency-svc-wx7jb [750.679047ms]
    Mar  9 16:41:24.136: INFO: Got endpoints: latency-svc-4b2fv [758.745506ms]
    Mar  9 16:41:24.138: INFO: Created: latency-svc-dwfmb
    Mar  9 16:41:24.222: INFO: Got endpoints: latency-svc-d6bv7 [794.670123ms]
    Mar  9 16:41:24.225: INFO: Created: latency-svc-qpjm6
    Mar  9 16:41:24.230: INFO: Got endpoints: latency-svc-xn8vf [752.845359ms]
    Mar  9 16:41:24.246: INFO: Created: latency-svc-wrbsq
    Mar  9 16:41:24.254: INFO: Created: latency-svc-jbv4w
    Mar  9 16:41:24.277: INFO: Got endpoints: latency-svc-mmhjq [749.003595ms]
    Mar  9 16:41:24.291: INFO: Created: latency-svc-b8r2v
    Mar  9 16:41:24.328: INFO: Got endpoints: latency-svc-t7b8v [745.782743ms]
    Mar  9 16:41:24.341: INFO: Created: latency-svc-vtrdd
    Mar  9 16:41:24.376: INFO: Got endpoints: latency-svc-lvnrl [746.113393ms]
    Mar  9 16:41:24.391: INFO: Created: latency-svc-zndcd
    Mar  9 16:41:24.427: INFO: Got endpoints: latency-svc-tvfjt [749.015696ms]
    Mar  9 16:41:24.449: INFO: Created: latency-svc-g52d6
    Mar  9 16:41:24.479: INFO: Got endpoints: latency-svc-s89wp [750.297482ms]
    Mar  9 16:41:24.499: INFO: Created: latency-svc-f9gjk
    Mar  9 16:41:24.529: INFO: Got endpoints: latency-svc-dm5z4 [750.30714ms]
    Mar  9 16:41:24.548: INFO: Created: latency-svc-wdxfk
    Mar  9 16:41:24.582: INFO: Got endpoints: latency-svc-njtlw [753.253607ms]
    Mar  9 16:41:24.598: INFO: Created: latency-svc-j4g8z
    Mar  9 16:41:24.628: INFO: Got endpoints: latency-svc-66sxl [746.123035ms]
    Mar  9 16:41:24.653: INFO: Created: latency-svc-t842f
    Mar  9 16:41:24.678: INFO: Got endpoints: latency-svc-qwnhz [748.15109ms]
    Mar  9 16:41:24.694: INFO: Created: latency-svc-t6gr8
    Mar  9 16:41:24.731: INFO: Got endpoints: latency-svc-qslth [753.091534ms]
    Mar  9 16:41:24.748: INFO: Created: latency-svc-fdrpb
    Mar  9 16:41:24.777: INFO: Got endpoints: latency-svc-7tmvh [750.682483ms]
    Mar  9 16:41:24.793: INFO: Created: latency-svc-5bmkr
    Mar  9 16:41:24.830: INFO: Got endpoints: latency-svc-dwfmb [753.890733ms]
    Mar  9 16:41:24.847: INFO: Created: latency-svc-67b52
    Mar  9 16:41:24.878: INFO: Got endpoints: latency-svc-qpjm6 [741.851638ms]
    Mar  9 16:41:24.894: INFO: Created: latency-svc-cf884
    Mar  9 16:41:24.929: INFO: Got endpoints: latency-svc-wrbsq [706.541616ms]
    Mar  9 16:41:24.944: INFO: Created: latency-svc-thrkb
    Mar  9 16:41:24.976: INFO: Got endpoints: latency-svc-jbv4w [746.686362ms]
    Mar  9 16:41:24.994: INFO: Created: latency-svc-cpgdv
    Mar  9 16:41:25.026: INFO: Got endpoints: latency-svc-b8r2v [749.160648ms]
    Mar  9 16:41:25.041: INFO: Created: latency-svc-nhj9n
    Mar  9 16:41:25.078: INFO: Got endpoints: latency-svc-vtrdd [749.234701ms]
    Mar  9 16:41:25.094: INFO: Created: latency-svc-h8x67
    Mar  9 16:41:25.127: INFO: Got endpoints: latency-svc-zndcd [750.466476ms]
    Mar  9 16:41:25.142: INFO: Created: latency-svc-4566f
    Mar  9 16:41:25.183: INFO: Got endpoints: latency-svc-g52d6 [755.413131ms]
    Mar  9 16:41:25.199: INFO: Created: latency-svc-544wv
    Mar  9 16:41:25.226: INFO: Got endpoints: latency-svc-f9gjk [746.903251ms]
    Mar  9 16:41:25.242: INFO: Created: latency-svc-52d7r
    Mar  9 16:41:25.277: INFO: Got endpoints: latency-svc-wdxfk [747.838673ms]
    Mar  9 16:41:25.292: INFO: Created: latency-svc-wclds
    Mar  9 16:41:25.327: INFO: Got endpoints: latency-svc-j4g8z [745.738845ms]
    Mar  9 16:41:25.342: INFO: Created: latency-svc-t7l25
    Mar  9 16:41:25.378: INFO: Got endpoints: latency-svc-t842f [749.493683ms]
    Mar  9 16:41:25.397: INFO: Created: latency-svc-ccz5x
    Mar  9 16:41:25.427: INFO: Got endpoints: latency-svc-t6gr8 [749.001973ms]
    Mar  9 16:41:25.450: INFO: Created: latency-svc-lx6zv
    Mar  9 16:41:25.477: INFO: Got endpoints: latency-svc-fdrpb [745.621874ms]
    Mar  9 16:41:25.492: INFO: Created: latency-svc-7tsqc
    Mar  9 16:41:25.526: INFO: Got endpoints: latency-svc-5bmkr [748.243066ms]
    Mar  9 16:41:25.540: INFO: Created: latency-svc-npdcw
    Mar  9 16:41:25.578: INFO: Got endpoints: latency-svc-67b52 [747.296054ms]
    Mar  9 16:41:25.592: INFO: Created: latency-svc-ppp44
    Mar  9 16:41:25.631: INFO: Got endpoints: latency-svc-cf884 [753.236095ms]
    Mar  9 16:41:25.648: INFO: Created: latency-svc-g7pkx
    Mar  9 16:41:25.676: INFO: Got endpoints: latency-svc-thrkb [746.595201ms]
    Mar  9 16:41:25.689: INFO: Created: latency-svc-2xpl4
    Mar  9 16:41:25.727: INFO: Got endpoints: latency-svc-cpgdv [750.392235ms]
    Mar  9 16:41:25.742: INFO: Created: latency-svc-5mcln
    Mar  9 16:41:25.777: INFO: Got endpoints: latency-svc-nhj9n [750.167795ms]
    Mar  9 16:41:25.792: INFO: Created: latency-svc-sgqnl
    Mar  9 16:41:25.827: INFO: Got endpoints: latency-svc-h8x67 [749.716271ms]
    Mar  9 16:41:25.841: INFO: Created: latency-svc-lsrhs
    Mar  9 16:41:25.878: INFO: Got endpoints: latency-svc-4566f [751.340012ms]
    Mar  9 16:41:25.893: INFO: Created: latency-svc-cgcws
    Mar  9 16:41:25.927: INFO: Got endpoints: latency-svc-544wv [744.574871ms]
    Mar  9 16:41:25.941: INFO: Created: latency-svc-d6ftv
    Mar  9 16:41:25.976: INFO: Got endpoints: latency-svc-52d7r [749.71143ms]
    Mar  9 16:41:25.990: INFO: Created: latency-svc-jpj8x
    Mar  9 16:41:26.026: INFO: Got endpoints: latency-svc-wclds [749.083761ms]
    Mar  9 16:41:26.039: INFO: Created: latency-svc-pqkbr
    Mar  9 16:41:26.077: INFO: Got endpoints: latency-svc-t7l25 [749.313343ms]
    Mar  9 16:41:26.090: INFO: Created: latency-svc-cb9qg
    Mar  9 16:41:26.129: INFO: Got endpoints: latency-svc-ccz5x [751.106025ms]
    Mar  9 16:41:26.143: INFO: Created: latency-svc-6tgmr
    Mar  9 16:41:26.176: INFO: Got endpoints: latency-svc-lx6zv [749.378844ms]
    Mar  9 16:41:26.190: INFO: Created: latency-svc-w9m6r
    Mar  9 16:41:26.226: INFO: Got endpoints: latency-svc-7tsqc [749.765646ms]
    Mar  9 16:41:26.243: INFO: Created: latency-svc-hp2b5
    Mar  9 16:41:26.277: INFO: Got endpoints: latency-svc-npdcw [751.618225ms]
    Mar  9 16:41:26.290: INFO: Created: latency-svc-ck5gp
    Mar  9 16:41:26.326: INFO: Got endpoints: latency-svc-ppp44 [748.337862ms]
    Mar  9 16:41:26.342: INFO: Created: latency-svc-n9qml
    Mar  9 16:41:26.377: INFO: Got endpoints: latency-svc-g7pkx [745.532261ms]
    Mar  9 16:41:26.390: INFO: Created: latency-svc-g98rn
    Mar  9 16:41:26.427: INFO: Got endpoints: latency-svc-2xpl4 [750.856478ms]
    Mar  9 16:41:26.446: INFO: Created: latency-svc-9rq9l
    Mar  9 16:41:26.476: INFO: Got endpoints: latency-svc-5mcln [749.142877ms]
    Mar  9 16:41:26.491: INFO: Created: latency-svc-8z58k
    Mar  9 16:41:26.526: INFO: Got endpoints: latency-svc-sgqnl [749.697231ms]
    Mar  9 16:41:26.542: INFO: Created: latency-svc-rh9l2
    Mar  9 16:41:26.580: INFO: Got endpoints: latency-svc-lsrhs [752.377804ms]
    Mar  9 16:41:26.595: INFO: Created: latency-svc-zfnrk
    Mar  9 16:41:26.628: INFO: Got endpoints: latency-svc-cgcws [750.026943ms]
    Mar  9 16:41:26.647: INFO: Created: latency-svc-tsskk
    Mar  9 16:41:26.680: INFO: Got endpoints: latency-svc-d6ftv [752.325116ms]
    Mar  9 16:41:26.697: INFO: Created: latency-svc-68x9c
    Mar  9 16:41:26.728: INFO: Got endpoints: latency-svc-jpj8x [752.222815ms]
    Mar  9 16:41:26.764: INFO: Created: latency-svc-88wm8
    Mar  9 16:41:26.778: INFO: Got endpoints: latency-svc-pqkbr [751.857471ms]
    Mar  9 16:41:26.791: INFO: Created: latency-svc-g5cf2
    Mar  9 16:41:26.827: INFO: Got endpoints: latency-svc-cb9qg [750.425437ms]
    Mar  9 16:41:26.844: INFO: Created: latency-svc-v568w
    Mar  9 16:41:26.879: INFO: Got endpoints: latency-svc-6tgmr [749.8524ms]
    Mar  9 16:41:26.898: INFO: Created: latency-svc-bkwxx
    Mar  9 16:41:26.927: INFO: Got endpoints: latency-svc-w9m6r [750.361922ms]
    Mar  9 16:41:26.943: INFO: Created: latency-svc-jzxwf
    Mar  9 16:41:26.977: INFO: Got endpoints: latency-svc-hp2b5 [750.402164ms]
    Mar  9 16:41:26.991: INFO: Created: latency-svc-xflzf
    Mar  9 16:41:27.027: INFO: Got endpoints: latency-svc-ck5gp [749.267261ms]
    Mar  9 16:41:27.041: INFO: Created: latency-svc-cxsl6
    Mar  9 16:41:27.081: INFO: Got endpoints: latency-svc-n9qml [754.156117ms]
    Mar  9 16:41:27.094: INFO: Created: latency-svc-fng8r
    Mar  9 16:41:27.126: INFO: Got endpoints: latency-svc-g98rn [748.506292ms]
    Mar  9 16:41:27.140: INFO: Created: latency-svc-dtqpl
    Mar  9 16:41:27.176: INFO: Got endpoints: latency-svc-9rq9l [749.754871ms]
    Mar  9 16:41:27.194: INFO: Created: latency-svc-n67wj
    Mar  9 16:41:27.228: INFO: Got endpoints: latency-svc-8z58k [752.079637ms]
    Mar  9 16:41:27.241: INFO: Created: latency-svc-j6rz7
    Mar  9 16:41:27.276: INFO: Got endpoints: latency-svc-rh9l2 [749.265199ms]
    Mar  9 16:41:27.326: INFO: Got endpoints: latency-svc-zfnrk [746.668326ms]
    Mar  9 16:41:27.377: INFO: Got endpoints: latency-svc-tsskk [748.129188ms]
    Mar  9 16:41:27.426: INFO: Got endpoints: latency-svc-68x9c [746.173738ms]
    Mar  9 16:41:27.478: INFO: Got endpoints: latency-svc-88wm8 [749.149827ms]
    Mar  9 16:41:27.527: INFO: Got endpoints: latency-svc-g5cf2 [749.112183ms]
    Mar  9 16:41:27.577: INFO: Got endpoints: latency-svc-v568w [749.868639ms]
    Mar  9 16:41:27.627: INFO: Got endpoints: latency-svc-bkwxx [747.756271ms]
    Mar  9 16:41:27.677: INFO: Got endpoints: latency-svc-jzxwf [749.962419ms]
    Mar  9 16:41:27.727: INFO: Got endpoints: latency-svc-xflzf [750.144284ms]
    Mar  9 16:41:27.779: INFO: Got endpoints: latency-svc-cxsl6 [751.672815ms]
    Mar  9 16:41:27.826: INFO: Got endpoints: latency-svc-fng8r [745.717228ms]
    Mar  9 16:41:27.879: INFO: Got endpoints: latency-svc-dtqpl [752.821285ms]
    Mar  9 16:41:27.927: INFO: Got endpoints: latency-svc-n67wj [750.15815ms]
    Mar  9 16:41:27.977: INFO: Got endpoints: latency-svc-j6rz7 [748.253078ms]
    Mar  9 16:41:27.977: INFO: Latencies: [27.640179ms 34.837742ms 49.608156ms 66.309697ms 81.000349ms 93.680157ms 109.689836ms 120.056988ms 132.016316ms 145.06548ms 154.680045ms 166.257238ms 170.767291ms 175.729517ms 178.978519ms 179.709005ms 180.965664ms 183.651573ms 185.233521ms 185.24278ms 185.861785ms 186.106974ms 186.565022ms 187.448124ms 188.231382ms 188.844177ms 190.883859ms 194.076664ms 195.773068ms 197.869926ms 201.876447ms 205.116034ms 205.546211ms 210.015785ms 211.079703ms 220.870355ms 245.943819ms 249.590196ms 256.098657ms 256.492452ms 257.651484ms 298.767081ms 333.215965ms 373.569698ms 412.491876ms 456.264704ms 479.765628ms 511.170193ms 537.841294ms 575.242417ms 598.085902ms 613.449371ms 640.720057ms 675.277373ms 706.541616ms 713.189252ms 741.851638ms 742.445829ms 743.362361ms 743.675176ms 744.574871ms 745.532261ms 745.621874ms 745.717228ms 745.72039ms 745.738845ms 745.782743ms 746.113393ms 746.123035ms 746.173738ms 746.569136ms 746.595201ms 746.668326ms 746.684942ms 746.686362ms 746.903251ms 746.939723ms 747.248144ms 747.296054ms 747.380642ms 747.394901ms 747.51961ms 747.756271ms 747.838673ms 748.003677ms 748.011978ms 748.051696ms 748.129188ms 748.133108ms 748.15109ms 748.243066ms 748.253078ms 748.337862ms 748.42029ms 748.506292ms 748.6426ms 748.843682ms 748.882452ms 748.960947ms 749.001973ms 749.003595ms 749.015696ms 749.034236ms 749.083761ms 749.098489ms 749.112183ms 749.120819ms 749.138712ms 749.142877ms 749.149827ms 749.160648ms 749.234701ms 749.253404ms 749.265199ms 749.267261ms 749.313343ms 749.378844ms 749.410154ms 749.467733ms 749.493683ms 749.59284ms 749.697231ms 749.71143ms 749.711675ms 749.716271ms 749.727462ms 749.734565ms 749.754871ms 749.765646ms 749.8524ms 749.868639ms 749.962419ms 750.010441ms 750.026943ms 750.034485ms 750.144284ms 750.15815ms 750.167795ms 750.214544ms 750.222552ms 750.249808ms 750.253713ms 750.288208ms 750.297482ms 750.30714ms 750.309964ms 750.361922ms 750.368526ms 750.392235ms 750.394379ms 750.402164ms 750.425437ms 750.466476ms 750.548154ms 750.563091ms 750.679047ms 750.682483ms 750.693138ms 750.703235ms 750.7597ms 750.856478ms 750.875233ms 750.94679ms 750.993173ms 751.104587ms 751.106025ms 751.116857ms 751.233502ms 751.340012ms 751.516965ms 751.618225ms 751.635346ms 751.672815ms 751.8153ms 751.85603ms 751.857471ms 751.962823ms 752.079637ms 752.222815ms 752.325116ms 752.377804ms 752.420753ms 752.693019ms 752.821285ms 752.845359ms 753.091534ms 753.097944ms 753.236095ms 753.253607ms 753.83292ms 753.890733ms 753.910573ms 754.156117ms 755.190006ms 755.413131ms 755.763735ms 756.580264ms 756.804058ms 758.745506ms 794.670123ms]
    Mar  9 16:41:27.977: INFO: 50 %ile: 749.003595ms
    Mar  9 16:41:27.977: INFO: 90 %ile: 752.377804ms
    Mar  9 16:41:27.977: INFO: 99 %ile: 758.745506ms
    Mar  9 16:41:27.977: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Mar  9 16:41:27.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5686" for this suite. 03/09/23 16:41:27.984
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:41:27.99
Mar  9 16:41:27.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename cronjob 03/09/23 16:41:27.992
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:28.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:28.005
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/09/23 16:41:28.008
STEP: Ensuring more than one job is running at a time 03/09/23 16:41:28.013
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/09/23 16:43:02.017
STEP: Removing cronjob 03/09/23 16:43:02.02
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  9 16:43:02.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8740" for this suite. 03/09/23 16:43:02.029
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":167,"skipped":3287,"failed":0}
------------------------------
â€¢ [SLOW TEST] [94.046 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:41:27.99
    Mar  9 16:41:27.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename cronjob 03/09/23 16:41:27.992
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:41:28.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:41:28.005
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/09/23 16:41:28.008
    STEP: Ensuring more than one job is running at a time 03/09/23 16:41:28.013
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/09/23 16:43:02.017
    STEP: Removing cronjob 03/09/23 16:43:02.02
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  9 16:43:02.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8740" for this suite. 03/09/23 16:43:02.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:43:02.039
Mar  9 16:43:02.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:43:02.04
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:02.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:02.054
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 03/09/23 16:43:02.058
Mar  9 16:43:02.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3827 api-versions'
Mar  9 16:43:02.130: INFO: stderr: ""
Mar  9 16:43:02.130: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:43:02.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3827" for this suite. 03/09/23 16:43:02.134
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":168,"skipped":3325,"failed":0}
------------------------------
â€¢ [0.100 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:43:02.039
    Mar  9 16:43:02.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:43:02.04
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:02.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:02.054
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 03/09/23 16:43:02.058
    Mar  9 16:43:02.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-3827 api-versions'
    Mar  9 16:43:02.130: INFO: stderr: ""
    Mar  9 16:43:02.130: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:43:02.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3827" for this suite. 03/09/23 16:43:02.134
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:43:02.139
Mar  9 16:43:02.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:43:02.14
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:02.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:02.155
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 03/09/23 16:43:02.158
STEP: Creating a ResourceQuota 03/09/23 16:43:07.161
STEP: Ensuring resource quota status is calculated 03/09/23 16:43:07.167
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:43:09.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-70" for this suite. 03/09/23 16:43:09.174
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":169,"skipped":3329,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.041 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:43:02.139
    Mar  9 16:43:02.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:43:02.14
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:02.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:02.155
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 03/09/23 16:43:02.158
    STEP: Creating a ResourceQuota 03/09/23 16:43:07.161
    STEP: Ensuring resource quota status is calculated 03/09/23 16:43:07.167
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:43:09.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-70" for this suite. 03/09/23 16:43:09.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:43:09.181
Mar  9 16:43:09.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:43:09.182
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:09.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:09.2
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:43:09.213
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:43:09.553
STEP: Deploying the webhook pod 03/09/23 16:43:09.56
STEP: Wait for the deployment to be ready 03/09/23 16:43:09.571
Mar  9 16:43:09.579: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:43:11.588
STEP: Verifying the service has paired with the endpoint 03/09/23 16:43:11.607
Mar  9 16:43:12.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 03/09/23 16:43:12.664
STEP: Creating a configMap that should be mutated 03/09/23 16:43:12.677
STEP: Deleting the collection of validation webhooks 03/09/23 16:43:12.704
STEP: Creating a configMap that should not be mutated 03/09/23 16:43:12.74
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:43:12.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1672" for this suite. 03/09/23 16:43:12.751
STEP: Destroying namespace "webhook-1672-markers" for this suite. 03/09/23 16:43:12.755
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":170,"skipped":3342,"failed":0}
------------------------------
â€¢ [3.616 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:43:09.181
    Mar  9 16:43:09.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:43:09.182
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:09.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:09.2
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:43:09.213
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:43:09.553
    STEP: Deploying the webhook pod 03/09/23 16:43:09.56
    STEP: Wait for the deployment to be ready 03/09/23 16:43:09.571
    Mar  9 16:43:09.579: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:43:11.588
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:43:11.607
    Mar  9 16:43:12.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 03/09/23 16:43:12.664
    STEP: Creating a configMap that should be mutated 03/09/23 16:43:12.677
    STEP: Deleting the collection of validation webhooks 03/09/23 16:43:12.704
    STEP: Creating a configMap that should not be mutated 03/09/23 16:43:12.74
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:43:12.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1672" for this suite. 03/09/23 16:43:12.751
    STEP: Destroying namespace "webhook-1672-markers" for this suite. 03/09/23 16:43:12.755
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:43:12.803
Mar  9 16:43:12.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:43:12.804
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:12.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:12.822
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:43:12.827
Mar  9 16:43:12.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87" in namespace "downward-api-8363" to be "Succeeded or Failed"
Mar  9 16:43:12.838: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430239ms
Mar  9 16:43:14.842: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87": Phase="Running", Reason="", readiness=false. Elapsed: 2.007306219s
Mar  9 16:43:16.844: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009509464s
STEP: Saw pod success 03/09/23 16:43:16.844
Mar  9 16:43:16.844: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87" satisfied condition "Succeeded or Failed"
Mar  9 16:43:16.846: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87 container client-container: <nil>
STEP: delete the pod 03/09/23 16:43:16.859
Mar  9 16:43:16.869: INFO: Waiting for pod downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87 to disappear
Mar  9 16:43:16.871: INFO: Pod downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 16:43:16.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8363" for this suite. 03/09/23 16:43:16.875
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":171,"skipped":3433,"failed":0}
------------------------------
â€¢ [4.077 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:43:12.803
    Mar  9 16:43:12.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:43:12.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:12.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:12.822
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:43:12.827
    Mar  9 16:43:12.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87" in namespace "downward-api-8363" to be "Succeeded or Failed"
    Mar  9 16:43:12.838: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430239ms
    Mar  9 16:43:14.842: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87": Phase="Running", Reason="", readiness=false. Elapsed: 2.007306219s
    Mar  9 16:43:16.844: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009509464s
    STEP: Saw pod success 03/09/23 16:43:16.844
    Mar  9 16:43:16.844: INFO: Pod "downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87" satisfied condition "Succeeded or Failed"
    Mar  9 16:43:16.846: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87 container client-container: <nil>
    STEP: delete the pod 03/09/23 16:43:16.859
    Mar  9 16:43:16.869: INFO: Waiting for pod downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87 to disappear
    Mar  9 16:43:16.871: INFO: Pod downwardapi-volume-689c262d-840a-4599-824b-1b5320d6ea87 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 16:43:16.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8363" for this suite. 03/09/23 16:43:16.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:43:16.884
Mar  9 16:43:16.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:43:16.885
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:16.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:16.899
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-1561bea9-f1cb-4122-a590-35ada0550a5a 03/09/23 16:43:16.902
STEP: Creating a pod to test consume configMaps 03/09/23 16:43:16.906
Mar  9 16:43:16.913: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6" in namespace "projected-7433" to be "Succeeded or Failed"
Mar  9 16:43:16.916: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.46279ms
Mar  9 16:43:18.919: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005523218s
Mar  9 16:43:20.921: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007889461s
STEP: Saw pod success 03/09/23 16:43:20.921
Mar  9 16:43:20.921: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6" satisfied condition "Succeeded or Failed"
Mar  9 16:43:20.925: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6 container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/09/23 16:43:20.93
Mar  9 16:43:20.943: INFO: Waiting for pod pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6 to disappear
Mar  9 16:43:20.945: INFO: Pod pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 16:43:20.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7433" for this suite. 03/09/23 16:43:20.949
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":172,"skipped":3504,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:43:16.884
    Mar  9 16:43:16.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:43:16.885
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:16.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:16.899
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-1561bea9-f1cb-4122-a590-35ada0550a5a 03/09/23 16:43:16.902
    STEP: Creating a pod to test consume configMaps 03/09/23 16:43:16.906
    Mar  9 16:43:16.913: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6" in namespace "projected-7433" to be "Succeeded or Failed"
    Mar  9 16:43:16.916: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.46279ms
    Mar  9 16:43:18.919: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005523218s
    Mar  9 16:43:20.921: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007889461s
    STEP: Saw pod success 03/09/23 16:43:20.921
    Mar  9 16:43:20.921: INFO: Pod "pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6" satisfied condition "Succeeded or Failed"
    Mar  9 16:43:20.925: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:43:20.93
    Mar  9 16:43:20.943: INFO: Waiting for pod pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6 to disappear
    Mar  9 16:43:20.945: INFO: Pod pod-projected-configmaps-8151253b-bbe1-4f92-9741-653e2f5d8cc6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 16:43:20.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7433" for this suite. 03/09/23 16:43:20.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:43:20.955
Mar  9 16:43:20.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:43:20.956
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:20.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:20.971
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 03/09/23 16:43:20.974
STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:43:20.978
STEP: Creating a ResourceQuota with not terminating scope 03/09/23 16:43:22.982
STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:43:22.987
STEP: Creating a long running pod 03/09/23 16:43:24.991
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/09/23 16:43:25.003
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/09/23 16:43:27.008
STEP: Deleting the pod 03/09/23 16:43:29.012
STEP: Ensuring resource quota status released the pod usage 03/09/23 16:43:29.021
STEP: Creating a terminating pod 03/09/23 16:43:31.026
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/09/23 16:43:31.036
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/09/23 16:43:33.04
STEP: Deleting the pod 03/09/23 16:43:35.045
STEP: Ensuring resource quota status released the pod usage 03/09/23 16:43:35.056
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:43:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5805" for this suite. 03/09/23 16:43:37.063
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":173,"skipped":3512,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.113 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:43:20.955
    Mar  9 16:43:20.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:43:20.956
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:20.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:20.971
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 03/09/23 16:43:20.974
    STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:43:20.978
    STEP: Creating a ResourceQuota with not terminating scope 03/09/23 16:43:22.982
    STEP: Ensuring ResourceQuota status is calculated 03/09/23 16:43:22.987
    STEP: Creating a long running pod 03/09/23 16:43:24.991
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/09/23 16:43:25.003
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/09/23 16:43:27.008
    STEP: Deleting the pod 03/09/23 16:43:29.012
    STEP: Ensuring resource quota status released the pod usage 03/09/23 16:43:29.021
    STEP: Creating a terminating pod 03/09/23 16:43:31.026
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/09/23 16:43:31.036
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/09/23 16:43:33.04
    STEP: Deleting the pod 03/09/23 16:43:35.045
    STEP: Ensuring resource quota status released the pod usage 03/09/23 16:43:35.056
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:43:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5805" for this suite. 03/09/23 16:43:37.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:43:37.069
Mar  9 16:43:37.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 16:43:37.071
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:37.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:37.084
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/09/23 16:43:37.087
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
 03/09/23 16:43:37.094
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
 03/09/23 16:43:37.094
STEP: creating a pod to probe DNS 03/09/23 16:43:37.094
STEP: submitting the pod to kubernetes 03/09/23 16:43:37.094
Mar  9 16:43:37.101: INFO: Waiting up to 15m0s for pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a" in namespace "dns-6110" to be "running"
Mar  9 16:43:37.104: INFO: Pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208086ms
Mar  9 16:43:39.108: INFO: Pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007061153s
Mar  9 16:43:39.108: INFO: Pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a" satisfied condition "running"
STEP: retrieving the pod 03/09/23 16:43:39.108
STEP: looking for the results for each expected name from probers 03/09/23 16:43:39.11
Mar  9 16:43:39.117: INFO: DNS probes using dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a succeeded

STEP: deleting the pod 03/09/23 16:43:39.117
STEP: changing the externalName to bar.example.com 03/09/23 16:43:39.126
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
 03/09/23 16:43:39.136
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
 03/09/23 16:43:39.137
STEP: creating a second pod to probe DNS 03/09/23 16:43:39.137
STEP: submitting the pod to kubernetes 03/09/23 16:43:39.139
Mar  9 16:43:39.145: INFO: Waiting up to 15m0s for pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f" in namespace "dns-6110" to be "running"
Mar  9 16:43:39.148: INFO: Pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.502927ms
Mar  9 16:43:41.152: INFO: Pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007142204s
Mar  9 16:43:41.153: INFO: Pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f" satisfied condition "running"
STEP: retrieving the pod 03/09/23 16:43:41.153
STEP: looking for the results for each expected name from probers 03/09/23 16:43:41.155
Mar  9 16:43:41.159: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:41.162: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:41.162: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

Mar  9 16:43:46.169: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:46.172: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:46.172: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

Mar  9 16:43:51.167: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:51.170: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:51.170: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

Mar  9 16:43:56.167: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:56.170: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:43:56.170: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

Mar  9 16:44:01.168: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:44:01.171: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:44:01.171: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

Mar  9 16:44:06.168: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:44:06.172: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  9 16:44:06.172: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

Mar  9 16:44:11.171: INFO: DNS probes using dns-test-a89a3659-41b6-462e-b89e-eae80907b87f succeeded

STEP: deleting the pod 03/09/23 16:44:11.171
STEP: changing the service to type=ClusterIP 03/09/23 16:44:11.18
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
 03/09/23 16:44:11.205
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
 03/09/23 16:44:11.205
STEP: creating a third pod to probe DNS 03/09/23 16:44:11.205
STEP: submitting the pod to kubernetes 03/09/23 16:44:11.207
Mar  9 16:44:11.219: INFO: Waiting up to 15m0s for pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a" in namespace "dns-6110" to be "running"
Mar  9 16:44:11.221: INFO: Pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.553115ms
Mar  9 16:44:13.226: INFO: Pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007367379s
Mar  9 16:44:13.226: INFO: Pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a" satisfied condition "running"
STEP: retrieving the pod 03/09/23 16:44:13.226
STEP: looking for the results for each expected name from probers 03/09/23 16:44:13.229
Mar  9 16:44:13.236: INFO: DNS probes using dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a succeeded

STEP: deleting the pod 03/09/23 16:44:13.236
STEP: deleting the test externalName service 03/09/23 16:44:13.245
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 16:44:13.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6110" for this suite. 03/09/23 16:44:13.265
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":174,"skipped":3527,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.202 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:43:37.069
    Mar  9 16:43:37.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 16:43:37.071
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:43:37.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:43:37.084
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/09/23 16:43:37.087
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
     03/09/23 16:43:37.094
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
     03/09/23 16:43:37.094
    STEP: creating a pod to probe DNS 03/09/23 16:43:37.094
    STEP: submitting the pod to kubernetes 03/09/23 16:43:37.094
    Mar  9 16:43:37.101: INFO: Waiting up to 15m0s for pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a" in namespace "dns-6110" to be "running"
    Mar  9 16:43:37.104: INFO: Pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208086ms
    Mar  9 16:43:39.108: INFO: Pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007061153s
    Mar  9 16:43:39.108: INFO: Pod "dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 16:43:39.108
    STEP: looking for the results for each expected name from probers 03/09/23 16:43:39.11
    Mar  9 16:43:39.117: INFO: DNS probes using dns-test-495ed1ac-ae52-4f78-8cf8-a908332bcd6a succeeded

    STEP: deleting the pod 03/09/23 16:43:39.117
    STEP: changing the externalName to bar.example.com 03/09/23 16:43:39.126
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
     03/09/23 16:43:39.136
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
     03/09/23 16:43:39.137
    STEP: creating a second pod to probe DNS 03/09/23 16:43:39.137
    STEP: submitting the pod to kubernetes 03/09/23 16:43:39.139
    Mar  9 16:43:39.145: INFO: Waiting up to 15m0s for pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f" in namespace "dns-6110" to be "running"
    Mar  9 16:43:39.148: INFO: Pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.502927ms
    Mar  9 16:43:41.152: INFO: Pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007142204s
    Mar  9 16:43:41.153: INFO: Pod "dns-test-a89a3659-41b6-462e-b89e-eae80907b87f" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 16:43:41.153
    STEP: looking for the results for each expected name from probers 03/09/23 16:43:41.155
    Mar  9 16:43:41.159: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:41.162: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:41.162: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

    Mar  9 16:43:46.169: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:46.172: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:46.172: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

    Mar  9 16:43:51.167: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:51.170: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:51.170: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

    Mar  9 16:43:56.167: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:56.170: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:43:56.170: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

    Mar  9 16:44:01.168: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:44:01.171: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:44:01.171: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

    Mar  9 16:44:06.168: INFO: File wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:44:06.172: INFO: File jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local from pod  dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  9 16:44:06.172: INFO: Lookups using dns-6110/dns-test-a89a3659-41b6-462e-b89e-eae80907b87f failed for: [wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local]

    Mar  9 16:44:11.171: INFO: DNS probes using dns-test-a89a3659-41b6-462e-b89e-eae80907b87f succeeded

    STEP: deleting the pod 03/09/23 16:44:11.171
    STEP: changing the service to type=ClusterIP 03/09/23 16:44:11.18
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
     03/09/23 16:44:11.205
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6110.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6110.svc.cluster.local; sleep 1; done
     03/09/23 16:44:11.205
    STEP: creating a third pod to probe DNS 03/09/23 16:44:11.205
    STEP: submitting the pod to kubernetes 03/09/23 16:44:11.207
    Mar  9 16:44:11.219: INFO: Waiting up to 15m0s for pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a" in namespace "dns-6110" to be "running"
    Mar  9 16:44:11.221: INFO: Pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.553115ms
    Mar  9 16:44:13.226: INFO: Pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007367379s
    Mar  9 16:44:13.226: INFO: Pod "dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 16:44:13.226
    STEP: looking for the results for each expected name from probers 03/09/23 16:44:13.229
    Mar  9 16:44:13.236: INFO: DNS probes using dns-test-836a6000-cd04-4ba1-8fc6-a043eda3945a succeeded

    STEP: deleting the pod 03/09/23 16:44:13.236
    STEP: deleting the test externalName service 03/09/23 16:44:13.245
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 16:44:13.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6110" for this suite. 03/09/23 16:44:13.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:44:13.274
Mar  9 16:44:13.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename namespaces 03/09/23 16:44:13.278
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:13.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:13.292
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 03/09/23 16:44:13.297
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:13.308
STEP: Creating a service in the namespace 03/09/23 16:44:13.311
STEP: Deleting the namespace 03/09/23 16:44:13.324
STEP: Waiting for the namespace to be removed. 03/09/23 16:44:13.333
STEP: Recreating the namespace 03/09/23 16:44:19.336
STEP: Verifying there is no service in the namespace 03/09/23 16:44:19.349
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:44:19.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7931" for this suite. 03/09/23 16:44:19.355
STEP: Destroying namespace "nsdeletetest-3420" for this suite. 03/09/23 16:44:19.359
Mar  9 16:44:19.362: INFO: Namespace nsdeletetest-3420 was already deleted
STEP: Destroying namespace "nsdeletetest-2949" for this suite. 03/09/23 16:44:19.362
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":175,"skipped":3559,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.092 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:44:13.274
    Mar  9 16:44:13.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename namespaces 03/09/23 16:44:13.278
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:13.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:13.292
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 03/09/23 16:44:13.297
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:13.308
    STEP: Creating a service in the namespace 03/09/23 16:44:13.311
    STEP: Deleting the namespace 03/09/23 16:44:13.324
    STEP: Waiting for the namespace to be removed. 03/09/23 16:44:13.333
    STEP: Recreating the namespace 03/09/23 16:44:19.336
    STEP: Verifying there is no service in the namespace 03/09/23 16:44:19.349
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:44:19.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7931" for this suite. 03/09/23 16:44:19.355
    STEP: Destroying namespace "nsdeletetest-3420" for this suite. 03/09/23 16:44:19.359
    Mar  9 16:44:19.362: INFO: Namespace nsdeletetest-3420 was already deleted
    STEP: Destroying namespace "nsdeletetest-2949" for this suite. 03/09/23 16:44:19.362
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:44:19.366
Mar  9 16:44:19.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:44:19.368
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:19.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:19.384
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 03/09/23 16:44:19.387
Mar  9 16:44:19.395: INFO: Waiting up to 5m0s for pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d" in namespace "downward-api-6903" to be "Succeeded or Failed"
Mar  9 16:44:19.399: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67052ms
Mar  9 16:44:21.403: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007457995s
Mar  9 16:44:23.403: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008177092s
STEP: Saw pod success 03/09/23 16:44:23.404
Mar  9 16:44:23.404: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d" satisfied condition "Succeeded or Failed"
Mar  9 16:44:23.406: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d container dapi-container: <nil>
STEP: delete the pod 03/09/23 16:44:23.413
Mar  9 16:44:23.422: INFO: Waiting for pod downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d to disappear
Mar  9 16:44:23.424: INFO: Pod downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  9 16:44:23.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6903" for this suite. 03/09/23 16:44:23.428
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":176,"skipped":3560,"failed":0}
------------------------------
â€¢ [4.066 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:44:19.366
    Mar  9 16:44:19.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:44:19.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:19.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:19.384
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 03/09/23 16:44:19.387
    Mar  9 16:44:19.395: INFO: Waiting up to 5m0s for pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d" in namespace "downward-api-6903" to be "Succeeded or Failed"
    Mar  9 16:44:19.399: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67052ms
    Mar  9 16:44:21.403: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007457995s
    Mar  9 16:44:23.403: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008177092s
    STEP: Saw pod success 03/09/23 16:44:23.404
    Mar  9 16:44:23.404: INFO: Pod "downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d" satisfied condition "Succeeded or Failed"
    Mar  9 16:44:23.406: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d container dapi-container: <nil>
    STEP: delete the pod 03/09/23 16:44:23.413
    Mar  9 16:44:23.422: INFO: Waiting for pod downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d to disappear
    Mar  9 16:44:23.424: INFO: Pod downward-api-30edfdd5-0c6d-45e8-befe-02c1d250125d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  9 16:44:23.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6903" for this suite. 03/09/23 16:44:23.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:44:23.433
Mar  9 16:44:23.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename watch 03/09/23 16:44:23.435
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:23.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:23.448
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/09/23 16:44:23.451
STEP: creating a new configmap 03/09/23 16:44:23.453
STEP: modifying the configmap once 03/09/23 16:44:23.457
STEP: closing the watch once it receives two notifications 03/09/23 16:44:23.463
Mar  9 16:44:23.463: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106239 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 16:44:23.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106240 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/09/23 16:44:23.463
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/09/23 16:44:23.469
STEP: deleting the configmap 03/09/23 16:44:23.471
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/09/23 16:44:23.475
Mar  9 16:44:23.475: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106241 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 16:44:23.475: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106242 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  9 16:44:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5563" for this suite. 03/09/23 16:44:23.479
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":177,"skipped":3565,"failed":0}
------------------------------
â€¢ [0.051 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:44:23.433
    Mar  9 16:44:23.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename watch 03/09/23 16:44:23.435
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:23.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:23.448
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/09/23 16:44:23.451
    STEP: creating a new configmap 03/09/23 16:44:23.453
    STEP: modifying the configmap once 03/09/23 16:44:23.457
    STEP: closing the watch once it receives two notifications 03/09/23 16:44:23.463
    Mar  9 16:44:23.463: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106239 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 16:44:23.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106240 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/09/23 16:44:23.463
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/09/23 16:44:23.469
    STEP: deleting the configmap 03/09/23 16:44:23.471
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/09/23 16:44:23.475
    Mar  9 16:44:23.475: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106241 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 16:44:23.475: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5563  be7f3bf2-85ab-47cd-981f-e2a41349a6f3 106242 0 2023-03-09 16:44:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-09 16:44:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  9 16:44:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5563" for this suite. 03/09/23 16:44:23.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:44:23.485
Mar  9 16:44:23.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:44:23.487
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:23.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:23.5
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:44:23.514
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:44:23.893
STEP: Deploying the webhook pod 03/09/23 16:44:23.9
STEP: Wait for the deployment to be ready 03/09/23 16:44:23.91
Mar  9 16:44:23.917: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:44:25.926
STEP: Verifying the service has paired with the endpoint 03/09/23 16:44:25.938
Mar  9 16:44:26.939: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/09/23 16:44:26.942
STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:26.943
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/09/23 16:44:26.957
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/09/23 16:44:27.965
STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:27.965
STEP: Having no error when timeout is longer than webhook latency 03/09/23 16:44:28.988
STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:28.988
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/09/23 16:44:34.02
STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:34.02
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:44:39.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5018" for this suite. 03/09/23 16:44:39.051
STEP: Destroying namespace "webhook-5018-markers" for this suite. 03/09/23 16:44:39.055
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":178,"skipped":3572,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.628 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:44:23.485
    Mar  9 16:44:23.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:44:23.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:23.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:23.5
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:44:23.514
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:44:23.893
    STEP: Deploying the webhook pod 03/09/23 16:44:23.9
    STEP: Wait for the deployment to be ready 03/09/23 16:44:23.91
    Mar  9 16:44:23.917: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:44:25.926
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:44:25.938
    Mar  9 16:44:26.939: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/09/23 16:44:26.942
    STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:26.943
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/09/23 16:44:26.957
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/09/23 16:44:27.965
    STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:27.965
    STEP: Having no error when timeout is longer than webhook latency 03/09/23 16:44:28.988
    STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:28.988
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/09/23 16:44:34.02
    STEP: Registering slow webhook via the AdmissionRegistration API 03/09/23 16:44:34.02
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:44:39.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5018" for this suite. 03/09/23 16:44:39.051
    STEP: Destroying namespace "webhook-5018-markers" for this suite. 03/09/23 16:44:39.055
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:44:39.114
Mar  9 16:44:39.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:44:39.115
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:39.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:39.138
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:44:39.163
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:44:39.68
STEP: Deploying the webhook pod 03/09/23 16:44:39.684
STEP: Wait for the deployment to be ready 03/09/23 16:44:39.694
Mar  9 16:44:39.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:44:41.709
STEP: Verifying the service has paired with the endpoint 03/09/23 16:44:41.722
Mar  9 16:44:42.723: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/09/23 16:44:42.726
STEP: create a pod that should be updated by the webhook 03/09/23 16:44:42.742
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:44:42.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4993" for this suite. 03/09/23 16:44:42.769
STEP: Destroying namespace "webhook-4993-markers" for this suite. 03/09/23 16:44:42.775
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":179,"skipped":3577,"failed":0}
------------------------------
â€¢ [3.703 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:44:39.114
    Mar  9 16:44:39.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:44:39.115
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:39.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:39.138
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:44:39.163
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:44:39.68
    STEP: Deploying the webhook pod 03/09/23 16:44:39.684
    STEP: Wait for the deployment to be ready 03/09/23 16:44:39.694
    Mar  9 16:44:39.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:44:41.709
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:44:41.722
    Mar  9 16:44:42.723: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/09/23 16:44:42.726
    STEP: create a pod that should be updated by the webhook 03/09/23 16:44:42.742
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:44:42.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4993" for this suite. 03/09/23 16:44:42.769
    STEP: Destroying namespace "webhook-4993-markers" for this suite. 03/09/23 16:44:42.775
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:44:42.818
Mar  9 16:44:42.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:44:42.819
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:42.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:42.837
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/09/23 16:44:42.842
Mar  9 16:44:42.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:44:45.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:45:03.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7265" for this suite. 03/09/23 16:45:03.43
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":180,"skipped":3588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.617 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:44:42.818
    Mar  9 16:44:42.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:44:42.819
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:44:42.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:44:42.837
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/09/23 16:44:42.842
    Mar  9 16:44:42.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:44:45.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:45:03.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7265" for this suite. 03/09/23 16:45:03.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:45:03.437
Mar  9 16:45:03.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 16:45:03.438
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:03.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:03.453
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar  9 16:45:03.462: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  9 16:45:08.466: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/09/23 16:45:08.466
Mar  9 16:45:08.466: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  9 16:45:10.471: INFO: Creating deployment "test-rollover-deployment"
Mar  9 16:45:10.477: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  9 16:45:12.485: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  9 16:45:12.490: INFO: Ensure that both replica sets have 1 created replica
Mar  9 16:45:12.494: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  9 16:45:12.503: INFO: Updating deployment test-rollover-deployment
Mar  9 16:45:12.503: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  9 16:45:14.509: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  9 16:45:14.515: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  9 16:45:14.520: INFO: all replica sets need to contain the pod-template-hash label
Mar  9 16:45:14.520: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 16:45:16.528: INFO: all replica sets need to contain the pod-template-hash label
Mar  9 16:45:16.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 16:45:18.527: INFO: all replica sets need to contain the pod-template-hash label
Mar  9 16:45:18.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 16:45:20.528: INFO: all replica sets need to contain the pod-template-hash label
Mar  9 16:45:20.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 16:45:22.528: INFO: all replica sets need to contain the pod-template-hash label
Mar  9 16:45:22.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 16:45:24.528: INFO: 
Mar  9 16:45:24.528: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 16:45:24.536: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5243  561e2395-aa39-404d-8886-d4ab93940655 106648 2 2023-03-09 16:45:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d9bf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:45:10 +0000 UTC,LastTransitionTime:2023-03-09 16:45:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-09 16:45:24 +0000 UTC,LastTransitionTime:2023-03-09 16:45:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  9 16:45:24.539: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5243  7b1e43b3-db84-4a21-bff3-67d16fad7264 106638 2 2023-03-09 16:45:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 561e2395-aa39-404d-8886-d4ab93940655 0xc007dec537 0xc007dec538}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"561e2395-aa39-404d-8886-d4ab93940655\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007dec5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:45:24.539: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  9 16:45:24.539: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5243  be33f137-b453-4f25-8720-f009a134acb1 106647 2 2023-03-09 16:45:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 561e2395-aa39-404d-8886-d4ab93940655 0xc007dec2e7 0xc007dec2e8}] [] [{e2e.test Update apps/v1 2023-03-09 16:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"561e2395-aa39-404d-8886-d4ab93940655\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007dec3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:45:24.539: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5243  29d74354-f1cd-4618-9a47-8a96528b645b 106597 2 2023-03-09 16:45:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 561e2395-aa39-404d-8886-d4ab93940655 0xc007dec417 0xc007dec418}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"561e2395-aa39-404d-8886-d4ab93940655\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007dec4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:45:24.542: INFO: Pod "test-rollover-deployment-6d45fd857b-dwdfn" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-dwdfn test-rollover-deployment-6d45fd857b- deployment-5243  0cba21b5-ea96-4e4f-a89a-8685ac403ad3 106613 0 2023-03-09 16:45:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:8a834472746c81c8de600494791b3b8811b92300b0dd1e0a4a4fd05777eef15c cni.projectcalico.org/podIP:10.244.42.242/32 cni.projectcalico.org/podIPs:10.244.42.242/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 7b1e43b3-db84-4a21-bff3-67d16fad7264 0xc007afbb37 0xc007afbb38}] [] [{kube-controller-manager Update v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1e43b3-db84-4a21-bff3-67d16fad7264\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:45:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:45:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bb9rz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bb9rz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.242,StartTime:2023-03-09 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:45:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://901896e0895a8df4ed1dc319b7e9421598ba0589c93de0cd526cf4b87233de20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 16:45:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5243" for this suite. 03/09/23 16:45:24.546
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":181,"skipped":3603,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.115 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:45:03.437
    Mar  9 16:45:03.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 16:45:03.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:03.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:03.453
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar  9 16:45:03.462: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar  9 16:45:08.466: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/09/23 16:45:08.466
    Mar  9 16:45:08.466: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar  9 16:45:10.471: INFO: Creating deployment "test-rollover-deployment"
    Mar  9 16:45:10.477: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar  9 16:45:12.485: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar  9 16:45:12.490: INFO: Ensure that both replica sets have 1 created replica
    Mar  9 16:45:12.494: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar  9 16:45:12.503: INFO: Updating deployment test-rollover-deployment
    Mar  9 16:45:12.503: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar  9 16:45:14.509: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar  9 16:45:14.515: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar  9 16:45:14.520: INFO: all replica sets need to contain the pod-template-hash label
    Mar  9 16:45:14.520: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 16:45:16.528: INFO: all replica sets need to contain the pod-template-hash label
    Mar  9 16:45:16.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 16:45:18.527: INFO: all replica sets need to contain the pod-template-hash label
    Mar  9 16:45:18.527: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 16:45:20.528: INFO: all replica sets need to contain the pod-template-hash label
    Mar  9 16:45:20.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 16:45:22.528: INFO: all replica sets need to contain the pod-template-hash label
    Mar  9 16:45:22.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 16, 45, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 16, 45, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 16:45:24.528: INFO: 
    Mar  9 16:45:24.528: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 16:45:24.536: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5243  561e2395-aa39-404d-8886-d4ab93940655 106648 2 2023-03-09 16:45:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d9bf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:45:10 +0000 UTC,LastTransitionTime:2023-03-09 16:45:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-09 16:45:24 +0000 UTC,LastTransitionTime:2023-03-09 16:45:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  9 16:45:24.539: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5243  7b1e43b3-db84-4a21-bff3-67d16fad7264 106638 2 2023-03-09 16:45:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 561e2395-aa39-404d-8886-d4ab93940655 0xc007dec537 0xc007dec538}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"561e2395-aa39-404d-8886-d4ab93940655\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007dec5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:45:24.539: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar  9 16:45:24.539: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5243  be33f137-b453-4f25-8720-f009a134acb1 106647 2 2023-03-09 16:45:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 561e2395-aa39-404d-8886-d4ab93940655 0xc007dec2e7 0xc007dec2e8}] [] [{e2e.test Update apps/v1 2023-03-09 16:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"561e2395-aa39-404d-8886-d4ab93940655\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007dec3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:45:24.539: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5243  29d74354-f1cd-4618-9a47-8a96528b645b 106597 2 2023-03-09 16:45:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 561e2395-aa39-404d-8886-d4ab93940655 0xc007dec417 0xc007dec418}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"561e2395-aa39-404d-8886-d4ab93940655\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007dec4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:45:24.542: INFO: Pod "test-rollover-deployment-6d45fd857b-dwdfn" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-dwdfn test-rollover-deployment-6d45fd857b- deployment-5243  0cba21b5-ea96-4e4f-a89a-8685ac403ad3 106613 0 2023-03-09 16:45:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:8a834472746c81c8de600494791b3b8811b92300b0dd1e0a4a4fd05777eef15c cni.projectcalico.org/podIP:10.244.42.242/32 cni.projectcalico.org/podIPs:10.244.42.242/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 7b1e43b3-db84-4a21-bff3-67d16fad7264 0xc007afbb37 0xc007afbb38}] [] [{kube-controller-manager Update v1 2023-03-09 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1e43b3-db84-4a21-bff3-67d16fad7264\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:45:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:45:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bb9rz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bb9rz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.242,StartTime:2023-03-09 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:45:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://901896e0895a8df4ed1dc319b7e9421598ba0589c93de0cd526cf4b87233de20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 16:45:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5243" for this suite. 03/09/23 16:45:24.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:45:24.555
Mar  9 16:45:24.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replication-controller 03/09/23 16:45:24.556
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:24.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:24.571
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 03/09/23 16:45:24.574
STEP: When the matched label of one of its pods change 03/09/23 16:45:24.579
Mar  9 16:45:24.582: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  9 16:45:29.586: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/09/23 16:45:29.6
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  9 16:45:30.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2249" for this suite. 03/09/23 16:45:30.615
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":182,"skipped":3653,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.067 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:45:24.555
    Mar  9 16:45:24.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replication-controller 03/09/23 16:45:24.556
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:24.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:24.571
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 03/09/23 16:45:24.574
    STEP: When the matched label of one of its pods change 03/09/23 16:45:24.579
    Mar  9 16:45:24.582: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar  9 16:45:29.586: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/09/23 16:45:29.6
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  9 16:45:30.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2249" for this suite. 03/09/23 16:45:30.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:45:30.623
Mar  9 16:45:30.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename gc 03/09/23 16:45:30.624
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:30.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:30.638
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar  9 16:45:30.674: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"436c9a7a-7f64-4a9b-aaa4-f1c533da8cef", Controller:(*bool)(0xc008199e8a), BlockOwnerDeletion:(*bool)(0xc008199e8b)}}
Mar  9 16:45:30.683: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1b754239-3481-4d7b-818e-379b42f34a07", Controller:(*bool)(0xc0081c80a2), BlockOwnerDeletion:(*bool)(0xc0081c80a3)}}
Mar  9 16:45:30.691: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6f7e3138-514f-4088-a838-92545e734bab", Controller:(*bool)(0xc0081c82d2), BlockOwnerDeletion:(*bool)(0xc0081c82d3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  9 16:45:35.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5824" for this suite. 03/09/23 16:45:35.706
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":183,"skipped":3663,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.089 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:45:30.623
    Mar  9 16:45:30.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename gc 03/09/23 16:45:30.624
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:30.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:30.638
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar  9 16:45:30.674: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"436c9a7a-7f64-4a9b-aaa4-f1c533da8cef", Controller:(*bool)(0xc008199e8a), BlockOwnerDeletion:(*bool)(0xc008199e8b)}}
    Mar  9 16:45:30.683: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1b754239-3481-4d7b-818e-379b42f34a07", Controller:(*bool)(0xc0081c80a2), BlockOwnerDeletion:(*bool)(0xc0081c80a3)}}
    Mar  9 16:45:30.691: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6f7e3138-514f-4088-a838-92545e734bab", Controller:(*bool)(0xc0081c82d2), BlockOwnerDeletion:(*bool)(0xc0081c82d3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  9 16:45:35.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5824" for this suite. 03/09/23 16:45:35.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:45:35.722
Mar  9 16:45:35.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:45:35.723
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:35.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:35.738
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Mar  9 16:45:35.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: creating the pod 03/09/23 16:45:35.742
STEP: submitting the pod to kubernetes 03/09/23 16:45:35.742
Mar  9 16:45:35.750: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053" in namespace "pods-9992" to be "running and ready"
Mar  9 16:45:35.752: INFO: Pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161895ms
Mar  9 16:45:35.752: INFO: The phase of Pod pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:45:37.755: INFO: Pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053": Phase="Running", Reason="", readiness=true. Elapsed: 2.00500411s
Mar  9 16:45:37.755: INFO: The phase of Pod pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053 is Running (Ready = true)
Mar  9 16:45:37.755: INFO: Pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:45:37.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9992" for this suite. 03/09/23 16:45:37.872
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":184,"skipped":3741,"failed":0}
------------------------------
â€¢ [2.155 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:45:35.722
    Mar  9 16:45:35.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:45:35.723
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:35.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:35.738
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Mar  9 16:45:35.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: creating the pod 03/09/23 16:45:35.742
    STEP: submitting the pod to kubernetes 03/09/23 16:45:35.742
    Mar  9 16:45:35.750: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053" in namespace "pods-9992" to be "running and ready"
    Mar  9 16:45:35.752: INFO: Pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161895ms
    Mar  9 16:45:35.752: INFO: The phase of Pod pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:45:37.755: INFO: Pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053": Phase="Running", Reason="", readiness=true. Elapsed: 2.00500411s
    Mar  9 16:45:37.755: INFO: The phase of Pod pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053 is Running (Ready = true)
    Mar  9 16:45:37.755: INFO: Pod "pod-exec-websocket-7e5adc43-77ae-4750-b87f-b2aa77829053" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:45:37.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9992" for this suite. 03/09/23 16:45:37.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:45:37.878
Mar  9 16:45:37.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename subpath 03/09/23 16:45:37.879
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:37.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:37.893
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/09/23 16:45:37.896
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-w7m4 03/09/23 16:45:37.903
STEP: Creating a pod to test atomic-volume-subpath 03/09/23 16:45:37.903
Mar  9 16:45:37.910: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w7m4" in namespace "subpath-3541" to be "Succeeded or Failed"
Mar  9 16:45:37.913: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641778ms
Mar  9 16:45:39.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00656052s
Mar  9 16:45:41.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 4.007279091s
Mar  9 16:45:43.916: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 6.006039373s
Mar  9 16:45:45.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 8.006591728s
Mar  9 16:45:47.916: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 10.005674036s
Mar  9 16:45:49.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 12.006752211s
Mar  9 16:45:51.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 14.006696437s
Mar  9 16:45:53.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 16.006409341s
Mar  9 16:45:55.916: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 18.006064411s
Mar  9 16:45:57.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 20.00685309s
Mar  9 16:45:59.918: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=false. Elapsed: 22.007478707s
Mar  9 16:46:01.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007385253s
STEP: Saw pod success 03/09/23 16:46:01.918
Mar  9 16:46:01.918: INFO: Pod "pod-subpath-test-configmap-w7m4" satisfied condition "Succeeded or Failed"
Mar  9 16:46:01.920: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-configmap-w7m4 container test-container-subpath-configmap-w7m4: <nil>
STEP: delete the pod 03/09/23 16:46:01.935
Mar  9 16:46:01.944: INFO: Waiting for pod pod-subpath-test-configmap-w7m4 to disappear
Mar  9 16:46:01.947: INFO: Pod pod-subpath-test-configmap-w7m4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w7m4 03/09/23 16:46:01.947
Mar  9 16:46:01.948: INFO: Deleting pod "pod-subpath-test-configmap-w7m4" in namespace "subpath-3541"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  9 16:46:01.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3541" for this suite. 03/09/23 16:46:01.954
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":185,"skipped":3767,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.080 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:45:37.878
    Mar  9 16:45:37.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename subpath 03/09/23 16:45:37.879
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:45:37.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:45:37.893
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/09/23 16:45:37.896
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-w7m4 03/09/23 16:45:37.903
    STEP: Creating a pod to test atomic-volume-subpath 03/09/23 16:45:37.903
    Mar  9 16:45:37.910: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w7m4" in namespace "subpath-3541" to be "Succeeded or Failed"
    Mar  9 16:45:37.913: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641778ms
    Mar  9 16:45:39.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00656052s
    Mar  9 16:45:41.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 4.007279091s
    Mar  9 16:45:43.916: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 6.006039373s
    Mar  9 16:45:45.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 8.006591728s
    Mar  9 16:45:47.916: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 10.005674036s
    Mar  9 16:45:49.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 12.006752211s
    Mar  9 16:45:51.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 14.006696437s
    Mar  9 16:45:53.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 16.006409341s
    Mar  9 16:45:55.916: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 18.006064411s
    Mar  9 16:45:57.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=true. Elapsed: 20.00685309s
    Mar  9 16:45:59.918: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Running", Reason="", readiness=false. Elapsed: 22.007478707s
    Mar  9 16:46:01.917: INFO: Pod "pod-subpath-test-configmap-w7m4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007385253s
    STEP: Saw pod success 03/09/23 16:46:01.918
    Mar  9 16:46:01.918: INFO: Pod "pod-subpath-test-configmap-w7m4" satisfied condition "Succeeded or Failed"
    Mar  9 16:46:01.920: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-configmap-w7m4 container test-container-subpath-configmap-w7m4: <nil>
    STEP: delete the pod 03/09/23 16:46:01.935
    Mar  9 16:46:01.944: INFO: Waiting for pod pod-subpath-test-configmap-w7m4 to disappear
    Mar  9 16:46:01.947: INFO: Pod pod-subpath-test-configmap-w7m4 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-w7m4 03/09/23 16:46:01.947
    Mar  9 16:46:01.948: INFO: Deleting pod "pod-subpath-test-configmap-w7m4" in namespace "subpath-3541"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  9 16:46:01.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3541" for this suite. 03/09/23 16:46:01.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:46:01.96
Mar  9 16:46:01.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:46:01.962
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:46:01.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:46:01.98
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:46:01.983
Mar  9 16:46:01.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe" in namespace "projected-544" to be "Succeeded or Failed"
Mar  9 16:46:01.993: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296209ms
Mar  9 16:46:03.998: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007200171s
Mar  9 16:46:05.998: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007184166s
STEP: Saw pod success 03/09/23 16:46:05.998
Mar  9 16:46:05.998: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe" satisfied condition "Succeeded or Failed"
Mar  9 16:46:06.001: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe container client-container: <nil>
STEP: delete the pod 03/09/23 16:46:06.007
Mar  9 16:46:06.017: INFO: Waiting for pod downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe to disappear
Mar  9 16:46:06.019: INFO: Pod downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 16:46:06.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-544" for this suite. 03/09/23 16:46:06.023
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":186,"skipped":3793,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:46:01.96
    Mar  9 16:46:01.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:46:01.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:46:01.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:46:01.98
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:46:01.983
    Mar  9 16:46:01.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe" in namespace "projected-544" to be "Succeeded or Failed"
    Mar  9 16:46:01.993: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296209ms
    Mar  9 16:46:03.998: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007200171s
    Mar  9 16:46:05.998: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007184166s
    STEP: Saw pod success 03/09/23 16:46:05.998
    Mar  9 16:46:05.998: INFO: Pod "downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe" satisfied condition "Succeeded or Failed"
    Mar  9 16:46:06.001: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe container client-container: <nil>
    STEP: delete the pod 03/09/23 16:46:06.007
    Mar  9 16:46:06.017: INFO: Waiting for pod downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe to disappear
    Mar  9 16:46:06.019: INFO: Pod downwardapi-volume-4ab39414-c967-46f6-a39a-d12df253bbfe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 16:46:06.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-544" for this suite. 03/09/23 16:46:06.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:46:06.028
Mar  9 16:46:06.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir-wrapper 03/09/23 16:46:06.029
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:46:06.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:46:06.044
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/09/23 16:46:06.047
STEP: Creating RC which spawns configmap-volume pods 03/09/23 16:46:06.285
Mar  9 16:46:06.391: INFO: Pod name wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc: Found 3 pods out of 5
Mar  9 16:46:11.399: INFO: Pod name wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/09/23 16:46:11.399
Mar  9 16:46:11.400: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:11.402: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.769832ms
Mar  9 16:46:13.406: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006786627s
Mar  9 16:46:15.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007187487s
Mar  9 16:46:17.406: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006701941s
Mar  9 16:46:19.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007791271s
Mar  9 16:46:21.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Running", Reason="", readiness=true. Elapsed: 10.007265423s
Mar  9 16:46:21.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g" satisfied condition "running"
Mar  9 16:46:21.407: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-b98z5" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:21.410: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-b98z5": Phase="Running", Reason="", readiness=true. Elapsed: 3.217815ms
Mar  9 16:46:21.410: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-b98z5" satisfied condition "running"
Mar  9 16:46:21.410: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-dm6nk" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:21.413: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-dm6nk": Phase="Running", Reason="", readiness=true. Elapsed: 3.124179ms
Mar  9 16:46:21.413: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-dm6nk" satisfied condition "running"
Mar  9 16:46:21.413: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-lmrtz" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:21.417: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-lmrtz": Phase="Running", Reason="", readiness=true. Elapsed: 3.169531ms
Mar  9 16:46:21.417: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-lmrtz" satisfied condition "running"
Mar  9 16:46:21.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-qtpv4" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:21.420: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-qtpv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.940578ms
Mar  9 16:46:21.420: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-qtpv4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc in namespace emptydir-wrapper-6753, will wait for the garbage collector to delete the pods 03/09/23 16:46:21.42
Mar  9 16:46:21.479: INFO: Deleting ReplicationController wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc took: 5.359898ms
Mar  9 16:46:21.579: INFO: Terminating ReplicationController wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc pods took: 100.70364ms
STEP: Creating RC which spawns configmap-volume pods 03/09/23 16:46:24.885
Mar  9 16:46:24.898: INFO: Pod name wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761: Found 0 pods out of 5
Mar  9 16:46:29.906: INFO: Pod name wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/09/23 16:46:29.906
Mar  9 16:46:29.907: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:29.910: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.091071ms
Mar  9 16:46:31.914: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007645338s
Mar  9 16:46:33.914: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007345803s
Mar  9 16:46:35.914: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007724485s
Mar  9 16:46:37.915: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008245261s
Mar  9 16:46:39.916: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Running", Reason="", readiness=true. Elapsed: 10.008967433s
Mar  9 16:46:39.916: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p" satisfied condition "running"
Mar  9 16:46:39.916: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-9vhs6" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:39.919: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-9vhs6": Phase="Running", Reason="", readiness=true. Elapsed: 3.120096ms
Mar  9 16:46:39.919: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-9vhs6" satisfied condition "running"
Mar  9 16:46:39.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-dx8v7" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:39.922: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-dx8v7": Phase="Running", Reason="", readiness=true. Elapsed: 3.145795ms
Mar  9 16:46:39.922: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-dx8v7" satisfied condition "running"
Mar  9 16:46:39.922: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-svhlb" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:39.925: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-svhlb": Phase="Running", Reason="", readiness=true. Elapsed: 3.109193ms
Mar  9 16:46:39.925: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-svhlb" satisfied condition "running"
Mar  9 16:46:39.925: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-xbmd8" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:39.928: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-xbmd8": Phase="Running", Reason="", readiness=true. Elapsed: 3.138925ms
Mar  9 16:46:39.928: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-xbmd8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761 in namespace emptydir-wrapper-6753, will wait for the garbage collector to delete the pods 03/09/23 16:46:39.928
Mar  9 16:46:39.987: INFO: Deleting ReplicationController wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761 took: 5.35748ms
Mar  9 16:46:40.088: INFO: Terminating ReplicationController wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761 pods took: 101.111749ms
STEP: Creating RC which spawns configmap-volume pods 03/09/23 16:46:43.593
Mar  9 16:46:43.607: INFO: Pod name wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b: Found 0 pods out of 5
Mar  9 16:46:48.615: INFO: Pod name wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/09/23 16:46:48.615
Mar  9 16:46:48.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:48.618: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831358ms
Mar  9 16:46:50.622: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00710178s
Mar  9 16:46:52.622: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007437865s
Mar  9 16:46:54.621: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006827436s
Mar  9 16:46:56.623: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008287772s
Mar  9 16:46:58.623: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.008533847s
Mar  9 16:46:58.623: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd" satisfied condition "running"
Mar  9 16:46:58.623: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-j6qkv" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:58.626: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-j6qkv": Phase="Running", Reason="", readiness=true. Elapsed: 3.211358ms
Mar  9 16:46:58.627: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-j6qkv" satisfied condition "running"
Mar  9 16:46:58.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lg8tn" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:58.629: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lg8tn": Phase="Running", Reason="", readiness=true. Elapsed: 2.822697ms
Mar  9 16:46:58.629: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lg8tn" satisfied condition "running"
Mar  9 16:46:58.629: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lkpvj" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:58.632: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lkpvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.855984ms
Mar  9 16:46:58.632: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lkpvj" satisfied condition "running"
Mar  9 16:46:58.632: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-qm7w5" in namespace "emptydir-wrapper-6753" to be "running"
Mar  9 16:46:58.635: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-qm7w5": Phase="Running", Reason="", readiness=true. Elapsed: 3.143344ms
Mar  9 16:46:58.635: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-qm7w5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b in namespace emptydir-wrapper-6753, will wait for the garbage collector to delete the pods 03/09/23 16:46:58.635
Mar  9 16:46:58.695: INFO: Deleting ReplicationController wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b took: 5.579556ms
Mar  9 16:46:58.796: INFO: Terminating ReplicationController wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b pods took: 101.115877ms
STEP: Cleaning up the configMaps 03/09/23 16:47:01.897
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  9 16:47:02.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6753" for this suite. 03/09/23 16:47:02.117
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":187,"skipped":3801,"failed":0}
------------------------------
â€¢ [SLOW TEST] [56.093 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:46:06.028
    Mar  9 16:46:06.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir-wrapper 03/09/23 16:46:06.029
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:46:06.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:46:06.044
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/09/23 16:46:06.047
    STEP: Creating RC which spawns configmap-volume pods 03/09/23 16:46:06.285
    Mar  9 16:46:06.391: INFO: Pod name wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc: Found 3 pods out of 5
    Mar  9 16:46:11.399: INFO: Pod name wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/09/23 16:46:11.399
    Mar  9 16:46:11.400: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:11.402: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.769832ms
    Mar  9 16:46:13.406: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006786627s
    Mar  9 16:46:15.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007187487s
    Mar  9 16:46:17.406: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006701941s
    Mar  9 16:46:19.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007791271s
    Mar  9 16:46:21.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g": Phase="Running", Reason="", readiness=true. Elapsed: 10.007265423s
    Mar  9 16:46:21.407: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-2gk5g" satisfied condition "running"
    Mar  9 16:46:21.407: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-b98z5" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:21.410: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-b98z5": Phase="Running", Reason="", readiness=true. Elapsed: 3.217815ms
    Mar  9 16:46:21.410: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-b98z5" satisfied condition "running"
    Mar  9 16:46:21.410: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-dm6nk" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:21.413: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-dm6nk": Phase="Running", Reason="", readiness=true. Elapsed: 3.124179ms
    Mar  9 16:46:21.413: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-dm6nk" satisfied condition "running"
    Mar  9 16:46:21.413: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-lmrtz" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:21.417: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-lmrtz": Phase="Running", Reason="", readiness=true. Elapsed: 3.169531ms
    Mar  9 16:46:21.417: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-lmrtz" satisfied condition "running"
    Mar  9 16:46:21.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-qtpv4" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:21.420: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-qtpv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.940578ms
    Mar  9 16:46:21.420: INFO: Pod "wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc-qtpv4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc in namespace emptydir-wrapper-6753, will wait for the garbage collector to delete the pods 03/09/23 16:46:21.42
    Mar  9 16:46:21.479: INFO: Deleting ReplicationController wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc took: 5.359898ms
    Mar  9 16:46:21.579: INFO: Terminating ReplicationController wrapped-volume-race-696b40ba-14ae-42ff-bf58-44ee7fc7bbdc pods took: 100.70364ms
    STEP: Creating RC which spawns configmap-volume pods 03/09/23 16:46:24.885
    Mar  9 16:46:24.898: INFO: Pod name wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761: Found 0 pods out of 5
    Mar  9 16:46:29.906: INFO: Pod name wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/09/23 16:46:29.906
    Mar  9 16:46:29.907: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:29.910: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.091071ms
    Mar  9 16:46:31.914: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007645338s
    Mar  9 16:46:33.914: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007345803s
    Mar  9 16:46:35.914: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007724485s
    Mar  9 16:46:37.915: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008245261s
    Mar  9 16:46:39.916: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p": Phase="Running", Reason="", readiness=true. Elapsed: 10.008967433s
    Mar  9 16:46:39.916: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-2vl2p" satisfied condition "running"
    Mar  9 16:46:39.916: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-9vhs6" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:39.919: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-9vhs6": Phase="Running", Reason="", readiness=true. Elapsed: 3.120096ms
    Mar  9 16:46:39.919: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-9vhs6" satisfied condition "running"
    Mar  9 16:46:39.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-dx8v7" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:39.922: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-dx8v7": Phase="Running", Reason="", readiness=true. Elapsed: 3.145795ms
    Mar  9 16:46:39.922: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-dx8v7" satisfied condition "running"
    Mar  9 16:46:39.922: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-svhlb" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:39.925: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-svhlb": Phase="Running", Reason="", readiness=true. Elapsed: 3.109193ms
    Mar  9 16:46:39.925: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-svhlb" satisfied condition "running"
    Mar  9 16:46:39.925: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-xbmd8" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:39.928: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-xbmd8": Phase="Running", Reason="", readiness=true. Elapsed: 3.138925ms
    Mar  9 16:46:39.928: INFO: Pod "wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761-xbmd8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761 in namespace emptydir-wrapper-6753, will wait for the garbage collector to delete the pods 03/09/23 16:46:39.928
    Mar  9 16:46:39.987: INFO: Deleting ReplicationController wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761 took: 5.35748ms
    Mar  9 16:46:40.088: INFO: Terminating ReplicationController wrapped-volume-race-d3bae322-4167-4665-801c-b2370a508761 pods took: 101.111749ms
    STEP: Creating RC which spawns configmap-volume pods 03/09/23 16:46:43.593
    Mar  9 16:46:43.607: INFO: Pod name wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b: Found 0 pods out of 5
    Mar  9 16:46:48.615: INFO: Pod name wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/09/23 16:46:48.615
    Mar  9 16:46:48.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:48.618: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831358ms
    Mar  9 16:46:50.622: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00710178s
    Mar  9 16:46:52.622: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007437865s
    Mar  9 16:46:54.621: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006827436s
    Mar  9 16:46:56.623: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008287772s
    Mar  9 16:46:58.623: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.008533847s
    Mar  9 16:46:58.623: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-hnfnd" satisfied condition "running"
    Mar  9 16:46:58.623: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-j6qkv" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:58.626: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-j6qkv": Phase="Running", Reason="", readiness=true. Elapsed: 3.211358ms
    Mar  9 16:46:58.627: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-j6qkv" satisfied condition "running"
    Mar  9 16:46:58.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lg8tn" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:58.629: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lg8tn": Phase="Running", Reason="", readiness=true. Elapsed: 2.822697ms
    Mar  9 16:46:58.629: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lg8tn" satisfied condition "running"
    Mar  9 16:46:58.629: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lkpvj" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:58.632: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lkpvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.855984ms
    Mar  9 16:46:58.632: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-lkpvj" satisfied condition "running"
    Mar  9 16:46:58.632: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-qm7w5" in namespace "emptydir-wrapper-6753" to be "running"
    Mar  9 16:46:58.635: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-qm7w5": Phase="Running", Reason="", readiness=true. Elapsed: 3.143344ms
    Mar  9 16:46:58.635: INFO: Pod "wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b-qm7w5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b in namespace emptydir-wrapper-6753, will wait for the garbage collector to delete the pods 03/09/23 16:46:58.635
    Mar  9 16:46:58.695: INFO: Deleting ReplicationController wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b took: 5.579556ms
    Mar  9 16:46:58.796: INFO: Terminating ReplicationController wrapped-volume-race-31a3bcd3-fcc9-405a-87ac-e1f4223a516b pods took: 101.115877ms
    STEP: Cleaning up the configMaps 03/09/23 16:47:01.897
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:47:02.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6753" for this suite. 03/09/23 16:47:02.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:02.122
Mar  9 16:47:02.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:47:02.123
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:02.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:02.141
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-8f0fcf5d-0abb-4bf2-acd7-949558127e2d 03/09/23 16:47:02.144
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:47:02.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7986" for this suite. 03/09/23 16:47:02.15
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":188,"skipped":3806,"failed":0}
------------------------------
â€¢ [0.033 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:02.122
    Mar  9 16:47:02.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:47:02.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:02.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:02.141
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-8f0fcf5d-0abb-4bf2-acd7-949558127e2d 03/09/23 16:47:02.144
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:47:02.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7986" for this suite. 03/09/23 16:47:02.15
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:02.155
Mar  9 16:47:02.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 16:47:02.156
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:02.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:02.172
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar  9 16:47:02.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:47:05.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6511" for this suite. 03/09/23 16:47:05.307
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":189,"skipped":3808,"failed":0}
------------------------------
â€¢ [3.157 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:02.155
    Mar  9 16:47:02.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 16:47:02.156
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:02.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:02.172
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar  9 16:47:02.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:47:05.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6511" for this suite. 03/09/23 16:47:05.307
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:05.312
Mar  9 16:47:05.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename job 03/09/23 16:47:05.313
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:05.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:05.328
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 03/09/23 16:47:05.331
STEP: Ensuring job reaches completions 03/09/23 16:47:05.337
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  9 16:47:17.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6040" for this suite. 03/09/23 16:47:17.344
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":190,"skipped":3812,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.038 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:05.312
    Mar  9 16:47:05.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename job 03/09/23 16:47:05.313
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:05.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:05.328
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 03/09/23 16:47:05.331
    STEP: Ensuring job reaches completions 03/09/23 16:47:05.337
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  9 16:47:17.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6040" for this suite. 03/09/23 16:47:17.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:17.352
Mar  9 16:47:17.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replication-controller 03/09/23 16:47:17.353
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:17.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:17.368
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 03/09/23 16:47:17.374
STEP: waiting for RC to be added 03/09/23 16:47:17.378
STEP: waiting for available Replicas 03/09/23 16:47:17.378
STEP: patching ReplicationController 03/09/23 16:47:18.395
STEP: waiting for RC to be modified 03/09/23 16:47:18.403
STEP: patching ReplicationController status 03/09/23 16:47:18.404
STEP: waiting for RC to be modified 03/09/23 16:47:18.408
STEP: waiting for available Replicas 03/09/23 16:47:18.408
STEP: fetching ReplicationController status 03/09/23 16:47:18.417
STEP: patching ReplicationController scale 03/09/23 16:47:18.419
STEP: waiting for RC to be modified 03/09/23 16:47:18.424
STEP: waiting for ReplicationController's scale to be the max amount 03/09/23 16:47:18.425
STEP: fetching ReplicationController; ensuring that it's patched 03/09/23 16:47:19.76
STEP: updating ReplicationController status 03/09/23 16:47:19.763
STEP: waiting for RC to be modified 03/09/23 16:47:19.769
STEP: listing all ReplicationControllers 03/09/23 16:47:19.769
STEP: checking that ReplicationController has expected values 03/09/23 16:47:19.771
STEP: deleting ReplicationControllers by collection 03/09/23 16:47:19.771
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/09/23 16:47:19.778
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  9 16:47:19.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5542" for this suite. 03/09/23 16:47:19.84
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":191,"skipped":3829,"failed":0}
------------------------------
â€¢ [2.494 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:17.352
    Mar  9 16:47:17.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replication-controller 03/09/23 16:47:17.353
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:17.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:17.368
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 03/09/23 16:47:17.374
    STEP: waiting for RC to be added 03/09/23 16:47:17.378
    STEP: waiting for available Replicas 03/09/23 16:47:17.378
    STEP: patching ReplicationController 03/09/23 16:47:18.395
    STEP: waiting for RC to be modified 03/09/23 16:47:18.403
    STEP: patching ReplicationController status 03/09/23 16:47:18.404
    STEP: waiting for RC to be modified 03/09/23 16:47:18.408
    STEP: waiting for available Replicas 03/09/23 16:47:18.408
    STEP: fetching ReplicationController status 03/09/23 16:47:18.417
    STEP: patching ReplicationController scale 03/09/23 16:47:18.419
    STEP: waiting for RC to be modified 03/09/23 16:47:18.424
    STEP: waiting for ReplicationController's scale to be the max amount 03/09/23 16:47:18.425
    STEP: fetching ReplicationController; ensuring that it's patched 03/09/23 16:47:19.76
    STEP: updating ReplicationController status 03/09/23 16:47:19.763
    STEP: waiting for RC to be modified 03/09/23 16:47:19.769
    STEP: listing all ReplicationControllers 03/09/23 16:47:19.769
    STEP: checking that ReplicationController has expected values 03/09/23 16:47:19.771
    STEP: deleting ReplicationControllers by collection 03/09/23 16:47:19.771
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/09/23 16:47:19.778
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  9 16:47:19.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5542" for this suite. 03/09/23 16:47:19.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:19.847
Mar  9 16:47:19.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:47:19.848
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:19.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:19.861
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Mar  9 16:47:19.866: INFO: Got root ca configmap in namespace "svcaccounts-1440"
Mar  9 16:47:19.872: INFO: Deleted root ca configmap in namespace "svcaccounts-1440"
STEP: waiting for a new root ca configmap created 03/09/23 16:47:20.372
Mar  9 16:47:20.376: INFO: Recreated root ca configmap in namespace "svcaccounts-1440"
Mar  9 16:47:20.380: INFO: Updated root ca configmap in namespace "svcaccounts-1440"
STEP: waiting for the root ca configmap reconciled 03/09/23 16:47:20.88
Mar  9 16:47:20.884: INFO: Reconciled root ca configmap in namespace "svcaccounts-1440"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  9 16:47:20.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1440" for this suite. 03/09/23 16:47:20.888
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":192,"skipped":3839,"failed":0}
------------------------------
â€¢ [1.047 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:19.847
    Mar  9 16:47:19.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:47:19.848
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:19.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:19.861
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Mar  9 16:47:19.866: INFO: Got root ca configmap in namespace "svcaccounts-1440"
    Mar  9 16:47:19.872: INFO: Deleted root ca configmap in namespace "svcaccounts-1440"
    STEP: waiting for a new root ca configmap created 03/09/23 16:47:20.372
    Mar  9 16:47:20.376: INFO: Recreated root ca configmap in namespace "svcaccounts-1440"
    Mar  9 16:47:20.380: INFO: Updated root ca configmap in namespace "svcaccounts-1440"
    STEP: waiting for the root ca configmap reconciled 03/09/23 16:47:20.88
    Mar  9 16:47:20.884: INFO: Reconciled root ca configmap in namespace "svcaccounts-1440"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  9 16:47:20.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1440" for this suite. 03/09/23 16:47:20.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:20.896
Mar  9 16:47:20.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:47:20.897
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:20.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:20.914
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-be25f756-4146-49b1-9235-e787301b6d7d 03/09/23 16:47:20.917
STEP: Creating a pod to test consume configMaps 03/09/23 16:47:20.923
Mar  9 16:47:20.931: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1" in namespace "projected-400" to be "Succeeded or Failed"
Mar  9 16:47:20.935: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075224ms
Mar  9 16:47:22.939: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008003346s
Mar  9 16:47:24.937: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006875628s
STEP: Saw pod success 03/09/23 16:47:24.938
Mar  9 16:47:24.938: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1" satisfied condition "Succeeded or Failed"
Mar  9 16:47:24.940: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:47:24.945
Mar  9 16:47:24.954: INFO: Waiting for pod pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1 to disappear
Mar  9 16:47:24.956: INFO: Pod pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 16:47:24.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-400" for this suite. 03/09/23 16:47:24.959
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":193,"skipped":3851,"failed":0}
------------------------------
â€¢ [4.069 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:20.896
    Mar  9 16:47:20.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:47:20.897
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:20.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:20.914
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-be25f756-4146-49b1-9235-e787301b6d7d 03/09/23 16:47:20.917
    STEP: Creating a pod to test consume configMaps 03/09/23 16:47:20.923
    Mar  9 16:47:20.931: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1" in namespace "projected-400" to be "Succeeded or Failed"
    Mar  9 16:47:20.935: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075224ms
    Mar  9 16:47:22.939: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008003346s
    Mar  9 16:47:24.937: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006875628s
    STEP: Saw pod success 03/09/23 16:47:24.938
    Mar  9 16:47:24.938: INFO: Pod "pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1" satisfied condition "Succeeded or Failed"
    Mar  9 16:47:24.940: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:47:24.945
    Mar  9 16:47:24.954: INFO: Waiting for pod pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1 to disappear
    Mar  9 16:47:24.956: INFO: Pod pod-projected-configmaps-f7a6d404-d05f-4288-b795-f71741d903d1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 16:47:24.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-400" for this suite. 03/09/23 16:47:24.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:24.965
Mar  9 16:47:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 16:47:24.966
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:24.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:24.98
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 03/09/23 16:47:24.982
Mar  9 16:47:24.992: INFO: Waiting up to 5m0s for pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d" in namespace "var-expansion-9634" to be "Succeeded or Failed"
Mar  9 16:47:24.994: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065961ms
Mar  9 16:47:26.998: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005486821s
Mar  9 16:47:28.998: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005331652s
STEP: Saw pod success 03/09/23 16:47:28.998
Mar  9 16:47:28.998: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d" satisfied condition "Succeeded or Failed"
Mar  9 16:47:29.000: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d container dapi-container: <nil>
STEP: delete the pod 03/09/23 16:47:29.006
Mar  9 16:47:29.014: INFO: Waiting for pod var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d to disappear
Mar  9 16:47:29.016: INFO: Pod var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 16:47:29.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9634" for this suite. 03/09/23 16:47:29.02
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":194,"skipped":3858,"failed":0}
------------------------------
â€¢ [4.059 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:24.965
    Mar  9 16:47:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 16:47:24.966
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:24.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:24.98
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 03/09/23 16:47:24.982
    Mar  9 16:47:24.992: INFO: Waiting up to 5m0s for pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d" in namespace "var-expansion-9634" to be "Succeeded or Failed"
    Mar  9 16:47:24.994: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065961ms
    Mar  9 16:47:26.998: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005486821s
    Mar  9 16:47:28.998: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005331652s
    STEP: Saw pod success 03/09/23 16:47:28.998
    Mar  9 16:47:28.998: INFO: Pod "var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d" satisfied condition "Succeeded or Failed"
    Mar  9 16:47:29.000: INFO: Trying to get logs from node tt-test-el8-003 pod var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d container dapi-container: <nil>
    STEP: delete the pod 03/09/23 16:47:29.006
    Mar  9 16:47:29.014: INFO: Waiting for pod var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d to disappear
    Mar  9 16:47:29.016: INFO: Pod var-expansion-e581accf-fae2-4fc9-9ed1-eaccdd81867d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 16:47:29.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9634" for this suite. 03/09/23 16:47:29.02
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:29.025
Mar  9 16:47:29.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:47:29.026
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:29.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:29.039
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:47:29.052
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:47:29.643
STEP: Deploying the webhook pod 03/09/23 16:47:29.65
STEP: Wait for the deployment to be ready 03/09/23 16:47:29.659
Mar  9 16:47:29.670: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:47:31.678
STEP: Verifying the service has paired with the endpoint 03/09/23 16:47:31.697
Mar  9 16:47:32.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/09/23 16:47:32.701
STEP: create a namespace for the webhook 03/09/23 16:47:32.716
STEP: create a configmap should be unconditionally rejected by the webhook 03/09/23 16:47:32.722
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:47:32.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2725" for this suite. 03/09/23 16:47:32.742
STEP: Destroying namespace "webhook-2725-markers" for this suite. 03/09/23 16:47:32.746
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":195,"skipped":3860,"failed":0}
------------------------------
â€¢ [3.761 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:29.025
    Mar  9 16:47:29.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:47:29.026
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:29.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:29.039
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:47:29.052
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:47:29.643
    STEP: Deploying the webhook pod 03/09/23 16:47:29.65
    STEP: Wait for the deployment to be ready 03/09/23 16:47:29.659
    Mar  9 16:47:29.670: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:47:31.678
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:47:31.697
    Mar  9 16:47:32.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/09/23 16:47:32.701
    STEP: create a namespace for the webhook 03/09/23 16:47:32.716
    STEP: create a configmap should be unconditionally rejected by the webhook 03/09/23 16:47:32.722
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:47:32.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2725" for this suite. 03/09/23 16:47:32.742
    STEP: Destroying namespace "webhook-2725-markers" for this suite. 03/09/23 16:47:32.746
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:32.787
Mar  9 16:47:32.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:47:32.788
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:32.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:32.804
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 03/09/23 16:47:32.809
STEP: Getting a ResourceQuota 03/09/23 16:47:32.813
STEP: Updating a ResourceQuota 03/09/23 16:47:32.816
STEP: Verifying a ResourceQuota was modified 03/09/23 16:47:32.824
STEP: Deleting a ResourceQuota 03/09/23 16:47:32.827
STEP: Verifying the deleted ResourceQuota 03/09/23 16:47:32.834
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:47:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5722" for this suite. 03/09/23 16:47:32.842
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":196,"skipped":3863,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:32.787
    Mar  9 16:47:32.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:47:32.788
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:32.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:32.804
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 03/09/23 16:47:32.809
    STEP: Getting a ResourceQuota 03/09/23 16:47:32.813
    STEP: Updating a ResourceQuota 03/09/23 16:47:32.816
    STEP: Verifying a ResourceQuota was modified 03/09/23 16:47:32.824
    STEP: Deleting a ResourceQuota 03/09/23 16:47:32.827
    STEP: Verifying the deleted ResourceQuota 03/09/23 16:47:32.834
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:47:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5722" for this suite. 03/09/23 16:47:32.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:32.848
Mar  9 16:47:32.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename events 03/09/23 16:47:32.849
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:32.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:32.865
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/09/23 16:47:32.869
STEP: listing all events in all namespaces 03/09/23 16:47:32.874
STEP: patching the test event 03/09/23 16:47:32.879
STEP: fetching the test event 03/09/23 16:47:32.884
STEP: updating the test event 03/09/23 16:47:32.886
STEP: getting the test event 03/09/23 16:47:32.895
STEP: deleting the test event 03/09/23 16:47:32.898
STEP: listing all events in all namespaces 03/09/23 16:47:32.903
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  9 16:47:32.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3868" for this suite. 03/09/23 16:47:32.911
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":197,"skipped":3872,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:32.848
    Mar  9 16:47:32.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename events 03/09/23 16:47:32.849
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:32.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:32.865
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/09/23 16:47:32.869
    STEP: listing all events in all namespaces 03/09/23 16:47:32.874
    STEP: patching the test event 03/09/23 16:47:32.879
    STEP: fetching the test event 03/09/23 16:47:32.884
    STEP: updating the test event 03/09/23 16:47:32.886
    STEP: getting the test event 03/09/23 16:47:32.895
    STEP: deleting the test event 03/09/23 16:47:32.898
    STEP: listing all events in all namespaces 03/09/23 16:47:32.903
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  9 16:47:32.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3868" for this suite. 03/09/23 16:47:32.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:32.918
Mar  9 16:47:32.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:47:32.919
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:32.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:32.934
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:47:32.948
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:47:33.485
STEP: Deploying the webhook pod 03/09/23 16:47:33.49
STEP: Wait for the deployment to be ready 03/09/23 16:47:33.5
Mar  9 16:47:33.506: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:47:35.514
STEP: Verifying the service has paired with the endpoint 03/09/23 16:47:35.531
Mar  9 16:47:36.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Mar  9 16:47:36.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/09/23 16:47:37.043
STEP: Creating a custom resource that should be denied by the webhook 03/09/23 16:47:37.058
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/09/23 16:47:39.093
STEP: Updating the custom resource with disallowed data should be denied 03/09/23 16:47:39.101
STEP: Deleting the custom resource should be denied 03/09/23 16:47:39.109
STEP: Remove the offending key and value from the custom resource data 03/09/23 16:47:39.114
STEP: Deleting the updated custom resource should be successful 03/09/23 16:47:39.122
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:47:39.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1793" for this suite. 03/09/23 16:47:39.65
STEP: Destroying namespace "webhook-1793-markers" for this suite. 03/09/23 16:47:39.654
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":198,"skipped":3906,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.782 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:32.918
    Mar  9 16:47:32.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:47:32.919
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:32.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:32.934
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:47:32.948
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:47:33.485
    STEP: Deploying the webhook pod 03/09/23 16:47:33.49
    STEP: Wait for the deployment to be ready 03/09/23 16:47:33.5
    Mar  9 16:47:33.506: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:47:35.514
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:47:35.531
    Mar  9 16:47:36.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Mar  9 16:47:36.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/09/23 16:47:37.043
    STEP: Creating a custom resource that should be denied by the webhook 03/09/23 16:47:37.058
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/09/23 16:47:39.093
    STEP: Updating the custom resource with disallowed data should be denied 03/09/23 16:47:39.101
    STEP: Deleting the custom resource should be denied 03/09/23 16:47:39.109
    STEP: Remove the offending key and value from the custom resource data 03/09/23 16:47:39.114
    STEP: Deleting the updated custom resource should be successful 03/09/23 16:47:39.122
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:47:39.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1793" for this suite. 03/09/23 16:47:39.65
    STEP: Destroying namespace "webhook-1793-markers" for this suite. 03/09/23 16:47:39.654
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:39.7
Mar  9 16:47:39.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:47:39.701
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:39.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:39.719
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:47:39.741
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:47:40.225
STEP: Deploying the webhook pod 03/09/23 16:47:40.231
STEP: Wait for the deployment to be ready 03/09/23 16:47:40.242
Mar  9 16:47:40.248: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 03/09/23 16:47:42.258
STEP: Verifying the service has paired with the endpoint 03/09/23 16:47:42.271
Mar  9 16:47:43.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Mar  9 16:47:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8125-crds.webhook.example.com via the AdmissionRegistration API 03/09/23 16:47:43.784
STEP: Creating a custom resource that should be mutated by the webhook 03/09/23 16:47:43.8
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:47:46.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9263" for this suite. 03/09/23 16:47:46.372
STEP: Destroying namespace "webhook-9263-markers" for this suite. 03/09/23 16:47:46.377
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":199,"skipped":3910,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:39.7
    Mar  9 16:47:39.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:47:39.701
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:39.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:39.719
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:47:39.741
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:47:40.225
    STEP: Deploying the webhook pod 03/09/23 16:47:40.231
    STEP: Wait for the deployment to be ready 03/09/23 16:47:40.242
    Mar  9 16:47:40.248: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 03/09/23 16:47:42.258
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:47:42.271
    Mar  9 16:47:43.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Mar  9 16:47:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8125-crds.webhook.example.com via the AdmissionRegistration API 03/09/23 16:47:43.784
    STEP: Creating a custom resource that should be mutated by the webhook 03/09/23 16:47:43.8
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:47:46.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9263" for this suite. 03/09/23 16:47:46.372
    STEP: Destroying namespace "webhook-9263-markers" for this suite. 03/09/23 16:47:46.377
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:46.46
Mar  9 16:47:46.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename certificates 03/09/23 16:47:46.461
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:46.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:46.483
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/09/23 16:47:48.232
STEP: getting /apis/certificates.k8s.io 03/09/23 16:47:48.234
STEP: getting /apis/certificates.k8s.io/v1 03/09/23 16:47:48.235
STEP: creating 03/09/23 16:47:48.236
STEP: getting 03/09/23 16:47:48.25
STEP: listing 03/09/23 16:47:48.252
STEP: watching 03/09/23 16:47:48.255
Mar  9 16:47:48.255: INFO: starting watch
STEP: patching 03/09/23 16:47:48.256
STEP: updating 03/09/23 16:47:48.261
Mar  9 16:47:48.266: INFO: waiting for watch events with expected annotations
Mar  9 16:47:48.266: INFO: saw patched and updated annotations
STEP: getting /approval 03/09/23 16:47:48.266
STEP: patching /approval 03/09/23 16:47:48.269
STEP: updating /approval 03/09/23 16:47:48.273
STEP: getting /status 03/09/23 16:47:48.278
STEP: patching /status 03/09/23 16:47:48.281
STEP: updating /status 03/09/23 16:47:48.287
STEP: deleting 03/09/23 16:47:48.293
STEP: deleting a collection 03/09/23 16:47:48.302
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:47:48.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8220" for this suite. 03/09/23 16:47:48.315
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":200,"skipped":3917,"failed":0}
------------------------------
â€¢ [1.860 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:46.46
    Mar  9 16:47:46.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename certificates 03/09/23 16:47:46.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:46.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:46.483
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/09/23 16:47:48.232
    STEP: getting /apis/certificates.k8s.io 03/09/23 16:47:48.234
    STEP: getting /apis/certificates.k8s.io/v1 03/09/23 16:47:48.235
    STEP: creating 03/09/23 16:47:48.236
    STEP: getting 03/09/23 16:47:48.25
    STEP: listing 03/09/23 16:47:48.252
    STEP: watching 03/09/23 16:47:48.255
    Mar  9 16:47:48.255: INFO: starting watch
    STEP: patching 03/09/23 16:47:48.256
    STEP: updating 03/09/23 16:47:48.261
    Mar  9 16:47:48.266: INFO: waiting for watch events with expected annotations
    Mar  9 16:47:48.266: INFO: saw patched and updated annotations
    STEP: getting /approval 03/09/23 16:47:48.266
    STEP: patching /approval 03/09/23 16:47:48.269
    STEP: updating /approval 03/09/23 16:47:48.273
    STEP: getting /status 03/09/23 16:47:48.278
    STEP: patching /status 03/09/23 16:47:48.281
    STEP: updating /status 03/09/23 16:47:48.287
    STEP: deleting 03/09/23 16:47:48.293
    STEP: deleting a collection 03/09/23 16:47:48.302
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:47:48.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-8220" for this suite. 03/09/23 16:47:48.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:48.32
Mar  9 16:47:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename disruption 03/09/23 16:47:48.322
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:48.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:48.334
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 03/09/23 16:47:48.341
STEP: Updating PodDisruptionBudget status 03/09/23 16:47:50.346
STEP: Waiting for all pods to be running 03/09/23 16:47:50.353
Mar  9 16:47:50.356: INFO: running pods: 0 < 1
STEP: locating a running pod 03/09/23 16:47:52.361
STEP: Waiting for the pdb to be processed 03/09/23 16:47:52.372
STEP: Patching PodDisruptionBudget status 03/09/23 16:47:52.377
STEP: Waiting for the pdb to be processed 03/09/23 16:47:52.384
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  9 16:47:52.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4244" for this suite. 03/09/23 16:47:52.39
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":201,"skipped":3931,"failed":0}
------------------------------
â€¢ [4.075 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:48.32
    Mar  9 16:47:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename disruption 03/09/23 16:47:48.322
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:48.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:48.334
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 03/09/23 16:47:48.341
    STEP: Updating PodDisruptionBudget status 03/09/23 16:47:50.346
    STEP: Waiting for all pods to be running 03/09/23 16:47:50.353
    Mar  9 16:47:50.356: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/09/23 16:47:52.361
    STEP: Waiting for the pdb to be processed 03/09/23 16:47:52.372
    STEP: Patching PodDisruptionBudget status 03/09/23 16:47:52.377
    STEP: Waiting for the pdb to be processed 03/09/23 16:47:52.384
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  9 16:47:52.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4244" for this suite. 03/09/23 16:47:52.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:52.396
Mar  9 16:47:52.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename endpointslice 03/09/23 16:47:52.398
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:52.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:52.411
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  9 16:47:54.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9587" for this suite. 03/09/23 16:47:54.471
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":202,"skipped":3937,"failed":0}
------------------------------
â€¢ [2.082 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:52.396
    Mar  9 16:47:52.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename endpointslice 03/09/23 16:47:52.398
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:52.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:52.411
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  9 16:47:54.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9587" for this suite. 03/09/23 16:47:54.471
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:54.478
Mar  9 16:47:54.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:47:54.48
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:54.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:54.504
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 03/09/23 16:47:54.511
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/09/23 16:47:54.516
STEP: patching the secret 03/09/23 16:47:54.52
STEP: deleting the secret using a LabelSelector 03/09/23 16:47:54.527
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/09/23 16:47:54.533
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:47:54.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5113" for this suite. 03/09/23 16:47:54.541
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":203,"skipped":3941,"failed":0}
------------------------------
â€¢ [0.069 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:54.478
    Mar  9 16:47:54.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:47:54.48
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:54.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:54.504
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 03/09/23 16:47:54.511
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/09/23 16:47:54.516
    STEP: patching the secret 03/09/23 16:47:54.52
    STEP: deleting the secret using a LabelSelector 03/09/23 16:47:54.527
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/09/23 16:47:54.533
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:47:54.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5113" for this suite. 03/09/23 16:47:54.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:54.549
Mar  9 16:47:54.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 16:47:54.55
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:54.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:54.564
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar  9 16:47:54.567: INFO: Creating simple deployment test-new-deployment
Mar  9 16:47:54.578: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 03/09/23 16:47:56.59
STEP: updating a scale subresource 03/09/23 16:47:56.592
STEP: verifying the deployment Spec.Replicas was modified 03/09/23 16:47:56.598
STEP: Patch a scale subresource 03/09/23 16:47:56.6
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 16:47:56.619: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1636  d18f727d-0681-46c4-9dfc-bbd8dc96cd4e 108809 3 2023-03-09 16:47:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-09 16:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006010f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:47:55 +0000 UTC,LastTransitionTime:2023-03-09 16:47:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-09 16:47:55 +0000 UTC,LastTransitionTime:2023-03-09 16:47:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  9 16:47:56.622: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1636  b93ee497-b5f3-410c-8237-07b4ba29e9f0 108814 2 2023-03-09 16:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d18f727d-0681-46c4-9dfc-bbd8dc96cd4e 0xc0060114a7 0xc0060114a8}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d18f727d-0681-46c4-9dfc-bbd8dc96cd4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006011538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:47:56.627: INFO: Pod "test-new-deployment-845c8977d9-84mpv" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-84mpv test-new-deployment-845c8977d9- deployment-1636  22192542-7615-4e8e-bdcb-0007ac905de7 108801 0 2023-03-09 16:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e5eccf59fa5b93062321ec4e54fc9cc33f7a16b2f48b97459c0685eb3cc772da cni.projectcalico.org/podIP:10.244.42.196/32 cni.projectcalico.org/podIPs:10.244.42.196/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b93ee497-b5f3-410c-8237-07b4ba29e9f0 0xc006011907 0xc006011908}] [] [{kube-controller-manager Update v1 2023-03-09 16:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93ee497-b5f3-410c-8237-07b4ba29e9f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmz7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmz7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.196,StartTime:2023-03-09 16:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:47:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://a2a27755af5342c8045ffb4b445e3e4caf4532f150dfc52b78202b5a5ef7260a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:47:56.627: INFO: Pod "test-new-deployment-845c8977d9-dpnlf" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-dpnlf test-new-deployment-845c8977d9- deployment-1636  59ef93d3-2715-47eb-bc44-8ac678159af4 108813 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b93ee497-b5f3-410c-8237-07b4ba29e9f0 0xc006011b27 0xc006011b28}] [] [{kube-controller-manager Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93ee497-b5f3-410c-8237-07b4ba29e9f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wvcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wvcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 16:47:56.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1636" for this suite. 03/09/23 16:47:56.632
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":204,"skipped":3966,"failed":0}
------------------------------
â€¢ [2.092 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:54.549
    Mar  9 16:47:54.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 16:47:54.55
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:54.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:54.564
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar  9 16:47:54.567: INFO: Creating simple deployment test-new-deployment
    Mar  9 16:47:54.578: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 03/09/23 16:47:56.59
    STEP: updating a scale subresource 03/09/23 16:47:56.592
    STEP: verifying the deployment Spec.Replicas was modified 03/09/23 16:47:56.598
    STEP: Patch a scale subresource 03/09/23 16:47:56.6
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 16:47:56.619: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1636  d18f727d-0681-46c4-9dfc-bbd8dc96cd4e 108809 3 2023-03-09 16:47:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-09 16:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006010f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:47:55 +0000 UTC,LastTransitionTime:2023-03-09 16:47:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-09 16:47:55 +0000 UTC,LastTransitionTime:2023-03-09 16:47:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  9 16:47:56.622: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1636  b93ee497-b5f3-410c-8237-07b4ba29e9f0 108814 2 2023-03-09 16:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d18f727d-0681-46c4-9dfc-bbd8dc96cd4e 0xc0060114a7 0xc0060114a8}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d18f727d-0681-46c4-9dfc-bbd8dc96cd4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006011538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:47:56.627: INFO: Pod "test-new-deployment-845c8977d9-84mpv" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-84mpv test-new-deployment-845c8977d9- deployment-1636  22192542-7615-4e8e-bdcb-0007ac905de7 108801 0 2023-03-09 16:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e5eccf59fa5b93062321ec4e54fc9cc33f7a16b2f48b97459c0685eb3cc772da cni.projectcalico.org/podIP:10.244.42.196/32 cni.projectcalico.org/podIPs:10.244.42.196/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b93ee497-b5f3-410c-8237-07b4ba29e9f0 0xc006011907 0xc006011908}] [] [{kube-controller-manager Update v1 2023-03-09 16:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93ee497-b5f3-410c-8237-07b4ba29e9f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmz7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmz7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.196,StartTime:2023-03-09 16:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:47:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://a2a27755af5342c8045ffb4b445e3e4caf4532f150dfc52b78202b5a5ef7260a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:47:56.627: INFO: Pod "test-new-deployment-845c8977d9-dpnlf" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-dpnlf test-new-deployment-845c8977d9- deployment-1636  59ef93d3-2715-47eb-bc44-8ac678159af4 108813 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b93ee497-b5f3-410c-8237-07b4ba29e9f0 0xc006011b27 0xc006011b28}] [] [{kube-controller-manager Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93ee497-b5f3-410c-8237-07b4ba29e9f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wvcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wvcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:47:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 16:47:56.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1636" for this suite. 03/09/23 16:47:56.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:47:56.646
Mar  9 16:47:56.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename watch 03/09/23 16:47:56.647
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:56.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:56.661
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/09/23 16:47:56.664
STEP: creating a new configmap 03/09/23 16:47:56.665
STEP: modifying the configmap once 03/09/23 16:47:56.669
STEP: changing the label value of the configmap 03/09/23 16:47:56.674
STEP: Expecting to observe a delete notification for the watched object 03/09/23 16:47:56.682
Mar  9 16:47:56.682: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108827 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 16:47:56.682: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108828 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 16:47:56.682: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108829 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/09/23 16:47:56.682
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/09/23 16:47:56.688
STEP: changing the label value of the configmap back 03/09/23 16:48:06.689
STEP: modifying the configmap a third time 03/09/23 16:48:06.695
STEP: deleting the configmap 03/09/23 16:48:06.702
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/09/23 16:48:06.707
Mar  9 16:48:06.707: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108902 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:48:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 16:48:06.707: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108903 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:48:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 16:48:06.707: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108904 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:48:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  9 16:48:06.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9965" for this suite. 03/09/23 16:48:06.711
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":205,"skipped":4008,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.070 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:47:56.646
    Mar  9 16:47:56.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename watch 03/09/23 16:47:56.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:47:56.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:47:56.661
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/09/23 16:47:56.664
    STEP: creating a new configmap 03/09/23 16:47:56.665
    STEP: modifying the configmap once 03/09/23 16:47:56.669
    STEP: changing the label value of the configmap 03/09/23 16:47:56.674
    STEP: Expecting to observe a delete notification for the watched object 03/09/23 16:47:56.682
    Mar  9 16:47:56.682: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108827 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 16:47:56.682: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108828 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 16:47:56.682: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108829 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:47:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/09/23 16:47:56.682
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/09/23 16:47:56.688
    STEP: changing the label value of the configmap back 03/09/23 16:48:06.689
    STEP: modifying the configmap a third time 03/09/23 16:48:06.695
    STEP: deleting the configmap 03/09/23 16:48:06.702
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/09/23 16:48:06.707
    Mar  9 16:48:06.707: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108902 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:48:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 16:48:06.707: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108903 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:48:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 16:48:06.707: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9965  dfaaa259-3294-4bde-bab4-df6b3ffcaa6c 108904 0 2023-03-09 16:47:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-09 16:48:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  9 16:48:06.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9965" for this suite. 03/09/23 16:48:06.711
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:48:06.717
Mar  9 16:48:06.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 16:48:06.718
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:06.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:06.733
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/09/23 16:48:06.736
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +notcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_udp@PTR;check="$$(dig +tcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_tcp@PTR;sleep 1; done
 03/09/23 16:48:06.757
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +notcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_udp@PTR;check="$$(dig +tcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_tcp@PTR;sleep 1; done
 03/09/23 16:48:06.758
STEP: creating a pod to probe DNS 03/09/23 16:48:06.758
STEP: submitting the pod to kubernetes 03/09/23 16:48:06.758
Mar  9 16:48:06.767: INFO: Waiting up to 15m0s for pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6" in namespace "dns-1033" to be "running"
Mar  9 16:48:06.771: INFO: Pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646616ms
Mar  9 16:48:08.775: INFO: Pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007771289s
Mar  9 16:48:08.775: INFO: Pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6" satisfied condition "running"
STEP: retrieving the pod 03/09/23 16:48:08.775
STEP: looking for the results for each expected name from probers 03/09/23 16:48:08.778
Mar  9 16:48:08.783: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.786: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.789: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.792: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.796: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.799: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.806: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.821: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.825: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.828: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.831: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.834: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.840: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:08.854: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

Mar  9 16:48:13.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.862: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.876: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.879: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.893: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.896: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.899: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.904: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.907: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.910: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:13.924: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

Mar  9 16:48:18.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.875: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.878: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.892: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.895: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.898: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.900: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.903: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.906: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.909: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:18.923: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

Mar  9 16:48:23.860: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.863: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.879: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.884: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.891: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.895: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.916: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.920: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.925: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.928: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.932: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.937: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.941: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.946: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:23.963: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

Mar  9 16:48:28.859: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.862: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.868: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.871: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.874: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.877: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.880: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.895: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.898: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.901: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.904: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.907: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.914: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.916: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:28.929: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

Mar  9 16:48:33.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.876: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.879: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.893: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.896: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.899: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.904: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.907: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.910: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:33.924: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

Mar  9 16:48:38.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:38.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:38.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:38.872: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:38.875: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:38.878: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
Mar  9 16:48:38.923: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc]

Mar  9 16:48:43.925: INFO: DNS probes using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 succeeded

STEP: deleting the pod 03/09/23 16:48:43.925
STEP: deleting the test service 03/09/23 16:48:43.94
STEP: deleting the test headless service 03/09/23 16:48:43.974
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 16:48:43.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1033" for this suite. 03/09/23 16:48:43.993
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":206,"skipped":4008,"failed":0}
------------------------------
â€¢ [SLOW TEST] [37.285 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:48:06.717
    Mar  9 16:48:06.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 16:48:06.718
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:06.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:06.733
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/09/23 16:48:06.736
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +notcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_udp@PTR;check="$$(dig +tcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_tcp@PTR;sleep 1; done
     03/09/23 16:48:06.757
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1033.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1033.svc;check="$$(dig +notcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_udp@PTR;check="$$(dig +tcp +noall +answer +search 253.133.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.133.253_tcp@PTR;sleep 1; done
     03/09/23 16:48:06.758
    STEP: creating a pod to probe DNS 03/09/23 16:48:06.758
    STEP: submitting the pod to kubernetes 03/09/23 16:48:06.758
    Mar  9 16:48:06.767: INFO: Waiting up to 15m0s for pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6" in namespace "dns-1033" to be "running"
    Mar  9 16:48:06.771: INFO: Pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646616ms
    Mar  9 16:48:08.775: INFO: Pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007771289s
    Mar  9 16:48:08.775: INFO: Pod "dns-test-af999903-f013-45a8-bd78-f6b520ffaca6" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 16:48:08.775
    STEP: looking for the results for each expected name from probers 03/09/23 16:48:08.778
    Mar  9 16:48:08.783: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.786: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.789: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.792: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.796: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.799: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.806: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.821: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.825: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.828: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.831: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.834: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.837: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.840: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:08.854: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

    Mar  9 16:48:13.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.862: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.876: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.879: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.893: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.896: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.899: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.904: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.907: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.910: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:13.924: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

    Mar  9 16:48:18.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.875: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.878: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.892: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.895: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.898: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.900: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.903: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.906: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.909: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:18.923: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

    Mar  9 16:48:23.860: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.863: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.879: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.884: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.891: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.895: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.916: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.920: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.925: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.928: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.932: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.937: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.941: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.946: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:23.963: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

    Mar  9 16:48:28.859: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.862: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.868: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.871: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.874: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.877: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.880: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.895: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.898: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.901: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.904: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.907: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.914: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.916: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:28.929: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

    Mar  9 16:48:33.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.876: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.879: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.893: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.896: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.899: INFO: Unable to read jessie_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.904: INFO: Unable to read jessie_udp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.907: INFO: Unable to read jessie_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.910: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:33.924: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033 wheezy_udp@dns-test-service.dns-1033.svc wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1033 jessie_tcp@dns-test-service.dns-1033 jessie_udp@dns-test-service.dns-1033.svc jessie_tcp@dns-test-service.dns-1033.svc jessie_udp@_http._tcp.dns-test-service.dns-1033.svc jessie_tcp@_http._tcp.dns-test-service.dns-1033.svc]

    Mar  9 16:48:38.858: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:38.861: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:38.864: INFO: Unable to read wheezy_udp@dns-test-service.dns-1033 from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:38.872: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:38.875: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:38.878: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc from pod dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6: the server could not find the requested resource (get pods dns-test-af999903-f013-45a8-bd78-f6b520ffaca6)
    Mar  9 16:48:38.923: INFO: Lookups using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1033 wheezy_tcp@dns-test-service.dns-1033.svc wheezy_udp@_http._tcp.dns-test-service.dns-1033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1033.svc]

    Mar  9 16:48:43.925: INFO: DNS probes using dns-1033/dns-test-af999903-f013-45a8-bd78-f6b520ffaca6 succeeded

    STEP: deleting the pod 03/09/23 16:48:43.925
    STEP: deleting the test service 03/09/23 16:48:43.94
    STEP: deleting the test headless service 03/09/23 16:48:43.974
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 16:48:43.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1033" for this suite. 03/09/23 16:48:43.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:48:44.007
Mar  9 16:48:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:48:44.01
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:44.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:44.03
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/09/23 16:48:44.034
Mar  9 16:48:44.042: INFO: Waiting up to 5m0s for pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde" in namespace "emptydir-9097" to be "Succeeded or Failed"
Mar  9 16:48:44.046: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59047ms
Mar  9 16:48:46.051: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008726891s
Mar  9 16:48:48.050: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008419357s
STEP: Saw pod success 03/09/23 16:48:48.05
Mar  9 16:48:48.050: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde" satisfied condition "Succeeded or Failed"
Mar  9 16:48:48.053: INFO: Trying to get logs from node tt-test-el8-003 pod pod-142822f4-01ea-4078-83c7-6f4efd14fdde container test-container: <nil>
STEP: delete the pod 03/09/23 16:48:48.059
Mar  9 16:48:48.067: INFO: Waiting for pod pod-142822f4-01ea-4078-83c7-6f4efd14fdde to disappear
Mar  9 16:48:48.069: INFO: Pod pod-142822f4-01ea-4078-83c7-6f4efd14fdde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:48:48.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9097" for this suite. 03/09/23 16:48:48.073
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":207,"skipped":4026,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:48:44.007
    Mar  9 16:48:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:48:44.01
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:44.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:44.03
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/09/23 16:48:44.034
    Mar  9 16:48:44.042: INFO: Waiting up to 5m0s for pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde" in namespace "emptydir-9097" to be "Succeeded or Failed"
    Mar  9 16:48:44.046: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59047ms
    Mar  9 16:48:46.051: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008726891s
    Mar  9 16:48:48.050: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008419357s
    STEP: Saw pod success 03/09/23 16:48:48.05
    Mar  9 16:48:48.050: INFO: Pod "pod-142822f4-01ea-4078-83c7-6f4efd14fdde" satisfied condition "Succeeded or Failed"
    Mar  9 16:48:48.053: INFO: Trying to get logs from node tt-test-el8-003 pod pod-142822f4-01ea-4078-83c7-6f4efd14fdde container test-container: <nil>
    STEP: delete the pod 03/09/23 16:48:48.059
    Mar  9 16:48:48.067: INFO: Waiting for pod pod-142822f4-01ea-4078-83c7-6f4efd14fdde to disappear
    Mar  9 16:48:48.069: INFO: Pod pod-142822f4-01ea-4078-83c7-6f4efd14fdde no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:48:48.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9097" for this suite. 03/09/23 16:48:48.073
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:48:48.078
Mar  9 16:48:48.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:48:48.079
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:48.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:48.093
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:48:48.106
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:48:48.64
STEP: Deploying the webhook pod 03/09/23 16:48:48.646
STEP: Wait for the deployment to be ready 03/09/23 16:48:48.656
Mar  9 16:48:48.663: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:48:50.673
STEP: Verifying the service has paired with the endpoint 03/09/23 16:48:50.688
Mar  9 16:48:51.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 03/09/23 16:48:51.692
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/09/23 16:48:51.694
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/09/23 16:48:51.694
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/09/23 16:48:51.694
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/09/23 16:48:51.695
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/09/23 16:48:51.695
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/09/23 16:48:51.696
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:48:51.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-714" for this suite. 03/09/23 16:48:51.7
STEP: Destroying namespace "webhook-714-markers" for this suite. 03/09/23 16:48:51.705
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":208,"skipped":4028,"failed":0}
------------------------------
â€¢ [3.672 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:48:48.078
    Mar  9 16:48:48.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:48:48.079
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:48.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:48.093
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:48:48.106
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:48:48.64
    STEP: Deploying the webhook pod 03/09/23 16:48:48.646
    STEP: Wait for the deployment to be ready 03/09/23 16:48:48.656
    Mar  9 16:48:48.663: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:48:50.673
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:48:50.688
    Mar  9 16:48:51.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 03/09/23 16:48:51.692
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/09/23 16:48:51.694
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/09/23 16:48:51.694
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/09/23 16:48:51.694
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/09/23 16:48:51.695
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/09/23 16:48:51.695
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/09/23 16:48:51.696
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:48:51.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-714" for this suite. 03/09/23 16:48:51.7
    STEP: Destroying namespace "webhook-714-markers" for this suite. 03/09/23 16:48:51.705
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:48:51.751
Mar  9 16:48:51.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-pred 03/09/23 16:48:51.753
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:51.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:51.778
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  9 16:48:51.786: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  9 16:48:51.799: INFO: Waiting for terminating namespaces to be deleted...
Mar  9 16:48:51.802: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-003 before test
Mar  9 16:48:51.809: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.809: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 16:48:51.809: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.809: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 16:48:51.809: INFO: csi-node-driver-8kqcp from calico-system started at 2023-03-09 16:12:55 +0000 UTC (2 container statuses recorded)
Mar  9 16:48:51.809: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 16:48:51.809: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 16:48:51.809: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.809: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 16:48:51.809: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.809: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  9 16:48:51.809: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 16:48:51.809: INFO: 	Container e2e ready: true, restart count 0
Mar  9 16:48:51.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 16:48:51.809: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 16:48:51.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 16:48:51.809: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 16:48:51.809: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-004 before test
Mar  9 16:48:51.819: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container calico-apiserver ready: true, restart count 0
Mar  9 16:48:51.819: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  9 16:48:51.819: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 16:48:51.819: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 16:48:51.819: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 16:48:51.819: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 16:48:51.819: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container webhook ready: true, restart count 0
Mar  9 16:48:51.819: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container coredns ready: true, restart count 0
Mar  9 16:48:51.819: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container coredns ready: true, restart count 0
Mar  9 16:48:51.819: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 16:48:51.819: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  9 16:48:51.819: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 16:48:51.819: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 16:48:51.819: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 16:48:51.819: INFO: 	Container tigera-operator ready: true, restart count 3
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node tt-test-el8-003 03/09/23 16:48:51.845
STEP: verifying the node has the label node tt-test-el8-004 03/09/23 16:48:51.862
Mar  9 16:48:51.885: INFO: Pod calico-apiserver-7c747f5cd5-b5vxx requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod calico-kube-controllers-764fd57778-m668z requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod calico-node-2bvv5 requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod calico-node-wh8hs requesting resource cpu=0m on Node tt-test-el8-003
Mar  9 16:48:51.885: INFO: Pod calico-typha-7cd8bd454-fdn6r requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod calico-typha-7cd8bd454-gbhkv requesting resource cpu=0m on Node tt-test-el8-003
Mar  9 16:48:51.885: INFO: Pod csi-node-driver-8kqcp requesting resource cpu=0m on Node tt-test-el8-003
Mar  9 16:48:51.885: INFO: Pod csi-node-driver-mmnqh requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod externalip-validation-webhook-76d97fbd6-96c5g requesting resource cpu=100m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod coredns-7b86c745f6-dj4rw requesting resource cpu=100m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod coredns-7b86c745f6-gblnw requesting resource cpu=100m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod kube-proxy-hrgxt requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod kube-proxy-k95qd requesting resource cpu=0m on Node tt-test-el8-003
Mar  9 16:48:51.885: INFO: Pod kubernetes-dashboard-5c84574c69-4r4nv requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod sonobuoy requesting resource cpu=0m on Node tt-test-el8-003
Mar  9 16:48:51.885: INFO: Pod sonobuoy-e2e-job-975b039fb38f48d3 requesting resource cpu=0m on Node tt-test-el8-003
Mar  9 16:48:51.885: INFO: Pod sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t requesting resource cpu=0m on Node tt-test-el8-004
Mar  9 16:48:51.885: INFO: Pod sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp requesting resource cpu=0m on Node tt-test-el8-003
Mar  9 16:48:51.885: INFO: Pod tigera-operator-7cbc46df58-t82qm requesting resource cpu=0m on Node tt-test-el8-004
STEP: Starting Pods to consume most of the cluster CPU. 03/09/23 16:48:51.885
Mar  9 16:48:51.885: INFO: Creating a pod which consumes cpu=2800m on Node tt-test-el8-003
Mar  9 16:48:51.895: INFO: Creating a pod which consumes cpu=2590m on Node tt-test-el8-004
Mar  9 16:48:51.905: INFO: Waiting up to 5m0s for pod "filler-pod-32838971-a428-4bca-b439-94600b711449" in namespace "sched-pred-6445" to be "running"
Mar  9 16:48:51.911: INFO: Pod "filler-pod-32838971-a428-4bca-b439-94600b711449": Phase="Pending", Reason="", readiness=false. Elapsed: 5.688795ms
Mar  9 16:48:53.915: INFO: Pod "filler-pod-32838971-a428-4bca-b439-94600b711449": Phase="Running", Reason="", readiness=true. Elapsed: 2.009529588s
Mar  9 16:48:53.915: INFO: Pod "filler-pod-32838971-a428-4bca-b439-94600b711449" satisfied condition "running"
Mar  9 16:48:53.915: INFO: Waiting up to 5m0s for pod "filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49" in namespace "sched-pred-6445" to be "running"
Mar  9 16:48:53.918: INFO: Pod "filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49": Phase="Running", Reason="", readiness=true. Elapsed: 2.581248ms
Mar  9 16:48:53.918: INFO: Pod "filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/09/23 16:48:53.918
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace338a111f4d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6445/filler-pod-32838971-a428-4bca-b439-94600b711449 to tt-test-el8-003] 03/09/23 16:48:53.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace33adce2c24], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/09/23 16:48:53.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace33b7552d30], Reason = [Created], Message = [Created container filler-pod-32838971-a428-4bca-b439-94600b711449] 03/09/23 16:48:53.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace33b949636a], Reason = [Started], Message = [Started container filler-pod-32838971-a428-4bca-b439-94600b711449] 03/09/23 16:48:53.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace338a94841d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6445/filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49 to tt-test-el8-004] 03/09/23 16:48:53.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace33aedad7d1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/09/23 16:48:53.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace33b791a4d3], Reason = [Created], Message = [Created container filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49] 03/09/23 16:48:53.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace33b92d356f], Reason = [Started], Message = [Started container filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49] 03/09/23 16:48:53.922
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174ace3402a96cca], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 03/09/23 16:48:53.932
STEP: removing the label node off the node tt-test-el8-003 03/09/23 16:48:54.93
STEP: verifying the node doesn't have the label node 03/09/23 16:48:54.943
STEP: removing the label node off the node tt-test-el8-004 03/09/23 16:48:54.949
STEP: verifying the node doesn't have the label node 03/09/23 16:48:54.96
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:48:54.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6445" for this suite. 03/09/23 16:48:54.967
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":209,"skipped":4037,"failed":0}
------------------------------
â€¢ [3.222 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:48:51.751
    Mar  9 16:48:51.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-pred 03/09/23 16:48:51.753
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:51.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:51.778
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  9 16:48:51.786: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  9 16:48:51.799: INFO: Waiting for terminating namespaces to be deleted...
    Mar  9 16:48:51.802: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-003 before test
    Mar  9 16:48:51.809: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.809: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.809: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: csi-node-driver-8kqcp from calico-system started at 2023-03-09 16:12:55 +0000 UTC (2 container statuses recorded)
    Mar  9 16:48:51.809: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.809: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.809: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 16:48:51.809: INFO: 	Container e2e ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 16:48:51.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 16:48:51.809: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-004 before test
    Mar  9 16:48:51.819: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container calico-apiserver ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container webhook ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 16:48:51.819: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 16:48:51.819: INFO: 	Container tigera-operator ready: true, restart count 3
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node tt-test-el8-003 03/09/23 16:48:51.845
    STEP: verifying the node has the label node tt-test-el8-004 03/09/23 16:48:51.862
    Mar  9 16:48:51.885: INFO: Pod calico-apiserver-7c747f5cd5-b5vxx requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod calico-kube-controllers-764fd57778-m668z requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod calico-node-2bvv5 requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod calico-node-wh8hs requesting resource cpu=0m on Node tt-test-el8-003
    Mar  9 16:48:51.885: INFO: Pod calico-typha-7cd8bd454-fdn6r requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod calico-typha-7cd8bd454-gbhkv requesting resource cpu=0m on Node tt-test-el8-003
    Mar  9 16:48:51.885: INFO: Pod csi-node-driver-8kqcp requesting resource cpu=0m on Node tt-test-el8-003
    Mar  9 16:48:51.885: INFO: Pod csi-node-driver-mmnqh requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod externalip-validation-webhook-76d97fbd6-96c5g requesting resource cpu=100m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod coredns-7b86c745f6-dj4rw requesting resource cpu=100m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod coredns-7b86c745f6-gblnw requesting resource cpu=100m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod kube-proxy-hrgxt requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod kube-proxy-k95qd requesting resource cpu=0m on Node tt-test-el8-003
    Mar  9 16:48:51.885: INFO: Pod kubernetes-dashboard-5c84574c69-4r4nv requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod sonobuoy requesting resource cpu=0m on Node tt-test-el8-003
    Mar  9 16:48:51.885: INFO: Pod sonobuoy-e2e-job-975b039fb38f48d3 requesting resource cpu=0m on Node tt-test-el8-003
    Mar  9 16:48:51.885: INFO: Pod sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t requesting resource cpu=0m on Node tt-test-el8-004
    Mar  9 16:48:51.885: INFO: Pod sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp requesting resource cpu=0m on Node tt-test-el8-003
    Mar  9 16:48:51.885: INFO: Pod tigera-operator-7cbc46df58-t82qm requesting resource cpu=0m on Node tt-test-el8-004
    STEP: Starting Pods to consume most of the cluster CPU. 03/09/23 16:48:51.885
    Mar  9 16:48:51.885: INFO: Creating a pod which consumes cpu=2800m on Node tt-test-el8-003
    Mar  9 16:48:51.895: INFO: Creating a pod which consumes cpu=2590m on Node tt-test-el8-004
    Mar  9 16:48:51.905: INFO: Waiting up to 5m0s for pod "filler-pod-32838971-a428-4bca-b439-94600b711449" in namespace "sched-pred-6445" to be "running"
    Mar  9 16:48:51.911: INFO: Pod "filler-pod-32838971-a428-4bca-b439-94600b711449": Phase="Pending", Reason="", readiness=false. Elapsed: 5.688795ms
    Mar  9 16:48:53.915: INFO: Pod "filler-pod-32838971-a428-4bca-b439-94600b711449": Phase="Running", Reason="", readiness=true. Elapsed: 2.009529588s
    Mar  9 16:48:53.915: INFO: Pod "filler-pod-32838971-a428-4bca-b439-94600b711449" satisfied condition "running"
    Mar  9 16:48:53.915: INFO: Waiting up to 5m0s for pod "filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49" in namespace "sched-pred-6445" to be "running"
    Mar  9 16:48:53.918: INFO: Pod "filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49": Phase="Running", Reason="", readiness=true. Elapsed: 2.581248ms
    Mar  9 16:48:53.918: INFO: Pod "filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/09/23 16:48:53.918
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace338a111f4d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6445/filler-pod-32838971-a428-4bca-b439-94600b711449 to tt-test-el8-003] 03/09/23 16:48:53.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace33adce2c24], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/09/23 16:48:53.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace33b7552d30], Reason = [Created], Message = [Created container filler-pod-32838971-a428-4bca-b439-94600b711449] 03/09/23 16:48:53.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-32838971-a428-4bca-b439-94600b711449.174ace33b949636a], Reason = [Started], Message = [Started container filler-pod-32838971-a428-4bca-b439-94600b711449] 03/09/23 16:48:53.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace338a94841d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6445/filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49 to tt-test-el8-004] 03/09/23 16:48:53.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace33aedad7d1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/09/23 16:48:53.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace33b791a4d3], Reason = [Created], Message = [Created container filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49] 03/09/23 16:48:53.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49.174ace33b92d356f], Reason = [Started], Message = [Started container filler-pod-f04597d0-8a48-459a-bf41-01563d9dca49] 03/09/23 16:48:53.922
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.174ace3402a96cca], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.] 03/09/23 16:48:53.932
    STEP: removing the label node off the node tt-test-el8-003 03/09/23 16:48:54.93
    STEP: verifying the node doesn't have the label node 03/09/23 16:48:54.943
    STEP: removing the label node off the node tt-test-el8-004 03/09/23 16:48:54.949
    STEP: verifying the node doesn't have the label node 03/09/23 16:48:54.96
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:48:54.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6445" for this suite. 03/09/23 16:48:54.967
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:48:54.979
Mar  9 16:48:54.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:48:54.981
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:54.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:54.996
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 03/09/23 16:48:54.999
Mar  9 16:48:54.999: INFO: Creating e2e-svc-a-88ghb
Mar  9 16:48:55.014: INFO: Creating e2e-svc-b-7jhb6
Mar  9 16:48:55.030: INFO: Creating e2e-svc-c-87kfp
STEP: deleting service collection 03/09/23 16:48:55.048
Mar  9 16:48:55.084: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:48:55.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8986" for this suite. 03/09/23 16:48:55.088
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":210,"skipped":4083,"failed":0}
------------------------------
â€¢ [0.115 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:48:54.979
    Mar  9 16:48:54.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:48:54.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:54.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:54.996
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 03/09/23 16:48:54.999
    Mar  9 16:48:54.999: INFO: Creating e2e-svc-a-88ghb
    Mar  9 16:48:55.014: INFO: Creating e2e-svc-b-7jhb6
    Mar  9 16:48:55.030: INFO: Creating e2e-svc-c-87kfp
    STEP: deleting service collection 03/09/23 16:48:55.048
    Mar  9 16:48:55.084: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:48:55.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8986" for this suite. 03/09/23 16:48:55.088
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:48:55.095
Mar  9 16:48:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:48:55.096
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:55.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:55.112
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 03/09/23 16:48:55.115
Mar  9 16:48:55.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 create -f -'
Mar  9 16:48:56.462: INFO: stderr: ""
Mar  9 16:48:56.463: INFO: stdout: "pod/pause created\n"
Mar  9 16:48:56.463: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  9 16:48:56.463: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8851" to be "running and ready"
Mar  9 16:48:56.466: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.245931ms
Mar  9 16:48:56.466: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tt-test-el8-003' to be 'Running' but was 'Pending'
Mar  9 16:48:58.470: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007804992s
Mar  9 16:48:58.470: INFO: Pod "pause" satisfied condition "running and ready"
Mar  9 16:48:58.470: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 03/09/23 16:48:58.47
Mar  9 16:48:58.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 label pods pause testing-label=testing-label-value'
Mar  9 16:48:58.548: INFO: stderr: ""
Mar  9 16:48:58.548: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/09/23 16:48:58.548
Mar  9 16:48:58.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get pod pause -L testing-label'
Mar  9 16:48:58.621: INFO: stderr: ""
Mar  9 16:48:58.621: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/09/23 16:48:58.621
Mar  9 16:48:58.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 label pods pause testing-label-'
Mar  9 16:48:58.701: INFO: stderr: ""
Mar  9 16:48:58.701: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/09/23 16:48:58.701
Mar  9 16:48:58.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get pod pause -L testing-label'
Mar  9 16:48:58.778: INFO: stderr: ""
Mar  9 16:48:58.778: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 03/09/23 16:48:58.778
Mar  9 16:48:58.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 delete --grace-period=0 --force -f -'
Mar  9 16:48:58.854: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:48:58.855: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  9 16:48:58.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get rc,svc -l name=pause --no-headers'
Mar  9 16:48:58.938: INFO: stderr: "No resources found in kubectl-8851 namespace.\n"
Mar  9 16:48:58.938: INFO: stdout: ""
Mar  9 16:48:58.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  9 16:48:59.006: INFO: stderr: ""
Mar  9 16:48:59.006: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:48:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8851" for this suite. 03/09/23 16:48:59.011
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":211,"skipped":4088,"failed":0}
------------------------------
â€¢ [3.921 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:48:55.095
    Mar  9 16:48:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:48:55.096
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:55.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:55.112
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 03/09/23 16:48:55.115
    Mar  9 16:48:55.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 create -f -'
    Mar  9 16:48:56.462: INFO: stderr: ""
    Mar  9 16:48:56.463: INFO: stdout: "pod/pause created\n"
    Mar  9 16:48:56.463: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar  9 16:48:56.463: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8851" to be "running and ready"
    Mar  9 16:48:56.466: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.245931ms
    Mar  9 16:48:56.466: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'tt-test-el8-003' to be 'Running' but was 'Pending'
    Mar  9 16:48:58.470: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007804992s
    Mar  9 16:48:58.470: INFO: Pod "pause" satisfied condition "running and ready"
    Mar  9 16:48:58.470: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 03/09/23 16:48:58.47
    Mar  9 16:48:58.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 label pods pause testing-label=testing-label-value'
    Mar  9 16:48:58.548: INFO: stderr: ""
    Mar  9 16:48:58.548: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/09/23 16:48:58.548
    Mar  9 16:48:58.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get pod pause -L testing-label'
    Mar  9 16:48:58.621: INFO: stderr: ""
    Mar  9 16:48:58.621: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/09/23 16:48:58.621
    Mar  9 16:48:58.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 label pods pause testing-label-'
    Mar  9 16:48:58.701: INFO: stderr: ""
    Mar  9 16:48:58.701: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/09/23 16:48:58.701
    Mar  9 16:48:58.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get pod pause -L testing-label'
    Mar  9 16:48:58.778: INFO: stderr: ""
    Mar  9 16:48:58.778: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 03/09/23 16:48:58.778
    Mar  9 16:48:58.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 delete --grace-period=0 --force -f -'
    Mar  9 16:48:58.854: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:48:58.855: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar  9 16:48:58.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get rc,svc -l name=pause --no-headers'
    Mar  9 16:48:58.938: INFO: stderr: "No resources found in kubectl-8851 namespace.\n"
    Mar  9 16:48:58.938: INFO: stdout: ""
    Mar  9 16:48:58.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-8851 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  9 16:48:59.006: INFO: stderr: ""
    Mar  9 16:48:59.006: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:48:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8851" for this suite. 03/09/23 16:48:59.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:48:59.017
Mar  9 16:48:59.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 16:48:59.018
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:59.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:59.033
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 03/09/23 16:48:59.036
STEP: waiting for pod running 03/09/23 16:48:59.042
Mar  9 16:48:59.043: INFO: Waiting up to 2m0s for pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" in namespace "var-expansion-3271" to be "running"
Mar  9 16:48:59.045: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535419ms
Mar  9 16:49:01.049: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006938696s
Mar  9 16:49:01.050: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" satisfied condition "running"
STEP: creating a file in subpath 03/09/23 16:49:01.05
Mar  9 16:49:01.053: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3271 PodName:var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:49:01.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:49:01.054: INFO: ExecWithOptions: Clientset creation
Mar  9 16:49:01.054: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3271/pods/var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/09/23 16:49:01.142
Mar  9 16:49:01.145: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3271 PodName:var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:49:01.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:49:01.146: INFO: ExecWithOptions: Clientset creation
Mar  9 16:49:01.146: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3271/pods/var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/09/23 16:49:01.218
Mar  9 16:49:01.729: INFO: Successfully updated pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2"
STEP: waiting for annotated pod running 03/09/23 16:49:01.729
Mar  9 16:49:01.730: INFO: Waiting up to 2m0s for pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" in namespace "var-expansion-3271" to be "running"
Mar  9 16:49:01.732: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.586631ms
Mar  9 16:49:01.732: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" satisfied condition "running"
STEP: deleting the pod gracefully 03/09/23 16:49:01.732
Mar  9 16:49:01.732: INFO: Deleting pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" in namespace "var-expansion-3271"
Mar  9 16:49:01.738: INFO: Wait up to 5m0s for pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 16:49:35.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3271" for this suite. 03/09/23 16:49:35.748
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":212,"skipped":4100,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.736 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:48:59.017
    Mar  9 16:48:59.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 16:48:59.018
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:48:59.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:48:59.033
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 03/09/23 16:48:59.036
    STEP: waiting for pod running 03/09/23 16:48:59.042
    Mar  9 16:48:59.043: INFO: Waiting up to 2m0s for pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" in namespace "var-expansion-3271" to be "running"
    Mar  9 16:48:59.045: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535419ms
    Mar  9 16:49:01.049: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006938696s
    Mar  9 16:49:01.050: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" satisfied condition "running"
    STEP: creating a file in subpath 03/09/23 16:49:01.05
    Mar  9 16:49:01.053: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3271 PodName:var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:49:01.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:49:01.054: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:49:01.054: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3271/pods/var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/09/23 16:49:01.142
    Mar  9 16:49:01.145: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3271 PodName:var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:49:01.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:49:01.146: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:49:01.146: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3271/pods/var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/09/23 16:49:01.218
    Mar  9 16:49:01.729: INFO: Successfully updated pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2"
    STEP: waiting for annotated pod running 03/09/23 16:49:01.729
    Mar  9 16:49:01.730: INFO: Waiting up to 2m0s for pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" in namespace "var-expansion-3271" to be "running"
    Mar  9 16:49:01.732: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.586631ms
    Mar  9 16:49:01.732: INFO: Pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" satisfied condition "running"
    STEP: deleting the pod gracefully 03/09/23 16:49:01.732
    Mar  9 16:49:01.732: INFO: Deleting pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" in namespace "var-expansion-3271"
    Mar  9 16:49:01.738: INFO: Wait up to 5m0s for pod "var-expansion-5728e859-eb52-408d-8ece-4570c231ecc2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 16:49:35.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3271" for this suite. 03/09/23 16:49:35.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:49:35.757
Mar  9 16:49:35.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename job 03/09/23 16:49:35.758
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:35.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:35.771
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 03/09/23 16:49:35.776
STEP: Patching the Job 03/09/23 16:49:35.781
STEP: Watching for Job to be patched 03/09/23 16:49:35.799
Mar  9 16:49:35.800: INFO: Event ADDED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  9 16:49:35.800: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  9 16:49:35.801: INFO: Event MODIFIED found for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/09/23 16:49:35.801
STEP: Watching for Job to be updated 03/09/23 16:49:35.808
Mar  9 16:49:35.810: INFO: Event MODIFIED found for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  9 16:49:35.810: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/09/23 16:49:35.81
Mar  9 16:49:35.813: INFO: Job: e2e-lpl8p as labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched]
STEP: Waiting for job to complete 03/09/23 16:49:35.813
STEP: Delete a job collection with a labelselector 03/09/23 16:49:43.817
STEP: Watching for Job to be deleted 03/09/23 16:49:43.823
Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  9 16:49:43.825: INFO: Event DELETED found for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/09/23 16:49:43.826
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  9 16:49:43.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1369" for this suite. 03/09/23 16:49:43.835
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":213,"skipped":4116,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.084 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:49:35.757
    Mar  9 16:49:35.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename job 03/09/23 16:49:35.758
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:35.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:35.771
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 03/09/23 16:49:35.776
    STEP: Patching the Job 03/09/23 16:49:35.781
    STEP: Watching for Job to be patched 03/09/23 16:49:35.799
    Mar  9 16:49:35.800: INFO: Event ADDED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  9 16:49:35.800: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  9 16:49:35.801: INFO: Event MODIFIED found for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/09/23 16:49:35.801
    STEP: Watching for Job to be updated 03/09/23 16:49:35.808
    Mar  9 16:49:35.810: INFO: Event MODIFIED found for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  9 16:49:35.810: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/09/23 16:49:35.81
    Mar  9 16:49:35.813: INFO: Job: e2e-lpl8p as labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched]
    STEP: Waiting for job to complete 03/09/23 16:49:35.813
    STEP: Delete a job collection with a labelselector 03/09/23 16:49:43.817
    STEP: Watching for Job to be deleted 03/09/23 16:49:43.823
    Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  9 16:49:43.825: INFO: Event MODIFIED observed for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  9 16:49:43.825: INFO: Event DELETED found for Job e2e-lpl8p in namespace job-1369 with labels: map[e2e-job-label:e2e-lpl8p e2e-lpl8p:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/09/23 16:49:43.826
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  9 16:49:43.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1369" for this suite. 03/09/23 16:49:43.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:49:43.842
Mar  9 16:49:43.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:49:43.843
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:43.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:43.862
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 03/09/23 16:49:43.866
STEP: Creating a ResourceQuota 03/09/23 16:49:48.869
STEP: Ensuring resource quota status is calculated 03/09/23 16:49:48.873
STEP: Creating a ReplicaSet 03/09/23 16:49:50.878
STEP: Ensuring resource quota status captures replicaset creation 03/09/23 16:49:50.893
STEP: Deleting a ReplicaSet 03/09/23 16:49:52.897
STEP: Ensuring resource quota status released usage 03/09/23 16:49:52.902
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:49:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8418" for this suite. 03/09/23 16:49:54.911
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":214,"skipped":4124,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:49:43.842
    Mar  9 16:49:43.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:49:43.843
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:43.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:43.862
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 03/09/23 16:49:43.866
    STEP: Creating a ResourceQuota 03/09/23 16:49:48.869
    STEP: Ensuring resource quota status is calculated 03/09/23 16:49:48.873
    STEP: Creating a ReplicaSet 03/09/23 16:49:50.878
    STEP: Ensuring resource quota status captures replicaset creation 03/09/23 16:49:50.893
    STEP: Deleting a ReplicaSet 03/09/23 16:49:52.897
    STEP: Ensuring resource quota status released usage 03/09/23 16:49:52.902
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:49:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8418" for this suite. 03/09/23 16:49:54.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:49:54.916
Mar  9 16:49:54.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename server-version 03/09/23 16:49:54.917
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:54.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:54.934
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/09/23 16:49:54.937
STEP: Confirm major version 03/09/23 16:49:54.938
Mar  9 16:49:54.938: INFO: Major version: 1
STEP: Confirm minor version 03/09/23 16:49:54.938
Mar  9 16:49:54.938: INFO: cleanMinorVersion: 25
Mar  9 16:49:54.938: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Mar  9 16:49:54.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6773" for this suite. 03/09/23 16:49:54.942
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":215,"skipped":4132,"failed":0}
------------------------------
â€¢ [0.030 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:49:54.916
    Mar  9 16:49:54.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename server-version 03/09/23 16:49:54.917
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:54.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:54.934
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/09/23 16:49:54.937
    STEP: Confirm major version 03/09/23 16:49:54.938
    Mar  9 16:49:54.938: INFO: Major version: 1
    STEP: Confirm minor version 03/09/23 16:49:54.938
    Mar  9 16:49:54.938: INFO: cleanMinorVersion: 25
    Mar  9 16:49:54.938: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Mar  9 16:49:54.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-6773" for this suite. 03/09/23 16:49:54.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:49:54.949
Mar  9 16:49:54.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:49:54.95
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:54.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:54.964
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:49:54.968
Mar  9 16:49:54.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657" in namespace "projected-2189" to be "Succeeded or Failed"
Mar  9 16:49:54.977: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.995537ms
Mar  9 16:49:56.981: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006758341s
Mar  9 16:49:58.982: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007363439s
STEP: Saw pod success 03/09/23 16:49:58.982
Mar  9 16:49:58.982: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657" satisfied condition "Succeeded or Failed"
Mar  9 16:49:58.984: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657 container client-container: <nil>
STEP: delete the pod 03/09/23 16:49:58.99
Mar  9 16:49:59.000: INFO: Waiting for pod downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657 to disappear
Mar  9 16:49:59.003: INFO: Pod downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 16:49:59.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2189" for this suite. 03/09/23 16:49:59.007
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":216,"skipped":4157,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:49:54.949
    Mar  9 16:49:54.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:49:54.95
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:54.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:54.964
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:49:54.968
    Mar  9 16:49:54.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657" in namespace "projected-2189" to be "Succeeded or Failed"
    Mar  9 16:49:54.977: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.995537ms
    Mar  9 16:49:56.981: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006758341s
    Mar  9 16:49:58.982: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007363439s
    STEP: Saw pod success 03/09/23 16:49:58.982
    Mar  9 16:49:58.982: INFO: Pod "downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657" satisfied condition "Succeeded or Failed"
    Mar  9 16:49:58.984: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657 container client-container: <nil>
    STEP: delete the pod 03/09/23 16:49:58.99
    Mar  9 16:49:59.000: INFO: Waiting for pod downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657 to disappear
    Mar  9 16:49:59.003: INFO: Pod downwardapi-volume-19de370f-79e0-4962-ab69-b7dcad16d657 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 16:49:59.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2189" for this suite. 03/09/23 16:49:59.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:49:59.014
Mar  9 16:49:59.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/09/23 16:49:59.015
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:59.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:59.033
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/09/23 16:49:59.036
STEP: Creating hostNetwork=false pod 03/09/23 16:49:59.037
Mar  9 16:49:59.044: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2914" to be "running and ready"
Mar  9 16:49:59.046: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287247ms
Mar  9 16:49:59.046: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:50:01.050: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006200242s
Mar  9 16:50:01.050: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar  9 16:50:01.050: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/09/23 16:50:01.053
Mar  9 16:50:01.058: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2914" to be "running and ready"
Mar  9 16:50:01.061: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110853ms
Mar  9 16:50:01.061: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:50:03.065: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007411893s
Mar  9 16:50:03.065: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar  9 16:50:03.065: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/09/23 16:50:03.068
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/09/23 16:50:03.068
Mar  9 16:50:03.068: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.069: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.069: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  9 16:50:03.152: INFO: Exec stderr: ""
Mar  9 16:50:03.152: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.153: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.153: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  9 16:50:03.230: INFO: Exec stderr: ""
Mar  9 16:50:03.230: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.231: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.231: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  9 16:50:03.302: INFO: Exec stderr: ""
Mar  9 16:50:03.302: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.303: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.303: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  9 16:50:03.385: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/09/23 16:50:03.385
Mar  9 16:50:03.385: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.386: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.386: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  9 16:50:03.473: INFO: Exec stderr: ""
Mar  9 16:50:03.473: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.474: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.474: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  9 16:50:03.545: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/09/23 16:50:03.545
Mar  9 16:50:03.546: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.546: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.546: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  9 16:50:03.618: INFO: Exec stderr: ""
Mar  9 16:50:03.618: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.619: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.619: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  9 16:50:03.692: INFO: Exec stderr: ""
Mar  9 16:50:03.692: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.693: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.693: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  9 16:50:03.761: INFO: Exec stderr: ""
Mar  9 16:50:03.761: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:50:03.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:50:03.762: INFO: ExecWithOptions: Clientset creation
Mar  9 16:50:03.762: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  9 16:50:03.832: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Mar  9 16:50:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2914" for this suite. 03/09/23 16:50:03.837
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":217,"skipped":4184,"failed":0}
------------------------------
â€¢ [4.828 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:49:59.014
    Mar  9 16:49:59.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/09/23 16:49:59.015
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:49:59.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:49:59.033
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/09/23 16:49:59.036
    STEP: Creating hostNetwork=false pod 03/09/23 16:49:59.037
    Mar  9 16:49:59.044: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2914" to be "running and ready"
    Mar  9 16:49:59.046: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287247ms
    Mar  9 16:49:59.046: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:50:01.050: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006200242s
    Mar  9 16:50:01.050: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar  9 16:50:01.050: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/09/23 16:50:01.053
    Mar  9 16:50:01.058: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2914" to be "running and ready"
    Mar  9 16:50:01.061: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110853ms
    Mar  9 16:50:01.061: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:50:03.065: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007411893s
    Mar  9 16:50:03.065: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar  9 16:50:03.065: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/09/23 16:50:03.068
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/09/23 16:50:03.068
    Mar  9 16:50:03.068: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.069: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.069: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  9 16:50:03.152: INFO: Exec stderr: ""
    Mar  9 16:50:03.152: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.153: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.153: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  9 16:50:03.230: INFO: Exec stderr: ""
    Mar  9 16:50:03.230: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.231: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.231: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  9 16:50:03.302: INFO: Exec stderr: ""
    Mar  9 16:50:03.302: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.303: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.303: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  9 16:50:03.385: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/09/23 16:50:03.385
    Mar  9 16:50:03.385: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.386: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.386: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  9 16:50:03.473: INFO: Exec stderr: ""
    Mar  9 16:50:03.473: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.474: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.474: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  9 16:50:03.545: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/09/23 16:50:03.545
    Mar  9 16:50:03.546: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.546: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.546: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  9 16:50:03.618: INFO: Exec stderr: ""
    Mar  9 16:50:03.618: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.619: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.619: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  9 16:50:03.692: INFO: Exec stderr: ""
    Mar  9 16:50:03.692: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.693: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.693: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  9 16:50:03.761: INFO: Exec stderr: ""
    Mar  9 16:50:03.761: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2914 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:50:03.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:50:03.762: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:50:03.762: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2914/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  9 16:50:03.832: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Mar  9 16:50:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-2914" for this suite. 03/09/23 16:50:03.837
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:03.842
Mar  9 16:50:03.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replicaset 03/09/23 16:50:03.843
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:03.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:03.861
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/09/23 16:50:03.864
Mar  9 16:50:03.870: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5781" to be "running and ready"
Mar  9 16:50:03.872: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.30287ms
Mar  9 16:50:03.872: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:50:05.877: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.006450556s
Mar  9 16:50:05.877: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar  9 16:50:05.877: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/09/23 16:50:05.879
STEP: Then the orphan pod is adopted 03/09/23 16:50:05.884
STEP: When the matched label of one of its pods change 03/09/23 16:50:06.891
Mar  9 16:50:06.893: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/09/23 16:50:06.904
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  9 16:50:07.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5781" for this suite. 03/09/23 16:50:07.914
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":218,"skipped":4188,"failed":0}
------------------------------
â€¢ [4.076 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:03.842
    Mar  9 16:50:03.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replicaset 03/09/23 16:50:03.843
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:03.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:03.861
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/09/23 16:50:03.864
    Mar  9 16:50:03.870: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5781" to be "running and ready"
    Mar  9 16:50:03.872: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.30287ms
    Mar  9 16:50:03.872: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:50:05.877: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.006450556s
    Mar  9 16:50:05.877: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar  9 16:50:05.877: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/09/23 16:50:05.879
    STEP: Then the orphan pod is adopted 03/09/23 16:50:05.884
    STEP: When the matched label of one of its pods change 03/09/23 16:50:06.891
    Mar  9 16:50:06.893: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/09/23 16:50:06.904
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  9 16:50:07.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5781" for this suite. 03/09/23 16:50:07.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:07.92
Mar  9 16:50:07.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 16:50:07.921
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:07.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:07.935
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/09/23 16:50:07.938
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_tcp@PTR;sleep 1; done
 03/09/23 16:50:07.958
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_tcp@PTR;sleep 1; done
 03/09/23 16:50:07.958
STEP: creating a pod to probe DNS 03/09/23 16:50:07.959
STEP: submitting the pod to kubernetes 03/09/23 16:50:07.959
Mar  9 16:50:07.970: INFO: Waiting up to 15m0s for pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f" in namespace "dns-2079" to be "running"
Mar  9 16:50:07.976: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055983ms
Mar  9 16:50:09.980: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009999062s
Mar  9 16:50:11.980: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.009586956s
Mar  9 16:50:11.980: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f" satisfied condition "running"
STEP: retrieving the pod 03/09/23 16:50:11.98
STEP: looking for the results for each expected name from probers 03/09/23 16:50:11.983
Mar  9 16:50:11.987: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:11.990: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:11.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:11.995: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:12.009: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:12.012: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:12.015: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:12.018: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:12.029: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

Mar  9 16:50:17.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.058: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.061: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.064: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.067: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:17.077: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

Mar  9 16:50:22.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.043: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.046: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.061: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.064: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:22.081: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

Mar  9 16:50:27.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.060: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.066: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.068: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:27.079: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

Mar  9 16:50:32.034: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.040: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.043: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.058: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.060: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.063: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.066: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:32.077: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

Mar  9 16:50:37.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.044: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.064: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.066: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.069: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.072: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
Mar  9 16:50:37.084: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

Mar  9 16:50:42.082: INFO: DNS probes using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f succeeded

STEP: deleting the pod 03/09/23 16:50:42.082
STEP: deleting the test service 03/09/23 16:50:42.098
STEP: deleting the test headless service 03/09/23 16:50:42.126
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 16:50:42.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2079" for this suite. 03/09/23 16:50:42.141
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":219,"skipped":4211,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.228 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:07.92
    Mar  9 16:50:07.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 16:50:07.921
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:07.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:07.935
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/09/23 16:50:07.938
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_tcp@PTR;sleep 1; done
     03/09/23 16:50:07.958
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2079.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2079.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2079.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.238.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.238.173_tcp@PTR;sleep 1; done
     03/09/23 16:50:07.958
    STEP: creating a pod to probe DNS 03/09/23 16:50:07.959
    STEP: submitting the pod to kubernetes 03/09/23 16:50:07.959
    Mar  9 16:50:07.970: INFO: Waiting up to 15m0s for pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f" in namespace "dns-2079" to be "running"
    Mar  9 16:50:07.976: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055983ms
    Mar  9 16:50:09.980: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009999062s
    Mar  9 16:50:11.980: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.009586956s
    Mar  9 16:50:11.980: INFO: Pod "dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 16:50:11.98
    STEP: looking for the results for each expected name from probers 03/09/23 16:50:11.983
    Mar  9 16:50:11.987: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:11.990: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:11.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:11.995: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:12.009: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:12.012: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:12.015: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:12.018: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:12.029: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

    Mar  9 16:50:17.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.058: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.061: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.064: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.067: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:17.077: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

    Mar  9 16:50:22.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.043: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.046: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.061: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.064: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:22.081: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

    Mar  9 16:50:27.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.060: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.066: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.068: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:27.079: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

    Mar  9 16:50:32.034: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.040: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.043: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.058: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.060: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.063: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.066: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:32.077: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

    Mar  9 16:50:37.035: INFO: Unable to read wheezy_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.044: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.064: INFO: Unable to read jessie_udp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.066: INFO: Unable to read jessie_tcp@dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.069: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.072: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local from pod dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f: the server could not find the requested resource (get pods dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f)
    Mar  9 16:50:37.084: INFO: Lookups using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f failed for: [wheezy_udp@dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@dns-test-service.dns-2079.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_udp@dns-test-service.dns-2079.svc.cluster.local jessie_tcp@dns-test-service.dns-2079.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2079.svc.cluster.local]

    Mar  9 16:50:42.082: INFO: DNS probes using dns-2079/dns-test-df22fd71-023e-41d1-9c24-3e339b9b0b4f succeeded

    STEP: deleting the pod 03/09/23 16:50:42.082
    STEP: deleting the test service 03/09/23 16:50:42.098
    STEP: deleting the test headless service 03/09/23 16:50:42.126
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 16:50:42.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2079" for this suite. 03/09/23 16:50:42.141
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:42.149
Mar  9 16:50:42.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename events 03/09/23 16:50:42.151
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:42.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:42.169
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/09/23 16:50:42.172
Mar  9 16:50:42.178: INFO: created test-event-1
Mar  9 16:50:42.181: INFO: created test-event-2
Mar  9 16:50:42.185: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/09/23 16:50:42.185
STEP: delete collection of events 03/09/23 16:50:42.187
Mar  9 16:50:42.187: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/09/23 16:50:42.202
Mar  9 16:50:42.202: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  9 16:50:42.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6554" for this suite. 03/09/23 16:50:42.21
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":220,"skipped":4215,"failed":0}
------------------------------
â€¢ [0.066 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:42.149
    Mar  9 16:50:42.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename events 03/09/23 16:50:42.151
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:42.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:42.169
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/09/23 16:50:42.172
    Mar  9 16:50:42.178: INFO: created test-event-1
    Mar  9 16:50:42.181: INFO: created test-event-2
    Mar  9 16:50:42.185: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/09/23 16:50:42.185
    STEP: delete collection of events 03/09/23 16:50:42.187
    Mar  9 16:50:42.187: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/09/23 16:50:42.202
    Mar  9 16:50:42.202: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  9 16:50:42.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6554" for this suite. 03/09/23 16:50:42.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:42.218
Mar  9 16:50:42.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubelet-test 03/09/23 16:50:42.219
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:42.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:42.234
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  9 16:50:46.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7930" for this suite. 03/09/23 16:50:46.256
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":221,"skipped":4244,"failed":0}
------------------------------
â€¢ [4.043 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:42.218
    Mar  9 16:50:42.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubelet-test 03/09/23 16:50:42.219
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:42.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:42.234
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  9 16:50:46.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7930" for this suite. 03/09/23 16:50:46.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:46.261
Mar  9 16:50:46.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename cronjob 03/09/23 16:50:46.263
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:46.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:46.277
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/09/23 16:50:46.28
STEP: creating 03/09/23 16:50:46.28
STEP: getting 03/09/23 16:50:46.284
STEP: listing 03/09/23 16:50:46.287
STEP: watching 03/09/23 16:50:46.289
Mar  9 16:50:46.289: INFO: starting watch
STEP: cluster-wide listing 03/09/23 16:50:46.29
STEP: cluster-wide watching 03/09/23 16:50:46.293
Mar  9 16:50:46.293: INFO: starting watch
STEP: patching 03/09/23 16:50:46.294
STEP: updating 03/09/23 16:50:46.3
Mar  9 16:50:46.307: INFO: waiting for watch events with expected annotations
Mar  9 16:50:46.307: INFO: saw patched and updated annotations
STEP: patching /status 03/09/23 16:50:46.307
STEP: updating /status 03/09/23 16:50:46.312
STEP: get /status 03/09/23 16:50:46.318
STEP: deleting 03/09/23 16:50:46.321
STEP: deleting a collection 03/09/23 16:50:46.333
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  9 16:50:46.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4259" for this suite. 03/09/23 16:50:46.345
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":222,"skipped":4265,"failed":0}
------------------------------
â€¢ [0.088 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:46.261
    Mar  9 16:50:46.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename cronjob 03/09/23 16:50:46.263
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:46.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:46.277
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/09/23 16:50:46.28
    STEP: creating 03/09/23 16:50:46.28
    STEP: getting 03/09/23 16:50:46.284
    STEP: listing 03/09/23 16:50:46.287
    STEP: watching 03/09/23 16:50:46.289
    Mar  9 16:50:46.289: INFO: starting watch
    STEP: cluster-wide listing 03/09/23 16:50:46.29
    STEP: cluster-wide watching 03/09/23 16:50:46.293
    Mar  9 16:50:46.293: INFO: starting watch
    STEP: patching 03/09/23 16:50:46.294
    STEP: updating 03/09/23 16:50:46.3
    Mar  9 16:50:46.307: INFO: waiting for watch events with expected annotations
    Mar  9 16:50:46.307: INFO: saw patched and updated annotations
    STEP: patching /status 03/09/23 16:50:46.307
    STEP: updating /status 03/09/23 16:50:46.312
    STEP: get /status 03/09/23 16:50:46.318
    STEP: deleting 03/09/23 16:50:46.321
    STEP: deleting a collection 03/09/23 16:50:46.333
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  9 16:50:46.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4259" for this suite. 03/09/23 16:50:46.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:46.35
Mar  9 16:50:46.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:50:46.351
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:46.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:46.364
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-40fba3de-afdc-44e4-9ebb-81847bdf99cb 03/09/23 16:50:46.37
STEP: Creating secret with name s-test-opt-upd-379bb200-8531-4354-967e-ab022d084754 03/09/23 16:50:46.374
STEP: Creating the pod 03/09/23 16:50:46.377
Mar  9 16:50:46.385: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529" in namespace "projected-779" to be "running and ready"
Mar  9 16:50:46.387: INFO: Pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472312ms
Mar  9 16:50:46.387: INFO: The phase of Pod pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:50:48.391: INFO: Pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529": Phase="Running", Reason="", readiness=true. Elapsed: 2.006160313s
Mar  9 16:50:48.391: INFO: The phase of Pod pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529 is Running (Ready = true)
Mar  9 16:50:48.391: INFO: Pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-40fba3de-afdc-44e4-9ebb-81847bdf99cb 03/09/23 16:50:48.409
STEP: Updating secret s-test-opt-upd-379bb200-8531-4354-967e-ab022d084754 03/09/23 16:50:48.414
STEP: Creating secret with name s-test-opt-create-6a81d5b1-a9d7-4b4f-9ecc-91b9ded26ccd 03/09/23 16:50:48.417
STEP: waiting to observe update in volume 03/09/23 16:50:48.421
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  9 16:50:52.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-779" for this suite. 03/09/23 16:50:52.453
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":223,"skipped":4281,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:46.35
    Mar  9 16:50:46.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:50:46.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:46.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:46.364
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-40fba3de-afdc-44e4-9ebb-81847bdf99cb 03/09/23 16:50:46.37
    STEP: Creating secret with name s-test-opt-upd-379bb200-8531-4354-967e-ab022d084754 03/09/23 16:50:46.374
    STEP: Creating the pod 03/09/23 16:50:46.377
    Mar  9 16:50:46.385: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529" in namespace "projected-779" to be "running and ready"
    Mar  9 16:50:46.387: INFO: Pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472312ms
    Mar  9 16:50:46.387: INFO: The phase of Pod pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:50:48.391: INFO: Pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529": Phase="Running", Reason="", readiness=true. Elapsed: 2.006160313s
    Mar  9 16:50:48.391: INFO: The phase of Pod pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529 is Running (Ready = true)
    Mar  9 16:50:48.391: INFO: Pod "pod-projected-secrets-97d4016a-d89d-462a-8cb5-14a06f1dd529" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-40fba3de-afdc-44e4-9ebb-81847bdf99cb 03/09/23 16:50:48.409
    STEP: Updating secret s-test-opt-upd-379bb200-8531-4354-967e-ab022d084754 03/09/23 16:50:48.414
    STEP: Creating secret with name s-test-opt-create-6a81d5b1-a9d7-4b4f-9ecc-91b9ded26ccd 03/09/23 16:50:48.417
    STEP: waiting to observe update in volume 03/09/23 16:50:48.421
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  9 16:50:52.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-779" for this suite. 03/09/23 16:50:52.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:52.46
Mar  9 16:50:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:50:52.461
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:52.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:52.476
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 03/09/23 16:50:52.485
STEP: watching for Pod to be ready 03/09/23 16:50:52.491
Mar  9 16:50:52.492: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar  9 16:50:52.498: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
Mar  9 16:50:52.507: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
Mar  9 16:50:53.031: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
Mar  9 16:50:54.127: INFO: Found Pod pod-test in namespace pods-3242 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/09/23 16:50:54.129
STEP: getting the Pod and ensuring that it's patched 03/09/23 16:50:54.139
STEP: replacing the Pod's status Ready condition to False 03/09/23 16:50:54.141
STEP: check the Pod again to ensure its Ready conditions are False 03/09/23 16:50:54.152
STEP: deleting the Pod via a Collection with a LabelSelector 03/09/23 16:50:54.152
STEP: watching for the Pod to be deleted 03/09/23 16:50:54.16
Mar  9 16:50:54.162: INFO: observed event type MODIFIED
Mar  9 16:50:56.133: INFO: observed event type MODIFIED
Mar  9 16:50:56.380: INFO: observed event type MODIFIED
Mar  9 16:50:57.136: INFO: observed event type MODIFIED
Mar  9 16:50:57.152: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:50:57.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3242" for this suite. 03/09/23 16:50:57.164
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":224,"skipped":4298,"failed":0}
------------------------------
â€¢ [4.709 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:52.46
    Mar  9 16:50:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:50:52.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:52.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:52.476
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 03/09/23 16:50:52.485
    STEP: watching for Pod to be ready 03/09/23 16:50:52.491
    Mar  9 16:50:52.492: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar  9 16:50:52.498: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
    Mar  9 16:50:52.507: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
    Mar  9 16:50:53.031: INFO: observed Pod pod-test in namespace pods-3242 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
    Mar  9 16:50:54.127: INFO: Found Pod pod-test in namespace pods-3242 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:50:52 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/09/23 16:50:54.129
    STEP: getting the Pod and ensuring that it's patched 03/09/23 16:50:54.139
    STEP: replacing the Pod's status Ready condition to False 03/09/23 16:50:54.141
    STEP: check the Pod again to ensure its Ready conditions are False 03/09/23 16:50:54.152
    STEP: deleting the Pod via a Collection with a LabelSelector 03/09/23 16:50:54.152
    STEP: watching for the Pod to be deleted 03/09/23 16:50:54.16
    Mar  9 16:50:54.162: INFO: observed event type MODIFIED
    Mar  9 16:50:56.133: INFO: observed event type MODIFIED
    Mar  9 16:50:56.380: INFO: observed event type MODIFIED
    Mar  9 16:50:57.136: INFO: observed event type MODIFIED
    Mar  9 16:50:57.152: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:50:57.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3242" for this suite. 03/09/23 16:50:57.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:50:57.17
Mar  9 16:50:57.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:50:57.171
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:57.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:57.187
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-0375e437-0f25-42d4-8c7a-e508c6240c25 03/09/23 16:50:57.19
STEP: Creating a pod to test consume secrets 03/09/23 16:50:57.193
Mar  9 16:50:57.201: INFO: Waiting up to 5m0s for pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785" in namespace "secrets-4926" to be "Succeeded or Failed"
Mar  9 16:50:57.203: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268315ms
Mar  9 16:50:59.207: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006138746s
Mar  9 16:51:01.206: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005382962s
STEP: Saw pod success 03/09/23 16:51:01.206
Mar  9 16:51:01.206: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785" satisfied condition "Succeeded or Failed"
Mar  9 16:51:01.209: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785 container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 16:51:01.214
Mar  9 16:51:01.222: INFO: Waiting for pod pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785 to disappear
Mar  9 16:51:01.224: INFO: Pod pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:51:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4926" for this suite. 03/09/23 16:51:01.228
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":225,"skipped":4306,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:50:57.17
    Mar  9 16:50:57.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:50:57.171
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:50:57.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:50:57.187
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-0375e437-0f25-42d4-8c7a-e508c6240c25 03/09/23 16:50:57.19
    STEP: Creating a pod to test consume secrets 03/09/23 16:50:57.193
    Mar  9 16:50:57.201: INFO: Waiting up to 5m0s for pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785" in namespace "secrets-4926" to be "Succeeded or Failed"
    Mar  9 16:50:57.203: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268315ms
    Mar  9 16:50:59.207: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006138746s
    Mar  9 16:51:01.206: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005382962s
    STEP: Saw pod success 03/09/23 16:51:01.206
    Mar  9 16:51:01.206: INFO: Pod "pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785" satisfied condition "Succeeded or Failed"
    Mar  9 16:51:01.209: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785 container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:51:01.214
    Mar  9 16:51:01.222: INFO: Waiting for pod pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785 to disappear
    Mar  9 16:51:01.224: INFO: Pod pod-secrets-b5b6f80a-9ce3-40c9-acab-48846f390785 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:51:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4926" for this suite. 03/09/23 16:51:01.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:51:01.24
Mar  9 16:51:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename namespaces 03/09/23 16:51:01.241
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:01.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:01.255
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 03/09/23 16:51:01.258
Mar  9 16:51:01.261: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/09/23 16:51:01.261
Mar  9 16:51:01.265: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/09/23 16:51:01.265
Mar  9 16:51:01.272: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:51:01.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6465" for this suite. 03/09/23 16:51:01.275
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":226,"skipped":4346,"failed":0}
------------------------------
â€¢ [0.040 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:51:01.24
    Mar  9 16:51:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename namespaces 03/09/23 16:51:01.241
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:01.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:01.255
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 03/09/23 16:51:01.258
    Mar  9 16:51:01.261: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/09/23 16:51:01.261
    Mar  9 16:51:01.265: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/09/23 16:51:01.265
    Mar  9 16:51:01.272: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:51:01.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6465" for this suite. 03/09/23 16:51:01.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:51:01.281
Mar  9 16:51:01.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 16:51:01.282
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:01.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:01.294
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-cac40660-7158-47f8-9b3d-8af6792cf886 in namespace container-probe-1587 03/09/23 16:51:01.297
Mar  9 16:51:01.303: INFO: Waiting up to 5m0s for pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886" in namespace "container-probe-1587" to be "not pending"
Mar  9 16:51:01.306: INFO: Pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191819ms
Mar  9 16:51:03.309: INFO: Pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886": Phase="Running", Reason="", readiness=true. Elapsed: 2.005575789s
Mar  9 16:51:03.309: INFO: Pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886" satisfied condition "not pending"
Mar  9 16:51:03.309: INFO: Started pod busybox-cac40660-7158-47f8-9b3d-8af6792cf886 in namespace container-probe-1587
STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:51:03.309
Mar  9 16:51:03.312: INFO: Initial restart count of pod busybox-cac40660-7158-47f8-9b3d-8af6792cf886 is 0
Mar  9 16:51:53.416: INFO: Restart count of pod container-probe-1587/busybox-cac40660-7158-47f8-9b3d-8af6792cf886 is now 1 (50.104660533s elapsed)
STEP: deleting the pod 03/09/23 16:51:53.416
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 16:51:53.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1587" for this suite. 03/09/23 16:51:53.432
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":227,"skipped":4354,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.155 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:51:01.281
    Mar  9 16:51:01.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 16:51:01.282
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:01.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:01.294
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-cac40660-7158-47f8-9b3d-8af6792cf886 in namespace container-probe-1587 03/09/23 16:51:01.297
    Mar  9 16:51:01.303: INFO: Waiting up to 5m0s for pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886" in namespace "container-probe-1587" to be "not pending"
    Mar  9 16:51:01.306: INFO: Pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191819ms
    Mar  9 16:51:03.309: INFO: Pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886": Phase="Running", Reason="", readiness=true. Elapsed: 2.005575789s
    Mar  9 16:51:03.309: INFO: Pod "busybox-cac40660-7158-47f8-9b3d-8af6792cf886" satisfied condition "not pending"
    Mar  9 16:51:03.309: INFO: Started pod busybox-cac40660-7158-47f8-9b3d-8af6792cf886 in namespace container-probe-1587
    STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:51:03.309
    Mar  9 16:51:03.312: INFO: Initial restart count of pod busybox-cac40660-7158-47f8-9b3d-8af6792cf886 is 0
    Mar  9 16:51:53.416: INFO: Restart count of pod container-probe-1587/busybox-cac40660-7158-47f8-9b3d-8af6792cf886 is now 1 (50.104660533s elapsed)
    STEP: deleting the pod 03/09/23 16:51:53.416
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 16:51:53.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1587" for this suite. 03/09/23 16:51:53.432
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:51:53.438
Mar  9 16:51:53.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 16:51:53.439
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:53.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:53.455
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar  9 16:51:53.459: INFO: Creating deployment "webserver-deployment"
Mar  9 16:51:53.463: INFO: Waiting for observed generation 1
Mar  9 16:51:55.468: INFO: Waiting for all required pods to come up
Mar  9 16:51:55.473: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/09/23 16:51:55.473
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sxk6s" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-flxdd" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-24dkw" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2h44m" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6424h" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b9c47" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kdrtl" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hg4wl" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lpkwk" in namespace "deployment-513" to be "running"
Mar  9 16:51:55.478: INFO: Pod "webserver-deployment-845c8977d9-sxk6s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984308ms
Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-flxdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.510418ms
Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-6424h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.352607ms
Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-2h44m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874746ms
Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-24dkw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.57619ms
Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-lpkwk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.883763ms
Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-hg4wl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.359575ms
Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-kdrtl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054416ms
Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-b9c47": Phase="Running", Reason="", readiness=true. Elapsed: 8.612649ms
Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-b9c47" satisfied condition "running"
Mar  9 16:51:57.482: INFO: Pod "webserver-deployment-845c8977d9-sxk6s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008152437s
Mar  9 16:51:57.482: INFO: Pod "webserver-deployment-845c8977d9-sxk6s" satisfied condition "running"
Mar  9 16:51:57.484: INFO: Pod "webserver-deployment-845c8977d9-flxdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009572676s
Mar  9 16:51:57.484: INFO: Pod "webserver-deployment-845c8977d9-flxdd" satisfied condition "running"
Mar  9 16:51:57.485: INFO: Pod "webserver-deployment-845c8977d9-2h44m": Phase="Running", Reason="", readiness=true. Elapsed: 2.010165333s
Mar  9 16:51:57.485: INFO: Pod "webserver-deployment-845c8977d9-2h44m" satisfied condition "running"
Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-6424h": Phase="Running", Reason="", readiness=true. Elapsed: 2.010347736s
Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-6424h" satisfied condition "running"
Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-24dkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011237117s
Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-24dkw" satisfied condition "running"
Mar  9 16:51:57.487: INFO: Pod "webserver-deployment-845c8977d9-hg4wl": Phase="Running", Reason="", readiness=true. Elapsed: 2.01099559s
Mar  9 16:51:57.487: INFO: Pod "webserver-deployment-845c8977d9-hg4wl" satisfied condition "running"
Mar  9 16:51:57.487: INFO: Pod "webserver-deployment-845c8977d9-kdrtl": Phase="Running", Reason="", readiness=true. Elapsed: 2.011340991s
Mar  9 16:51:57.488: INFO: Pod "webserver-deployment-845c8977d9-lpkwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010912425s
Mar  9 16:51:57.488: INFO: Pod "webserver-deployment-845c8977d9-lpkwk" satisfied condition "running"
Mar  9 16:51:57.488: INFO: Pod "webserver-deployment-845c8977d9-kdrtl" satisfied condition "running"
Mar  9 16:51:57.488: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  9 16:51:57.492: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  9 16:51:57.501: INFO: Updating deployment webserver-deployment
Mar  9 16:51:57.501: INFO: Waiting for observed generation 2
Mar  9 16:51:59.507: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  9 16:51:59.510: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  9 16:51:59.513: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  9 16:51:59.521: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  9 16:51:59.521: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  9 16:51:59.523: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  9 16:51:59.528: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  9 16:51:59.528: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  9 16:51:59.536: INFO: Updating deployment webserver-deployment
Mar  9 16:51:59.536: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  9 16:51:59.545: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  9 16:51:59.551: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 16:51:59.558: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-513  ec705938-9b92-4487-bd8c-aa04e1b0eebf 110531 3 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00625c4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:51:56 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-09 16:51:57 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  9 16:51:59.564: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-513  f4ffe0a1-d0b0-442b-8584-861abf8bb522 110535 3 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ec705938-9b92-4487-bd8c-aa04e1b0eebf 0xc00370c807 0xc00370c808}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec705938-9b92-4487-bd8c-aa04e1b0eebf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370c8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:51:59.564: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  9 16:51:59.564: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-513  78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 110532 3 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ec705938-9b92-4487-bd8c-aa04e1b0eebf 0xc00370c907 0xc00370c908}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec705938-9b92-4487-bd8c-aa04e1b0eebf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370c998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:51:59.583: INFO: Pod "webserver-deployment-69b7448995-6x8dv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6x8dv webserver-deployment-69b7448995- deployment-513  cae16657-b37b-4326-ba95-715d7de7b151 110510 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7afda42b5acf913cd46479d8efaed4347e3b93a0a2fc96e900e1acd5a0658325 cni.projectcalico.org/podIP:10.244.42.235/32 cni.projectcalico.org/podIPs:10.244.42.235/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625c897 0xc00625c898}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rlcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rlcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.592: INFO: Pod "webserver-deployment-69b7448995-7hsw7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7hsw7 webserver-deployment-69b7448995- deployment-513  939e8726-00e0-4a73-9cd2-15c36d3d7fde 110524 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:885ab416854377cda5608ca89ac3e2e5c794e851639ffe315c448fd6da8eeb1a cni.projectcalico.org/podIP:10.244.88.233/32 cni.projectcalico.org/podIPs:10.244.88.233/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625cac7 0xc00625cac8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6k9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6k9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.233,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.593: INFO: Pod "webserver-deployment-69b7448995-ft5lk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ft5lk webserver-deployment-69b7448995- deployment-513  74356fe9-0457-40a0-84a6-f7db59f2886a 110502 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:78fdb8a9752bf7b3c552a44825b10857204ab584c2203c92db7c76f330bafffe cni.projectcalico.org/podIP:10.244.42.240/32 cni.projectcalico.org/podIPs:10.244.42.240/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625cd47 0xc00625cd48}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s97ts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s97ts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.593: INFO: Pod "webserver-deployment-69b7448995-h9lqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-h9lqz webserver-deployment-69b7448995- deployment-513  e5ededcf-752d-4725-9ebc-ec15a094dab9 110528 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8878eefdf31d13689f7ca1a89569682d625d0f667722e4abe617cb8a376dfd3e cni.projectcalico.org/podIP:10.244.88.209/32 cni.projectcalico.org/podIPs:10.244.88.209/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625cf77 0xc00625cf78}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9th7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9th7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.209,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.594: INFO: Pod "webserver-deployment-69b7448995-r5wfv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-r5wfv webserver-deployment-69b7448995- deployment-513  f107b970-a3d0-4e91-9f93-b771ec084c0b 110539 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625d1b7 0xc00625d1b8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8h88k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8h88k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.594: INFO: Pod "webserver-deployment-69b7448995-zp785" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zp785 webserver-deployment-69b7448995- deployment-513  10761c65-dc14-4d9e-afd8-488cad5cda69 110521 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:66e26c630c9bce79e35b35e7cddb05068ff3964f1e6f209c354b0430a00e20e8 cni.projectcalico.org/podIP:10.244.42.232/32 cni.projectcalico.org/podIPs:10.244.42.232/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625d330 0xc00625d331}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-699mt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-699mt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.232,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.595: INFO: Pod "webserver-deployment-845c8977d9-24dkw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-24dkw webserver-deployment-845c8977d9- deployment-513  e056e5db-cdfe-4cb1-b8fb-16c0bfa1dcd0 110405 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0849155c5fe3ac1942a7808060c298f9d921d486c7559478654d7593b16f8204 cni.projectcalico.org/podIP:10.244.42.200/32 cni.projectcalico.org/podIPs:10.244.42.200/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625d5a7 0xc00625d5a8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qrwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qrwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.200,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4725375cac827df521b121c1054b78268db2309d53f6177bff71fe6a151ffafd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.595: INFO: Pod "webserver-deployment-845c8977d9-6424h" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6424h webserver-deployment-845c8977d9- deployment-513  90f1ec02-55d0-41c8-bb1d-6aeb437e4614 110400 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cc2f0ed25b7e5a3aebce630763e4fb30877a6c362240a37906543f815980cff1 cni.projectcalico.org/podIP:10.244.88.226/32 cni.projectcalico.org/podIPs:10.244.88.226/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625d7d7 0xc00625d7d8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvc29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvc29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.226,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://54c8faebc713c013f6477411960506ef335f21abeef46698374f3dc26a630ec1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.595: INFO: Pod "webserver-deployment-845c8977d9-6rz97" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6rz97 webserver-deployment-845c8977d9- deployment-513  ae673c29-adc0-4f67-b4cf-5b4e5164104b 110543 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625d9f7 0xc00625d9f8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqbrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqbrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.596: INFO: Pod "webserver-deployment-845c8977d9-b6fx9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b6fx9 webserver-deployment-845c8977d9- deployment-513  e8b16ec2-d409-47bf-b587-63650dbea2d2 110546 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625db60 0xc00625db61}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqtx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqtx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.596: INFO: Pod "webserver-deployment-845c8977d9-b9c47" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b9c47 webserver-deployment-845c8977d9- deployment-513  cac89747-55d5-411e-a0d9-971d2b107e4f 110391 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:501c7235496240e350ff47550712587b1274d9c6702021eff6bc6fdff2ca3b75 cni.projectcalico.org/podIP:10.244.88.231/32 cni.projectcalico.org/podIPs:10.244.88.231/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625dce0 0xc00625dce1}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nxph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nxph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.231,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://58052bf2f38afeb79bd32d6c706ccb364afad15a4b3210c23560e9aeb16f17fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.596: INFO: Pod "webserver-deployment-845c8977d9-bt9xf" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bt9xf webserver-deployment-845c8977d9- deployment-513  6bb24d1f-0734-4641-87ef-374cfb77b139 110540 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625dee7 0xc00625dee8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94qjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94qjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.597: INFO: Pod "webserver-deployment-845c8977d9-flxdd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-flxdd webserver-deployment-845c8977d9- deployment-513  7895ad9e-6de3-4f1f-9535-d0a7917f8ff3 110419 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cf1a279a3c26f11da098246ac7d1b05b94c2e5b89fbc0fd945a48c6d279c62d5 cni.projectcalico.org/podIP:10.244.88.212/32 cni.projectcalico.org/podIPs:10.244.88.212/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb4180 0xc001cb4181}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84bk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84bk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.212,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6829a8f8972f78ed548a29624285e2273d0e368f9386198455b4608ddd45a9bc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.597: INFO: Pod "webserver-deployment-845c8977d9-hg4wl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hg4wl webserver-deployment-845c8977d9- deployment-513  d31fff39-b4b0-4c3f-8e9a-be12f64d4055 110397 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:49e39d1870c3db64f87683fc663fa1934e4bb3ecfa583e79df95e21ecdf0a74d cni.projectcalico.org/podIP:10.244.42.198/32 cni.projectcalico.org/podIPs:10.244.42.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb47a7 0xc001cb47a8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7lfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7lfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.198,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://92dced46fb0b029952f80e845be272286f9ff04dd17e670c30fef59edfdf526b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.597: INFO: Pod "webserver-deployment-845c8977d9-jdtfq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jdtfq webserver-deployment-845c8977d9- deployment-513  664f8e99-d3d8-4fcc-96d5-f8afdfb3fbdd 110536 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb4b97 0xc001cb4b98}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgkz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgkz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-kdrtl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kdrtl webserver-deployment-845c8977d9- deployment-513  7a96534a-7d6e-4929-a1ed-674497974f82 110407 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3cb2e4ff59f405b16b9097f251b5db579e7ae3ecdffb8b74e2f882ed9b61f2aa cni.projectcalico.org/podIP:10.244.88.218/32 cni.projectcalico.org/podIPs:10.244.88.218/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb4f00 0xc001cb4f01}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4t287,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4t287,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.218,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0496094bc05623db1fa5204b0425eea944cef8b61259124e3d6ac5eac2b82edf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-kk4pk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kk4pk webserver-deployment-845c8977d9- deployment-513  883650da-5bfe-45d3-b2bd-91f7e4f5a51f 110381 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d1b0b6e1a56423bcd97730105bc60e7b2bbb3f9b39856fc993ac89331e2f72d9 cni.projectcalico.org/podIP:10.244.42.251/32 cni.projectcalico.org/podIPs:10.244.42.251/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5127 0xc001cb5128}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98wpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98wpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.251,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b3051ee462b61d8cebb03984953e41b8dc3fafc5f65eac20a3f6b2e5dfb4407f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-ntnkb" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ntnkb webserver-deployment-845c8977d9- deployment-513  a825d96d-9002-432a-8607-7308bf6b2583 110542 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5337 0xc001cb5338}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfrwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfrwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-qk6c4" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qk6c4 webserver-deployment-845c8977d9- deployment-513  06df3ccc-300f-4637-a715-9a768aea57c0 110541 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5480 0xc001cb5481}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqx4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqx4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  9 16:51:59.599: INFO: Pod "webserver-deployment-845c8977d9-sxk6s" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sxk6s webserver-deployment-845c8977d9- deployment-513  f052e991-327e-4aad-926f-23f3cf7cf5e3 110414 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:02109f151b8edac222abc87b3f63153917fb40c75e7287879dab77bf4a9601d6 cni.projectcalico.org/podIP:10.244.88.236/32 cni.projectcalico.org/podIPs:10.244.88.236/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5600 0xc001cb5601}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tcxz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tcxz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.236,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4a96ea99327548e9f73731a4ea338fdf1445c8478a971fd59b5742dedd946534,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 16:51:59.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-513" for this suite. 03/09/23 16:51:59.626
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":228,"skipped":4356,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.200 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:51:53.438
    Mar  9 16:51:53.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 16:51:53.439
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:53.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:53.455
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar  9 16:51:53.459: INFO: Creating deployment "webserver-deployment"
    Mar  9 16:51:53.463: INFO: Waiting for observed generation 1
    Mar  9 16:51:55.468: INFO: Waiting for all required pods to come up
    Mar  9 16:51:55.473: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/09/23 16:51:55.473
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sxk6s" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-flxdd" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-24dkw" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2h44m" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6424h" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b9c47" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kdrtl" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hg4wl" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.474: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lpkwk" in namespace "deployment-513" to be "running"
    Mar  9 16:51:55.478: INFO: Pod "webserver-deployment-845c8977d9-sxk6s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984308ms
    Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-flxdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.510418ms
    Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-6424h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.352607ms
    Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-2h44m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874746ms
    Mar  9 16:51:55.481: INFO: Pod "webserver-deployment-845c8977d9-24dkw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.57619ms
    Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-lpkwk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.883763ms
    Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-hg4wl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.359575ms
    Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-kdrtl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054416ms
    Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-b9c47": Phase="Running", Reason="", readiness=true. Elapsed: 8.612649ms
    Mar  9 16:51:55.484: INFO: Pod "webserver-deployment-845c8977d9-b9c47" satisfied condition "running"
    Mar  9 16:51:57.482: INFO: Pod "webserver-deployment-845c8977d9-sxk6s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008152437s
    Mar  9 16:51:57.482: INFO: Pod "webserver-deployment-845c8977d9-sxk6s" satisfied condition "running"
    Mar  9 16:51:57.484: INFO: Pod "webserver-deployment-845c8977d9-flxdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009572676s
    Mar  9 16:51:57.484: INFO: Pod "webserver-deployment-845c8977d9-flxdd" satisfied condition "running"
    Mar  9 16:51:57.485: INFO: Pod "webserver-deployment-845c8977d9-2h44m": Phase="Running", Reason="", readiness=true. Elapsed: 2.010165333s
    Mar  9 16:51:57.485: INFO: Pod "webserver-deployment-845c8977d9-2h44m" satisfied condition "running"
    Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-6424h": Phase="Running", Reason="", readiness=true. Elapsed: 2.010347736s
    Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-6424h" satisfied condition "running"
    Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-24dkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011237117s
    Mar  9 16:51:57.486: INFO: Pod "webserver-deployment-845c8977d9-24dkw" satisfied condition "running"
    Mar  9 16:51:57.487: INFO: Pod "webserver-deployment-845c8977d9-hg4wl": Phase="Running", Reason="", readiness=true. Elapsed: 2.01099559s
    Mar  9 16:51:57.487: INFO: Pod "webserver-deployment-845c8977d9-hg4wl" satisfied condition "running"
    Mar  9 16:51:57.487: INFO: Pod "webserver-deployment-845c8977d9-kdrtl": Phase="Running", Reason="", readiness=true. Elapsed: 2.011340991s
    Mar  9 16:51:57.488: INFO: Pod "webserver-deployment-845c8977d9-lpkwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010912425s
    Mar  9 16:51:57.488: INFO: Pod "webserver-deployment-845c8977d9-lpkwk" satisfied condition "running"
    Mar  9 16:51:57.488: INFO: Pod "webserver-deployment-845c8977d9-kdrtl" satisfied condition "running"
    Mar  9 16:51:57.488: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar  9 16:51:57.492: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar  9 16:51:57.501: INFO: Updating deployment webserver-deployment
    Mar  9 16:51:57.501: INFO: Waiting for observed generation 2
    Mar  9 16:51:59.507: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar  9 16:51:59.510: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar  9 16:51:59.513: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  9 16:51:59.521: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar  9 16:51:59.521: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar  9 16:51:59.523: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  9 16:51:59.528: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar  9 16:51:59.528: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar  9 16:51:59.536: INFO: Updating deployment webserver-deployment
    Mar  9 16:51:59.536: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar  9 16:51:59.545: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar  9 16:51:59.551: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 16:51:59.558: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-513  ec705938-9b92-4487-bd8c-aa04e1b0eebf 110531 3 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00625c4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:51:56 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-09 16:51:57 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar  9 16:51:59.564: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-513  f4ffe0a1-d0b0-442b-8584-861abf8bb522 110535 3 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ec705938-9b92-4487-bd8c-aa04e1b0eebf 0xc00370c807 0xc00370c808}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec705938-9b92-4487-bd8c-aa04e1b0eebf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370c8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:51:59.564: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar  9 16:51:59.564: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-513  78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 110532 3 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ec705938-9b92-4487-bd8c-aa04e1b0eebf 0xc00370c907 0xc00370c908}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec705938-9b92-4487-bd8c-aa04e1b0eebf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370c998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:51:59.583: INFO: Pod "webserver-deployment-69b7448995-6x8dv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6x8dv webserver-deployment-69b7448995- deployment-513  cae16657-b37b-4326-ba95-715d7de7b151 110510 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7afda42b5acf913cd46479d8efaed4347e3b93a0a2fc96e900e1acd5a0658325 cni.projectcalico.org/podIP:10.244.42.235/32 cni.projectcalico.org/podIPs:10.244.42.235/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625c897 0xc00625c898}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rlcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rlcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.592: INFO: Pod "webserver-deployment-69b7448995-7hsw7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7hsw7 webserver-deployment-69b7448995- deployment-513  939e8726-00e0-4a73-9cd2-15c36d3d7fde 110524 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:885ab416854377cda5608ca89ac3e2e5c794e851639ffe315c448fd6da8eeb1a cni.projectcalico.org/podIP:10.244.88.233/32 cni.projectcalico.org/podIPs:10.244.88.233/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625cac7 0xc00625cac8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6k9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6k9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.233,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.593: INFO: Pod "webserver-deployment-69b7448995-ft5lk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ft5lk webserver-deployment-69b7448995- deployment-513  74356fe9-0457-40a0-84a6-f7db59f2886a 110502 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:78fdb8a9752bf7b3c552a44825b10857204ab584c2203c92db7c76f330bafffe cni.projectcalico.org/podIP:10.244.42.240/32 cni.projectcalico.org/podIPs:10.244.42.240/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625cd47 0xc00625cd48}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s97ts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s97ts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.593: INFO: Pod "webserver-deployment-69b7448995-h9lqz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-h9lqz webserver-deployment-69b7448995- deployment-513  e5ededcf-752d-4725-9ebc-ec15a094dab9 110528 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8878eefdf31d13689f7ca1a89569682d625d0f667722e4abe617cb8a376dfd3e cni.projectcalico.org/podIP:10.244.88.209/32 cni.projectcalico.org/podIPs:10.244.88.209/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625cf77 0xc00625cf78}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9th7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9th7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.209,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.594: INFO: Pod "webserver-deployment-69b7448995-r5wfv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-r5wfv webserver-deployment-69b7448995- deployment-513  f107b970-a3d0-4e91-9f93-b771ec084c0b 110539 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625d1b7 0xc00625d1b8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8h88k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8h88k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.594: INFO: Pod "webserver-deployment-69b7448995-zp785" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zp785 webserver-deployment-69b7448995- deployment-513  10761c65-dc14-4d9e-afd8-488cad5cda69 110521 0 2023-03-09 16:51:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:66e26c630c9bce79e35b35e7cddb05068ff3964f1e6f209c354b0430a00e20e8 cni.projectcalico.org/podIP:10.244.42.232/32 cni.projectcalico.org/podIPs:10.244.42.232/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f4ffe0a1-d0b0-442b-8584-861abf8bb522 0xc00625d330 0xc00625d331}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ffe0a1-d0b0-442b-8584-861abf8bb522\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-699mt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-699mt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.232,StartTime:2023-03-09 16:51:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.595: INFO: Pod "webserver-deployment-845c8977d9-24dkw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-24dkw webserver-deployment-845c8977d9- deployment-513  e056e5db-cdfe-4cb1-b8fb-16c0bfa1dcd0 110405 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0849155c5fe3ac1942a7808060c298f9d921d486c7559478654d7593b16f8204 cni.projectcalico.org/podIP:10.244.42.200/32 cni.projectcalico.org/podIPs:10.244.42.200/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625d5a7 0xc00625d5a8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qrwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qrwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.200,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4725375cac827df521b121c1054b78268db2309d53f6177bff71fe6a151ffafd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.595: INFO: Pod "webserver-deployment-845c8977d9-6424h" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6424h webserver-deployment-845c8977d9- deployment-513  90f1ec02-55d0-41c8-bb1d-6aeb437e4614 110400 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cc2f0ed25b7e5a3aebce630763e4fb30877a6c362240a37906543f815980cff1 cni.projectcalico.org/podIP:10.244.88.226/32 cni.projectcalico.org/podIPs:10.244.88.226/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625d7d7 0xc00625d7d8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvc29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvc29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.226,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://54c8faebc713c013f6477411960506ef335f21abeef46698374f3dc26a630ec1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.595: INFO: Pod "webserver-deployment-845c8977d9-6rz97" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6rz97 webserver-deployment-845c8977d9- deployment-513  ae673c29-adc0-4f67-b4cf-5b4e5164104b 110543 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625d9f7 0xc00625d9f8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqbrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqbrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.596: INFO: Pod "webserver-deployment-845c8977d9-b6fx9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b6fx9 webserver-deployment-845c8977d9- deployment-513  e8b16ec2-d409-47bf-b587-63650dbea2d2 110546 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625db60 0xc00625db61}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqtx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqtx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.596: INFO: Pod "webserver-deployment-845c8977d9-b9c47" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b9c47 webserver-deployment-845c8977d9- deployment-513  cac89747-55d5-411e-a0d9-971d2b107e4f 110391 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:501c7235496240e350ff47550712587b1274d9c6702021eff6bc6fdff2ca3b75 cni.projectcalico.org/podIP:10.244.88.231/32 cni.projectcalico.org/podIPs:10.244.88.231/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625dce0 0xc00625dce1}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nxph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nxph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.231,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://58052bf2f38afeb79bd32d6c706ccb364afad15a4b3210c23560e9aeb16f17fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.596: INFO: Pod "webserver-deployment-845c8977d9-bt9xf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bt9xf webserver-deployment-845c8977d9- deployment-513  6bb24d1f-0734-4641-87ef-374cfb77b139 110540 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc00625dee7 0xc00625dee8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94qjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94qjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.597: INFO: Pod "webserver-deployment-845c8977d9-flxdd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-flxdd webserver-deployment-845c8977d9- deployment-513  7895ad9e-6de3-4f1f-9535-d0a7917f8ff3 110419 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cf1a279a3c26f11da098246ac7d1b05b94c2e5b89fbc0fd945a48c6d279c62d5 cni.projectcalico.org/podIP:10.244.88.212/32 cni.projectcalico.org/podIPs:10.244.88.212/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb4180 0xc001cb4181}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84bk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84bk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.212,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6829a8f8972f78ed548a29624285e2273d0e368f9386198455b4608ddd45a9bc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.597: INFO: Pod "webserver-deployment-845c8977d9-hg4wl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hg4wl webserver-deployment-845c8977d9- deployment-513  d31fff39-b4b0-4c3f-8e9a-be12f64d4055 110397 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:49e39d1870c3db64f87683fc663fa1934e4bb3ecfa583e79df95e21ecdf0a74d cni.projectcalico.org/podIP:10.244.42.198/32 cni.projectcalico.org/podIPs:10.244.42.198/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb47a7 0xc001cb47a8}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7lfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7lfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.198,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://92dced46fb0b029952f80e845be272286f9ff04dd17e670c30fef59edfdf526b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.597: INFO: Pod "webserver-deployment-845c8977d9-jdtfq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jdtfq webserver-deployment-845c8977d9- deployment-513  664f8e99-d3d8-4fcc-96d5-f8afdfb3fbdd 110536 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb4b97 0xc001cb4b98}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgkz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgkz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-kdrtl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kdrtl webserver-deployment-845c8977d9- deployment-513  7a96534a-7d6e-4929-a1ed-674497974f82 110407 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3cb2e4ff59f405b16b9097f251b5db579e7ae3ecdffb8b74e2f882ed9b61f2aa cni.projectcalico.org/podIP:10.244.88.218/32 cni.projectcalico.org/podIPs:10.244.88.218/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb4f00 0xc001cb4f01}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4t287,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4t287,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.218,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0496094bc05623db1fa5204b0425eea944cef8b61259124e3d6ac5eac2b82edf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-kk4pk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kk4pk webserver-deployment-845c8977d9- deployment-513  883650da-5bfe-45d3-b2bd-91f7e4f5a51f 110381 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d1b0b6e1a56423bcd97730105bc60e7b2bbb3f9b39856fc993ac89331e2f72d9 cni.projectcalico.org/podIP:10.244.42.251/32 cni.projectcalico.org/podIPs:10.244.42.251/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5127 0xc001cb5128}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98wpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98wpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.251,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b3051ee462b61d8cebb03984953e41b8dc3fafc5f65eac20a3f6b2e5dfb4407f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-ntnkb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ntnkb webserver-deployment-845c8977d9- deployment-513  a825d96d-9002-432a-8607-7308bf6b2583 110542 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5337 0xc001cb5338}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfrwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfrwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.598: INFO: Pod "webserver-deployment-845c8977d9-qk6c4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qk6c4 webserver-deployment-845c8977d9- deployment-513  06df3ccc-300f-4637-a715-9a768aea57c0 110541 0 2023-03-09 16:51:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5480 0xc001cb5481}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqx4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqx4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  9 16:51:59.599: INFO: Pod "webserver-deployment-845c8977d9-sxk6s" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sxk6s webserver-deployment-845c8977d9- deployment-513  f052e991-327e-4aad-926f-23f3cf7cf5e3 110414 0 2023-03-09 16:51:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:02109f151b8edac222abc87b3f63153917fb40c75e7287879dab77bf4a9601d6 cni.projectcalico.org/podIP:10.244.88.236/32 cni.projectcalico.org/podIPs:10.244.88.236/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 78bbcaa7-39b7-4dfa-aec0-8d2f266d659c 0xc001cb5600 0xc001cb5601}] [] [{kube-controller-manager Update v1 2023-03-09 16:51:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bbcaa7-39b7-4dfa-aec0-8d2f266d659c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 16:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 16:51:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.88.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tcxz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tcxz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:51:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.231.104,PodIP:10.244.88.236,StartTime:2023-03-09 16:51:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4a96ea99327548e9f73731a4ea338fdf1445c8478a971fd59b5742dedd946534,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.88.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 16:51:59.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-513" for this suite. 03/09/23 16:51:59.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:51:59.641
Mar  9 16:51:59.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-preemption 03/09/23 16:51:59.643
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:59.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:59.677
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  9 16:51:59.692: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  9 16:52:59.729: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 03/09/23 16:52:59.731
Mar  9 16:52:59.752: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  9 16:52:59.757: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  9 16:52:59.774: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  9 16:52:59.779: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/09/23 16:52:59.779
Mar  9 16:52:59.779: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6333" to be "running"
Mar  9 16:52:59.783: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.745812ms
Mar  9 16:53:01.786: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007581463s
Mar  9 16:53:03.786: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007688164s
Mar  9 16:53:05.788: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.008707425s
Mar  9 16:53:05.788: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  9 16:53:05.788: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6333" to be "running"
Mar  9 16:53:05.790: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.468663ms
Mar  9 16:53:05.790: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  9 16:53:05.790: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6333" to be "running"
Mar  9 16:53:05.792: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.38541ms
Mar  9 16:53:07.796: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005543837s
Mar  9 16:53:07.796: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  9 16:53:07.796: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6333" to be "running"
Mar  9 16:53:07.798: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.744808ms
Mar  9 16:53:07.798: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/09/23 16:53:07.798
Mar  9 16:53:07.810: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar  9 16:53:07.813: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.69613ms
Mar  9 16:53:09.817: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006955246s
Mar  9 16:53:11.816: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006030887s
Mar  9 16:53:11.816: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:53:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6333" for this suite. 03/09/23 16:53:11.842
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":229,"skipped":4398,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.237 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:51:59.641
    Mar  9 16:51:59.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-preemption 03/09/23 16:51:59.643
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:51:59.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:51:59.677
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  9 16:51:59.692: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  9 16:52:59.729: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 03/09/23 16:52:59.731
    Mar  9 16:52:59.752: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  9 16:52:59.757: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  9 16:52:59.774: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  9 16:52:59.779: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/09/23 16:52:59.779
    Mar  9 16:52:59.779: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6333" to be "running"
    Mar  9 16:52:59.783: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.745812ms
    Mar  9 16:53:01.786: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007581463s
    Mar  9 16:53:03.786: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007688164s
    Mar  9 16:53:05.788: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.008707425s
    Mar  9 16:53:05.788: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  9 16:53:05.788: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6333" to be "running"
    Mar  9 16:53:05.790: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.468663ms
    Mar  9 16:53:05.790: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  9 16:53:05.790: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6333" to be "running"
    Mar  9 16:53:05.792: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.38541ms
    Mar  9 16:53:07.796: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005543837s
    Mar  9 16:53:07.796: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  9 16:53:07.796: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6333" to be "running"
    Mar  9 16:53:07.798: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.744808ms
    Mar  9 16:53:07.798: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/09/23 16:53:07.798
    Mar  9 16:53:07.810: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar  9 16:53:07.813: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.69613ms
    Mar  9 16:53:09.817: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006955246s
    Mar  9 16:53:11.816: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006030887s
    Mar  9 16:53:11.816: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:53:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6333" for this suite. 03/09/23 16:53:11.842
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:53:11.88
Mar  9 16:53:11.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-preemption 03/09/23 16:53:11.882
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:53:11.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:53:11.895
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  9 16:53:11.909: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  9 16:54:11.942: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:54:11.945
Mar  9 16:54:11.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-preemption-path 03/09/23 16:54:11.946
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:11.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:11.962
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Mar  9 16:54:11.977: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar  9 16:54:11.980: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Mar  9 16:54:11.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5676" for this suite. 03/09/23 16:54:12
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  9 16:54:12.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3705" for this suite. 03/09/23 16:54:12.017
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":230,"skipped":4425,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.182 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:53:11.88
    Mar  9 16:53:11.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-preemption 03/09/23 16:53:11.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:53:11.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:53:11.895
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  9 16:53:11.909: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  9 16:54:11.942: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:54:11.945
    Mar  9 16:54:11.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-preemption-path 03/09/23 16:54:11.946
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:11.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:11.962
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Mar  9 16:54:11.977: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar  9 16:54:11.980: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Mar  9 16:54:11.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-5676" for this suite. 03/09/23 16:54:12
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 16:54:12.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3705" for this suite. 03/09/23 16:54:12.017
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:54:12.064
Mar  9 16:54:12.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 16:54:12.066
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:12.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:12.08
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar  9 16:54:12.083: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  9 16:54:12.091: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  9 16:54:17.097: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/09/23 16:54:17.097
Mar  9 16:54:17.097: INFO: Creating deployment "test-rolling-update-deployment"
Mar  9 16:54:17.101: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  9 16:54:17.108: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  9 16:54:19.115: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  9 16:54:19.117: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 16:54:19.124: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3513  ae37f006-7651-42b9-bd96-2d4b5001c099 111320 1 2023-03-09 16:54:17 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006011328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:54:17 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-09 16:54:18 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  9 16:54:19.127: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3513  54bc3b94-f743-4af0-a15f-2e326f41a0c0 111310 1 2023-03-09 16:54:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ae37f006-7651-42b9-bd96-2d4b5001c099 0xc006011827 0xc006011828}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae37f006-7651-42b9-bd96-2d4b5001c099\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060118d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:54:19.127: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  9 16:54:19.127: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3513  d268f6e1-bc23-4245-af8f-7a86e48ac5ad 111319 2 2023-03-09 16:54:12 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ae37f006-7651-42b9-bd96-2d4b5001c099 0xc0060116f7 0xc0060116f8}] [] [{e2e.test Update apps/v1 2023-03-09 16:54:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae37f006-7651-42b9-bd96-2d4b5001c099\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0060117b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  9 16:54:19.130: INFO: Pod "test-rolling-update-deployment-78f575d8ff-bxgwp" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-bxgwp test-rolling-update-deployment-78f575d8ff- deployment-3513  5cf0a4a5-4dd7-4e48-99bd-b0149d85c631 111309 0 2023-03-09 16:54:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:f8af6fb504920938c7d8cd59ae22c8895f400668e1e356268fd2f4bd45a7f9d6 cni.projectcalico.org/podIP:10.244.42.236/32 cni.projectcalico.org/podIPs:10.244.42.236/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 54bc3b94-f743-4af0-a15f-2e326f41a0c0 0xc0048e55d7 0xc0048e55d8}] [] [{calico Update v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54bc3b94-f743-4af0-a15f-2e326f41a0c0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cs2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cs2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.236,StartTime:2023-03-09 16:54:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:54:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://c104c880764a69f55d70f7fd24a65544b06f2d3994a698c2f24c52f0392e9cef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 16:54:19.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3513" for this suite. 03/09/23 16:54:19.133
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":231,"skipped":4464,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.076 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:54:12.064
    Mar  9 16:54:12.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 16:54:12.066
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:12.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:12.08
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar  9 16:54:12.083: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar  9 16:54:12.091: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  9 16:54:17.097: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/09/23 16:54:17.097
    Mar  9 16:54:17.097: INFO: Creating deployment "test-rolling-update-deployment"
    Mar  9 16:54:17.101: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar  9 16:54:17.108: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar  9 16:54:19.115: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar  9 16:54:19.117: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 16:54:19.124: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3513  ae37f006-7651-42b9-bd96-2d4b5001c099 111320 1 2023-03-09 16:54:17 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006011328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-09 16:54:17 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-09 16:54:18 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  9 16:54:19.127: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3513  54bc3b94-f743-4af0-a15f-2e326f41a0c0 111310 1 2023-03-09 16:54:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ae37f006-7651-42b9-bd96-2d4b5001c099 0xc006011827 0xc006011828}] [] [{kube-controller-manager Update apps/v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae37f006-7651-42b9-bd96-2d4b5001c099\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060118d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:54:19.127: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar  9 16:54:19.127: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3513  d268f6e1-bc23-4245-af8f-7a86e48ac5ad 111319 2 2023-03-09 16:54:12 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ae37f006-7651-42b9-bd96-2d4b5001c099 0xc0060116f7 0xc0060116f8}] [] [{e2e.test Update apps/v1 2023-03-09 16:54:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae37f006-7651-42b9-bd96-2d4b5001c099\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0060117b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 16:54:19.130: INFO: Pod "test-rolling-update-deployment-78f575d8ff-bxgwp" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-bxgwp test-rolling-update-deployment-78f575d8ff- deployment-3513  5cf0a4a5-4dd7-4e48-99bd-b0149d85c631 111309 0 2023-03-09 16:54:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:f8af6fb504920938c7d8cd59ae22c8895f400668e1e356268fd2f4bd45a7f9d6 cni.projectcalico.org/podIP:10.244.42.236/32 cni.projectcalico.org/podIPs:10.244.42.236/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 54bc3b94-f743-4af0-a15f-2e326f41a0c0 0xc0048e55d7 0xc0048e55d8}] [] [{calico Update v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-09 16:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54bc3b94-f743-4af0-a15f-2e326f41a0c0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 16:54:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cs2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cs2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 16:54:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.236,StartTime:2023-03-09 16:54:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 16:54:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://c104c880764a69f55d70f7fd24a65544b06f2d3994a698c2f24c52f0392e9cef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 16:54:19.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3513" for this suite. 03/09/23 16:54:19.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:54:19.142
Mar  9 16:54:19.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename containers 03/09/23 16:54:19.143
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:19.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:19.156
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 03/09/23 16:54:19.159
Mar  9 16:54:19.166: INFO: Waiting up to 5m0s for pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd" in namespace "containers-2776" to be "Succeeded or Failed"
Mar  9 16:54:19.168: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374933ms
Mar  9 16:54:21.172: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005780694s
Mar  9 16:54:23.172: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006013478s
STEP: Saw pod success 03/09/23 16:54:23.172
Mar  9 16:54:23.172: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd" satisfied condition "Succeeded or Failed"
Mar  9 16:54:23.175: INFO: Trying to get logs from node tt-test-el8-003 pod client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:54:23.189
Mar  9 16:54:23.200: INFO: Waiting for pod client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd to disappear
Mar  9 16:54:23.203: INFO: Pod client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  9 16:54:23.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2776" for this suite. 03/09/23 16:54:23.206
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":232,"skipped":4473,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:54:19.142
    Mar  9 16:54:19.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename containers 03/09/23 16:54:19.143
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:19.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:19.156
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 03/09/23 16:54:19.159
    Mar  9 16:54:19.166: INFO: Waiting up to 5m0s for pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd" in namespace "containers-2776" to be "Succeeded or Failed"
    Mar  9 16:54:19.168: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374933ms
    Mar  9 16:54:21.172: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005780694s
    Mar  9 16:54:23.172: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006013478s
    STEP: Saw pod success 03/09/23 16:54:23.172
    Mar  9 16:54:23.172: INFO: Pod "client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd" satisfied condition "Succeeded or Failed"
    Mar  9 16:54:23.175: INFO: Trying to get logs from node tt-test-el8-003 pod client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:54:23.189
    Mar  9 16:54:23.200: INFO: Waiting for pod client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd to disappear
    Mar  9 16:54:23.203: INFO: Pod client-containers-86fa7ea8-0929-4067-a181-aa7ef5d77bbd no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  9 16:54:23.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2776" for this suite. 03/09/23 16:54:23.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:54:23.212
Mar  9 16:54:23.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:54:23.213
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:23.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:23.226
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 03/09/23 16:54:23.229
Mar  9 16:54:23.235: INFO: created test-pod-1
Mar  9 16:54:23.240: INFO: created test-pod-2
Mar  9 16:54:23.246: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/09/23 16:54:23.246
Mar  9 16:54:23.246: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6440' to be running and ready
Mar  9 16:54:23.263: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  9 16:54:23.263: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  9 16:54:23.263: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  9 16:54:23.263: INFO: 0 / 3 pods in namespace 'pods-6440' are running and ready (0 seconds elapsed)
Mar  9 16:54:23.263: INFO: expected 0 pod replicas in namespace 'pods-6440', 0 are Running and Ready.
Mar  9 16:54:23.263: INFO: POD         NODE             PHASE    GRACE  CONDITIONS
Mar  9 16:54:23.263: INFO: test-pod-1  tt-test-el8-003  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  }]
Mar  9 16:54:23.263: INFO: test-pod-2  tt-test-el8-003  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  }]
Mar  9 16:54:23.263: INFO: test-pod-3  tt-test-el8-003  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  }]
Mar  9 16:54:23.263: INFO: 
Mar  9 16:54:25.272: INFO: 3 / 3 pods in namespace 'pods-6440' are running and ready (2 seconds elapsed)
Mar  9 16:54:25.272: INFO: expected 0 pod replicas in namespace 'pods-6440', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/09/23 16:54:25.288
Mar  9 16:54:25.291: INFO: Pod quantity 3 is different from expected quantity 0
Mar  9 16:54:26.294: INFO: Pod quantity 3 is different from expected quantity 0
Mar  9 16:54:27.295: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:54:28.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6440" for this suite. 03/09/23 16:54:28.297
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":233,"skipped":4486,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.091 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:54:23.212
    Mar  9 16:54:23.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:54:23.213
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:23.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:23.226
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 03/09/23 16:54:23.229
    Mar  9 16:54:23.235: INFO: created test-pod-1
    Mar  9 16:54:23.240: INFO: created test-pod-2
    Mar  9 16:54:23.246: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/09/23 16:54:23.246
    Mar  9 16:54:23.246: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6440' to be running and ready
    Mar  9 16:54:23.263: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  9 16:54:23.263: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  9 16:54:23.263: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  9 16:54:23.263: INFO: 0 / 3 pods in namespace 'pods-6440' are running and ready (0 seconds elapsed)
    Mar  9 16:54:23.263: INFO: expected 0 pod replicas in namespace 'pods-6440', 0 are Running and Ready.
    Mar  9 16:54:23.263: INFO: POD         NODE             PHASE    GRACE  CONDITIONS
    Mar  9 16:54:23.263: INFO: test-pod-1  tt-test-el8-003  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  }]
    Mar  9 16:54:23.263: INFO: test-pod-2  tt-test-el8-003  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  }]
    Mar  9 16:54:23.263: INFO: test-pod-3  tt-test-el8-003  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 16:54:23 +0000 UTC  }]
    Mar  9 16:54:23.263: INFO: 
    Mar  9 16:54:25.272: INFO: 3 / 3 pods in namespace 'pods-6440' are running and ready (2 seconds elapsed)
    Mar  9 16:54:25.272: INFO: expected 0 pod replicas in namespace 'pods-6440', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/09/23 16:54:25.288
    Mar  9 16:54:25.291: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  9 16:54:26.294: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  9 16:54:27.295: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:54:28.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6440" for this suite. 03/09/23 16:54:28.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:54:28.304
Mar  9 16:54:28.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename gc 03/09/23 16:54:28.305
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:28.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:28.319
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/09/23 16:54:28.322
STEP: delete the rc 03/09/23 16:54:33.331
STEP: wait for all pods to be garbage collected 03/09/23 16:54:33.336
STEP: Gathering metrics 03/09/23 16:54:38.343
Mar  9 16:54:38.360: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
Mar  9 16:54:38.362: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.594298ms
Mar  9 16:54:38.363: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
Mar  9 16:54:38.363: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
Mar  9 16:54:38.431: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  9 16:54:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8864" for this suite. 03/09/23 16:54:38.435
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":234,"skipped":4491,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.136 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:54:28.304
    Mar  9 16:54:28.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename gc 03/09/23 16:54:28.305
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:28.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:28.319
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/09/23 16:54:28.322
    STEP: delete the rc 03/09/23 16:54:33.331
    STEP: wait for all pods to be garbage collected 03/09/23 16:54:33.336
    STEP: Gathering metrics 03/09/23 16:54:38.343
    Mar  9 16:54:38.360: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
    Mar  9 16:54:38.362: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.594298ms
    Mar  9 16:54:38.363: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
    Mar  9 16:54:38.363: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
    Mar  9 16:54:38.431: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  9 16:54:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8864" for this suite. 03/09/23 16:54:38.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:54:38.441
Mar  9 16:54:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 16:54:38.442
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:38.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:38.456
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-3485c7c1-4a42-4f3e-a098-f9a55bc5ff1d 03/09/23 16:54:38.459
STEP: Creating a pod to test consume configMaps 03/09/23 16:54:38.462
Mar  9 16:54:38.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566" in namespace "configmap-1909" to be "Succeeded or Failed"
Mar  9 16:54:38.471: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896907ms
Mar  9 16:54:40.475: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006818728s
Mar  9 16:54:42.475: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00637104s
STEP: Saw pod success 03/09/23 16:54:42.475
Mar  9 16:54:42.475: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566" satisfied condition "Succeeded or Failed"
Mar  9 16:54:42.477: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566 container configmap-volume-test: <nil>
STEP: delete the pod 03/09/23 16:54:42.483
Mar  9 16:54:42.492: INFO: Waiting for pod pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566 to disappear
Mar  9 16:54:42.494: INFO: Pod pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 16:54:42.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1909" for this suite. 03/09/23 16:54:42.498
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":235,"skipped":4509,"failed":0}
------------------------------
â€¢ [4.062 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:54:38.441
    Mar  9 16:54:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 16:54:38.442
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:38.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:38.456
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-3485c7c1-4a42-4f3e-a098-f9a55bc5ff1d 03/09/23 16:54:38.459
    STEP: Creating a pod to test consume configMaps 03/09/23 16:54:38.462
    Mar  9 16:54:38.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566" in namespace "configmap-1909" to be "Succeeded or Failed"
    Mar  9 16:54:38.471: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896907ms
    Mar  9 16:54:40.475: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006818728s
    Mar  9 16:54:42.475: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00637104s
    STEP: Saw pod success 03/09/23 16:54:42.475
    Mar  9 16:54:42.475: INFO: Pod "pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566" satisfied condition "Succeeded or Failed"
    Mar  9 16:54:42.477: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566 container configmap-volume-test: <nil>
    STEP: delete the pod 03/09/23 16:54:42.483
    Mar  9 16:54:42.492: INFO: Waiting for pod pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566 to disappear
    Mar  9 16:54:42.494: INFO: Pod pod-configmaps-54a0c2f6-b7c7-4e5d-bc17-a564be85c566 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 16:54:42.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1909" for this suite. 03/09/23 16:54:42.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:54:42.504
Mar  9 16:54:42.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:54:42.505
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:42.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:42.517
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-8207 03/09/23 16:54:42.52
Mar  9 16:54:42.528: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8207" to be "running and ready"
Mar  9 16:54:42.530: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406521ms
Mar  9 16:54:42.530: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:54:44.535: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.006873319s
Mar  9 16:54:44.535: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  9 16:54:44.535: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  9 16:54:44.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  9 16:54:44.690: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  9 16:54:44.690: INFO: stdout: "iptables"
Mar  9 16:54:44.690: INFO: proxyMode: iptables
Mar  9 16:54:44.697: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  9 16:54:44.701: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8207 03/09/23 16:54:44.701
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8207 03/09/23 16:54:44.717
I0309 16:54:44.725012      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8207, replica count: 3
I0309 16:54:47.776494      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 16:54:47.785: INFO: Creating new exec pod
Mar  9 16:54:47.790: INFO: Waiting up to 5m0s for pod "execpod-affinityk5ggj" in namespace "services-8207" to be "running"
Mar  9 16:54:47.793: INFO: Pod "execpod-affinityk5ggj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19189ms
Mar  9 16:54:49.797: INFO: Pod "execpod-affinityk5ggj": Phase="Running", Reason="", readiness=true. Elapsed: 2.006719202s
Mar  9 16:54:49.797: INFO: Pod "execpod-affinityk5ggj" satisfied condition "running"
Mar  9 16:54:50.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar  9 16:54:50.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  9 16:54:50.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:54:50.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.141.85 80'
Mar  9 16:54:51.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.141.85 80\nConnection to 10.100.141.85 80 port [tcp/http] succeeded!\n"
Mar  9 16:54:51.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:54:51.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 32176'
Mar  9 16:54:51.270: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 32176\nConnection to 100.100.230.140 32176 port [tcp/*] succeeded!\n"
Mar  9 16:54:51.270: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:54:51.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 32176'
Mar  9 16:54:51.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 32176\nConnection to 100.100.231.104 32176 port [tcp/*] succeeded!\n"
Mar  9 16:54:51.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:54:51.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:32176/ ; done'
Mar  9 16:54:51.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n"
Mar  9 16:54:51.637: INFO: stdout: "\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk"
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
Mar  9 16:54:51.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.100.230.140:32176/'
Mar  9 16:54:51.782: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n"
Mar  9 16:54:51.782: INFO: stdout: "affinity-nodeport-timeout-prvwk"
Mar  9 16:55:11.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.100.230.140:32176/'
Mar  9 16:55:11.933: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n"
Mar  9 16:55:11.933: INFO: stdout: "affinity-nodeport-timeout-x5pqd"
Mar  9 16:55:11.933: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8207, will wait for the garbage collector to delete the pods 03/09/23 16:55:11.945
Mar  9 16:55:12.004: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.678658ms
Mar  9 16:55:12.105: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.820924ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:55:14.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8207" for this suite. 03/09/23 16:55:14.132
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":236,"skipped":4523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [31.634 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:54:42.504
    Mar  9 16:54:42.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:54:42.505
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:54:42.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:54:42.517
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-8207 03/09/23 16:54:42.52
    Mar  9 16:54:42.528: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8207" to be "running and ready"
    Mar  9 16:54:42.530: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406521ms
    Mar  9 16:54:42.530: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:54:44.535: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.006873319s
    Mar  9 16:54:44.535: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  9 16:54:44.535: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  9 16:54:44.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  9 16:54:44.690: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar  9 16:54:44.690: INFO: stdout: "iptables"
    Mar  9 16:54:44.690: INFO: proxyMode: iptables
    Mar  9 16:54:44.697: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  9 16:54:44.701: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-8207 03/09/23 16:54:44.701
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-8207 03/09/23 16:54:44.717
    I0309 16:54:44.725012      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8207, replica count: 3
    I0309 16:54:47.776494      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 16:54:47.785: INFO: Creating new exec pod
    Mar  9 16:54:47.790: INFO: Waiting up to 5m0s for pod "execpod-affinityk5ggj" in namespace "services-8207" to be "running"
    Mar  9 16:54:47.793: INFO: Pod "execpod-affinityk5ggj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19189ms
    Mar  9 16:54:49.797: INFO: Pod "execpod-affinityk5ggj": Phase="Running", Reason="", readiness=true. Elapsed: 2.006719202s
    Mar  9 16:54:49.797: INFO: Pod "execpod-affinityk5ggj" satisfied condition "running"
    Mar  9 16:54:50.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Mar  9 16:54:50.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Mar  9 16:54:50.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:54:50.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.141.85 80'
    Mar  9 16:54:51.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.141.85 80\nConnection to 10.100.141.85 80 port [tcp/http] succeeded!\n"
    Mar  9 16:54:51.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:54:51.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 32176'
    Mar  9 16:54:51.270: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 32176\nConnection to 100.100.230.140 32176 port [tcp/*] succeeded!\n"
    Mar  9 16:54:51.270: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:54:51.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 32176'
    Mar  9 16:54:51.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 32176\nConnection to 100.100.231.104 32176 port [tcp/*] succeeded!\n"
    Mar  9 16:54:51.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:54:51.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:32176/ ; done'
    Mar  9 16:54:51.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n"
    Mar  9 16:54:51.637: INFO: stdout: "\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk\naffinity-nodeport-timeout-prvwk"
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Received response from host: affinity-nodeport-timeout-prvwk
    Mar  9 16:54:51.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.100.230.140:32176/'
    Mar  9 16:54:51.782: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n"
    Mar  9 16:54:51.782: INFO: stdout: "affinity-nodeport-timeout-prvwk"
    Mar  9 16:55:11.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-8207 exec execpod-affinityk5ggj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.100.230.140:32176/'
    Mar  9 16:55:11.933: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.100.230.140:32176/\n"
    Mar  9 16:55:11.933: INFO: stdout: "affinity-nodeport-timeout-x5pqd"
    Mar  9 16:55:11.933: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8207, will wait for the garbage collector to delete the pods 03/09/23 16:55:11.945
    Mar  9 16:55:12.004: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.678658ms
    Mar  9 16:55:12.105: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.820924ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:55:14.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8207" for this suite. 03/09/23 16:55:14.132
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:55:14.139
Mar  9 16:55:14.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename disruption 03/09/23 16:55:14.14
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:14.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:14.155
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 03/09/23 16:55:14.158
STEP: Waiting for the pdb to be processed 03/09/23 16:55:14.163
STEP: updating the pdb 03/09/23 16:55:14.17
STEP: Waiting for the pdb to be processed 03/09/23 16:55:14.18
STEP: patching the pdb 03/09/23 16:55:14.185
STEP: Waiting for the pdb to be processed 03/09/23 16:55:14.193
STEP: Waiting for the pdb to be deleted 03/09/23 16:55:16.206
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  9 16:55:16.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2658" for this suite. 03/09/23 16:55:16.211
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":237,"skipped":4542,"failed":0}
------------------------------
â€¢ [2.078 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:55:14.139
    Mar  9 16:55:14.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename disruption 03/09/23 16:55:14.14
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:14.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:14.155
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 03/09/23 16:55:14.158
    STEP: Waiting for the pdb to be processed 03/09/23 16:55:14.163
    STEP: updating the pdb 03/09/23 16:55:14.17
    STEP: Waiting for the pdb to be processed 03/09/23 16:55:14.18
    STEP: patching the pdb 03/09/23 16:55:14.185
    STEP: Waiting for the pdb to be processed 03/09/23 16:55:14.193
    STEP: Waiting for the pdb to be deleted 03/09/23 16:55:16.206
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  9 16:55:16.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2658" for this suite. 03/09/23 16:55:16.211
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:55:16.217
Mar  9 16:55:16.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:55:16.218
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:16.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:16.235
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 03/09/23 16:55:16.238
Mar  9 16:55:16.238: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  9 16:55:16.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
Mar  9 16:55:16.501: INFO: stderr: ""
Mar  9 16:55:16.501: INFO: stdout: "service/agnhost-replica created\n"
Mar  9 16:55:16.501: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  9 16:55:16.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
Mar  9 16:55:16.823: INFO: stderr: ""
Mar  9 16:55:16.823: INFO: stdout: "service/agnhost-primary created\n"
Mar  9 16:55:16.823: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  9 16:55:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
Mar  9 16:55:17.146: INFO: stderr: ""
Mar  9 16:55:17.146: INFO: stdout: "service/frontend created\n"
Mar  9 16:55:17.146: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  9 16:55:17.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
Mar  9 16:55:17.399: INFO: stderr: ""
Mar  9 16:55:17.399: INFO: stdout: "deployment.apps/frontend created\n"
Mar  9 16:55:17.399: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  9 16:55:17.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
Mar  9 16:55:17.677: INFO: stderr: ""
Mar  9 16:55:17.677: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  9 16:55:17.677: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  9 16:55:17.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
Mar  9 16:55:17.998: INFO: stderr: ""
Mar  9 16:55:17.998: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/09/23 16:55:17.998
Mar  9 16:55:17.999: INFO: Waiting for all frontend pods to be Running.
Mar  9 16:55:23.050: INFO: Waiting for frontend to serve content.
Mar  9 16:55:23.059: INFO: Trying to add a new entry to the guestbook.
Mar  9 16:55:23.069: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/09/23 16:55:23.075
Mar  9 16:55:23.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
Mar  9 16:55:23.172: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:55:23.172: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/09/23 16:55:23.172
Mar  9 16:55:23.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
Mar  9 16:55:23.281: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:55:23.281: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/09/23 16:55:23.281
Mar  9 16:55:23.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
Mar  9 16:55:23.386: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:55:23.386: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/09/23 16:55:23.386
Mar  9 16:55:23.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
Mar  9 16:55:23.470: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:55:23.470: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/09/23 16:55:23.471
Mar  9 16:55:23.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
Mar  9 16:55:23.598: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:55:23.598: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/09/23 16:55:23.598
Mar  9 16:55:23.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
Mar  9 16:55:23.712: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  9 16:55:23.712: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:55:23.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2120" for this suite. 03/09/23 16:55:23.721
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":238,"skipped":4543,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.516 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:55:16.217
    Mar  9 16:55:16.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:55:16.218
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:16.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:16.235
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 03/09/23 16:55:16.238
    Mar  9 16:55:16.238: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar  9 16:55:16.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
    Mar  9 16:55:16.501: INFO: stderr: ""
    Mar  9 16:55:16.501: INFO: stdout: "service/agnhost-replica created\n"
    Mar  9 16:55:16.501: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar  9 16:55:16.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
    Mar  9 16:55:16.823: INFO: stderr: ""
    Mar  9 16:55:16.823: INFO: stdout: "service/agnhost-primary created\n"
    Mar  9 16:55:16.823: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar  9 16:55:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
    Mar  9 16:55:17.146: INFO: stderr: ""
    Mar  9 16:55:17.146: INFO: stdout: "service/frontend created\n"
    Mar  9 16:55:17.146: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar  9 16:55:17.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
    Mar  9 16:55:17.399: INFO: stderr: ""
    Mar  9 16:55:17.399: INFO: stdout: "deployment.apps/frontend created\n"
    Mar  9 16:55:17.399: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  9 16:55:17.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
    Mar  9 16:55:17.677: INFO: stderr: ""
    Mar  9 16:55:17.677: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar  9 16:55:17.677: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  9 16:55:17.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 create -f -'
    Mar  9 16:55:17.998: INFO: stderr: ""
    Mar  9 16:55:17.998: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/09/23 16:55:17.998
    Mar  9 16:55:17.999: INFO: Waiting for all frontend pods to be Running.
    Mar  9 16:55:23.050: INFO: Waiting for frontend to serve content.
    Mar  9 16:55:23.059: INFO: Trying to add a new entry to the guestbook.
    Mar  9 16:55:23.069: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/09/23 16:55:23.075
    Mar  9 16:55:23.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
    Mar  9 16:55:23.172: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:55:23.172: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/09/23 16:55:23.172
    Mar  9 16:55:23.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
    Mar  9 16:55:23.281: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:55:23.281: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/09/23 16:55:23.281
    Mar  9 16:55:23.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
    Mar  9 16:55:23.386: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:55:23.386: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/09/23 16:55:23.386
    Mar  9 16:55:23.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
    Mar  9 16:55:23.470: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:55:23.470: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/09/23 16:55:23.471
    Mar  9 16:55:23.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
    Mar  9 16:55:23.598: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:55:23.598: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/09/23 16:55:23.598
    Mar  9 16:55:23.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
    Mar  9 16:55:23.712: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  9 16:55:23.712: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:55:23.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2120" for this suite. 03/09/23 16:55:23.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:55:23.734
Mar  9 16:55:23.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:55:23.735
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:23.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:23.753
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 16:55:23.757
Mar  9 16:55:23.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9702 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Mar  9 16:55:23.889: INFO: stderr: ""
Mar  9 16:55:23.889: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/09/23 16:55:23.889
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Mar  9 16:55:23.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9702 delete pods e2e-test-httpd-pod'
Mar  9 16:55:27.302: INFO: stderr: ""
Mar  9 16:55:27.302: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:55:27.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9702" for this suite. 03/09/23 16:55:27.308
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":239,"skipped":4556,"failed":0}
------------------------------
â€¢ [3.581 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:55:23.734
    Mar  9 16:55:23.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:55:23.735
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:23.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:23.753
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/09/23 16:55:23.757
    Mar  9 16:55:23.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9702 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Mar  9 16:55:23.889: INFO: stderr: ""
    Mar  9 16:55:23.889: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/09/23 16:55:23.889
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Mar  9 16:55:23.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9702 delete pods e2e-test-httpd-pod'
    Mar  9 16:55:27.302: INFO: stderr: ""
    Mar  9 16:55:27.302: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:55:27.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9702" for this suite. 03/09/23 16:55:27.308
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:55:27.318
Mar  9 16:55:27.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 16:55:27.323
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:27.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:27.337
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Mar  9 16:55:27.346: INFO: Waiting up to 5m0s for pod "server-envvars-6465571f-04da-4009-a630-4e357660af59" in namespace "pods-9197" to be "running and ready"
Mar  9 16:55:27.349: INFO: Pod "server-envvars-6465571f-04da-4009-a630-4e357660af59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204909ms
Mar  9 16:55:27.349: INFO: The phase of Pod server-envvars-6465571f-04da-4009-a630-4e357660af59 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:55:29.352: INFO: Pod "server-envvars-6465571f-04da-4009-a630-4e357660af59": Phase="Running", Reason="", readiness=true. Elapsed: 2.005760257s
Mar  9 16:55:29.352: INFO: The phase of Pod server-envvars-6465571f-04da-4009-a630-4e357660af59 is Running (Ready = true)
Mar  9 16:55:29.352: INFO: Pod "server-envvars-6465571f-04da-4009-a630-4e357660af59" satisfied condition "running and ready"
Mar  9 16:55:29.376: INFO: Waiting up to 5m0s for pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858" in namespace "pods-9197" to be "Succeeded or Failed"
Mar  9 16:55:29.379: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858": Phase="Pending", Reason="", readiness=false. Elapsed: 3.698009ms
Mar  9 16:55:31.384: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007954096s
Mar  9 16:55:33.384: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008092741s
STEP: Saw pod success 03/09/23 16:55:33.384
Mar  9 16:55:33.384: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858" satisfied condition "Succeeded or Failed"
Mar  9 16:55:33.387: INFO: Trying to get logs from node tt-test-el8-003 pod client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858 container env3cont: <nil>
STEP: delete the pod 03/09/23 16:55:33.394
Mar  9 16:55:33.402: INFO: Waiting for pod client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858 to disappear
Mar  9 16:55:33.405: INFO: Pod client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 16:55:33.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9197" for this suite. 03/09/23 16:55:33.409
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":240,"skipped":4559,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.099 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:55:27.318
    Mar  9 16:55:27.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 16:55:27.323
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:27.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:27.337
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Mar  9 16:55:27.346: INFO: Waiting up to 5m0s for pod "server-envvars-6465571f-04da-4009-a630-4e357660af59" in namespace "pods-9197" to be "running and ready"
    Mar  9 16:55:27.349: INFO: Pod "server-envvars-6465571f-04da-4009-a630-4e357660af59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204909ms
    Mar  9 16:55:27.349: INFO: The phase of Pod server-envvars-6465571f-04da-4009-a630-4e357660af59 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:55:29.352: INFO: Pod "server-envvars-6465571f-04da-4009-a630-4e357660af59": Phase="Running", Reason="", readiness=true. Elapsed: 2.005760257s
    Mar  9 16:55:29.352: INFO: The phase of Pod server-envvars-6465571f-04da-4009-a630-4e357660af59 is Running (Ready = true)
    Mar  9 16:55:29.352: INFO: Pod "server-envvars-6465571f-04da-4009-a630-4e357660af59" satisfied condition "running and ready"
    Mar  9 16:55:29.376: INFO: Waiting up to 5m0s for pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858" in namespace "pods-9197" to be "Succeeded or Failed"
    Mar  9 16:55:29.379: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858": Phase="Pending", Reason="", readiness=false. Elapsed: 3.698009ms
    Mar  9 16:55:31.384: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007954096s
    Mar  9 16:55:33.384: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008092741s
    STEP: Saw pod success 03/09/23 16:55:33.384
    Mar  9 16:55:33.384: INFO: Pod "client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858" satisfied condition "Succeeded or Failed"
    Mar  9 16:55:33.387: INFO: Trying to get logs from node tt-test-el8-003 pod client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858 container env3cont: <nil>
    STEP: delete the pod 03/09/23 16:55:33.394
    Mar  9 16:55:33.402: INFO: Waiting for pod client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858 to disappear
    Mar  9 16:55:33.405: INFO: Pod client-envvars-49fe0fa2-6cea-4495-bbfe-55e222a6c858 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 16:55:33.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9197" for this suite. 03/09/23 16:55:33.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:55:33.416
Mar  9 16:55:33.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename security-context-test 03/09/23 16:55:33.417
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:33.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:33.432
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Mar  9 16:55:33.441: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba" in namespace "security-context-test-779" to be "Succeeded or Failed"
Mar  9 16:55:33.443: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188423ms
Mar  9 16:55:35.448: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006614465s
Mar  9 16:55:37.447: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005625998s
Mar  9 16:55:39.448: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006424923s
Mar  9 16:55:39.448: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  9 16:55:39.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-779" for this suite. 03/09/23 16:55:39.457
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":241,"skipped":4578,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.045 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:55:33.416
    Mar  9 16:55:33.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename security-context-test 03/09/23 16:55:33.417
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:33.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:33.432
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Mar  9 16:55:33.441: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba" in namespace "security-context-test-779" to be "Succeeded or Failed"
    Mar  9 16:55:33.443: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188423ms
    Mar  9 16:55:35.448: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006614465s
    Mar  9 16:55:37.447: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005625998s
    Mar  9 16:55:39.448: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006424923s
    Mar  9 16:55:39.448: INFO: Pod "alpine-nnp-false-ea98cfea-c650-4773-852f-87d92bf075ba" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  9 16:55:39.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-779" for this suite. 03/09/23 16:55:39.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:55:39.465
Mar  9 16:55:39.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 16:55:39.467
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:39.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:39.482
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2044 03/09/23 16:55:39.485
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-2044 03/09/23 16:55:39.496
Mar  9 16:55:39.504: INFO: Found 0 stateful pods, waiting for 1
Mar  9 16:55:49.509: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/09/23 16:55:49.514
STEP: Getting /status 03/09/23 16:55:49.524
Mar  9 16:55:49.527: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/09/23 16:55:49.527
Mar  9 16:55:49.535: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/09/23 16:55:49.535
Mar  9 16:55:49.537: INFO: Observed &StatefulSet event: ADDED
Mar  9 16:55:49.537: INFO: Found Statefulset ss in namespace statefulset-2044 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  9 16:55:49.537: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/09/23 16:55:49.537
Mar  9 16:55:49.537: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  9 16:55:49.543: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/09/23 16:55:49.543
Mar  9 16:55:49.545: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 16:55:49.545: INFO: Deleting all statefulset in ns statefulset-2044
Mar  9 16:55:49.548: INFO: Scaling statefulset ss to 0
Mar  9 16:55:59.563: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 16:55:59.565: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 16:55:59.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2044" for this suite. 03/09/23 16:55:59.585
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":242,"skipped":4647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.125 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:55:39.465
    Mar  9 16:55:39.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 16:55:39.467
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:39.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:39.482
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2044 03/09/23 16:55:39.485
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-2044 03/09/23 16:55:39.496
    Mar  9 16:55:39.504: INFO: Found 0 stateful pods, waiting for 1
    Mar  9 16:55:49.509: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/09/23 16:55:49.514
    STEP: Getting /status 03/09/23 16:55:49.524
    Mar  9 16:55:49.527: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/09/23 16:55:49.527
    Mar  9 16:55:49.535: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/09/23 16:55:49.535
    Mar  9 16:55:49.537: INFO: Observed &StatefulSet event: ADDED
    Mar  9 16:55:49.537: INFO: Found Statefulset ss in namespace statefulset-2044 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  9 16:55:49.537: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/09/23 16:55:49.537
    Mar  9 16:55:49.537: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  9 16:55:49.543: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/09/23 16:55:49.543
    Mar  9 16:55:49.545: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 16:55:49.545: INFO: Deleting all statefulset in ns statefulset-2044
    Mar  9 16:55:49.548: INFO: Scaling statefulset ss to 0
    Mar  9 16:55:59.563: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 16:55:59.565: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 16:55:59.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2044" for this suite. 03/09/23 16:55:59.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:55:59.592
Mar  9 16:55:59.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:55:59.593
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:59.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:59.607
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-6226 03/09/23 16:55:59.61
STEP: creating service affinity-nodeport in namespace services-6226 03/09/23 16:55:59.61
STEP: creating replication controller affinity-nodeport in namespace services-6226 03/09/23 16:55:59.625
I0309 16:55:59.632180      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6226, replica count: 3
I0309 16:56:02.684089      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 16:56:02.693: INFO: Creating new exec pod
Mar  9 16:56:02.698: INFO: Waiting up to 5m0s for pod "execpod-affinityzcbbx" in namespace "services-6226" to be "running"
Mar  9 16:56:02.701: INFO: Pod "execpod-affinityzcbbx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27058ms
Mar  9 16:56:04.705: INFO: Pod "execpod-affinityzcbbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.00604228s
Mar  9 16:56:04.705: INFO: Pod "execpod-affinityzcbbx" satisfied condition "running"
Mar  9 16:56:05.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar  9 16:56:05.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  9 16:56:05.865: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:56:05.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.100.32 80'
Mar  9 16:56:06.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.100.32 80\nConnection to 10.109.100.32 80 port [tcp/http] succeeded!\n"
Mar  9 16:56:06.020: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:56:06.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 31653'
Mar  9 16:56:06.174: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 31653\nConnection to 100.100.230.140 31653 port [tcp/*] succeeded!\n"
Mar  9 16:56:06.174: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:56:06.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 31653'
Mar  9 16:56:06.314: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 31653\nConnection to 100.100.231.104 31653 port [tcp/*] succeeded!\n"
Mar  9 16:56:06.314: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 16:56:06.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31653/ ; done'
Mar  9 16:56:06.545: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n"
Mar  9 16:56:06.545: INFO: stdout: "\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq"
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
Mar  9 16:56:06.545: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6226, will wait for the garbage collector to delete the pods 03/09/23 16:56:06.553
Mar  9 16:56:06.613: INFO: Deleting ReplicationController affinity-nodeport took: 4.621811ms
Mar  9 16:56:06.713: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.712915ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:56:08.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6226" for this suite. 03/09/23 16:56:08.342
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":243,"skipped":4665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.756 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:55:59.592
    Mar  9 16:55:59.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:55:59.593
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:55:59.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:55:59.607
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-6226 03/09/23 16:55:59.61
    STEP: creating service affinity-nodeport in namespace services-6226 03/09/23 16:55:59.61
    STEP: creating replication controller affinity-nodeport in namespace services-6226 03/09/23 16:55:59.625
    I0309 16:55:59.632180      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6226, replica count: 3
    I0309 16:56:02.684089      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 16:56:02.693: INFO: Creating new exec pod
    Mar  9 16:56:02.698: INFO: Waiting up to 5m0s for pod "execpod-affinityzcbbx" in namespace "services-6226" to be "running"
    Mar  9 16:56:02.701: INFO: Pod "execpod-affinityzcbbx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27058ms
    Mar  9 16:56:04.705: INFO: Pod "execpod-affinityzcbbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.00604228s
    Mar  9 16:56:04.705: INFO: Pod "execpod-affinityzcbbx" satisfied condition "running"
    Mar  9 16:56:05.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Mar  9 16:56:05.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar  9 16:56:05.865: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:56:05.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.100.32 80'
    Mar  9 16:56:06.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.100.32 80\nConnection to 10.109.100.32 80 port [tcp/http] succeeded!\n"
    Mar  9 16:56:06.020: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:56:06.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.230.140 31653'
    Mar  9 16:56:06.174: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.230.140 31653\nConnection to 100.100.230.140 31653 port [tcp/*] succeeded!\n"
    Mar  9 16:56:06.174: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:56:06.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.100.231.104 31653'
    Mar  9 16:56:06.314: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.100.231.104 31653\nConnection to 100.100.231.104 31653 port [tcp/*] succeeded!\n"
    Mar  9 16:56:06.314: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 16:56:06.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-6226 exec execpod-affinityzcbbx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.230.140:31653/ ; done'
    Mar  9 16:56:06.545: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.230.140:31653/\n"
    Mar  9 16:56:06.545: INFO: stdout: "\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq\naffinity-nodeport-7mcnq"
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Received response from host: affinity-nodeport-7mcnq
    Mar  9 16:56:06.545: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6226, will wait for the garbage collector to delete the pods 03/09/23 16:56:06.553
    Mar  9 16:56:06.613: INFO: Deleting ReplicationController affinity-nodeport took: 4.621811ms
    Mar  9 16:56:06.713: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.712915ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:56:08.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6226" for this suite. 03/09/23 16:56:08.342
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:56:08.349
Mar  9 16:56:08.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename security-context 03/09/23 16:56:08.351
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:08.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:08.365
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/09/23 16:56:08.368
Mar  9 16:56:08.375: INFO: Waiting up to 5m0s for pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e" in namespace "security-context-2114" to be "Succeeded or Failed"
Mar  9 16:56:08.379: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292884ms
Mar  9 16:56:10.383: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007234298s
Mar  9 16:56:12.382: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006546322s
STEP: Saw pod success 03/09/23 16:56:12.382
Mar  9 16:56:12.382: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e" satisfied condition "Succeeded or Failed"
Mar  9 16:56:12.385: INFO: Trying to get logs from node tt-test-el8-003 pod security-context-411b5165-500b-4929-8216-f9d7a2886e1e container test-container: <nil>
STEP: delete the pod 03/09/23 16:56:12.39
Mar  9 16:56:12.399: INFO: Waiting for pod security-context-411b5165-500b-4929-8216-f9d7a2886e1e to disappear
Mar  9 16:56:12.401: INFO: Pod security-context-411b5165-500b-4929-8216-f9d7a2886e1e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  9 16:56:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2114" for this suite. 03/09/23 16:56:12.405
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":244,"skipped":4675,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:56:08.349
    Mar  9 16:56:08.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename security-context 03/09/23 16:56:08.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:08.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:08.365
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/09/23 16:56:08.368
    Mar  9 16:56:08.375: INFO: Waiting up to 5m0s for pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e" in namespace "security-context-2114" to be "Succeeded or Failed"
    Mar  9 16:56:08.379: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292884ms
    Mar  9 16:56:10.383: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007234298s
    Mar  9 16:56:12.382: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006546322s
    STEP: Saw pod success 03/09/23 16:56:12.382
    Mar  9 16:56:12.382: INFO: Pod "security-context-411b5165-500b-4929-8216-f9d7a2886e1e" satisfied condition "Succeeded or Failed"
    Mar  9 16:56:12.385: INFO: Trying to get logs from node tt-test-el8-003 pod security-context-411b5165-500b-4929-8216-f9d7a2886e1e container test-container: <nil>
    STEP: delete the pod 03/09/23 16:56:12.39
    Mar  9 16:56:12.399: INFO: Waiting for pod security-context-411b5165-500b-4929-8216-f9d7a2886e1e to disappear
    Mar  9 16:56:12.401: INFO: Pod security-context-411b5165-500b-4929-8216-f9d7a2886e1e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  9 16:56:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2114" for this suite. 03/09/23 16:56:12.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:56:12.41
Mar  9 16:56:12.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 16:56:12.411
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:12.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:12.424
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 16:56:12.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4306" for this suite. 03/09/23 16:56:12.432
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":245,"skipped":4682,"failed":0}
------------------------------
â€¢ [0.026 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:56:12.41
    Mar  9 16:56:12.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 16:56:12.411
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:12.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:12.424
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 16:56:12.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4306" for this suite. 03/09/23 16:56:12.432
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:56:12.437
Mar  9 16:56:12.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename gc 03/09/23 16:56:12.438
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:12.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:12.451
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/09/23 16:56:12.458
STEP: delete the rc 03/09/23 16:56:17.469
STEP: wait for the rc to be deleted 03/09/23 16:56:17.476
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/09/23 16:56:22.479
STEP: Gathering metrics 03/09/23 16:56:52.495
Mar  9 16:56:52.513: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
Mar  9 16:56:52.515: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.587958ms
Mar  9 16:56:52.515: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
Mar  9 16:56:52.515: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
Mar  9 16:56:52.581: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  9 16:56:52.581: INFO: Deleting pod "simpletest.rc-2l9hz" in namespace "gc-2621"
Mar  9 16:56:52.591: INFO: Deleting pod "simpletest.rc-2mgsd" in namespace "gc-2621"
Mar  9 16:56:52.603: INFO: Deleting pod "simpletest.rc-2pnzg" in namespace "gc-2621"
Mar  9 16:56:52.613: INFO: Deleting pod "simpletest.rc-2r4fp" in namespace "gc-2621"
Mar  9 16:56:52.623: INFO: Deleting pod "simpletest.rc-2vhlt" in namespace "gc-2621"
Mar  9 16:56:52.631: INFO: Deleting pod "simpletest.rc-457rb" in namespace "gc-2621"
Mar  9 16:56:52.642: INFO: Deleting pod "simpletest.rc-497mc" in namespace "gc-2621"
Mar  9 16:56:52.654: INFO: Deleting pod "simpletest.rc-4fbjm" in namespace "gc-2621"
Mar  9 16:56:52.665: INFO: Deleting pod "simpletest.rc-4mc8d" in namespace "gc-2621"
Mar  9 16:56:52.674: INFO: Deleting pod "simpletest.rc-58rjd" in namespace "gc-2621"
Mar  9 16:56:52.685: INFO: Deleting pod "simpletest.rc-5rt45" in namespace "gc-2621"
Mar  9 16:56:52.696: INFO: Deleting pod "simpletest.rc-67xrx" in namespace "gc-2621"
Mar  9 16:56:52.707: INFO: Deleting pod "simpletest.rc-6qb7w" in namespace "gc-2621"
Mar  9 16:56:52.718: INFO: Deleting pod "simpletest.rc-6qw7f" in namespace "gc-2621"
Mar  9 16:56:52.729: INFO: Deleting pod "simpletest.rc-6rlpk" in namespace "gc-2621"
Mar  9 16:56:52.740: INFO: Deleting pod "simpletest.rc-6sgmh" in namespace "gc-2621"
Mar  9 16:56:52.749: INFO: Deleting pod "simpletest.rc-6v4sp" in namespace "gc-2621"
Mar  9 16:56:52.758: INFO: Deleting pod "simpletest.rc-7566t" in namespace "gc-2621"
Mar  9 16:56:52.766: INFO: Deleting pod "simpletest.rc-7b4wj" in namespace "gc-2621"
Mar  9 16:56:52.775: INFO: Deleting pod "simpletest.rc-7g4qm" in namespace "gc-2621"
Mar  9 16:56:52.783: INFO: Deleting pod "simpletest.rc-7knvr" in namespace "gc-2621"
Mar  9 16:56:52.791: INFO: Deleting pod "simpletest.rc-7sfs8" in namespace "gc-2621"
Mar  9 16:56:52.801: INFO: Deleting pod "simpletest.rc-82kb5" in namespace "gc-2621"
Mar  9 16:56:52.811: INFO: Deleting pod "simpletest.rc-86qtr" in namespace "gc-2621"
Mar  9 16:56:52.821: INFO: Deleting pod "simpletest.rc-8875j" in namespace "gc-2621"
Mar  9 16:56:52.828: INFO: Deleting pod "simpletest.rc-8fpll" in namespace "gc-2621"
Mar  9 16:56:52.842: INFO: Deleting pod "simpletest.rc-8v4g5" in namespace "gc-2621"
Mar  9 16:56:52.849: INFO: Deleting pod "simpletest.rc-9sw22" in namespace "gc-2621"
Mar  9 16:56:52.858: INFO: Deleting pod "simpletest.rc-b2769" in namespace "gc-2621"
Mar  9 16:56:52.869: INFO: Deleting pod "simpletest.rc-b27xw" in namespace "gc-2621"
Mar  9 16:56:52.876: INFO: Deleting pod "simpletest.rc-b7cbl" in namespace "gc-2621"
Mar  9 16:56:52.886: INFO: Deleting pod "simpletest.rc-bjgwt" in namespace "gc-2621"
Mar  9 16:56:52.895: INFO: Deleting pod "simpletest.rc-bvkml" in namespace "gc-2621"
Mar  9 16:56:52.904: INFO: Deleting pod "simpletest.rc-c2hb6" in namespace "gc-2621"
Mar  9 16:56:52.916: INFO: Deleting pod "simpletest.rc-c7vs9" in namespace "gc-2621"
Mar  9 16:56:52.925: INFO: Deleting pod "simpletest.rc-cv4f7" in namespace "gc-2621"
Mar  9 16:56:52.935: INFO: Deleting pod "simpletest.rc-dcgr6" in namespace "gc-2621"
Mar  9 16:56:52.944: INFO: Deleting pod "simpletest.rc-f7t5h" in namespace "gc-2621"
Mar  9 16:56:52.953: INFO: Deleting pod "simpletest.rc-frn6n" in namespace "gc-2621"
Mar  9 16:56:52.962: INFO: Deleting pod "simpletest.rc-gcz4f" in namespace "gc-2621"
Mar  9 16:56:52.972: INFO: Deleting pod "simpletest.rc-gv4t9" in namespace "gc-2621"
Mar  9 16:56:52.982: INFO: Deleting pod "simpletest.rc-gz2t2" in namespace "gc-2621"
Mar  9 16:56:52.991: INFO: Deleting pod "simpletest.rc-h2fv8" in namespace "gc-2621"
Mar  9 16:56:53.000: INFO: Deleting pod "simpletest.rc-h8wj7" in namespace "gc-2621"
Mar  9 16:56:53.009: INFO: Deleting pod "simpletest.rc-hr8cf" in namespace "gc-2621"
Mar  9 16:56:53.018: INFO: Deleting pod "simpletest.rc-jcn5v" in namespace "gc-2621"
Mar  9 16:56:53.028: INFO: Deleting pod "simpletest.rc-jgjtb" in namespace "gc-2621"
Mar  9 16:56:53.039: INFO: Deleting pod "simpletest.rc-jnnpd" in namespace "gc-2621"
Mar  9 16:56:53.048: INFO: Deleting pod "simpletest.rc-kf8ph" in namespace "gc-2621"
Mar  9 16:56:53.056: INFO: Deleting pod "simpletest.rc-l8frw" in namespace "gc-2621"
Mar  9 16:56:53.064: INFO: Deleting pod "simpletest.rc-l8p2c" in namespace "gc-2621"
Mar  9 16:56:53.073: INFO: Deleting pod "simpletest.rc-lczlf" in namespace "gc-2621"
Mar  9 16:56:53.081: INFO: Deleting pod "simpletest.rc-ltlsg" in namespace "gc-2621"
Mar  9 16:56:53.089: INFO: Deleting pod "simpletest.rc-mgtbd" in namespace "gc-2621"
Mar  9 16:56:53.097: INFO: Deleting pod "simpletest.rc-mx6tq" in namespace "gc-2621"
Mar  9 16:56:53.106: INFO: Deleting pod "simpletest.rc-mzcfv" in namespace "gc-2621"
Mar  9 16:56:53.115: INFO: Deleting pod "simpletest.rc-n74b9" in namespace "gc-2621"
Mar  9 16:56:53.124: INFO: Deleting pod "simpletest.rc-nxkdd" in namespace "gc-2621"
Mar  9 16:56:53.137: INFO: Deleting pod "simpletest.rc-nzjrj" in namespace "gc-2621"
Mar  9 16:56:53.145: INFO: Deleting pod "simpletest.rc-p57t9" in namespace "gc-2621"
Mar  9 16:56:53.190: INFO: Deleting pod "simpletest.rc-pc8nc" in namespace "gc-2621"
Mar  9 16:56:53.242: INFO: Deleting pod "simpletest.rc-pmxwc" in namespace "gc-2621"
Mar  9 16:56:53.290: INFO: Deleting pod "simpletest.rc-q65gq" in namespace "gc-2621"
Mar  9 16:56:53.341: INFO: Deleting pod "simpletest.rc-qftgz" in namespace "gc-2621"
Mar  9 16:56:53.393: INFO: Deleting pod "simpletest.rc-qmtbv" in namespace "gc-2621"
Mar  9 16:56:53.441: INFO: Deleting pod "simpletest.rc-qtc5j" in namespace "gc-2621"
Mar  9 16:56:53.492: INFO: Deleting pod "simpletest.rc-qtgfg" in namespace "gc-2621"
Mar  9 16:56:53.561: INFO: Deleting pod "simpletest.rc-r69wj" in namespace "gc-2621"
Mar  9 16:56:53.593: INFO: Deleting pod "simpletest.rc-rc57h" in namespace "gc-2621"
Mar  9 16:56:53.641: INFO: Deleting pod "simpletest.rc-rk2hg" in namespace "gc-2621"
Mar  9 16:56:53.689: INFO: Deleting pod "simpletest.rc-rl48r" in namespace "gc-2621"
Mar  9 16:56:53.746: INFO: Deleting pod "simpletest.rc-rlk54" in namespace "gc-2621"
Mar  9 16:56:53.792: INFO: Deleting pod "simpletest.rc-rrlnt" in namespace "gc-2621"
Mar  9 16:56:53.840: INFO: Deleting pod "simpletest.rc-rv6hx" in namespace "gc-2621"
Mar  9 16:56:53.890: INFO: Deleting pod "simpletest.rc-s2cpf" in namespace "gc-2621"
Mar  9 16:56:53.939: INFO: Deleting pod "simpletest.rc-s2pv6" in namespace "gc-2621"
Mar  9 16:56:53.992: INFO: Deleting pod "simpletest.rc-sf6rp" in namespace "gc-2621"
Mar  9 16:56:54.045: INFO: Deleting pod "simpletest.rc-sfh5l" in namespace "gc-2621"
Mar  9 16:56:54.088: INFO: Deleting pod "simpletest.rc-snzdb" in namespace "gc-2621"
Mar  9 16:56:54.141: INFO: Deleting pod "simpletest.rc-t5q8r" in namespace "gc-2621"
Mar  9 16:56:54.192: INFO: Deleting pod "simpletest.rc-tfhz7" in namespace "gc-2621"
Mar  9 16:56:54.239: INFO: Deleting pod "simpletest.rc-tp29t" in namespace "gc-2621"
Mar  9 16:56:54.290: INFO: Deleting pod "simpletest.rc-tx2kd" in namespace "gc-2621"
Mar  9 16:56:54.338: INFO: Deleting pod "simpletest.rc-tx7wl" in namespace "gc-2621"
Mar  9 16:56:54.392: INFO: Deleting pod "simpletest.rc-vcxks" in namespace "gc-2621"
Mar  9 16:56:54.440: INFO: Deleting pod "simpletest.rc-vjfsj" in namespace "gc-2621"
Mar  9 16:56:54.492: INFO: Deleting pod "simpletest.rc-vr69l" in namespace "gc-2621"
Mar  9 16:56:54.542: INFO: Deleting pod "simpletest.rc-wpp8s" in namespace "gc-2621"
Mar  9 16:56:54.590: INFO: Deleting pod "simpletest.rc-wtbgj" in namespace "gc-2621"
Mar  9 16:56:54.641: INFO: Deleting pod "simpletest.rc-x87fh" in namespace "gc-2621"
Mar  9 16:56:54.692: INFO: Deleting pod "simpletest.rc-xb5t4" in namespace "gc-2621"
Mar  9 16:56:54.739: INFO: Deleting pod "simpletest.rc-xhkbl" in namespace "gc-2621"
Mar  9 16:56:54.792: INFO: Deleting pod "simpletest.rc-xmtt8" in namespace "gc-2621"
Mar  9 16:56:54.842: INFO: Deleting pod "simpletest.rc-xqcj8" in namespace "gc-2621"
Mar  9 16:56:54.894: INFO: Deleting pod "simpletest.rc-xvmm6" in namespace "gc-2621"
Mar  9 16:56:54.942: INFO: Deleting pod "simpletest.rc-xw6pj" in namespace "gc-2621"
Mar  9 16:56:54.995: INFO: Deleting pod "simpletest.rc-z2w4p" in namespace "gc-2621"
Mar  9 16:56:55.039: INFO: Deleting pod "simpletest.rc-zfxh9" in namespace "gc-2621"
Mar  9 16:56:55.091: INFO: Deleting pod "simpletest.rc-zj8w9" in namespace "gc-2621"
Mar  9 16:56:55.141: INFO: Deleting pod "simpletest.rc-zqdd2" in namespace "gc-2621"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  9 16:56:55.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2621" for this suite. 03/09/23 16:56:55.235
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":246,"skipped":4692,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.849 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:56:12.437
    Mar  9 16:56:12.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename gc 03/09/23 16:56:12.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:12.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:12.451
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/09/23 16:56:12.458
    STEP: delete the rc 03/09/23 16:56:17.469
    STEP: wait for the rc to be deleted 03/09/23 16:56:17.476
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/09/23 16:56:22.479
    STEP: Gathering metrics 03/09/23 16:56:52.495
    Mar  9 16:56:52.513: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
    Mar  9 16:56:52.515: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.587958ms
    Mar  9 16:56:52.515: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
    Mar  9 16:56:52.515: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
    Mar  9 16:56:52.581: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  9 16:56:52.581: INFO: Deleting pod "simpletest.rc-2l9hz" in namespace "gc-2621"
    Mar  9 16:56:52.591: INFO: Deleting pod "simpletest.rc-2mgsd" in namespace "gc-2621"
    Mar  9 16:56:52.603: INFO: Deleting pod "simpletest.rc-2pnzg" in namespace "gc-2621"
    Mar  9 16:56:52.613: INFO: Deleting pod "simpletest.rc-2r4fp" in namespace "gc-2621"
    Mar  9 16:56:52.623: INFO: Deleting pod "simpletest.rc-2vhlt" in namespace "gc-2621"
    Mar  9 16:56:52.631: INFO: Deleting pod "simpletest.rc-457rb" in namespace "gc-2621"
    Mar  9 16:56:52.642: INFO: Deleting pod "simpletest.rc-497mc" in namespace "gc-2621"
    Mar  9 16:56:52.654: INFO: Deleting pod "simpletest.rc-4fbjm" in namespace "gc-2621"
    Mar  9 16:56:52.665: INFO: Deleting pod "simpletest.rc-4mc8d" in namespace "gc-2621"
    Mar  9 16:56:52.674: INFO: Deleting pod "simpletest.rc-58rjd" in namespace "gc-2621"
    Mar  9 16:56:52.685: INFO: Deleting pod "simpletest.rc-5rt45" in namespace "gc-2621"
    Mar  9 16:56:52.696: INFO: Deleting pod "simpletest.rc-67xrx" in namespace "gc-2621"
    Mar  9 16:56:52.707: INFO: Deleting pod "simpletest.rc-6qb7w" in namespace "gc-2621"
    Mar  9 16:56:52.718: INFO: Deleting pod "simpletest.rc-6qw7f" in namespace "gc-2621"
    Mar  9 16:56:52.729: INFO: Deleting pod "simpletest.rc-6rlpk" in namespace "gc-2621"
    Mar  9 16:56:52.740: INFO: Deleting pod "simpletest.rc-6sgmh" in namespace "gc-2621"
    Mar  9 16:56:52.749: INFO: Deleting pod "simpletest.rc-6v4sp" in namespace "gc-2621"
    Mar  9 16:56:52.758: INFO: Deleting pod "simpletest.rc-7566t" in namespace "gc-2621"
    Mar  9 16:56:52.766: INFO: Deleting pod "simpletest.rc-7b4wj" in namespace "gc-2621"
    Mar  9 16:56:52.775: INFO: Deleting pod "simpletest.rc-7g4qm" in namespace "gc-2621"
    Mar  9 16:56:52.783: INFO: Deleting pod "simpletest.rc-7knvr" in namespace "gc-2621"
    Mar  9 16:56:52.791: INFO: Deleting pod "simpletest.rc-7sfs8" in namespace "gc-2621"
    Mar  9 16:56:52.801: INFO: Deleting pod "simpletest.rc-82kb5" in namespace "gc-2621"
    Mar  9 16:56:52.811: INFO: Deleting pod "simpletest.rc-86qtr" in namespace "gc-2621"
    Mar  9 16:56:52.821: INFO: Deleting pod "simpletest.rc-8875j" in namespace "gc-2621"
    Mar  9 16:56:52.828: INFO: Deleting pod "simpletest.rc-8fpll" in namespace "gc-2621"
    Mar  9 16:56:52.842: INFO: Deleting pod "simpletest.rc-8v4g5" in namespace "gc-2621"
    Mar  9 16:56:52.849: INFO: Deleting pod "simpletest.rc-9sw22" in namespace "gc-2621"
    Mar  9 16:56:52.858: INFO: Deleting pod "simpletest.rc-b2769" in namespace "gc-2621"
    Mar  9 16:56:52.869: INFO: Deleting pod "simpletest.rc-b27xw" in namespace "gc-2621"
    Mar  9 16:56:52.876: INFO: Deleting pod "simpletest.rc-b7cbl" in namespace "gc-2621"
    Mar  9 16:56:52.886: INFO: Deleting pod "simpletest.rc-bjgwt" in namespace "gc-2621"
    Mar  9 16:56:52.895: INFO: Deleting pod "simpletest.rc-bvkml" in namespace "gc-2621"
    Mar  9 16:56:52.904: INFO: Deleting pod "simpletest.rc-c2hb6" in namespace "gc-2621"
    Mar  9 16:56:52.916: INFO: Deleting pod "simpletest.rc-c7vs9" in namespace "gc-2621"
    Mar  9 16:56:52.925: INFO: Deleting pod "simpletest.rc-cv4f7" in namespace "gc-2621"
    Mar  9 16:56:52.935: INFO: Deleting pod "simpletest.rc-dcgr6" in namespace "gc-2621"
    Mar  9 16:56:52.944: INFO: Deleting pod "simpletest.rc-f7t5h" in namespace "gc-2621"
    Mar  9 16:56:52.953: INFO: Deleting pod "simpletest.rc-frn6n" in namespace "gc-2621"
    Mar  9 16:56:52.962: INFO: Deleting pod "simpletest.rc-gcz4f" in namespace "gc-2621"
    Mar  9 16:56:52.972: INFO: Deleting pod "simpletest.rc-gv4t9" in namespace "gc-2621"
    Mar  9 16:56:52.982: INFO: Deleting pod "simpletest.rc-gz2t2" in namespace "gc-2621"
    Mar  9 16:56:52.991: INFO: Deleting pod "simpletest.rc-h2fv8" in namespace "gc-2621"
    Mar  9 16:56:53.000: INFO: Deleting pod "simpletest.rc-h8wj7" in namespace "gc-2621"
    Mar  9 16:56:53.009: INFO: Deleting pod "simpletest.rc-hr8cf" in namespace "gc-2621"
    Mar  9 16:56:53.018: INFO: Deleting pod "simpletest.rc-jcn5v" in namespace "gc-2621"
    Mar  9 16:56:53.028: INFO: Deleting pod "simpletest.rc-jgjtb" in namespace "gc-2621"
    Mar  9 16:56:53.039: INFO: Deleting pod "simpletest.rc-jnnpd" in namespace "gc-2621"
    Mar  9 16:56:53.048: INFO: Deleting pod "simpletest.rc-kf8ph" in namespace "gc-2621"
    Mar  9 16:56:53.056: INFO: Deleting pod "simpletest.rc-l8frw" in namespace "gc-2621"
    Mar  9 16:56:53.064: INFO: Deleting pod "simpletest.rc-l8p2c" in namespace "gc-2621"
    Mar  9 16:56:53.073: INFO: Deleting pod "simpletest.rc-lczlf" in namespace "gc-2621"
    Mar  9 16:56:53.081: INFO: Deleting pod "simpletest.rc-ltlsg" in namespace "gc-2621"
    Mar  9 16:56:53.089: INFO: Deleting pod "simpletest.rc-mgtbd" in namespace "gc-2621"
    Mar  9 16:56:53.097: INFO: Deleting pod "simpletest.rc-mx6tq" in namespace "gc-2621"
    Mar  9 16:56:53.106: INFO: Deleting pod "simpletest.rc-mzcfv" in namespace "gc-2621"
    Mar  9 16:56:53.115: INFO: Deleting pod "simpletest.rc-n74b9" in namespace "gc-2621"
    Mar  9 16:56:53.124: INFO: Deleting pod "simpletest.rc-nxkdd" in namespace "gc-2621"
    Mar  9 16:56:53.137: INFO: Deleting pod "simpletest.rc-nzjrj" in namespace "gc-2621"
    Mar  9 16:56:53.145: INFO: Deleting pod "simpletest.rc-p57t9" in namespace "gc-2621"
    Mar  9 16:56:53.190: INFO: Deleting pod "simpletest.rc-pc8nc" in namespace "gc-2621"
    Mar  9 16:56:53.242: INFO: Deleting pod "simpletest.rc-pmxwc" in namespace "gc-2621"
    Mar  9 16:56:53.290: INFO: Deleting pod "simpletest.rc-q65gq" in namespace "gc-2621"
    Mar  9 16:56:53.341: INFO: Deleting pod "simpletest.rc-qftgz" in namespace "gc-2621"
    Mar  9 16:56:53.393: INFO: Deleting pod "simpletest.rc-qmtbv" in namespace "gc-2621"
    Mar  9 16:56:53.441: INFO: Deleting pod "simpletest.rc-qtc5j" in namespace "gc-2621"
    Mar  9 16:56:53.492: INFO: Deleting pod "simpletest.rc-qtgfg" in namespace "gc-2621"
    Mar  9 16:56:53.561: INFO: Deleting pod "simpletest.rc-r69wj" in namespace "gc-2621"
    Mar  9 16:56:53.593: INFO: Deleting pod "simpletest.rc-rc57h" in namespace "gc-2621"
    Mar  9 16:56:53.641: INFO: Deleting pod "simpletest.rc-rk2hg" in namespace "gc-2621"
    Mar  9 16:56:53.689: INFO: Deleting pod "simpletest.rc-rl48r" in namespace "gc-2621"
    Mar  9 16:56:53.746: INFO: Deleting pod "simpletest.rc-rlk54" in namespace "gc-2621"
    Mar  9 16:56:53.792: INFO: Deleting pod "simpletest.rc-rrlnt" in namespace "gc-2621"
    Mar  9 16:56:53.840: INFO: Deleting pod "simpletest.rc-rv6hx" in namespace "gc-2621"
    Mar  9 16:56:53.890: INFO: Deleting pod "simpletest.rc-s2cpf" in namespace "gc-2621"
    Mar  9 16:56:53.939: INFO: Deleting pod "simpletest.rc-s2pv6" in namespace "gc-2621"
    Mar  9 16:56:53.992: INFO: Deleting pod "simpletest.rc-sf6rp" in namespace "gc-2621"
    Mar  9 16:56:54.045: INFO: Deleting pod "simpletest.rc-sfh5l" in namespace "gc-2621"
    Mar  9 16:56:54.088: INFO: Deleting pod "simpletest.rc-snzdb" in namespace "gc-2621"
    Mar  9 16:56:54.141: INFO: Deleting pod "simpletest.rc-t5q8r" in namespace "gc-2621"
    Mar  9 16:56:54.192: INFO: Deleting pod "simpletest.rc-tfhz7" in namespace "gc-2621"
    Mar  9 16:56:54.239: INFO: Deleting pod "simpletest.rc-tp29t" in namespace "gc-2621"
    Mar  9 16:56:54.290: INFO: Deleting pod "simpletest.rc-tx2kd" in namespace "gc-2621"
    Mar  9 16:56:54.338: INFO: Deleting pod "simpletest.rc-tx7wl" in namespace "gc-2621"
    Mar  9 16:56:54.392: INFO: Deleting pod "simpletest.rc-vcxks" in namespace "gc-2621"
    Mar  9 16:56:54.440: INFO: Deleting pod "simpletest.rc-vjfsj" in namespace "gc-2621"
    Mar  9 16:56:54.492: INFO: Deleting pod "simpletest.rc-vr69l" in namespace "gc-2621"
    Mar  9 16:56:54.542: INFO: Deleting pod "simpletest.rc-wpp8s" in namespace "gc-2621"
    Mar  9 16:56:54.590: INFO: Deleting pod "simpletest.rc-wtbgj" in namespace "gc-2621"
    Mar  9 16:56:54.641: INFO: Deleting pod "simpletest.rc-x87fh" in namespace "gc-2621"
    Mar  9 16:56:54.692: INFO: Deleting pod "simpletest.rc-xb5t4" in namespace "gc-2621"
    Mar  9 16:56:54.739: INFO: Deleting pod "simpletest.rc-xhkbl" in namespace "gc-2621"
    Mar  9 16:56:54.792: INFO: Deleting pod "simpletest.rc-xmtt8" in namespace "gc-2621"
    Mar  9 16:56:54.842: INFO: Deleting pod "simpletest.rc-xqcj8" in namespace "gc-2621"
    Mar  9 16:56:54.894: INFO: Deleting pod "simpletest.rc-xvmm6" in namespace "gc-2621"
    Mar  9 16:56:54.942: INFO: Deleting pod "simpletest.rc-xw6pj" in namespace "gc-2621"
    Mar  9 16:56:54.995: INFO: Deleting pod "simpletest.rc-z2w4p" in namespace "gc-2621"
    Mar  9 16:56:55.039: INFO: Deleting pod "simpletest.rc-zfxh9" in namespace "gc-2621"
    Mar  9 16:56:55.091: INFO: Deleting pod "simpletest.rc-zj8w9" in namespace "gc-2621"
    Mar  9 16:56:55.141: INFO: Deleting pod "simpletest.rc-zqdd2" in namespace "gc-2621"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  9 16:56:55.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2621" for this suite. 03/09/23 16:56:55.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:56:55.29
Mar  9 16:56:55.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:56:55.292
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:55.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:55.308
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:56:55.312
Mar  9 16:56:55.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b" in namespace "projected-2379" to be "Succeeded or Failed"
Mar  9 16:56:55.325: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.734475ms
Mar  9 16:56:57.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009601465s
Mar  9 16:56:59.331: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011307193s
Mar  9 16:57:01.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009690859s
Mar  9 16:57:03.330: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01026096s
Mar  9 16:57:05.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009608419s
Mar  9 16:57:07.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.010188167s
STEP: Saw pod success 03/09/23 16:57:07.329
Mar  9 16:57:07.330: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b" satisfied condition "Succeeded or Failed"
Mar  9 16:57:07.332: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b container client-container: <nil>
STEP: delete the pod 03/09/23 16:57:07.337
Mar  9 16:57:07.347: INFO: Waiting for pod downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b to disappear
Mar  9 16:57:07.349: INFO: Pod downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 16:57:07.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2379" for this suite. 03/09/23 16:57:07.353
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":247,"skipped":4702,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.069 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:56:55.29
    Mar  9 16:56:55.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:56:55.292
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:56:55.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:56:55.308
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:56:55.312
    Mar  9 16:56:55.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b" in namespace "projected-2379" to be "Succeeded or Failed"
    Mar  9 16:56:55.325: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.734475ms
    Mar  9 16:56:57.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009601465s
    Mar  9 16:56:59.331: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011307193s
    Mar  9 16:57:01.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009690859s
    Mar  9 16:57:03.330: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01026096s
    Mar  9 16:57:05.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009608419s
    Mar  9 16:57:07.329: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.010188167s
    STEP: Saw pod success 03/09/23 16:57:07.329
    Mar  9 16:57:07.330: INFO: Pod "downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b" satisfied condition "Succeeded or Failed"
    Mar  9 16:57:07.332: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b container client-container: <nil>
    STEP: delete the pod 03/09/23 16:57:07.337
    Mar  9 16:57:07.347: INFO: Waiting for pod downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b to disappear
    Mar  9 16:57:07.349: INFO: Pod downwardapi-volume-6324b240-587e-485e-bd8d-ace42365e60b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 16:57:07.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2379" for this suite. 03/09/23 16:57:07.353
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:07.359
Mar  9 16:57:07.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 16:57:07.36
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:07.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:07.376
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 03/09/23 16:57:07.379
STEP: Creating a ResourceQuota 03/09/23 16:57:12.382
STEP: Ensuring resource quota status is calculated 03/09/23 16:57:12.387
STEP: Creating a ReplicationController 03/09/23 16:57:14.39
STEP: Ensuring resource quota status captures replication controller creation 03/09/23 16:57:14.401
STEP: Deleting a ReplicationController 03/09/23 16:57:16.405
STEP: Ensuring resource quota status released usage 03/09/23 16:57:16.409
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 16:57:18.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4316" for this suite. 03/09/23 16:57:18.417
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":248,"skipped":4703,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:07.359
    Mar  9 16:57:07.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 16:57:07.36
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:07.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:07.376
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 03/09/23 16:57:07.379
    STEP: Creating a ResourceQuota 03/09/23 16:57:12.382
    STEP: Ensuring resource quota status is calculated 03/09/23 16:57:12.387
    STEP: Creating a ReplicationController 03/09/23 16:57:14.39
    STEP: Ensuring resource quota status captures replication controller creation 03/09/23 16:57:14.401
    STEP: Deleting a ReplicationController 03/09/23 16:57:16.405
    STEP: Ensuring resource quota status released usage 03/09/23 16:57:16.409
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 16:57:18.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4316" for this suite. 03/09/23 16:57:18.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:18.423
Mar  9 16:57:18.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:57:18.424
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:18.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:18.438
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 03/09/23 16:57:18.441
Mar  9 16:57:18.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca" in namespace "downward-api-1105" to be "Succeeded or Failed"
Mar  9 16:57:18.451: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449578ms
Mar  9 16:57:20.456: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007273743s
Mar  9 16:57:22.456: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007165215s
STEP: Saw pod success 03/09/23 16:57:22.456
Mar  9 16:57:22.456: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca" satisfied condition "Succeeded or Failed"
Mar  9 16:57:22.459: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca container client-container: <nil>
STEP: delete the pod 03/09/23 16:57:22.464
Mar  9 16:57:22.473: INFO: Waiting for pod downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca to disappear
Mar  9 16:57:22.476: INFO: Pod downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 16:57:22.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1105" for this suite. 03/09/23 16:57:22.479
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":249,"skipped":4718,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:18.423
    Mar  9 16:57:18.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:57:18.424
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:18.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:18.438
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 03/09/23 16:57:18.441
    Mar  9 16:57:18.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca" in namespace "downward-api-1105" to be "Succeeded or Failed"
    Mar  9 16:57:18.451: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.449578ms
    Mar  9 16:57:20.456: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007273743s
    Mar  9 16:57:22.456: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007165215s
    STEP: Saw pod success 03/09/23 16:57:22.456
    Mar  9 16:57:22.456: INFO: Pod "downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca" satisfied condition "Succeeded or Failed"
    Mar  9 16:57:22.459: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca container client-container: <nil>
    STEP: delete the pod 03/09/23 16:57:22.464
    Mar  9 16:57:22.473: INFO: Waiting for pod downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca to disappear
    Mar  9 16:57:22.476: INFO: Pod downwardapi-volume-3b9d149c-662e-4910-8e0b-b12c5ea37bca no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 16:57:22.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1105" for this suite. 03/09/23 16:57:22.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:22.484
Mar  9 16:57:22.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename containers 03/09/23 16:57:22.486
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:22.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:22.5
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Mar  9 16:57:22.508: INFO: Waiting up to 5m0s for pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd" in namespace "containers-5508" to be "running"
Mar  9 16:57:22.511: INFO: Pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.526569ms
Mar  9 16:57:24.523: INFO: Pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.014222844s
Mar  9 16:57:24.523: INFO: Pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  9 16:57:24.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5508" for this suite. 03/09/23 16:57:24.533
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":250,"skipped":4723,"failed":0}
------------------------------
â€¢ [2.053 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:22.484
    Mar  9 16:57:22.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename containers 03/09/23 16:57:22.486
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:22.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:22.5
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Mar  9 16:57:22.508: INFO: Waiting up to 5m0s for pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd" in namespace "containers-5508" to be "running"
    Mar  9 16:57:22.511: INFO: Pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.526569ms
    Mar  9 16:57:24.523: INFO: Pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.014222844s
    Mar  9 16:57:24.523: INFO: Pod "client-containers-e4cf9988-0c2e-4f7b-bc2e-1765758165dd" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  9 16:57:24.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5508" for this suite. 03/09/23 16:57:24.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:24.54
Mar  9 16:57:24.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:57:24.541
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:24.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:24.554
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 03/09/23 16:57:24.556
Mar  9 16:57:24.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  9 16:57:24.630: INFO: stderr: ""
Mar  9 16:57:24.630: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 03/09/23 16:57:24.63
Mar  9 16:57:24.630: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  9 16:57:24.630: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4137" to be "running and ready, or succeeded"
Mar  9 16:57:24.633: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.493264ms
Mar  9 16:57:24.633: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tt-test-el8-003' to be 'Running' but was 'Pending'
Mar  9 16:57:26.637: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006396429s
Mar  9 16:57:26.637: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  9 16:57:26.637: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/09/23 16:57:26.637
Mar  9 16:57:26.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator'
Mar  9 16:57:26.710: INFO: stderr: ""
Mar  9 16:57:26.710: INFO: stdout: "I0309 16:57:25.351593       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/7ln 560\nI0309 16:57:25.551736       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/dv5v 348\nI0309 16:57:25.752295       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/k7lt 244\nI0309 16:57:25.952665       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/bbd 482\nI0309 16:57:26.152014       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/b5l6 258\nI0309 16:57:26.352359       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/k5rq 517\nI0309 16:57:26.551636       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/mssv 560\n"
STEP: limiting log lines 03/09/23 16:57:26.71
Mar  9 16:57:26.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --tail=1'
Mar  9 16:57:26.786: INFO: stderr: ""
Mar  9 16:57:26.786: INFO: stdout: "I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
Mar  9 16:57:26.786: INFO: got output "I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
STEP: limiting log bytes 03/09/23 16:57:26.786
Mar  9 16:57:26.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --limit-bytes=1'
Mar  9 16:57:26.861: INFO: stderr: ""
Mar  9 16:57:26.861: INFO: stdout: "I"
Mar  9 16:57:26.861: INFO: got output "I"
STEP: exposing timestamps 03/09/23 16:57:26.861
Mar  9 16:57:26.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  9 16:57:26.935: INFO: stderr: ""
Mar  9 16:57:26.935: INFO: stdout: "2023-03-09T16:57:26.752122065Z I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
Mar  9 16:57:26.935: INFO: got output "2023-03-09T16:57:26.752122065Z I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
STEP: restricting to a time range 03/09/23 16:57:26.935
Mar  9 16:57:29.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --since=1s'
Mar  9 16:57:29.511: INFO: stderr: ""
Mar  9 16:57:29.511: INFO: stdout: "I0309 16:57:28.551948       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/tpqn 508\nI0309 16:57:28.752329       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/djq 314\nI0309 16:57:28.952686       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/llc4 350\nI0309 16:57:29.152053       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/gs8 296\nI0309 16:57:29.352326       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/kr7q 200\n"
Mar  9 16:57:29.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --since=24h'
Mar  9 16:57:29.584: INFO: stderr: ""
Mar  9 16:57:29.584: INFO: stdout: "I0309 16:57:25.351593       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/7ln 560\nI0309 16:57:25.551736       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/dv5v 348\nI0309 16:57:25.752295       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/k7lt 244\nI0309 16:57:25.952665       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/bbd 482\nI0309 16:57:26.152014       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/b5l6 258\nI0309 16:57:26.352359       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/k5rq 517\nI0309 16:57:26.551636       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/mssv 560\nI0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\nI0309 16:57:26.952351       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/mwh 584\nI0309 16:57:27.151638       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/n9bx 521\nI0309 16:57:27.352027       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/wwx 299\nI0309 16:57:27.552368       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/vmzx 400\nI0309 16:57:27.751632       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/w824 389\nI0309 16:57:27.952021       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/lb2 332\nI0309 16:57:28.152179       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/6c2 215\nI0309 16:57:28.352576       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/xzt 354\nI0309 16:57:28.551948       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/tpqn 508\nI0309 16:57:28.752329       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/djq 314\nI0309 16:57:28.952686       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/llc4 350\nI0309 16:57:29.152053       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/gs8 296\nI0309 16:57:29.352326       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/kr7q 200\nI0309 16:57:29.553222       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/t8s9 573\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Mar  9 16:57:29.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 delete pod logs-generator'
Mar  9 16:57:30.338: INFO: stderr: ""
Mar  9 16:57:30.338: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:57:30.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4137" for this suite. 03/09/23 16:57:30.342
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":251,"skipped":4766,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.807 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:24.54
    Mar  9 16:57:24.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:57:24.541
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:24.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:24.554
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 03/09/23 16:57:24.556
    Mar  9 16:57:24.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar  9 16:57:24.630: INFO: stderr: ""
    Mar  9 16:57:24.630: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 03/09/23 16:57:24.63
    Mar  9 16:57:24.630: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar  9 16:57:24.630: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4137" to be "running and ready, or succeeded"
    Mar  9 16:57:24.633: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.493264ms
    Mar  9 16:57:24.633: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'tt-test-el8-003' to be 'Running' but was 'Pending'
    Mar  9 16:57:26.637: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006396429s
    Mar  9 16:57:26.637: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar  9 16:57:26.637: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/09/23 16:57:26.637
    Mar  9 16:57:26.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator'
    Mar  9 16:57:26.710: INFO: stderr: ""
    Mar  9 16:57:26.710: INFO: stdout: "I0309 16:57:25.351593       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/7ln 560\nI0309 16:57:25.551736       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/dv5v 348\nI0309 16:57:25.752295       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/k7lt 244\nI0309 16:57:25.952665       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/bbd 482\nI0309 16:57:26.152014       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/b5l6 258\nI0309 16:57:26.352359       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/k5rq 517\nI0309 16:57:26.551636       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/mssv 560\n"
    STEP: limiting log lines 03/09/23 16:57:26.71
    Mar  9 16:57:26.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --tail=1'
    Mar  9 16:57:26.786: INFO: stderr: ""
    Mar  9 16:57:26.786: INFO: stdout: "I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
    Mar  9 16:57:26.786: INFO: got output "I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
    STEP: limiting log bytes 03/09/23 16:57:26.786
    Mar  9 16:57:26.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --limit-bytes=1'
    Mar  9 16:57:26.861: INFO: stderr: ""
    Mar  9 16:57:26.861: INFO: stdout: "I"
    Mar  9 16:57:26.861: INFO: got output "I"
    STEP: exposing timestamps 03/09/23 16:57:26.861
    Mar  9 16:57:26.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar  9 16:57:26.935: INFO: stderr: ""
    Mar  9 16:57:26.935: INFO: stdout: "2023-03-09T16:57:26.752122065Z I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
    Mar  9 16:57:26.935: INFO: got output "2023-03-09T16:57:26.752122065Z I0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\n"
    STEP: restricting to a time range 03/09/23 16:57:26.935
    Mar  9 16:57:29.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --since=1s'
    Mar  9 16:57:29.511: INFO: stderr: ""
    Mar  9 16:57:29.511: INFO: stdout: "I0309 16:57:28.551948       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/tpqn 508\nI0309 16:57:28.752329       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/djq 314\nI0309 16:57:28.952686       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/llc4 350\nI0309 16:57:29.152053       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/gs8 296\nI0309 16:57:29.352326       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/kr7q 200\n"
    Mar  9 16:57:29.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 logs logs-generator logs-generator --since=24h'
    Mar  9 16:57:29.584: INFO: stderr: ""
    Mar  9 16:57:29.584: INFO: stdout: "I0309 16:57:25.351593       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/7ln 560\nI0309 16:57:25.551736       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/dv5v 348\nI0309 16:57:25.752295       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/k7lt 244\nI0309 16:57:25.952665       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/bbd 482\nI0309 16:57:26.152014       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/b5l6 258\nI0309 16:57:26.352359       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/k5rq 517\nI0309 16:57:26.551636       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/mssv 560\nI0309 16:57:26.752038       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/wzf 205\nI0309 16:57:26.952351       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/mwh 584\nI0309 16:57:27.151638       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/n9bx 521\nI0309 16:57:27.352027       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/wwx 299\nI0309 16:57:27.552368       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/vmzx 400\nI0309 16:57:27.751632       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/w824 389\nI0309 16:57:27.952021       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/lb2 332\nI0309 16:57:28.152179       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/6c2 215\nI0309 16:57:28.352576       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/xzt 354\nI0309 16:57:28.551948       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/tpqn 508\nI0309 16:57:28.752329       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/djq 314\nI0309 16:57:28.952686       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/llc4 350\nI0309 16:57:29.152053       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/gs8 296\nI0309 16:57:29.352326       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/kr7q 200\nI0309 16:57:29.553222       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/t8s9 573\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Mar  9 16:57:29.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-4137 delete pod logs-generator'
    Mar  9 16:57:30.338: INFO: stderr: ""
    Mar  9 16:57:30.338: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:57:30.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4137" for this suite. 03/09/23 16:57:30.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:30.348
Mar  9 16:57:30.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:57:30.35
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:30.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:30.369
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/09/23 16:57:30.372
Mar  9 16:57:30.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:57:34.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:57:50.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8635" for this suite. 03/09/23 16:57:50.174
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":252,"skipped":4795,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.830 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:30.348
    Mar  9 16:57:30.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:57:30.35
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:30.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:30.369
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/09/23 16:57:30.372
    Mar  9 16:57:30.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:57:34.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:57:50.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8635" for this suite. 03/09/23 16:57:50.174
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:50.179
Mar  9 16:57:50.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 16:57:50.18
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:50.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:50.196
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Mar  9 16:57:50.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2431 version'
Mar  9 16:57:50.260: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar  9 16:57:50.260: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T14:05:25Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7+1.el8\", GitCommit:\"26eb3f5fe6860b07d4d25a7044e4bba35088dc3a\", GitTreeState:\"clean\", BuildDate:\"2023-03-07T21:37:30Z\", GoVersion:\"go1.19.4 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 16:57:50.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2431" for this suite. 03/09/23 16:57:50.265
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":253,"skipped":4795,"failed":0}
------------------------------
â€¢ [0.091 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:50.179
    Mar  9 16:57:50.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 16:57:50.18
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:50.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:50.196
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Mar  9 16:57:50.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-2431 version'
    Mar  9 16:57:50.260: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar  9 16:57:50.260: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T14:05:25Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7+1.el8\", GitCommit:\"26eb3f5fe6860b07d4d25a7044e4bba35088dc3a\", GitTreeState:\"clean\", BuildDate:\"2023-03-07T21:37:30Z\", GoVersion:\"go1.19.4 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 16:57:50.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2431" for this suite. 03/09/23 16:57:50.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:50.271
Mar  9 16:57:50.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:57:50.273
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:50.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:50.286
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  03/09/23 16:57:50.289
Mar  9 16:57:50.295: INFO: Waiting up to 5m0s for pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c" in namespace "svcaccounts-316" to be "Succeeded or Failed"
Mar  9 16:57:50.298: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701211ms
Mar  9 16:57:52.302: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00724411s
Mar  9 16:57:54.301: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006059732s
STEP: Saw pod success 03/09/23 16:57:54.301
Mar  9 16:57:54.301: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c" satisfied condition "Succeeded or Failed"
Mar  9 16:57:54.304: INFO: Trying to get logs from node tt-test-el8-003 pod test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:57:54.31
Mar  9 16:57:54.318: INFO: Waiting for pod test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c to disappear
Mar  9 16:57:54.321: INFO: Pod test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  9 16:57:54.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-316" for this suite. 03/09/23 16:57:54.324
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":254,"skipped":4811,"failed":0}
------------------------------
â€¢ [4.058 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:50.271
    Mar  9 16:57:50.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename svcaccounts 03/09/23 16:57:50.273
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:50.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:50.286
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  03/09/23 16:57:50.289
    Mar  9 16:57:50.295: INFO: Waiting up to 5m0s for pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c" in namespace "svcaccounts-316" to be "Succeeded or Failed"
    Mar  9 16:57:50.298: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701211ms
    Mar  9 16:57:52.302: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00724411s
    Mar  9 16:57:54.301: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006059732s
    STEP: Saw pod success 03/09/23 16:57:54.301
    Mar  9 16:57:54.301: INFO: Pod "test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c" satisfied condition "Succeeded or Failed"
    Mar  9 16:57:54.304: INFO: Trying to get logs from node tt-test-el8-003 pod test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:57:54.31
    Mar  9 16:57:54.318: INFO: Waiting for pod test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c to disappear
    Mar  9 16:57:54.321: INFO: Pod test-pod-5c757f51-4495-4e80-95a3-d735236eaf6c no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  9 16:57:54.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-316" for this suite. 03/09/23 16:57:54.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:54.331
Mar  9 16:57:54.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 16:57:54.332
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:54.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:54.346
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar  9 16:57:54.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:57:54.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2627" for this suite. 03/09/23 16:57:54.892
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":255,"skipped":4824,"failed":0}
------------------------------
â€¢ [0.569 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:54.331
    Mar  9 16:57:54.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 16:57:54.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:54.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:54.346
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar  9 16:57:54.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:57:54.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2627" for this suite. 03/09/23 16:57:54.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:54.902
Mar  9 16:57:54.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 16:57:54.903
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:54.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:54.917
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 03/09/23 16:57:54.919
Mar  9 16:57:54.927: INFO: Waiting up to 5m0s for pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4" in namespace "downward-api-4729" to be "running and ready"
Mar  9 16:57:54.930: INFO: Pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11486ms
Mar  9 16:57:54.930: INFO: The phase of Pod annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:57:56.935: INFO: Pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00729891s
Mar  9 16:57:56.935: INFO: The phase of Pod annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4 is Running (Ready = true)
Mar  9 16:57:56.935: INFO: Pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4" satisfied condition "running and ready"
Mar  9 16:57:57.454: INFO: Successfully updated pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 16:57:59.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4729" for this suite. 03/09/23 16:57:59.473
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":256,"skipped":4866,"failed":0}
------------------------------
â€¢ [4.579 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:54.902
    Mar  9 16:57:54.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 16:57:54.903
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:54.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:54.917
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 03/09/23 16:57:54.919
    Mar  9 16:57:54.927: INFO: Waiting up to 5m0s for pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4" in namespace "downward-api-4729" to be "running and ready"
    Mar  9 16:57:54.930: INFO: Pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11486ms
    Mar  9 16:57:54.930: INFO: The phase of Pod annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:57:56.935: INFO: Pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00729891s
    Mar  9 16:57:56.935: INFO: The phase of Pod annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4 is Running (Ready = true)
    Mar  9 16:57:56.935: INFO: Pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4" satisfied condition "running and ready"
    Mar  9 16:57:57.454: INFO: Successfully updated pod "annotationupdate79af65d6-6661-49a1-8e62-cab21e80f3e4"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 16:57:59.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4729" for this suite. 03/09/23 16:57:59.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:57:59.483
Mar  9 16:57:59.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:57:59.484
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:59.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:59.5
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Mar  9 16:57:59.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/09/23 16:58:04.484
Mar  9 16:58:04.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 create -f -'
Mar  9 16:58:05.465: INFO: stderr: ""
Mar  9 16:58:05.465: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  9 16:58:05.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 delete e2e-test-crd-publish-openapi-7959-crds test-cr'
Mar  9 16:58:05.540: INFO: stderr: ""
Mar  9 16:58:05.540: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  9 16:58:05.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 apply -f -'
Mar  9 16:58:05.792: INFO: stderr: ""
Mar  9 16:58:05.792: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  9 16:58:05.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 delete e2e-test-crd-publish-openapi-7959-crds test-cr'
Mar  9 16:58:05.866: INFO: stderr: ""
Mar  9 16:58:05.866: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/09/23 16:58:05.866
Mar  9 16:58:05.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 explain e2e-test-crd-publish-openapi-7959-crds'
Mar  9 16:58:06.131: INFO: stderr: ""
Mar  9 16:58:06.131: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7959-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:58:08.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9853" for this suite. 03/09/23 16:58:08.938
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":257,"skipped":4873,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.461 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:57:59.483
    Mar  9 16:57:59.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 16:57:59.484
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:57:59.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:57:59.5
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Mar  9 16:57:59.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/09/23 16:58:04.484
    Mar  9 16:58:04.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 create -f -'
    Mar  9 16:58:05.465: INFO: stderr: ""
    Mar  9 16:58:05.465: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  9 16:58:05.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 delete e2e-test-crd-publish-openapi-7959-crds test-cr'
    Mar  9 16:58:05.540: INFO: stderr: ""
    Mar  9 16:58:05.540: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar  9 16:58:05.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 apply -f -'
    Mar  9 16:58:05.792: INFO: stderr: ""
    Mar  9 16:58:05.792: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  9 16:58:05.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 --namespace=crd-publish-openapi-9853 delete e2e-test-crd-publish-openapi-7959-crds test-cr'
    Mar  9 16:58:05.866: INFO: stderr: ""
    Mar  9 16:58:05.866: INFO: stdout: "e2e-test-crd-publish-openapi-7959-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/09/23 16:58:05.866
    Mar  9 16:58:05.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-9853 explain e2e-test-crd-publish-openapi-7959-crds'
    Mar  9 16:58:06.131: INFO: stderr: ""
    Mar  9 16:58:06.131: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7959-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:58:08.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9853" for this suite. 03/09/23 16:58:08.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:08.945
Mar  9 16:58:08.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:58:08.947
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:08.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:08.964
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/09/23 16:58:08.967
Mar  9 16:58:08.975: INFO: Waiting up to 5m0s for pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b" in namespace "emptydir-7641" to be "Succeeded or Failed"
Mar  9 16:58:08.978: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406407ms
Mar  9 16:58:10.982: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006111626s
Mar  9 16:58:12.982: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006819669s
STEP: Saw pod success 03/09/23 16:58:12.982
Mar  9 16:58:12.983: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b" satisfied condition "Succeeded or Failed"
Mar  9 16:58:12.985: INFO: Trying to get logs from node tt-test-el8-003 pod pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b container test-container: <nil>
STEP: delete the pod 03/09/23 16:58:12.991
Mar  9 16:58:12.999: INFO: Waiting for pod pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b to disappear
Mar  9 16:58:13.001: INFO: Pod pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:58:13.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7641" for this suite. 03/09/23 16:58:13.005
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":258,"skipped":4888,"failed":0}
------------------------------
â€¢ [4.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:08.945
    Mar  9 16:58:08.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:58:08.947
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:08.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:08.964
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/09/23 16:58:08.967
    Mar  9 16:58:08.975: INFO: Waiting up to 5m0s for pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b" in namespace "emptydir-7641" to be "Succeeded or Failed"
    Mar  9 16:58:08.978: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406407ms
    Mar  9 16:58:10.982: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006111626s
    Mar  9 16:58:12.982: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006819669s
    STEP: Saw pod success 03/09/23 16:58:12.982
    Mar  9 16:58:12.983: INFO: Pod "pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b" satisfied condition "Succeeded or Failed"
    Mar  9 16:58:12.985: INFO: Trying to get logs from node tt-test-el8-003 pod pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b container test-container: <nil>
    STEP: delete the pod 03/09/23 16:58:12.991
    Mar  9 16:58:12.999: INFO: Waiting for pod pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b to disappear
    Mar  9 16:58:13.001: INFO: Pod pod-f5f8d946-ed69-47a7-b509-a3c068ffaa3b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:58:13.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7641" for this suite. 03/09/23 16:58:13.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:13.013
Mar  9 16:58:13.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename runtimeclass 03/09/23 16:58:13.014
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:13.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:13.028
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar  9 16:58:13.041: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5497 to be scheduled
Mar  9 16:58:13.043: INFO: 1 pods are not scheduled: [runtimeclass-5497/test-runtimeclass-runtimeclass-5497-preconfigured-handler-hbcfb(5e9d561c-78a8-4bc5-be88-1a14da6b6214)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  9 16:58:15.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5497" for this suite. 03/09/23 16:58:15.056
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":259,"skipped":4904,"failed":0}
------------------------------
â€¢ [2.048 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:13.013
    Mar  9 16:58:13.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename runtimeclass 03/09/23 16:58:13.014
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:13.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:13.028
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar  9 16:58:13.041: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5497 to be scheduled
    Mar  9 16:58:13.043: INFO: 1 pods are not scheduled: [runtimeclass-5497/test-runtimeclass-runtimeclass-5497-preconfigured-handler-hbcfb(5e9d561c-78a8-4bc5-be88-1a14da6b6214)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  9 16:58:15.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5497" for this suite. 03/09/23 16:58:15.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:15.062
Mar  9 16:58:15.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 16:58:15.063
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:15.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:15.077
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-116d5390-cee9-4fdb-93e5-1464f080fbf8 03/09/23 16:58:15.08
STEP: Creating a pod to test consume configMaps 03/09/23 16:58:15.084
Mar  9 16:58:15.091: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd" in namespace "projected-3161" to be "Succeeded or Failed"
Mar  9 16:58:15.094: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415609ms
Mar  9 16:58:17.098: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006295561s
Mar  9 16:58:19.097: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005913509s
STEP: Saw pod success 03/09/23 16:58:19.097
Mar  9 16:58:19.098: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd" satisfied condition "Succeeded or Failed"
Mar  9 16:58:19.100: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd container agnhost-container: <nil>
STEP: delete the pod 03/09/23 16:58:19.106
Mar  9 16:58:19.116: INFO: Waiting for pod pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd to disappear
Mar  9 16:58:19.118: INFO: Pod pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 16:58:19.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3161" for this suite. 03/09/23 16:58:19.121
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":260,"skipped":4916,"failed":0}
------------------------------
â€¢ [4.064 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:15.062
    Mar  9 16:58:15.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 16:58:15.063
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:15.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:15.077
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-116d5390-cee9-4fdb-93e5-1464f080fbf8 03/09/23 16:58:15.08
    STEP: Creating a pod to test consume configMaps 03/09/23 16:58:15.084
    Mar  9 16:58:15.091: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd" in namespace "projected-3161" to be "Succeeded or Failed"
    Mar  9 16:58:15.094: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415609ms
    Mar  9 16:58:17.098: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006295561s
    Mar  9 16:58:19.097: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005913509s
    STEP: Saw pod success 03/09/23 16:58:19.097
    Mar  9 16:58:19.098: INFO: Pod "pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd" satisfied condition "Succeeded or Failed"
    Mar  9 16:58:19.100: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 16:58:19.106
    Mar  9 16:58:19.116: INFO: Waiting for pod pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd to disappear
    Mar  9 16:58:19.118: INFO: Pod pod-projected-configmaps-32f6b30f-6d8c-4fe7-a7ff-ab2fadcf4edd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 16:58:19.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3161" for this suite. 03/09/23 16:58:19.121
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:19.126
Mar  9 16:58:19.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 16:58:19.128
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:19.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:19.142
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-f2f754dc-f048-4d5c-8446-4fd8a716a391 03/09/23 16:58:19.149
STEP: Creating configMap with name cm-test-opt-upd-610ebae5-ff8c-4a32-9c72-d18b465ebebd 03/09/23 16:58:19.152
STEP: Creating the pod 03/09/23 16:58:19.156
Mar  9 16:58:19.163: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34" in namespace "configmap-7413" to be "running and ready"
Mar  9 16:58:19.165: INFO: Pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271844ms
Mar  9 16:58:19.165: INFO: The phase of Pod pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:58:21.170: INFO: Pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34": Phase="Running", Reason="", readiness=true. Elapsed: 2.006960616s
Mar  9 16:58:21.170: INFO: The phase of Pod pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34 is Running (Ready = true)
Mar  9 16:58:21.170: INFO: Pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-f2f754dc-f048-4d5c-8446-4fd8a716a391 03/09/23 16:58:21.188
STEP: Updating configmap cm-test-opt-upd-610ebae5-ff8c-4a32-9c72-d18b465ebebd 03/09/23 16:58:21.192
STEP: Creating configMap with name cm-test-opt-create-df206f0e-39aa-4a03-8fc7-7ab454fdae85 03/09/23 16:58:21.196
STEP: waiting to observe update in volume 03/09/23 16:58:21.2
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 16:58:23.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7413" for this suite. 03/09/23 16:58:23.23
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":261,"skipped":4919,"failed":0}
------------------------------
â€¢ [4.110 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:19.126
    Mar  9 16:58:19.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 16:58:19.128
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:19.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:19.142
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-f2f754dc-f048-4d5c-8446-4fd8a716a391 03/09/23 16:58:19.149
    STEP: Creating configMap with name cm-test-opt-upd-610ebae5-ff8c-4a32-9c72-d18b465ebebd 03/09/23 16:58:19.152
    STEP: Creating the pod 03/09/23 16:58:19.156
    Mar  9 16:58:19.163: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34" in namespace "configmap-7413" to be "running and ready"
    Mar  9 16:58:19.165: INFO: Pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271844ms
    Mar  9 16:58:19.165: INFO: The phase of Pod pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:58:21.170: INFO: Pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34": Phase="Running", Reason="", readiness=true. Elapsed: 2.006960616s
    Mar  9 16:58:21.170: INFO: The phase of Pod pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34 is Running (Ready = true)
    Mar  9 16:58:21.170: INFO: Pod "pod-configmaps-a4a0ea22-c6b7-491c-9627-4b5756020b34" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-f2f754dc-f048-4d5c-8446-4fd8a716a391 03/09/23 16:58:21.188
    STEP: Updating configmap cm-test-opt-upd-610ebae5-ff8c-4a32-9c72-d18b465ebebd 03/09/23 16:58:21.192
    STEP: Creating configMap with name cm-test-opt-create-df206f0e-39aa-4a03-8fc7-7ab454fdae85 03/09/23 16:58:21.196
    STEP: waiting to observe update in volume 03/09/23 16:58:21.2
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 16:58:23.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7413" for this suite. 03/09/23 16:58:23.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:23.237
Mar  9 16:58:23.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 16:58:23.239
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:23.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:23.254
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Mar  9 16:58:23.265: INFO: Waiting up to 2m0s for pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" in namespace "var-expansion-5896" to be "container 0 failed with reason CreateContainerConfigError"
Mar  9 16:58:23.267: INFO: Pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649448ms
Mar  9 16:58:25.270: INFO: Pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005788086s
Mar  9 16:58:25.270: INFO: Pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  9 16:58:25.271: INFO: Deleting pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" in namespace "var-expansion-5896"
Mar  9 16:58:25.278: INFO: Wait up to 5m0s for pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 16:58:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5896" for this suite. 03/09/23 16:58:29.288
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":262,"skipped":4927,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.056 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:23.237
    Mar  9 16:58:23.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 16:58:23.239
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:23.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:23.254
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Mar  9 16:58:23.265: INFO: Waiting up to 2m0s for pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" in namespace "var-expansion-5896" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  9 16:58:23.267: INFO: Pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649448ms
    Mar  9 16:58:25.270: INFO: Pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005788086s
    Mar  9 16:58:25.270: INFO: Pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  9 16:58:25.271: INFO: Deleting pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" in namespace "var-expansion-5896"
    Mar  9 16:58:25.278: INFO: Wait up to 5m0s for pod "var-expansion-15466458-7a8b-471a-b1e2-d804d1663ddf" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 16:58:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5896" for this suite. 03/09/23 16:58:29.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:29.294
Mar  9 16:58:29.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:58:29.295
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:29.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:29.319
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 03/09/23 16:58:29.322
Mar  9 16:58:29.331: INFO: Waiting up to 5m0s for pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683" in namespace "emptydir-8403" to be "Succeeded or Failed"
Mar  9 16:58:29.334: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073302ms
Mar  9 16:58:31.338: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007395336s
Mar  9 16:58:33.338: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007411232s
STEP: Saw pod success 03/09/23 16:58:33.338
Mar  9 16:58:33.339: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683" satisfied condition "Succeeded or Failed"
Mar  9 16:58:33.341: INFO: Trying to get logs from node tt-test-el8-003 pod pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683 container test-container: <nil>
STEP: delete the pod 03/09/23 16:58:33.347
Mar  9 16:58:33.359: INFO: Waiting for pod pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683 to disappear
Mar  9 16:58:33.362: INFO: Pod pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:58:33.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8403" for this suite. 03/09/23 16:58:33.365
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":263,"skipped":4941,"failed":0}
------------------------------
â€¢ [4.076 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:29.294
    Mar  9 16:58:29.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:58:29.295
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:29.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:29.319
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/09/23 16:58:29.322
    Mar  9 16:58:29.331: INFO: Waiting up to 5m0s for pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683" in namespace "emptydir-8403" to be "Succeeded or Failed"
    Mar  9 16:58:29.334: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073302ms
    Mar  9 16:58:31.338: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007395336s
    Mar  9 16:58:33.338: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007411232s
    STEP: Saw pod success 03/09/23 16:58:33.338
    Mar  9 16:58:33.339: INFO: Pod "pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683" satisfied condition "Succeeded or Failed"
    Mar  9 16:58:33.341: INFO: Trying to get logs from node tt-test-el8-003 pod pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683 container test-container: <nil>
    STEP: delete the pod 03/09/23 16:58:33.347
    Mar  9 16:58:33.359: INFO: Waiting for pod pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683 to disappear
    Mar  9 16:58:33.362: INFO: Pod pod-0304ceb9-84cb-49db-bf2a-8717d3cfd683 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:58:33.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8403" for this suite. 03/09/23 16:58:33.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:33.371
Mar  9 16:58:33.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 16:58:33.373
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:33.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:33.386
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 16:58:33.398
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:58:33.93
STEP: Deploying the webhook pod 03/09/23 16:58:33.937
STEP: Wait for the deployment to be ready 03/09/23 16:58:33.947
Mar  9 16:58:33.953: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 16:58:35.962
STEP: Verifying the service has paired with the endpoint 03/09/23 16:58:35.992
Mar  9 16:58:36.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 03/09/23 16:58:36.995
STEP: Creating a custom resource definition that should be denied by the webhook 03/09/23 16:58:37.01
Mar  9 16:58:37.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 16:58:37.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7144" for this suite. 03/09/23 16:58:37.029
STEP: Destroying namespace "webhook-7144-markers" for this suite. 03/09/23 16:58:37.033
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":264,"skipped":4951,"failed":0}
------------------------------
â€¢ [3.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:33.371
    Mar  9 16:58:33.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 16:58:33.373
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:33.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:33.386
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 16:58:33.398
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 16:58:33.93
    STEP: Deploying the webhook pod 03/09/23 16:58:33.937
    STEP: Wait for the deployment to be ready 03/09/23 16:58:33.947
    Mar  9 16:58:33.953: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 16:58:35.962
    STEP: Verifying the service has paired with the endpoint 03/09/23 16:58:35.992
    Mar  9 16:58:36.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/09/23 16:58:36.995
    STEP: Creating a custom resource definition that should be denied by the webhook 03/09/23 16:58:37.01
    Mar  9 16:58:37.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 16:58:37.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7144" for this suite. 03/09/23 16:58:37.029
    STEP: Destroying namespace "webhook-7144-markers" for this suite. 03/09/23 16:58:37.033
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:37.081
Mar  9 16:58:37.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-runtime 03/09/23 16:58:37.082
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:37.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:37.099
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 03/09/23 16:58:37.102
STEP: wait for the container to reach Failed 03/09/23 16:58:37.111
STEP: get the container status 03/09/23 16:58:41.126
STEP: the container should be terminated 03/09/23 16:58:41.129
STEP: the termination message should be set 03/09/23 16:58:41.129
Mar  9 16:58:41.129: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/09/23 16:58:41.129
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  9 16:58:41.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3327" for this suite. 03/09/23 16:58:41.143
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":265,"skipped":5003,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:37.081
    Mar  9 16:58:37.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-runtime 03/09/23 16:58:37.082
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:37.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:37.099
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 03/09/23 16:58:37.102
    STEP: wait for the container to reach Failed 03/09/23 16:58:37.111
    STEP: get the container status 03/09/23 16:58:41.126
    STEP: the container should be terminated 03/09/23 16:58:41.129
    STEP: the termination message should be set 03/09/23 16:58:41.129
    Mar  9 16:58:41.129: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/09/23 16:58:41.129
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  9 16:58:41.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3327" for this suite. 03/09/23 16:58:41.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:41.15
Mar  9 16:58:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 16:58:41.151
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:41.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:41.166
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 03/09/23 16:58:41.168
Mar  9 16:58:41.176: INFO: Waiting up to 5m0s for pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389" in namespace "emptydir-3457" to be "Succeeded or Failed"
Mar  9 16:58:41.180: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124035ms
Mar  9 16:58:43.184: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008297752s
Mar  9 16:58:45.184: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008733987s
STEP: Saw pod success 03/09/23 16:58:45.184
Mar  9 16:58:45.185: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389" satisfied condition "Succeeded or Failed"
Mar  9 16:58:45.187: INFO: Trying to get logs from node tt-test-el8-003 pod pod-50d2abec-2516-48c7-b935-f6e0e3cb5389 container test-container: <nil>
STEP: delete the pod 03/09/23 16:58:45.193
Mar  9 16:58:45.203: INFO: Waiting for pod pod-50d2abec-2516-48c7-b935-f6e0e3cb5389 to disappear
Mar  9 16:58:45.205: INFO: Pod pod-50d2abec-2516-48c7-b935-f6e0e3cb5389 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 16:58:45.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3457" for this suite. 03/09/23 16:58:45.208
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":266,"skipped":5023,"failed":0}
------------------------------
â€¢ [4.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:41.15
    Mar  9 16:58:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 16:58:41.151
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:41.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:41.166
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/09/23 16:58:41.168
    Mar  9 16:58:41.176: INFO: Waiting up to 5m0s for pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389" in namespace "emptydir-3457" to be "Succeeded or Failed"
    Mar  9 16:58:41.180: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124035ms
    Mar  9 16:58:43.184: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008297752s
    Mar  9 16:58:45.184: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008733987s
    STEP: Saw pod success 03/09/23 16:58:45.184
    Mar  9 16:58:45.185: INFO: Pod "pod-50d2abec-2516-48c7-b935-f6e0e3cb5389" satisfied condition "Succeeded or Failed"
    Mar  9 16:58:45.187: INFO: Trying to get logs from node tt-test-el8-003 pod pod-50d2abec-2516-48c7-b935-f6e0e3cb5389 container test-container: <nil>
    STEP: delete the pod 03/09/23 16:58:45.193
    Mar  9 16:58:45.203: INFO: Waiting for pod pod-50d2abec-2516-48c7-b935-f6e0e3cb5389 to disappear
    Mar  9 16:58:45.205: INFO: Pod pod-50d2abec-2516-48c7-b935-f6e0e3cb5389 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 16:58:45.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3457" for this suite. 03/09/23 16:58:45.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:45.214
Mar  9 16:58:45.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 16:58:45.215
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:45.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:45.228
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-d1b8c8d6-3789-461f-84f8-5f5117d1e430 03/09/23 16:58:45.234
STEP: Creating the pod 03/09/23 16:58:45.238
Mar  9 16:58:45.245: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8" in namespace "configmap-8396" to be "running"
Mar  9 16:58:45.248: INFO: Pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.118133ms
Mar  9 16:58:47.252: INFO: Pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8": Phase="Running", Reason="", readiness=false. Elapsed: 2.006895542s
Mar  9 16:58:47.252: INFO: Pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8" satisfied condition "running"
STEP: Waiting for pod with text data 03/09/23 16:58:47.252
STEP: Waiting for pod with binary data 03/09/23 16:58:47.257
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 16:58:47.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8396" for this suite. 03/09/23 16:58:47.266
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":267,"skipped":5031,"failed":0}
------------------------------
â€¢ [2.060 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:45.214
    Mar  9 16:58:45.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 16:58:45.215
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:45.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:45.228
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-d1b8c8d6-3789-461f-84f8-5f5117d1e430 03/09/23 16:58:45.234
    STEP: Creating the pod 03/09/23 16:58:45.238
    Mar  9 16:58:45.245: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8" in namespace "configmap-8396" to be "running"
    Mar  9 16:58:45.248: INFO: Pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.118133ms
    Mar  9 16:58:47.252: INFO: Pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8": Phase="Running", Reason="", readiness=false. Elapsed: 2.006895542s
    Mar  9 16:58:47.252: INFO: Pod "pod-configmaps-c6845e38-d017-405a-ac9d-30c56ddfb2a8" satisfied condition "running"
    STEP: Waiting for pod with text data 03/09/23 16:58:47.252
    STEP: Waiting for pod with binary data 03/09/23 16:58:47.257
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 16:58:47.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8396" for this suite. 03/09/23 16:58:47.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:47.275
Mar  9 16:58:47.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename security-context 03/09/23 16:58:47.276
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:47.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:47.29
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/09/23 16:58:47.293
Mar  9 16:58:47.300: INFO: Waiting up to 5m0s for pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087" in namespace "security-context-1463" to be "Succeeded or Failed"
Mar  9 16:58:47.303: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419751ms
Mar  9 16:58:49.308: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007394519s
Mar  9 16:58:51.306: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006317173s
STEP: Saw pod success 03/09/23 16:58:51.306
Mar  9 16:58:51.307: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087" satisfied condition "Succeeded or Failed"
Mar  9 16:58:51.309: INFO: Trying to get logs from node tt-test-el8-003 pod security-context-7732b67d-33ae-4a13-9297-abc5099d2087 container test-container: <nil>
STEP: delete the pod 03/09/23 16:58:51.314
Mar  9 16:58:51.325: INFO: Waiting for pod security-context-7732b67d-33ae-4a13-9297-abc5099d2087 to disappear
Mar  9 16:58:51.328: INFO: Pod security-context-7732b67d-33ae-4a13-9297-abc5099d2087 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  9 16:58:51.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1463" for this suite. 03/09/23 16:58:51.332
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":268,"skipped":5041,"failed":0}
------------------------------
â€¢ [4.062 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:47.275
    Mar  9 16:58:47.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename security-context 03/09/23 16:58:47.276
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:47.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:47.29
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/09/23 16:58:47.293
    Mar  9 16:58:47.300: INFO: Waiting up to 5m0s for pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087" in namespace "security-context-1463" to be "Succeeded or Failed"
    Mar  9 16:58:47.303: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419751ms
    Mar  9 16:58:49.308: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007394519s
    Mar  9 16:58:51.306: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006317173s
    STEP: Saw pod success 03/09/23 16:58:51.306
    Mar  9 16:58:51.307: INFO: Pod "security-context-7732b67d-33ae-4a13-9297-abc5099d2087" satisfied condition "Succeeded or Failed"
    Mar  9 16:58:51.309: INFO: Trying to get logs from node tt-test-el8-003 pod security-context-7732b67d-33ae-4a13-9297-abc5099d2087 container test-container: <nil>
    STEP: delete the pod 03/09/23 16:58:51.314
    Mar  9 16:58:51.325: INFO: Waiting for pod security-context-7732b67d-33ae-4a13-9297-abc5099d2087 to disappear
    Mar  9 16:58:51.328: INFO: Pod security-context-7732b67d-33ae-4a13-9297-abc5099d2087 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  9 16:58:51.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-1463" for this suite. 03/09/23 16:58:51.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:51.337
Mar  9 16:58:51.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename csistoragecapacity 03/09/23 16:58:51.338
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:51.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:51.353
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/09/23 16:58:51.356
STEP: getting /apis/storage.k8s.io 03/09/23 16:58:51.358
STEP: getting /apis/storage.k8s.io/v1 03/09/23 16:58:51.359
STEP: creating 03/09/23 16:58:51.36
STEP: watching 03/09/23 16:58:51.373
Mar  9 16:58:51.373: INFO: starting watch
STEP: getting 03/09/23 16:58:51.378
STEP: listing in namespace 03/09/23 16:58:51.38
STEP: listing across namespaces 03/09/23 16:58:51.383
STEP: patching 03/09/23 16:58:51.385
STEP: updating 03/09/23 16:58:51.389
Mar  9 16:58:51.393: INFO: waiting for watch events with expected annotations in namespace
Mar  9 16:58:51.393: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/09/23 16:58:51.393
STEP: deleting a collection 03/09/23 16:58:51.402
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Mar  9 16:58:51.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-9077" for this suite. 03/09/23 16:58:51.415
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":269,"skipped":5055,"failed":0}
------------------------------
â€¢ [0.083 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:51.337
    Mar  9 16:58:51.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename csistoragecapacity 03/09/23 16:58:51.338
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:51.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:51.353
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/09/23 16:58:51.356
    STEP: getting /apis/storage.k8s.io 03/09/23 16:58:51.358
    STEP: getting /apis/storage.k8s.io/v1 03/09/23 16:58:51.359
    STEP: creating 03/09/23 16:58:51.36
    STEP: watching 03/09/23 16:58:51.373
    Mar  9 16:58:51.373: INFO: starting watch
    STEP: getting 03/09/23 16:58:51.378
    STEP: listing in namespace 03/09/23 16:58:51.38
    STEP: listing across namespaces 03/09/23 16:58:51.383
    STEP: patching 03/09/23 16:58:51.385
    STEP: updating 03/09/23 16:58:51.389
    Mar  9 16:58:51.393: INFO: waiting for watch events with expected annotations in namespace
    Mar  9 16:58:51.393: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/09/23 16:58:51.393
    STEP: deleting a collection 03/09/23 16:58:51.402
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Mar  9 16:58:51.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-9077" for this suite. 03/09/23 16:58:51.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:51.421
Mar  9 16:58:51.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename ephemeral-containers-test 03/09/23 16:58:51.422
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:51.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:51.435
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/09/23 16:58:51.438
Mar  9 16:58:51.445: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3417" to be "running and ready"
Mar  9 16:58:51.448: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373622ms
Mar  9 16:58:51.448: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  9 16:58:53.452: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006705722s
Mar  9 16:58:53.452: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar  9 16:58:53.452: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/09/23 16:58:53.455
Mar  9 16:58:53.467: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3417" to be "container debugger running"
Mar  9 16:58:53.470: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.721556ms
Mar  9 16:58:55.474: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007291418s
Mar  9 16:58:57.474: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006832949s
Mar  9 16:58:57.474: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/09/23 16:58:57.474
Mar  9 16:58:57.474: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3417 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 16:58:57.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 16:58:57.475: INFO: ExecWithOptions: Clientset creation
Mar  9 16:58:57.475: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-3417/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar  9 16:58:57.554: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  9 16:58:57.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-3417" for this suite. 03/09/23 16:58:57.563
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":270,"skipped":5062,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.147 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:51.421
    Mar  9 16:58:51.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/09/23 16:58:51.422
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:51.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:51.435
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/09/23 16:58:51.438
    Mar  9 16:58:51.445: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3417" to be "running and ready"
    Mar  9 16:58:51.448: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373622ms
    Mar  9 16:58:51.448: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 16:58:53.452: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006705722s
    Mar  9 16:58:53.452: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar  9 16:58:53.452: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/09/23 16:58:53.455
    Mar  9 16:58:53.467: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3417" to be "container debugger running"
    Mar  9 16:58:53.470: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.721556ms
    Mar  9 16:58:55.474: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007291418s
    Mar  9 16:58:57.474: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006832949s
    Mar  9 16:58:57.474: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/09/23 16:58:57.474
    Mar  9 16:58:57.474: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3417 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 16:58:57.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 16:58:57.475: INFO: ExecWithOptions: Clientset creation
    Mar  9 16:58:57.475: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-3417/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar  9 16:58:57.554: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  9 16:58:57.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-3417" for this suite. 03/09/23 16:58:57.563
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:58:57.568
Mar  9 16:58:57.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 16:58:57.569
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:57.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:57.581
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-11bb1458-4a4e-459b-a97f-40c4fcac9403 03/09/23 16:58:57.584
STEP: Creating a pod to test consume secrets 03/09/23 16:58:57.588
Mar  9 16:58:57.596: INFO: Waiting up to 5m0s for pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7" in namespace "secrets-8573" to be "Succeeded or Failed"
Mar  9 16:58:57.599: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.649935ms
Mar  9 16:58:59.604: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008273607s
Mar  9 16:59:01.604: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008007036s
STEP: Saw pod success 03/09/23 16:59:01.604
Mar  9 16:59:01.604: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7" satisfied condition "Succeeded or Failed"
Mar  9 16:59:01.607: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7 container secret-env-test: <nil>
STEP: delete the pod 03/09/23 16:59:01.612
Mar  9 16:59:01.622: INFO: Waiting for pod pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7 to disappear
Mar  9 16:59:01.625: INFO: Pod pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  9 16:59:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8573" for this suite. 03/09/23 16:59:01.628
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":271,"skipped":5062,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:58:57.568
    Mar  9 16:58:57.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 16:58:57.569
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:58:57.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:58:57.581
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-11bb1458-4a4e-459b-a97f-40c4fcac9403 03/09/23 16:58:57.584
    STEP: Creating a pod to test consume secrets 03/09/23 16:58:57.588
    Mar  9 16:58:57.596: INFO: Waiting up to 5m0s for pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7" in namespace "secrets-8573" to be "Succeeded or Failed"
    Mar  9 16:58:57.599: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.649935ms
    Mar  9 16:58:59.604: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008273607s
    Mar  9 16:59:01.604: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008007036s
    STEP: Saw pod success 03/09/23 16:59:01.604
    Mar  9 16:59:01.604: INFO: Pod "pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7" satisfied condition "Succeeded or Failed"
    Mar  9 16:59:01.607: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7 container secret-env-test: <nil>
    STEP: delete the pod 03/09/23 16:59:01.612
    Mar  9 16:59:01.622: INFO: Waiting for pod pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7 to disappear
    Mar  9 16:59:01.625: INFO: Pod pod-secrets-20a1cda9-680c-4a9e-a40a-894650d960d7 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 16:59:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8573" for this suite. 03/09/23 16:59:01.628
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 16:59:01.635
Mar  9 16:59:01.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 16:59:01.637
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:59:01.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:59:01.649
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4 in namespace container-probe-5783 03/09/23 16:59:01.653
Mar  9 16:59:01.660: INFO: Waiting up to 5m0s for pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4" in namespace "container-probe-5783" to be "not pending"
Mar  9 16:59:01.663: INFO: Pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814107ms
Mar  9 16:59:03.668: INFO: Pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00717007s
Mar  9 16:59:03.668: INFO: Pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4" satisfied condition "not pending"
Mar  9 16:59:03.668: INFO: Started pod liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4 in namespace container-probe-5783
STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:59:03.668
Mar  9 16:59:03.671: INFO: Initial restart count of pod liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4 is 0
STEP: deleting the pod 03/09/23 17:03:04.154
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 17:03:04.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5783" for this suite. 03/09/23 17:03:04.171
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":272,"skipped":5062,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.541 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 16:59:01.635
    Mar  9 16:59:01.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 16:59:01.637
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 16:59:01.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 16:59:01.649
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4 in namespace container-probe-5783 03/09/23 16:59:01.653
    Mar  9 16:59:01.660: INFO: Waiting up to 5m0s for pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4" in namespace "container-probe-5783" to be "not pending"
    Mar  9 16:59:01.663: INFO: Pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814107ms
    Mar  9 16:59:03.668: INFO: Pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00717007s
    Mar  9 16:59:03.668: INFO: Pod "liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4" satisfied condition "not pending"
    Mar  9 16:59:03.668: INFO: Started pod liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4 in namespace container-probe-5783
    STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 16:59:03.668
    Mar  9 16:59:03.671: INFO: Initial restart count of pod liveness-4a08ad33-d83a-4c7b-be1c-04ef459c71b4 is 0
    STEP: deleting the pod 03/09/23 17:03:04.154
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 17:03:04.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5783" for this suite. 03/09/23 17:03:04.171
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:03:04.178
Mar  9 17:03:04.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pods 03/09/23 17:03:04.179
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:04.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:04.192
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 03/09/23 17:03:04.195
Mar  9 17:03:04.203: INFO: Waiting up to 5m0s for pod "pod-p99kl" in namespace "pods-476" to be "running"
Mar  9 17:03:04.205: INFO: Pod "pod-p99kl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464138ms
Mar  9 17:03:06.210: INFO: Pod "pod-p99kl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007313149s
Mar  9 17:03:06.210: INFO: Pod "pod-p99kl" satisfied condition "running"
STEP: patching /status 03/09/23 17:03:06.21
Mar  9 17:03:06.219: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  9 17:03:06.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-476" for this suite. 03/09/23 17:03:06.223
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":273,"skipped":5066,"failed":0}
------------------------------
â€¢ [2.049 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:03:04.178
    Mar  9 17:03:04.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pods 03/09/23 17:03:04.179
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:04.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:04.192
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 03/09/23 17:03:04.195
    Mar  9 17:03:04.203: INFO: Waiting up to 5m0s for pod "pod-p99kl" in namespace "pods-476" to be "running"
    Mar  9 17:03:04.205: INFO: Pod "pod-p99kl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464138ms
    Mar  9 17:03:06.210: INFO: Pod "pod-p99kl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007313149s
    Mar  9 17:03:06.210: INFO: Pod "pod-p99kl" satisfied condition "running"
    STEP: patching /status 03/09/23 17:03:06.21
    Mar  9 17:03:06.219: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  9 17:03:06.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-476" for this suite. 03/09/23 17:03:06.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:03:06.228
Mar  9 17:03:06.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename conformance-tests 03/09/23 17:03:06.23
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:06.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:06.245
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/09/23 17:03:06.248
Mar  9 17:03:06.248: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Mar  9 17:03:06.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-486" for this suite. 03/09/23 17:03:06.257
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":274,"skipped":5078,"failed":0}
------------------------------
â€¢ [0.034 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:03:06.228
    Mar  9 17:03:06.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename conformance-tests 03/09/23 17:03:06.23
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:06.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:06.245
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/09/23 17:03:06.248
    Mar  9 17:03:06.248: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Mar  9 17:03:06.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-486" for this suite. 03/09/23 17:03:06.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:03:06.263
Mar  9 17:03:06.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 17:03:06.264
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:06.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:06.278
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-ac51e823-0821-4964-ba0a-928a8294381d 03/09/23 17:03:06.28
STEP: Creating a pod to test consume configMaps 03/09/23 17:03:06.284
Mar  9 17:03:06.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1" in namespace "configmap-2326" to be "Succeeded or Failed"
Mar  9 17:03:06.294: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.360433ms
Mar  9 17:03:08.298: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007967781s
Mar  9 17:03:10.298: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007992486s
STEP: Saw pod success 03/09/23 17:03:10.298
Mar  9 17:03:10.298: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1" satisfied condition "Succeeded or Failed"
Mar  9 17:03:10.301: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 17:03:10.313
Mar  9 17:03:10.324: INFO: Waiting for pod pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1 to disappear
Mar  9 17:03:10.327: INFO: Pod pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 17:03:10.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2326" for this suite. 03/09/23 17:03:10.331
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":275,"skipped":5098,"failed":0}
------------------------------
â€¢ [4.072 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:03:06.263
    Mar  9 17:03:06.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 17:03:06.264
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:06.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:06.278
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-ac51e823-0821-4964-ba0a-928a8294381d 03/09/23 17:03:06.28
    STEP: Creating a pod to test consume configMaps 03/09/23 17:03:06.284
    Mar  9 17:03:06.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1" in namespace "configmap-2326" to be "Succeeded or Failed"
    Mar  9 17:03:06.294: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.360433ms
    Mar  9 17:03:08.298: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007967781s
    Mar  9 17:03:10.298: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007992486s
    STEP: Saw pod success 03/09/23 17:03:10.298
    Mar  9 17:03:10.298: INFO: Pod "pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1" satisfied condition "Succeeded or Failed"
    Mar  9 17:03:10.301: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 17:03:10.313
    Mar  9 17:03:10.324: INFO: Waiting for pod pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1 to disappear
    Mar  9 17:03:10.327: INFO: Pod pod-configmaps-3d4bc515-4b7b-4a9e-8d0f-34fba45366c1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 17:03:10.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2326" for this suite. 03/09/23 17:03:10.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:03:10.337
Mar  9 17:03:10.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename cronjob 03/09/23 17:03:10.338
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:10.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:10.353
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/09/23 17:03:10.356
STEP: Ensuring a job is scheduled 03/09/23 17:03:10.361
STEP: Ensuring exactly one is scheduled 03/09/23 17:04:00.366
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/09/23 17:04:00.369
STEP: Ensuring the job is replaced with a new one 03/09/23 17:04:00.371
STEP: Removing cronjob 03/09/23 17:05:00.377
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  9 17:05:00.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6855" for this suite. 03/09/23 17:05:00.387
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":276,"skipped":5114,"failed":0}
------------------------------
â€¢ [SLOW TEST] [110.057 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:03:10.337
    Mar  9 17:03:10.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename cronjob 03/09/23 17:03:10.338
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:03:10.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:03:10.353
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/09/23 17:03:10.356
    STEP: Ensuring a job is scheduled 03/09/23 17:03:10.361
    STEP: Ensuring exactly one is scheduled 03/09/23 17:04:00.366
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/09/23 17:04:00.369
    STEP: Ensuring the job is replaced with a new one 03/09/23 17:04:00.371
    STEP: Removing cronjob 03/09/23 17:05:00.377
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  9 17:05:00.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6855" for this suite. 03/09/23 17:05:00.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:00.394
Mar  9 17:05:00.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename disruption 03/09/23 17:05:00.396
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:00.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:00.424
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 03/09/23 17:05:00.433
STEP: Waiting for all pods to be running 03/09/23 17:05:02.463
Mar  9 17:05:02.469: INFO: running pods: 0 < 3
Mar  9 17:05:04.475: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  9 17:05:06.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8608" for this suite. 03/09/23 17:05:06.481
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":277,"skipped":5126,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.093 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:00.394
    Mar  9 17:05:00.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename disruption 03/09/23 17:05:00.396
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:00.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:00.424
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 03/09/23 17:05:00.433
    STEP: Waiting for all pods to be running 03/09/23 17:05:02.463
    Mar  9 17:05:02.469: INFO: running pods: 0 < 3
    Mar  9 17:05:04.475: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  9 17:05:06.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8608" for this suite. 03/09/23 17:05:06.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:06.488
Mar  9 17:05:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 17:05:06.489
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:06.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:06.503
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Mar  9 17:05:06.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/09/23 17:05:09.441
Mar  9 17:05:09.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 create -f -'
Mar  9 17:05:10.360: INFO: stderr: ""
Mar  9 17:05:10.360: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  9 17:05:10.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 delete e2e-test-crd-publish-openapi-2130-crds test-cr'
Mar  9 17:05:10.432: INFO: stderr: ""
Mar  9 17:05:10.432: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  9 17:05:10.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 apply -f -'
Mar  9 17:05:10.710: INFO: stderr: ""
Mar  9 17:05:10.710: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  9 17:05:10.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 delete e2e-test-crd-publish-openapi-2130-crds test-cr'
Mar  9 17:05:10.784: INFO: stderr: ""
Mar  9 17:05:10.784: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/09/23 17:05:10.784
Mar  9 17:05:10.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 explain e2e-test-crd-publish-openapi-2130-crds'
Mar  9 17:05:11.061: INFO: stderr: ""
Mar  9 17:05:11.061: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2130-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:05:13.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4408" for this suite. 03/09/23 17:05:13.971
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":278,"skipped":5131,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.488 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:06.488
    Mar  9 17:05:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 17:05:06.489
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:06.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:06.503
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Mar  9 17:05:06.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/09/23 17:05:09.441
    Mar  9 17:05:09.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 create -f -'
    Mar  9 17:05:10.360: INFO: stderr: ""
    Mar  9 17:05:10.360: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  9 17:05:10.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 delete e2e-test-crd-publish-openapi-2130-crds test-cr'
    Mar  9 17:05:10.432: INFO: stderr: ""
    Mar  9 17:05:10.432: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar  9 17:05:10.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 apply -f -'
    Mar  9 17:05:10.710: INFO: stderr: ""
    Mar  9 17:05:10.710: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  9 17:05:10.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 --namespace=crd-publish-openapi-4408 delete e2e-test-crd-publish-openapi-2130-crds test-cr'
    Mar  9 17:05:10.784: INFO: stderr: ""
    Mar  9 17:05:10.784: INFO: stdout: "e2e-test-crd-publish-openapi-2130-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/09/23 17:05:10.784
    Mar  9 17:05:10.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=crd-publish-openapi-4408 explain e2e-test-crd-publish-openapi-2130-crds'
    Mar  9 17:05:11.061: INFO: stderr: ""
    Mar  9 17:05:11.061: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2130-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:05:13.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4408" for this suite. 03/09/23 17:05:13.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:13.977
Mar  9 17:05:13.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename endpointslicemirroring 03/09/23 17:05:13.978
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:13.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:13.994
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/09/23 17:05:14.011
Mar  9 17:05:14.018: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/09/23 17:05:16.023
Mar  9 17:05:16.030: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 03/09/23 17:05:18.034
Mar  9 17:05:18.041: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Mar  9 17:05:20.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1057" for this suite. 03/09/23 17:05:20.049
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":279,"skipped":5142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.077 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:13.977
    Mar  9 17:05:13.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename endpointslicemirroring 03/09/23 17:05:13.978
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:13.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:13.994
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/09/23 17:05:14.011
    Mar  9 17:05:14.018: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/09/23 17:05:16.023
    Mar  9 17:05:16.030: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 03/09/23 17:05:18.034
    Mar  9 17:05:18.041: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Mar  9 17:05:20.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-1057" for this suite. 03/09/23 17:05:20.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:20.055
Mar  9 17:05:20.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename podtemplate 03/09/23 17:05:20.056
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:20.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:20.07
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  9 17:05:20.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5778" for this suite. 03/09/23 17:05:20.099
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":280,"skipped":5166,"failed":0}
------------------------------
â€¢ [0.049 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:20.055
    Mar  9 17:05:20.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename podtemplate 03/09/23 17:05:20.056
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:20.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:20.07
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  9 17:05:20.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5778" for this suite. 03/09/23 17:05:20.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:20.105
Mar  9 17:05:20.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 17:05:20.106
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:20.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:20.12
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 17:05:20.133
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:05:20.77
STEP: Deploying the webhook pod 03/09/23 17:05:20.777
STEP: Wait for the deployment to be ready 03/09/23 17:05:20.788
Mar  9 17:05:20.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 17:05:22.804
STEP: Verifying the service has paired with the endpoint 03/09/23 17:05:22.82
Mar  9 17:05:23.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Mar  9 17:05:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9910-crds.webhook.example.com via the AdmissionRegistration API 03/09/23 17:05:24.336
STEP: Creating a custom resource while v1 is storage version 03/09/23 17:05:24.352
STEP: Patching Custom Resource Definition to set v2 as storage 03/09/23 17:05:26.423
STEP: Patching the custom resource while v2 is storage version 03/09/23 17:05:26.446
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:05:27.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8428" for this suite. 03/09/23 17:05:27.036
STEP: Destroying namespace "webhook-8428-markers" for this suite. 03/09/23 17:05:27.041
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":281,"skipped":5178,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.003 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:20.105
    Mar  9 17:05:20.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 17:05:20.106
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:20.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:20.12
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 17:05:20.133
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:05:20.77
    STEP: Deploying the webhook pod 03/09/23 17:05:20.777
    STEP: Wait for the deployment to be ready 03/09/23 17:05:20.788
    Mar  9 17:05:20.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 17:05:22.804
    STEP: Verifying the service has paired with the endpoint 03/09/23 17:05:22.82
    Mar  9 17:05:23.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Mar  9 17:05:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9910-crds.webhook.example.com via the AdmissionRegistration API 03/09/23 17:05:24.336
    STEP: Creating a custom resource while v1 is storage version 03/09/23 17:05:24.352
    STEP: Patching Custom Resource Definition to set v2 as storage 03/09/23 17:05:26.423
    STEP: Patching the custom resource while v2 is storage version 03/09/23 17:05:26.446
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:05:27.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8428" for this suite. 03/09/23 17:05:27.036
    STEP: Destroying namespace "webhook-8428-markers" for this suite. 03/09/23 17:05:27.041
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:27.108
Mar  9 17:05:27.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 17:05:27.11
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:27.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:27.136
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-3523/configmap-test-d2e016ce-2690-43f7-836e-8ed6bc653570 03/09/23 17:05:27.14
STEP: Creating a pod to test consume configMaps 03/09/23 17:05:27.144
Mar  9 17:05:27.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44" in namespace "configmap-3523" to be "Succeeded or Failed"
Mar  9 17:05:27.154: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166342ms
Mar  9 17:05:29.157: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007120638s
Mar  9 17:05:31.157: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007063767s
STEP: Saw pod success 03/09/23 17:05:31.157
Mar  9 17:05:31.158: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44" satisfied condition "Succeeded or Failed"
Mar  9 17:05:31.160: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44 container env-test: <nil>
STEP: delete the pod 03/09/23 17:05:31.176
Mar  9 17:05:31.187: INFO: Waiting for pod pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44 to disappear
Mar  9 17:05:31.189: INFO: Pod pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 17:05:31.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3523" for this suite. 03/09/23 17:05:31.193
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":282,"skipped":5184,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:27.108
    Mar  9 17:05:27.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 17:05:27.11
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:27.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:27.136
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-3523/configmap-test-d2e016ce-2690-43f7-836e-8ed6bc653570 03/09/23 17:05:27.14
    STEP: Creating a pod to test consume configMaps 03/09/23 17:05:27.144
    Mar  9 17:05:27.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44" in namespace "configmap-3523" to be "Succeeded or Failed"
    Mar  9 17:05:27.154: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166342ms
    Mar  9 17:05:29.157: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007120638s
    Mar  9 17:05:31.157: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007063767s
    STEP: Saw pod success 03/09/23 17:05:31.157
    Mar  9 17:05:31.158: INFO: Pod "pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44" satisfied condition "Succeeded or Failed"
    Mar  9 17:05:31.160: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44 container env-test: <nil>
    STEP: delete the pod 03/09/23 17:05:31.176
    Mar  9 17:05:31.187: INFO: Waiting for pod pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44 to disappear
    Mar  9 17:05:31.189: INFO: Pod pod-configmaps-41e840d9-d111-494a-84b5-4399507c6f44 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 17:05:31.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3523" for this suite. 03/09/23 17:05:31.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:31.199
Mar  9 17:05:31.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 17:05:31.2
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:31.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:31.213
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-7d75cde3-b9f2-4674-a51b-acc2c596db63 03/09/23 17:05:31.216
STEP: Creating a pod to test consume configMaps 03/09/23 17:05:31.22
Mar  9 17:05:31.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253" in namespace "configmap-2293" to be "Succeeded or Failed"
Mar  9 17:05:31.229: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253": Phase="Pending", Reason="", readiness=false. Elapsed: 2.435229ms
Mar  9 17:05:33.233: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006866866s
Mar  9 17:05:35.234: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007154856s
STEP: Saw pod success 03/09/23 17:05:35.234
Mar  9 17:05:35.234: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253" satisfied condition "Succeeded or Failed"
Mar  9 17:05:35.236: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 17:05:35.241
Mar  9 17:05:35.252: INFO: Waiting for pod pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253 to disappear
Mar  9 17:05:35.254: INFO: Pod pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 17:05:35.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2293" for this suite. 03/09/23 17:05:35.258
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":283,"skipped":5194,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:31.199
    Mar  9 17:05:31.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 17:05:31.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:31.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:31.213
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-7d75cde3-b9f2-4674-a51b-acc2c596db63 03/09/23 17:05:31.216
    STEP: Creating a pod to test consume configMaps 03/09/23 17:05:31.22
    Mar  9 17:05:31.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253" in namespace "configmap-2293" to be "Succeeded or Failed"
    Mar  9 17:05:31.229: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253": Phase="Pending", Reason="", readiness=false. Elapsed: 2.435229ms
    Mar  9 17:05:33.233: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006866866s
    Mar  9 17:05:35.234: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007154856s
    STEP: Saw pod success 03/09/23 17:05:35.234
    Mar  9 17:05:35.234: INFO: Pod "pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253" satisfied condition "Succeeded or Failed"
    Mar  9 17:05:35.236: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 17:05:35.241
    Mar  9 17:05:35.252: INFO: Waiting for pod pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253 to disappear
    Mar  9 17:05:35.254: INFO: Pod pod-configmaps-eb29642a-4973-4272-985e-ec2e2568e253 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 17:05:35.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2293" for this suite. 03/09/23 17:05:35.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:35.264
Mar  9 17:05:35.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename var-expansion 03/09/23 17:05:35.265
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:35.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:35.278
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Mar  9 17:05:35.289: INFO: Waiting up to 2m0s for pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" in namespace "var-expansion-3805" to be "container 0 failed with reason CreateContainerConfigError"
Mar  9 17:05:35.292: INFO: Pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604272ms
Mar  9 17:05:37.296: INFO: Pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007070098s
Mar  9 17:05:37.296: INFO: Pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  9 17:05:37.296: INFO: Deleting pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" in namespace "var-expansion-3805"
Mar  9 17:05:37.301: INFO: Wait up to 5m0s for pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  9 17:05:41.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3805" for this suite. 03/09/23 17:05:41.311
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":284,"skipped":5218,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.052 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:35.264
    Mar  9 17:05:35.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename var-expansion 03/09/23 17:05:35.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:35.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:35.278
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Mar  9 17:05:35.289: INFO: Waiting up to 2m0s for pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" in namespace "var-expansion-3805" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  9 17:05:35.292: INFO: Pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604272ms
    Mar  9 17:05:37.296: INFO: Pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007070098s
    Mar  9 17:05:37.296: INFO: Pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  9 17:05:37.296: INFO: Deleting pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" in namespace "var-expansion-3805"
    Mar  9 17:05:37.301: INFO: Wait up to 5m0s for pod "var-expansion-516d219f-c070-478c-a789-76cb5e0d509d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  9 17:05:41.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3805" for this suite. 03/09/23 17:05:41.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:41.319
Mar  9 17:05:41.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 17:05:41.32
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:41.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:41.334
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 17:05:41.351
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:05:41.522
STEP: Deploying the webhook pod 03/09/23 17:05:41.527
STEP: Wait for the deployment to be ready 03/09/23 17:05:41.538
Mar  9 17:05:41.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 17:05:43.553
STEP: Verifying the service has paired with the endpoint 03/09/23 17:05:43.567
Mar  9 17:05:44.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 03/09/23 17:05:44.57
STEP: create a pod 03/09/23 17:05:44.585
Mar  9 17:05:44.590: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6989" to be "running"
Mar  9 17:05:44.592: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583403ms
Mar  9 17:05:46.596: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006273802s
Mar  9 17:05:46.596: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/09/23 17:05:46.596
Mar  9 17:05:46.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=webhook-6989 attach --namespace=webhook-6989 to-be-attached-pod -i -c=container1'
Mar  9 17:05:46.680: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:05:46.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6989" for this suite. 03/09/23 17:05:46.689
STEP: Destroying namespace "webhook-6989-markers" for this suite. 03/09/23 17:05:46.693
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":285,"skipped":5262,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.413 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:41.319
    Mar  9 17:05:41.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 17:05:41.32
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:41.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:41.334
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 17:05:41.351
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:05:41.522
    STEP: Deploying the webhook pod 03/09/23 17:05:41.527
    STEP: Wait for the deployment to be ready 03/09/23 17:05:41.538
    Mar  9 17:05:41.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 17:05:43.553
    STEP: Verifying the service has paired with the endpoint 03/09/23 17:05:43.567
    Mar  9 17:05:44.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 03/09/23 17:05:44.57
    STEP: create a pod 03/09/23 17:05:44.585
    Mar  9 17:05:44.590: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6989" to be "running"
    Mar  9 17:05:44.592: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583403ms
    Mar  9 17:05:46.596: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006273802s
    Mar  9 17:05:46.596: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/09/23 17:05:46.596
    Mar  9 17:05:46.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=webhook-6989 attach --namespace=webhook-6989 to-be-attached-pod -i -c=container1'
    Mar  9 17:05:46.680: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:05:46.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6989" for this suite. 03/09/23 17:05:46.689
    STEP: Destroying namespace "webhook-6989-markers" for this suite. 03/09/23 17:05:46.693
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:46.749
Mar  9 17:05:46.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename security-context-test 03/09/23 17:05:46.756
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:46.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:46.78
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Mar  9 17:05:46.792: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c" in namespace "security-context-test-2004" to be "Succeeded or Failed"
Mar  9 17:05:46.794: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584617ms
Mar  9 17:05:48.798: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006425755s
Mar  9 17:05:50.798: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006415269s
Mar  9 17:05:50.798: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  9 17:05:50.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2004" for this suite. 03/09/23 17:05:50.802
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":286,"skipped":5428,"failed":0}
------------------------------
â€¢ [4.058 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:46.749
    Mar  9 17:05:46.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename security-context-test 03/09/23 17:05:46.756
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:46.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:46.78
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Mar  9 17:05:46.792: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c" in namespace "security-context-test-2004" to be "Succeeded or Failed"
    Mar  9 17:05:46.794: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584617ms
    Mar  9 17:05:48.798: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006425755s
    Mar  9 17:05:50.798: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006415269s
    Mar  9 17:05:50.798: INFO: Pod "busybox-readonly-false-d3861234-ad1b-4b86-83b2-1ec3bb71224c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  9 17:05:50.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2004" for this suite. 03/09/23 17:05:50.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:05:50.809
Mar  9 17:05:50.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 17:05:50.81
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:50.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:50.827
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-1001 03/09/23 17:05:50.83
STEP: creating service affinity-clusterip-transition in namespace services-1001 03/09/23 17:05:50.83
STEP: creating replication controller affinity-clusterip-transition in namespace services-1001 03/09/23 17:05:50.845
I0309 17:05:50.854167      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1001, replica count: 3
I0309 17:05:53.905121      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 17:05:53.910: INFO: Creating new exec pod
Mar  9 17:05:53.915: INFO: Waiting up to 5m0s for pod "execpod-affinityns7mp" in namespace "services-1001" to be "running"
Mar  9 17:05:53.918: INFO: Pod "execpod-affinityns7mp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.686331ms
Mar  9 17:05:55.922: INFO: Pod "execpod-affinityns7mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.006468559s
Mar  9 17:05:55.922: INFO: Pod "execpod-affinityns7mp" satisfied condition "running"
Mar  9 17:05:56.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar  9 17:05:57.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  9 17:05:57.116: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:05:57.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.114.46 80'
Mar  9 17:05:57.277: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.114.46 80\nConnection to 10.107.114.46 80 port [tcp/http] succeeded!\n"
Mar  9 17:05:57.277: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:05:57.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
Mar  9 17:05:57.608: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
Mar  9 17:05:57.608: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56"
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
Mar  9 17:06:27.843: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
Mar  9 17:06:27.843: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56"
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:27.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
Mar  9 17:06:28.133: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
Mar  9 17:06:28.133: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-g4t56"
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
Mar  9 17:06:58.361: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
Mar  9 17:06:58.361: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56"
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
Mar  9 17:06:58.361: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1001, will wait for the garbage collector to delete the pods 03/09/23 17:06:58.373
Mar  9 17:06:58.435: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.443378ms
Mar  9 17:06:58.536: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.94179ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 17:07:00.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1001" for this suite. 03/09/23 17:07:00.964
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":287,"skipped":5450,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.162 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:05:50.809
    Mar  9 17:05:50.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 17:05:50.81
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:05:50.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:05:50.827
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-1001 03/09/23 17:05:50.83
    STEP: creating service affinity-clusterip-transition in namespace services-1001 03/09/23 17:05:50.83
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1001 03/09/23 17:05:50.845
    I0309 17:05:50.854167      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1001, replica count: 3
    I0309 17:05:53.905121      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 17:05:53.910: INFO: Creating new exec pod
    Mar  9 17:05:53.915: INFO: Waiting up to 5m0s for pod "execpod-affinityns7mp" in namespace "services-1001" to be "running"
    Mar  9 17:05:53.918: INFO: Pod "execpod-affinityns7mp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.686331ms
    Mar  9 17:05:55.922: INFO: Pod "execpod-affinityns7mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.006468559s
    Mar  9 17:05:55.922: INFO: Pod "execpod-affinityns7mp" satisfied condition "running"
    Mar  9 17:05:56.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Mar  9 17:05:57.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar  9 17:05:57.116: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:05:57.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.114.46 80'
    Mar  9 17:05:57.277: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.114.46 80\nConnection to 10.107.114.46 80 port [tcp/http] succeeded!\n"
    Mar  9 17:05:57.277: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:05:57.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
    Mar  9 17:05:57.608: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
    Mar  9 17:05:57.608: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56"
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:05:57.608: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
    Mar  9 17:06:27.843: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
    Mar  9 17:06:27.843: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56"
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.843: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:27.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
    Mar  9 17:06:28.133: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
    Mar  9 17:06:28.133: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-64npc\naffinity-clusterip-transition-s9hwl\naffinity-clusterip-transition-g4t56"
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-64npc
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-s9hwl
    Mar  9 17:06:28.133: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-1001 exec execpod-affinityns7mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.114.46:80/ ; done'
    Mar  9 17:06:58.361: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.114.46:80/\n"
    Mar  9 17:06:58.361: INFO: stdout: "\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56\naffinity-clusterip-transition-g4t56"
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Received response from host: affinity-clusterip-transition-g4t56
    Mar  9 17:06:58.361: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1001, will wait for the garbage collector to delete the pods 03/09/23 17:06:58.373
    Mar  9 17:06:58.435: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.443378ms
    Mar  9 17:06:58.536: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.94179ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 17:07:00.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1001" for this suite. 03/09/23 17:07:00.964
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:00.972
Mar  9 17:07:00.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 17:07:00.973
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:00.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:00.99
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 03/09/23 17:07:00.994
Mar  9 17:07:01.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba" in namespace "projected-150" to be "Succeeded or Failed"
Mar  9 17:07:01.006: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.332296ms
Mar  9 17:07:03.009: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006367505s
Mar  9 17:07:05.011: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008568665s
STEP: Saw pod success 03/09/23 17:07:05.011
Mar  9 17:07:05.012: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba" satisfied condition "Succeeded or Failed"
Mar  9 17:07:05.014: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba container client-container: <nil>
STEP: delete the pod 03/09/23 17:07:05.02
Mar  9 17:07:05.029: INFO: Waiting for pod downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba to disappear
Mar  9 17:07:05.033: INFO: Pod downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 17:07:05.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-150" for this suite. 03/09/23 17:07:05.037
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":288,"skipped":5450,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:00.972
    Mar  9 17:07:00.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 17:07:00.973
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:00.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:00.99
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 03/09/23 17:07:00.994
    Mar  9 17:07:01.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba" in namespace "projected-150" to be "Succeeded or Failed"
    Mar  9 17:07:01.006: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.332296ms
    Mar  9 17:07:03.009: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006367505s
    Mar  9 17:07:05.011: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008568665s
    STEP: Saw pod success 03/09/23 17:07:05.011
    Mar  9 17:07:05.012: INFO: Pod "downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba" satisfied condition "Succeeded or Failed"
    Mar  9 17:07:05.014: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba container client-container: <nil>
    STEP: delete the pod 03/09/23 17:07:05.02
    Mar  9 17:07:05.029: INFO: Waiting for pod downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba to disappear
    Mar  9 17:07:05.033: INFO: Pod downwardapi-volume-6778108e-29be-4c75-bdfa-13218a9a4eba no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 17:07:05.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-150" for this suite. 03/09/23 17:07:05.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:05.042
Mar  9 17:07:05.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename podtemplate 03/09/23 17:07:05.044
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:05.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:05.057
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/09/23 17:07:05.06
STEP: Replace a pod template 03/09/23 17:07:05.064
Mar  9 17:07:05.071: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  9 17:07:05.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5024" for this suite. 03/09/23 17:07:05.075
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":289,"skipped":5463,"failed":0}
------------------------------
â€¢ [0.038 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:05.042
    Mar  9 17:07:05.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename podtemplate 03/09/23 17:07:05.044
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:05.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:05.057
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/09/23 17:07:05.06
    STEP: Replace a pod template 03/09/23 17:07:05.064
    Mar  9 17:07:05.071: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  9 17:07:05.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5024" for this suite. 03/09/23 17:07:05.075
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:05.081
Mar  9 17:07:05.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 17:07:05.082
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:05.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:05.097
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3286 03/09/23 17:07:05.099
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-3286 03/09/23 17:07:05.105
Mar  9 17:07:05.113: INFO: Found 0 stateful pods, waiting for 1
Mar  9 17:07:15.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/09/23 17:07:15.122
STEP: updating a scale subresource 03/09/23 17:07:15.124
STEP: verifying the statefulset Spec.Replicas was modified 03/09/23 17:07:15.13
STEP: Patch a scale subresource 03/09/23 17:07:15.132
STEP: verifying the statefulset Spec.Replicas was modified 03/09/23 17:07:15.143
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 17:07:15.147: INFO: Deleting all statefulset in ns statefulset-3286
Mar  9 17:07:15.149: INFO: Scaling statefulset ss to 0
Mar  9 17:07:25.167: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 17:07:25.169: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 17:07:25.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3286" for this suite. 03/09/23 17:07:25.186
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":290,"skipped":5465,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.110 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:05.081
    Mar  9 17:07:05.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 17:07:05.082
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:05.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:05.097
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3286 03/09/23 17:07:05.099
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-3286 03/09/23 17:07:05.105
    Mar  9 17:07:05.113: INFO: Found 0 stateful pods, waiting for 1
    Mar  9 17:07:15.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/09/23 17:07:15.122
    STEP: updating a scale subresource 03/09/23 17:07:15.124
    STEP: verifying the statefulset Spec.Replicas was modified 03/09/23 17:07:15.13
    STEP: Patch a scale subresource 03/09/23 17:07:15.132
    STEP: verifying the statefulset Spec.Replicas was modified 03/09/23 17:07:15.143
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 17:07:15.147: INFO: Deleting all statefulset in ns statefulset-3286
    Mar  9 17:07:15.149: INFO: Scaling statefulset ss to 0
    Mar  9 17:07:25.167: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 17:07:25.169: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 17:07:25.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3286" for this suite. 03/09/23 17:07:25.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:25.194
Mar  9 17:07:25.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 17:07:25.195
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:25.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:25.21
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3935 03/09/23 17:07:25.213
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 03/09/23 17:07:25.219
STEP: Creating pod with conflicting port in namespace statefulset-3935 03/09/23 17:07:25.224
STEP: Waiting until pod test-pod will start running in namespace statefulset-3935 03/09/23 17:07:25.232
Mar  9 17:07:25.232: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3935" to be "running"
Mar  9 17:07:25.234: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533273ms
Mar  9 17:07:27.238: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006803229s
Mar  9 17:07:27.239: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3935 03/09/23 17:07:27.239
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3935 03/09/23 17:07:27.243
Mar  9 17:07:27.254: INFO: Observed stateful pod in namespace: statefulset-3935, name: ss-0, uid: 60b3ff1e-017e-48e3-8fc5-f4a87f32ce88, status phase: Pending. Waiting for statefulset controller to delete.
Mar  9 17:07:27.266: INFO: Observed stateful pod in namespace: statefulset-3935, name: ss-0, uid: 60b3ff1e-017e-48e3-8fc5-f4a87f32ce88, status phase: Failed. Waiting for statefulset controller to delete.
Mar  9 17:07:27.275: INFO: Observed stateful pod in namespace: statefulset-3935, name: ss-0, uid: 60b3ff1e-017e-48e3-8fc5-f4a87f32ce88, status phase: Failed. Waiting for statefulset controller to delete.
Mar  9 17:07:27.277: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3935
STEP: Removing pod with conflicting port in namespace statefulset-3935 03/09/23 17:07:27.277
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3935 and will be in running state 03/09/23 17:07:27.289
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 17:07:31.303: INFO: Deleting all statefulset in ns statefulset-3935
Mar  9 17:07:31.306: INFO: Scaling statefulset ss to 0
Mar  9 17:07:41.322: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 17:07:41.325: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 17:07:41.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3935" for this suite. 03/09/23 17:07:41.338
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":291,"skipped":5502,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.150 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:25.194
    Mar  9 17:07:25.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 17:07:25.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:25.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:25.21
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3935 03/09/23 17:07:25.213
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 03/09/23 17:07:25.219
    STEP: Creating pod with conflicting port in namespace statefulset-3935 03/09/23 17:07:25.224
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3935 03/09/23 17:07:25.232
    Mar  9 17:07:25.232: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3935" to be "running"
    Mar  9 17:07:25.234: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533273ms
    Mar  9 17:07:27.238: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006803229s
    Mar  9 17:07:27.239: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3935 03/09/23 17:07:27.239
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3935 03/09/23 17:07:27.243
    Mar  9 17:07:27.254: INFO: Observed stateful pod in namespace: statefulset-3935, name: ss-0, uid: 60b3ff1e-017e-48e3-8fc5-f4a87f32ce88, status phase: Pending. Waiting for statefulset controller to delete.
    Mar  9 17:07:27.266: INFO: Observed stateful pod in namespace: statefulset-3935, name: ss-0, uid: 60b3ff1e-017e-48e3-8fc5-f4a87f32ce88, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  9 17:07:27.275: INFO: Observed stateful pod in namespace: statefulset-3935, name: ss-0, uid: 60b3ff1e-017e-48e3-8fc5-f4a87f32ce88, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  9 17:07:27.277: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3935
    STEP: Removing pod with conflicting port in namespace statefulset-3935 03/09/23 17:07:27.277
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3935 and will be in running state 03/09/23 17:07:27.289
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 17:07:31.303: INFO: Deleting all statefulset in ns statefulset-3935
    Mar  9 17:07:31.306: INFO: Scaling statefulset ss to 0
    Mar  9 17:07:41.322: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 17:07:41.325: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 17:07:41.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3935" for this suite. 03/09/23 17:07:41.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:41.345
Mar  9 17:07:41.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-webhook 03/09/23 17:07:41.346
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:41.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:41.362
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/09/23 17:07:41.365
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/09/23 17:07:41.99
STEP: Deploying the custom resource conversion webhook pod 03/09/23 17:07:41.997
STEP: Wait for the deployment to be ready 03/09/23 17:07:42.007
Mar  9 17:07:42.013: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 17:07:44.023
STEP: Verifying the service has paired with the endpoint 03/09/23 17:07:44.036
Mar  9 17:07:45.038: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar  9 17:07:45.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Creating a v1 custom resource 03/09/23 17:07:47.636
STEP: v2 custom resource should be converted 03/09/23 17:07:47.64
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:07:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5776" for this suite. 03/09/23 17:07:48.159
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":292,"skipped":5510,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.856 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:41.345
    Mar  9 17:07:41.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-webhook 03/09/23 17:07:41.346
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:41.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:41.362
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/09/23 17:07:41.365
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/09/23 17:07:41.99
    STEP: Deploying the custom resource conversion webhook pod 03/09/23 17:07:41.997
    STEP: Wait for the deployment to be ready 03/09/23 17:07:42.007
    Mar  9 17:07:42.013: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 17:07:44.023
    STEP: Verifying the service has paired with the endpoint 03/09/23 17:07:44.036
    Mar  9 17:07:45.038: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar  9 17:07:45.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Creating a v1 custom resource 03/09/23 17:07:47.636
    STEP: v2 custom resource should be converted 03/09/23 17:07:47.64
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:07:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5776" for this suite. 03/09/23 17:07:48.159
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:48.201
Mar  9 17:07:48.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 17:07:48.202
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:48.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:48.224
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-1acae837-e14d-4474-8a6b-291687f69d2d 03/09/23 17:07:48.228
STEP: Creating a pod to test consume secrets 03/09/23 17:07:48.236
Mar  9 17:07:48.244: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486" in namespace "projected-109" to be "Succeeded or Failed"
Mar  9 17:07:48.248: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486": Phase="Pending", Reason="", readiness=false. Elapsed: 3.795629ms
Mar  9 17:07:50.253: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008301605s
Mar  9 17:07:52.252: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007217988s
STEP: Saw pod success 03/09/23 17:07:52.252
Mar  9 17:07:52.252: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486" satisfied condition "Succeeded or Failed"
Mar  9 17:07:52.254: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/09/23 17:07:52.259
Mar  9 17:07:52.271: INFO: Waiting for pod pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486 to disappear
Mar  9 17:07:52.273: INFO: Pod pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  9 17:07:52.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-109" for this suite. 03/09/23 17:07:52.276
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":293,"skipped":5510,"failed":0}
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:48.201
    Mar  9 17:07:48.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 17:07:48.202
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:48.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:48.224
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-1acae837-e14d-4474-8a6b-291687f69d2d 03/09/23 17:07:48.228
    STEP: Creating a pod to test consume secrets 03/09/23 17:07:48.236
    Mar  9 17:07:48.244: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486" in namespace "projected-109" to be "Succeeded or Failed"
    Mar  9 17:07:48.248: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486": Phase="Pending", Reason="", readiness=false. Elapsed: 3.795629ms
    Mar  9 17:07:50.253: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008301605s
    Mar  9 17:07:52.252: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007217988s
    STEP: Saw pod success 03/09/23 17:07:52.252
    Mar  9 17:07:52.252: INFO: Pod "pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486" satisfied condition "Succeeded or Failed"
    Mar  9 17:07:52.254: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 17:07:52.259
    Mar  9 17:07:52.271: INFO: Waiting for pod pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486 to disappear
    Mar  9 17:07:52.273: INFO: Pod pod-projected-secrets-69682f0f-b338-49e9-9b30-874fd0a4c486 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  9 17:07:52.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-109" for this suite. 03/09/23 17:07:52.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:52.284
Mar  9 17:07:52.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename secrets 03/09/23 17:07:52.285
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:52.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:52.299
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-cfa852f4-468c-4fe0-ae36-581726683685 03/09/23 17:07:52.302
STEP: Creating a pod to test consume secrets 03/09/23 17:07:52.306
Mar  9 17:07:52.313: INFO: Waiting up to 5m0s for pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de" in namespace "secrets-6589" to be "Succeeded or Failed"
Mar  9 17:07:52.315: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.247496ms
Mar  9 17:07:54.319: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005755871s
Mar  9 17:07:56.320: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007247861s
STEP: Saw pod success 03/09/23 17:07:56.32
Mar  9 17:07:56.321: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de" satisfied condition "Succeeded or Failed"
Mar  9 17:07:56.323: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de container secret-volume-test: <nil>
STEP: delete the pod 03/09/23 17:07:56.329
Mar  9 17:07:56.339: INFO: Waiting for pod pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de to disappear
Mar  9 17:07:56.341: INFO: Pod pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  9 17:07:56.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6589" for this suite. 03/09/23 17:07:56.35
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":294,"skipped":5540,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:52.284
    Mar  9 17:07:52.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename secrets 03/09/23 17:07:52.285
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:52.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:52.299
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-cfa852f4-468c-4fe0-ae36-581726683685 03/09/23 17:07:52.302
    STEP: Creating a pod to test consume secrets 03/09/23 17:07:52.306
    Mar  9 17:07:52.313: INFO: Waiting up to 5m0s for pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de" in namespace "secrets-6589" to be "Succeeded or Failed"
    Mar  9 17:07:52.315: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.247496ms
    Mar  9 17:07:54.319: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005755871s
    Mar  9 17:07:56.320: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007247861s
    STEP: Saw pod success 03/09/23 17:07:56.32
    Mar  9 17:07:56.321: INFO: Pod "pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de" satisfied condition "Succeeded or Failed"
    Mar  9 17:07:56.323: INFO: Trying to get logs from node tt-test-el8-003 pod pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de container secret-volume-test: <nil>
    STEP: delete the pod 03/09/23 17:07:56.329
    Mar  9 17:07:56.339: INFO: Waiting for pod pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de to disappear
    Mar  9 17:07:56.341: INFO: Pod pod-secrets-a7a06890-4b84-4752-b899-b7bd34fd36de no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  9 17:07:56.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6589" for this suite. 03/09/23 17:07:56.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:56.377
Mar  9 17:07:56.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 17:07:56.378
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:56.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:56.402
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 03/09/23 17:07:56.406
Mar  9 17:07:56.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9543 create -f -'
Mar  9 17:07:57.834: INFO: stderr: ""
Mar  9 17:07:57.834: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/09/23 17:07:57.834
Mar  9 17:07:58.839: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 17:07:58.839: INFO: Found 0 / 1
Mar  9 17:07:59.839: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 17:07:59.839: INFO: Found 1 / 1
Mar  9 17:07:59.839: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/09/23 17:07:59.839
Mar  9 17:07:59.841: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 17:07:59.841: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  9 17:07:59.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9543 patch pod agnhost-primary-w9z49 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  9 17:07:59.921: INFO: stderr: ""
Mar  9 17:07:59.921: INFO: stdout: "pod/agnhost-primary-w9z49 patched\n"
STEP: checking annotations 03/09/23 17:07:59.921
Mar  9 17:07:59.924: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 17:07:59.924: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 17:07:59.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9543" for this suite. 03/09/23 17:07:59.927
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":295,"skipped":5600,"failed":0}
------------------------------
â€¢ [3.555 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:56.377
    Mar  9 17:07:56.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 17:07:56.378
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:56.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:56.402
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 03/09/23 17:07:56.406
    Mar  9 17:07:56.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9543 create -f -'
    Mar  9 17:07:57.834: INFO: stderr: ""
    Mar  9 17:07:57.834: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/09/23 17:07:57.834
    Mar  9 17:07:58.839: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 17:07:58.839: INFO: Found 0 / 1
    Mar  9 17:07:59.839: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 17:07:59.839: INFO: Found 1 / 1
    Mar  9 17:07:59.839: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/09/23 17:07:59.839
    Mar  9 17:07:59.841: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 17:07:59.841: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  9 17:07:59.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-9543 patch pod agnhost-primary-w9z49 -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar  9 17:07:59.921: INFO: stderr: ""
    Mar  9 17:07:59.921: INFO: stdout: "pod/agnhost-primary-w9z49 patched\n"
    STEP: checking annotations 03/09/23 17:07:59.921
    Mar  9 17:07:59.924: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 17:07:59.924: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 17:07:59.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9543" for this suite. 03/09/23 17:07:59.927
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:07:59.933
Mar  9 17:07:59.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 17:07:59.934
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:59.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:59.947
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 17:07:59.961
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:08:00.412
STEP: Deploying the webhook pod 03/09/23 17:08:00.418
STEP: Wait for the deployment to be ready 03/09/23 17:08:00.428
Mar  9 17:08:00.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 17:08:02.445
STEP: Verifying the service has paired with the endpoint 03/09/23 17:08:02.458
Mar  9 17:08:03.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 03/09/23 17:08:03.461
STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:03.48
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/09/23 17:08:03.488
STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:03.496
STEP: Patching a validating webhook configuration's rules to include the create operation 03/09/23 17:08:03.504
STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:03.511
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:08:03.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2135" for this suite. 03/09/23 17:08:03.527
STEP: Destroying namespace "webhook-2135-markers" for this suite. 03/09/23 17:08:03.531
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":296,"skipped":5600,"failed":0}
------------------------------
â€¢ [3.653 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:07:59.933
    Mar  9 17:07:59.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 17:07:59.934
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:07:59.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:07:59.947
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 17:07:59.961
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:08:00.412
    STEP: Deploying the webhook pod 03/09/23 17:08:00.418
    STEP: Wait for the deployment to be ready 03/09/23 17:08:00.428
    Mar  9 17:08:00.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 17:08:02.445
    STEP: Verifying the service has paired with the endpoint 03/09/23 17:08:02.458
    Mar  9 17:08:03.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 03/09/23 17:08:03.461
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:03.48
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/09/23 17:08:03.488
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:03.496
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/09/23 17:08:03.504
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:03.511
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:08:03.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2135" for this suite. 03/09/23 17:08:03.527
    STEP: Destroying namespace "webhook-2135-markers" for this suite. 03/09/23 17:08:03.531
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:08:03.587
Mar  9 17:08:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 17:08:03.588
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:03.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:03.606
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 17:08:03.626
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:08:03.995
STEP: Deploying the webhook pod 03/09/23 17:08:04
STEP: Wait for the deployment to be ready 03/09/23 17:08:04.011
Mar  9 17:08:04.017: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 17:08:06.026
STEP: Verifying the service has paired with the endpoint 03/09/23 17:08:06.041
Mar  9 17:08:07.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 03/09/23 17:08:07.093
STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:08.17
STEP: Deleting the collection of validation webhooks 03/09/23 17:08:08.197
STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:08.235
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:08:08.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1241" for this suite. 03/09/23 17:08:08.247
STEP: Destroying namespace "webhook-1241-markers" for this suite. 03/09/23 17:08:08.251
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":297,"skipped":5603,"failed":0}
------------------------------
â€¢ [4.708 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:08:03.587
    Mar  9 17:08:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 17:08:03.588
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:03.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:03.606
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 17:08:03.626
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:08:03.995
    STEP: Deploying the webhook pod 03/09/23 17:08:04
    STEP: Wait for the deployment to be ready 03/09/23 17:08:04.011
    Mar  9 17:08:04.017: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 17:08:06.026
    STEP: Verifying the service has paired with the endpoint 03/09/23 17:08:06.041
    Mar  9 17:08:07.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 03/09/23 17:08:07.093
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:08.17
    STEP: Deleting the collection of validation webhooks 03/09/23 17:08:08.197
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/09/23 17:08:08.235
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:08:08.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1241" for this suite. 03/09/23 17:08:08.247
    STEP: Destroying namespace "webhook-1241-markers" for this suite. 03/09/23 17:08:08.251
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:08:08.295
Mar  9 17:08:08.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename disruption 03/09/23 17:08:08.297
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:08.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:08.319
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 03/09/23 17:08:08.324
STEP: Waiting for the pdb to be processed 03/09/23 17:08:08.329
STEP: First trying to evict a pod which shouldn't be evictable 03/09/23 17:08:10.345
STEP: Waiting for all pods to be running 03/09/23 17:08:10.345
Mar  9 17:08:10.352: INFO: pods: 0 < 3
STEP: locating a running pod 03/09/23 17:08:12.357
STEP: Updating the pdb to allow a pod to be evicted 03/09/23 17:08:12.366
STEP: Waiting for the pdb to be processed 03/09/23 17:08:12.373
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/09/23 17:08:14.38
STEP: Waiting for all pods to be running 03/09/23 17:08:14.38
STEP: Waiting for the pdb to observed all healthy pods 03/09/23 17:08:14.383
STEP: Patching the pdb to disallow a pod to be evicted 03/09/23 17:08:14.398
STEP: Waiting for the pdb to be processed 03/09/23 17:08:14.418
STEP: Waiting for all pods to be running 03/09/23 17:08:16.424
STEP: locating a running pod 03/09/23 17:08:16.428
STEP: Deleting the pdb to allow a pod to be evicted 03/09/23 17:08:16.435
STEP: Waiting for the pdb to be deleted 03/09/23 17:08:16.439
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/09/23 17:08:16.441
STEP: Waiting for all pods to be running 03/09/23 17:08:16.441
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  9 17:08:16.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9526" for this suite. 03/09/23 17:08:16.457
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":298,"skipped":5604,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.168 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:08:08.295
    Mar  9 17:08:08.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename disruption 03/09/23 17:08:08.297
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:08.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:08.319
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 03/09/23 17:08:08.324
    STEP: Waiting for the pdb to be processed 03/09/23 17:08:08.329
    STEP: First trying to evict a pod which shouldn't be evictable 03/09/23 17:08:10.345
    STEP: Waiting for all pods to be running 03/09/23 17:08:10.345
    Mar  9 17:08:10.352: INFO: pods: 0 < 3
    STEP: locating a running pod 03/09/23 17:08:12.357
    STEP: Updating the pdb to allow a pod to be evicted 03/09/23 17:08:12.366
    STEP: Waiting for the pdb to be processed 03/09/23 17:08:12.373
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/09/23 17:08:14.38
    STEP: Waiting for all pods to be running 03/09/23 17:08:14.38
    STEP: Waiting for the pdb to observed all healthy pods 03/09/23 17:08:14.383
    STEP: Patching the pdb to disallow a pod to be evicted 03/09/23 17:08:14.398
    STEP: Waiting for the pdb to be processed 03/09/23 17:08:14.418
    STEP: Waiting for all pods to be running 03/09/23 17:08:16.424
    STEP: locating a running pod 03/09/23 17:08:16.428
    STEP: Deleting the pdb to allow a pod to be evicted 03/09/23 17:08:16.435
    STEP: Waiting for the pdb to be deleted 03/09/23 17:08:16.439
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/09/23 17:08:16.441
    STEP: Waiting for all pods to be running 03/09/23 17:08:16.441
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  9 17:08:16.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9526" for this suite. 03/09/23 17:08:16.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:08:16.465
Mar  9 17:08:16.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename gc 03/09/23 17:08:16.466
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:16.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:16.489
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/09/23 17:08:16.496
STEP: delete the rc 03/09/23 17:08:21.504
STEP: wait for the rc to be deleted 03/09/23 17:08:21.511
Mar  9 17:08:22.528: INFO: 80 pods remaining
Mar  9 17:08:22.528: INFO: 80 pods has nil DeletionTimestamp
Mar  9 17:08:22.528: INFO: 
Mar  9 17:08:23.531: INFO: 72 pods remaining
Mar  9 17:08:23.531: INFO: 72 pods has nil DeletionTimestamp
Mar  9 17:08:23.531: INFO: 
Mar  9 17:08:24.539: INFO: 60 pods remaining
Mar  9 17:08:24.539: INFO: 60 pods has nil DeletionTimestamp
Mar  9 17:08:24.539: INFO: 
Mar  9 17:08:25.525: INFO: 40 pods remaining
Mar  9 17:08:25.525: INFO: 40 pods has nil DeletionTimestamp
Mar  9 17:08:25.525: INFO: 
Mar  9 17:08:26.528: INFO: 32 pods remaining
Mar  9 17:08:26.528: INFO: 32 pods has nil DeletionTimestamp
Mar  9 17:08:26.529: INFO: 
Mar  9 17:08:27.522: INFO: 20 pods remaining
Mar  9 17:08:27.522: INFO: 20 pods has nil DeletionTimestamp
Mar  9 17:08:27.522: INFO: 
STEP: Gathering metrics 03/09/23 17:08:28.518
Mar  9 17:08:28.537: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
Mar  9 17:08:28.539: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.526577ms
Mar  9 17:08:28.539: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
Mar  9 17:08:28.539: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
Mar  9 17:08:28.651: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  9 17:08:28.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9956" for this suite. 03/09/23 17:08:28.656
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":299,"skipped":5616,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.204 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:08:16.465
    Mar  9 17:08:16.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename gc 03/09/23 17:08:16.466
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:16.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:16.489
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/09/23 17:08:16.496
    STEP: delete the rc 03/09/23 17:08:21.504
    STEP: wait for the rc to be deleted 03/09/23 17:08:21.511
    Mar  9 17:08:22.528: INFO: 80 pods remaining
    Mar  9 17:08:22.528: INFO: 80 pods has nil DeletionTimestamp
    Mar  9 17:08:22.528: INFO: 
    Mar  9 17:08:23.531: INFO: 72 pods remaining
    Mar  9 17:08:23.531: INFO: 72 pods has nil DeletionTimestamp
    Mar  9 17:08:23.531: INFO: 
    Mar  9 17:08:24.539: INFO: 60 pods remaining
    Mar  9 17:08:24.539: INFO: 60 pods has nil DeletionTimestamp
    Mar  9 17:08:24.539: INFO: 
    Mar  9 17:08:25.525: INFO: 40 pods remaining
    Mar  9 17:08:25.525: INFO: 40 pods has nil DeletionTimestamp
    Mar  9 17:08:25.525: INFO: 
    Mar  9 17:08:26.528: INFO: 32 pods remaining
    Mar  9 17:08:26.528: INFO: 32 pods has nil DeletionTimestamp
    Mar  9 17:08:26.529: INFO: 
    Mar  9 17:08:27.522: INFO: 20 pods remaining
    Mar  9 17:08:27.522: INFO: 20 pods has nil DeletionTimestamp
    Mar  9 17:08:27.522: INFO: 
    STEP: Gathering metrics 03/09/23 17:08:28.518
    Mar  9 17:08:28.537: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
    Mar  9 17:08:28.539: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.526577ms
    Mar  9 17:08:28.539: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
    Mar  9 17:08:28.539: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
    Mar  9 17:08:28.651: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  9 17:08:28.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9956" for this suite. 03/09/23 17:08:28.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:08:28.671
Mar  9 17:08:28.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pod-network-test 03/09/23 17:08:28.672
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:28.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:28.696
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1591 03/09/23 17:08:28.7
STEP: creating a selector 03/09/23 17:08:28.701
STEP: Creating the service pods in kubernetes 03/09/23 17:08:28.701
Mar  9 17:08:28.701: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  9 17:08:28.735: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1591" to be "running and ready"
Mar  9 17:08:28.738: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38518ms
Mar  9 17:08:28.738: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:30.743: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530285s
Mar  9 17:08:30.743: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:32.745: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010275908s
Mar  9 17:08:32.745: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:34.742: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007312542s
Mar  9 17:08:34.742: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:36.743: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007931373s
Mar  9 17:08:36.743: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:38.742: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006767019s
Mar  9 17:08:38.742: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:40.741: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006628729s
Mar  9 17:08:40.741: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:42.741: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006659161s
Mar  9 17:08:42.741: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:08:44.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00684387s
Mar  9 17:08:44.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:08:46.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007532291s
Mar  9 17:08:46.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:08:48.741: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006599658s
Mar  9 17:08:48.741: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:08:50.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.00681436s
Mar  9 17:08:50.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:08:52.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 24.007598957s
Mar  9 17:08:52.742: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  9 17:08:52.742: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  9 17:08:52.745: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1591" to be "running and ready"
Mar  9 17:08:52.747: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.170707ms
Mar  9 17:08:52.747: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  9 17:08:52.747: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/09/23 17:08:52.75
Mar  9 17:08:52.760: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1591" to be "running"
Mar  9 17:08:52.763: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289259ms
Mar  9 17:08:54.767: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007724291s
Mar  9 17:08:54.767: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  9 17:08:54.770: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1591" to be "running"
Mar  9 17:08:54.772: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.317986ms
Mar  9 17:08:54.772: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  9 17:08:54.774: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar  9 17:08:54.774: INFO: Going to poll 10.244.42.227 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Mar  9 17:08:54.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.42.227:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1591 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 17:08:54.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 17:08:54.778: INFO: ExecWithOptions: Clientset creation
Mar  9 17:08:54.778: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1591/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.42.227%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  9 17:08:54.864: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  9 17:08:54.864: INFO: Going to poll 10.244.88.234 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Mar  9 17:08:54.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.88.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1591 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 17:08:54.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 17:08:54.867: INFO: ExecWithOptions: Clientset creation
Mar  9 17:08:54.867: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1591/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.88.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  9 17:08:54.944: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  9 17:08:54.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1591" for this suite. 03/09/23 17:08:54.949
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":300,"skipped":5663,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.284 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:08:28.671
    Mar  9 17:08:28.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pod-network-test 03/09/23 17:08:28.672
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:28.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:28.696
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1591 03/09/23 17:08:28.7
    STEP: creating a selector 03/09/23 17:08:28.701
    STEP: Creating the service pods in kubernetes 03/09/23 17:08:28.701
    Mar  9 17:08:28.701: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  9 17:08:28.735: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1591" to be "running and ready"
    Mar  9 17:08:28.738: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38518ms
    Mar  9 17:08:28.738: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:30.743: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530285s
    Mar  9 17:08:30.743: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:32.745: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010275908s
    Mar  9 17:08:32.745: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:34.742: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007312542s
    Mar  9 17:08:34.742: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:36.743: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007931373s
    Mar  9 17:08:36.743: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:38.742: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006767019s
    Mar  9 17:08:38.742: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:40.741: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006628729s
    Mar  9 17:08:40.741: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:42.741: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006659161s
    Mar  9 17:08:42.741: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:08:44.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00684387s
    Mar  9 17:08:44.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:08:46.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007532291s
    Mar  9 17:08:46.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:08:48.741: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006599658s
    Mar  9 17:08:48.741: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:08:50.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.00681436s
    Mar  9 17:08:50.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:08:52.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 24.007598957s
    Mar  9 17:08:52.742: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  9 17:08:52.742: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  9 17:08:52.745: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1591" to be "running and ready"
    Mar  9 17:08:52.747: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.170707ms
    Mar  9 17:08:52.747: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  9 17:08:52.747: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/09/23 17:08:52.75
    Mar  9 17:08:52.760: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1591" to be "running"
    Mar  9 17:08:52.763: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289259ms
    Mar  9 17:08:54.767: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007724291s
    Mar  9 17:08:54.767: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  9 17:08:54.770: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1591" to be "running"
    Mar  9 17:08:54.772: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.317986ms
    Mar  9 17:08:54.772: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  9 17:08:54.774: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar  9 17:08:54.774: INFO: Going to poll 10.244.42.227 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Mar  9 17:08:54.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.42.227:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1591 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 17:08:54.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 17:08:54.778: INFO: ExecWithOptions: Clientset creation
    Mar  9 17:08:54.778: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1591/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.42.227%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  9 17:08:54.864: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  9 17:08:54.864: INFO: Going to poll 10.244.88.234 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Mar  9 17:08:54.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.88.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1591 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 17:08:54.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 17:08:54.867: INFO: ExecWithOptions: Clientset creation
    Mar  9 17:08:54.867: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1591/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.88.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  9 17:08:54.944: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  9 17:08:54.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1591" for this suite. 03/09/23 17:08:54.949
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:08:54.955
Mar  9 17:08:54.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 17:08:54.956
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:54.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:54.973
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar  9 17:08:54.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:08:55.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2092" for this suite. 03/09/23 17:08:56.002
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":301,"skipped":5664,"failed":0}
------------------------------
â€¢ [1.055 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:08:54.955
    Mar  9 17:08:54.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename custom-resource-definition 03/09/23 17:08:54.956
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:54.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:54.973
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar  9 17:08:54.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:08:55.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2092" for this suite. 03/09/23 17:08:56.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:08:56.012
Mar  9 17:08:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename ingressclass 03/09/23 17:08:56.013
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:56.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:56.03
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/09/23 17:08:56.033
STEP: getting /apis/networking.k8s.io 03/09/23 17:08:56.035
STEP: getting /apis/networking.k8s.iov1 03/09/23 17:08:56.037
STEP: creating 03/09/23 17:08:56.038
STEP: getting 03/09/23 17:08:56.05
STEP: listing 03/09/23 17:08:56.052
STEP: watching 03/09/23 17:08:56.054
Mar  9 17:08:56.054: INFO: starting watch
STEP: patching 03/09/23 17:08:56.056
STEP: updating 03/09/23 17:08:56.061
Mar  9 17:08:56.066: INFO: waiting for watch events with expected annotations
Mar  9 17:08:56.066: INFO: saw patched and updated annotations
STEP: deleting 03/09/23 17:08:56.066
STEP: deleting a collection 03/09/23 17:08:56.074
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Mar  9 17:08:56.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1957" for this suite. 03/09/23 17:08:56.088
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":302,"skipped":5677,"failed":0}
------------------------------
â€¢ [0.081 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:08:56.012
    Mar  9 17:08:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename ingressclass 03/09/23 17:08:56.013
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:56.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:56.03
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/09/23 17:08:56.033
    STEP: getting /apis/networking.k8s.io 03/09/23 17:08:56.035
    STEP: getting /apis/networking.k8s.iov1 03/09/23 17:08:56.037
    STEP: creating 03/09/23 17:08:56.038
    STEP: getting 03/09/23 17:08:56.05
    STEP: listing 03/09/23 17:08:56.052
    STEP: watching 03/09/23 17:08:56.054
    Mar  9 17:08:56.054: INFO: starting watch
    STEP: patching 03/09/23 17:08:56.056
    STEP: updating 03/09/23 17:08:56.061
    Mar  9 17:08:56.066: INFO: waiting for watch events with expected annotations
    Mar  9 17:08:56.066: INFO: saw patched and updated annotations
    STEP: deleting 03/09/23 17:08:56.066
    STEP: deleting a collection 03/09/23 17:08:56.074
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Mar  9 17:08:56.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-1957" for this suite. 03/09/23 17:08:56.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:08:56.094
Mar  9 17:08:56.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 17:08:56.095
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:56.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:56.115
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar  9 17:08:56.126: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  9 17:09:01.132: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/09/23 17:09:01.132
Mar  9 17:09:01.132: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/09/23 17:09:01.145
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 17:09:01.157: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-722  a0387dad-2a79-4c21-8a36-3764a46bca05 120063 1 2023-03-09 17:09:01 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-09 17:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003513688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  9 17:09:01.161: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar  9 17:09:01.161: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  9 17:09:01.161: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-722  c253f780-8a0a-4d6f-9d0b-4ad5749d09d1 120064 1 2023-03-09 17:08:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment a0387dad-2a79-4c21-8a36-3764a46bca05 0xc0035139d7 0xc0035139d8}] [] [{e2e.test Update apps/v1 2023-03-09 17:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 17:08:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-09 17:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"a0387dad-2a79-4c21-8a36-3764a46bca05\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003513a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  9 17:09:01.166: INFO: Pod "test-cleanup-controller-v5h72" is available:
&Pod{ObjectMeta:{test-cleanup-controller-v5h72 test-cleanup-controller- deployment-722  85cbb025-eb94-47d2-a8bd-481c163b0d95 120020 0 2023-03-09 17:08:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:721a3249e8dfdc230967af01e5cf736467b6330e9032087b1546d95f339b8b30 cni.projectcalico.org/podIP:10.244.42.226/32 cni.projectcalico.org/podIPs:10.244.42.226/32] [{apps/v1 ReplicaSet test-cleanup-controller c253f780-8a0a-4d6f-9d0b-4ad5749d09d1 0xc003513db7 0xc003513db8}] [] [{calico Update v1 2023-03-09 17:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-09 17:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c253f780-8a0a-4d6f-9d0b-4ad5749d09d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 17:08:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqphb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqphb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.226,StartTime:2023-03-09 17:08:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 17:08:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://3c77cca865d2a0eb56beb27ea1af7da995afd74eb89717bf0b6b73830cc96fe0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 17:09:01.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-722" for this suite. 03/09/23 17:09:01.179
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":303,"skipped":5699,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.100 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:08:56.094
    Mar  9 17:08:56.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 17:08:56.095
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:08:56.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:08:56.115
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar  9 17:08:56.126: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar  9 17:09:01.132: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/09/23 17:09:01.132
    Mar  9 17:09:01.132: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/09/23 17:09:01.145
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 17:09:01.157: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-722  a0387dad-2a79-4c21-8a36-3764a46bca05 120063 1 2023-03-09 17:09:01 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-09 17:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003513688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  9 17:09:01.161: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Mar  9 17:09:01.161: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Mar  9 17:09:01.161: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-722  c253f780-8a0a-4d6f-9d0b-4ad5749d09d1 120064 1 2023-03-09 17:08:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment a0387dad-2a79-4c21-8a36-3764a46bca05 0xc0035139d7 0xc0035139d8}] [] [{e2e.test Update apps/v1 2023-03-09 17:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 17:08:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-09 17:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"a0387dad-2a79-4c21-8a36-3764a46bca05\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003513a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 17:09:01.166: INFO: Pod "test-cleanup-controller-v5h72" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-v5h72 test-cleanup-controller- deployment-722  85cbb025-eb94-47d2-a8bd-481c163b0d95 120020 0 2023-03-09 17:08:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:721a3249e8dfdc230967af01e5cf736467b6330e9032087b1546d95f339b8b30 cni.projectcalico.org/podIP:10.244.42.226/32 cni.projectcalico.org/podIPs:10.244.42.226/32] [{apps/v1 ReplicaSet test-cleanup-controller c253f780-8a0a-4d6f-9d0b-4ad5749d09d1 0xc003513db7 0xc003513db8}] [] [{calico Update v1 2023-03-09 17:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-09 17:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c253f780-8a0a-4d6f-9d0b-4ad5749d09d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-09 17:08:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqphb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqphb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:08:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.226,StartTime:2023-03-09 17:08:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 17:08:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://3c77cca865d2a0eb56beb27ea1af7da995afd74eb89717bf0b6b73830cc96fe0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 17:09:01.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-722" for this suite. 03/09/23 17:09:01.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:01.195
Mar  9 17:09:01.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replication-controller 03/09/23 17:09:01.197
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:01.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:01.232
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Mar  9 17:09:01.237: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/09/23 17:09:02.258
STEP: Checking rc "condition-test" has the desired failure condition set 03/09/23 17:09:02.264
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/09/23 17:09:03.27
Mar  9 17:09:03.279: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/09/23 17:09:03.279
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  9 17:09:04.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6148" for this suite. 03/09/23 17:09:04.288
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":304,"skipped":5704,"failed":0}
------------------------------
â€¢ [3.098 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:01.195
    Mar  9 17:09:01.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replication-controller 03/09/23 17:09:01.197
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:01.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:01.232
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Mar  9 17:09:01.237: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/09/23 17:09:02.258
    STEP: Checking rc "condition-test" has the desired failure condition set 03/09/23 17:09:02.264
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/09/23 17:09:03.27
    Mar  9 17:09:03.279: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/09/23 17:09:03.279
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  9 17:09:04.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6148" for this suite. 03/09/23 17:09:04.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:04.296
Mar  9 17:09:04.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename namespaces 03/09/23 17:09:04.298
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:04.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:04.312
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 03/09/23 17:09:04.315
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:04.325
STEP: Creating a pod in the namespace 03/09/23 17:09:04.328
STEP: Waiting for the pod to have running status 03/09/23 17:09:04.334
Mar  9 17:09:04.334: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7641" to be "running"
Mar  9 17:09:04.337: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662604ms
Mar  9 17:09:06.340: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005725084s
Mar  9 17:09:08.341: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006664312s
Mar  9 17:09:08.341: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/09/23 17:09:08.341
STEP: Waiting for the namespace to be removed. 03/09/23 17:09:08.346
STEP: Recreating the namespace 03/09/23 17:09:19.35
STEP: Verifying there are no pods in the namespace 03/09/23 17:09:19.361
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  9 17:09:19.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3677" for this suite. 03/09/23 17:09:19.369
STEP: Destroying namespace "nsdeletetest-7641" for this suite. 03/09/23 17:09:19.375
Mar  9 17:09:19.378: INFO: Namespace nsdeletetest-7641 was already deleted
STEP: Destroying namespace "nsdeletetest-198" for this suite. 03/09/23 17:09:19.378
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":305,"skipped":5746,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.088 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:04.296
    Mar  9 17:09:04.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename namespaces 03/09/23 17:09:04.298
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:04.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:04.312
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 03/09/23 17:09:04.315
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:04.325
    STEP: Creating a pod in the namespace 03/09/23 17:09:04.328
    STEP: Waiting for the pod to have running status 03/09/23 17:09:04.334
    Mar  9 17:09:04.334: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7641" to be "running"
    Mar  9 17:09:04.337: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662604ms
    Mar  9 17:09:06.340: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005725084s
    Mar  9 17:09:08.341: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006664312s
    Mar  9 17:09:08.341: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/09/23 17:09:08.341
    STEP: Waiting for the namespace to be removed. 03/09/23 17:09:08.346
    STEP: Recreating the namespace 03/09/23 17:09:19.35
    STEP: Verifying there are no pods in the namespace 03/09/23 17:09:19.361
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 17:09:19.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3677" for this suite. 03/09/23 17:09:19.369
    STEP: Destroying namespace "nsdeletetest-7641" for this suite. 03/09/23 17:09:19.375
    Mar  9 17:09:19.378: INFO: Namespace nsdeletetest-7641 was already deleted
    STEP: Destroying namespace "nsdeletetest-198" for this suite. 03/09/23 17:09:19.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:19.386
Mar  9 17:09:19.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename svcaccounts 03/09/23 17:09:19.388
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:19.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:19.402
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Mar  9 17:09:19.421: INFO: Waiting up to 5m0s for pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1" in namespace "svcaccounts-591" to be "running"
Mar  9 17:09:19.425: INFO: Pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685339ms
Mar  9 17:09:21.429: INFO: Pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007782078s
Mar  9 17:09:21.429: INFO: Pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1" satisfied condition "running"
STEP: reading a file in the container 03/09/23 17:09:21.429
Mar  9 17:09:21.429: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-591 pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/09/23 17:09:21.574
Mar  9 17:09:21.574: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-591 pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/09/23 17:09:21.729
Mar  9 17:09:21.730: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-591 pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar  9 17:09:21.881: INFO: Got root ca configmap in namespace "svcaccounts-591"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  9 17:09:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-591" for this suite. 03/09/23 17:09:21.888
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":306,"skipped":5774,"failed":0}
------------------------------
â€¢ [2.508 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:19.386
    Mar  9 17:09:19.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename svcaccounts 03/09/23 17:09:19.388
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:19.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:19.402
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Mar  9 17:09:19.421: INFO: Waiting up to 5m0s for pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1" in namespace "svcaccounts-591" to be "running"
    Mar  9 17:09:19.425: INFO: Pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685339ms
    Mar  9 17:09:21.429: INFO: Pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007782078s
    Mar  9 17:09:21.429: INFO: Pod "pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1" satisfied condition "running"
    STEP: reading a file in the container 03/09/23 17:09:21.429
    Mar  9 17:09:21.429: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-591 pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/09/23 17:09:21.574
    Mar  9 17:09:21.574: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-591 pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/09/23 17:09:21.729
    Mar  9 17:09:21.730: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-591 pod-service-account-c8b48c85-fa03-4c03-a5e8-34cd137d38e1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar  9 17:09:21.881: INFO: Got root ca configmap in namespace "svcaccounts-591"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  9 17:09:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-591" for this suite. 03/09/23 17:09:21.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:21.895
Mar  9 17:09:21.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 17:09:21.896
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:21.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:21.911
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 03/09/23 17:09:21.914
Mar  9 17:09:21.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: rename a version 03/09/23 17:09:32.82
STEP: check the new version name is served 03/09/23 17:09:32.837
STEP: check the old version name is removed 03/09/23 17:09:36.718
STEP: check the other version is not changed 03/09/23 17:09:38.693
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:09:45.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4041" for this suite. 03/09/23 17:09:45.416
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":307,"skipped":5790,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.527 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:21.895
    Mar  9 17:09:21.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 17:09:21.896
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:21.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:21.911
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 03/09/23 17:09:21.914
    Mar  9 17:09:21.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: rename a version 03/09/23 17:09:32.82
    STEP: check the new version name is served 03/09/23 17:09:32.837
    STEP: check the old version name is removed 03/09/23 17:09:36.718
    STEP: check the other version is not changed 03/09/23 17:09:38.693
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:09:45.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4041" for this suite. 03/09/23 17:09:45.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:45.422
Mar  9 17:09:45.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replication-controller 03/09/23 17:09:45.424
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:45.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:45.437
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 03/09/23 17:09:45.44
Mar  9 17:09:45.448: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4981" to be "running and ready"
Mar  9 17:09:45.450: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.564788ms
Mar  9 17:09:45.450: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:09:47.455: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.00714936s
Mar  9 17:09:47.455: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar  9 17:09:47.455: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/09/23 17:09:47.458
STEP: Then the orphan pod is adopted 03/09/23 17:09:47.462
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  9 17:09:48.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4981" for this suite. 03/09/23 17:09:48.472
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":308,"skipped":5802,"failed":0}
------------------------------
â€¢ [3.054 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:45.422
    Mar  9 17:09:45.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replication-controller 03/09/23 17:09:45.424
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:45.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:45.437
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/09/23 17:09:45.44
    Mar  9 17:09:45.448: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4981" to be "running and ready"
    Mar  9 17:09:45.450: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.564788ms
    Mar  9 17:09:45.450: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:09:47.455: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.00714936s
    Mar  9 17:09:47.455: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar  9 17:09:47.455: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/09/23 17:09:47.458
    STEP: Then the orphan pod is adopted 03/09/23 17:09:47.462
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  9 17:09:48.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4981" for this suite. 03/09/23 17:09:48.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:48.478
Mar  9 17:09:48.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 17:09:48.479
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:48.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:48.494
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 03/09/23 17:09:48.497
Mar  9 17:09:48.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0" in namespace "downward-api-5231" to be "Succeeded or Failed"
Mar  9 17:09:48.506: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365034ms
Mar  9 17:09:50.510: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006622902s
Mar  9 17:09:52.510: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006591846s
STEP: Saw pod success 03/09/23 17:09:52.51
Mar  9 17:09:52.510: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0" satisfied condition "Succeeded or Failed"
Mar  9 17:09:52.513: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0 container client-container: <nil>
STEP: delete the pod 03/09/23 17:09:52.527
Mar  9 17:09:52.537: INFO: Waiting for pod downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0 to disappear
Mar  9 17:09:52.540: INFO: Pod downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 17:09:52.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5231" for this suite. 03/09/23 17:09:52.544
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":309,"skipped":5823,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:48.478
    Mar  9 17:09:48.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 17:09:48.479
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:48.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:48.494
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 03/09/23 17:09:48.497
    Mar  9 17:09:48.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0" in namespace "downward-api-5231" to be "Succeeded or Failed"
    Mar  9 17:09:48.506: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365034ms
    Mar  9 17:09:50.510: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006622902s
    Mar  9 17:09:52.510: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006591846s
    STEP: Saw pod success 03/09/23 17:09:52.51
    Mar  9 17:09:52.510: INFO: Pod "downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0" satisfied condition "Succeeded or Failed"
    Mar  9 17:09:52.513: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0 container client-container: <nil>
    STEP: delete the pod 03/09/23 17:09:52.527
    Mar  9 17:09:52.537: INFO: Waiting for pod downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0 to disappear
    Mar  9 17:09:52.540: INFO: Pod downwardapi-volume-02a73a59-b052-4ed0-8003-c6cd561061a0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 17:09:52.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5231" for this suite. 03/09/23 17:09:52.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:52.549
Mar  9 17:09:52.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename events 03/09/23 17:09:52.55
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:52.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:52.564
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/09/23 17:09:52.567
STEP: get a list of Events with a label in the current namespace 03/09/23 17:09:52.58
STEP: delete a list of events 03/09/23 17:09:52.583
Mar  9 17:09:52.583: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/09/23 17:09:52.598
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  9 17:09:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-927" for this suite. 03/09/23 17:09:52.604
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":310,"skipped":5835,"failed":0}
------------------------------
â€¢ [0.059 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:52.549
    Mar  9 17:09:52.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename events 03/09/23 17:09:52.55
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:52.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:52.564
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/09/23 17:09:52.567
    STEP: get a list of Events with a label in the current namespace 03/09/23 17:09:52.58
    STEP: delete a list of events 03/09/23 17:09:52.583
    Mar  9 17:09:52.583: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/09/23 17:09:52.598
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  9 17:09:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-927" for this suite. 03/09/23 17:09:52.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:52.609
Mar  9 17:09:52.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename containers 03/09/23 17:09:52.61
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:52.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:52.626
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 03/09/23 17:09:52.629
Mar  9 17:09:52.635: INFO: Waiting up to 5m0s for pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff" in namespace "containers-3573" to be "Succeeded or Failed"
Mar  9 17:09:52.637: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27919ms
Mar  9 17:09:54.641: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005894194s
Mar  9 17:09:56.641: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006020525s
STEP: Saw pod success 03/09/23 17:09:56.641
Mar  9 17:09:56.641: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff" satisfied condition "Succeeded or Failed"
Mar  9 17:09:56.644: INFO: Trying to get logs from node tt-test-el8-003 pod client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff container agnhost-container: <nil>
STEP: delete the pod 03/09/23 17:09:56.649
Mar  9 17:09:56.657: INFO: Waiting for pod client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff to disappear
Mar  9 17:09:56.660: INFO: Pod client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  9 17:09:56.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3573" for this suite. 03/09/23 17:09:56.664
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":311,"skipped":5855,"failed":0}
------------------------------
â€¢ [4.059 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:52.609
    Mar  9 17:09:52.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename containers 03/09/23 17:09:52.61
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:52.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:52.626
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 03/09/23 17:09:52.629
    Mar  9 17:09:52.635: INFO: Waiting up to 5m0s for pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff" in namespace "containers-3573" to be "Succeeded or Failed"
    Mar  9 17:09:52.637: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27919ms
    Mar  9 17:09:54.641: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005894194s
    Mar  9 17:09:56.641: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006020525s
    STEP: Saw pod success 03/09/23 17:09:56.641
    Mar  9 17:09:56.641: INFO: Pod "client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff" satisfied condition "Succeeded or Failed"
    Mar  9 17:09:56.644: INFO: Trying to get logs from node tt-test-el8-003 pod client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 17:09:56.649
    Mar  9 17:09:56.657: INFO: Waiting for pod client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff to disappear
    Mar  9 17:09:56.660: INFO: Pod client-containers-1ffa9649-4f2a-49cf-b2a5-23de145c1eff no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  9 17:09:56.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3573" for this suite. 03/09/23 17:09:56.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:56.672
Mar  9 17:09:56.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename proxy 03/09/23 17:09:56.673
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:56.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:56.687
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar  9 17:09:56.690: INFO: Creating pod...
Mar  9 17:09:56.697: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6360" to be "running"
Mar  9 17:09:56.699: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250565ms
Mar  9 17:09:58.703: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005720673s
Mar  9 17:09:58.703: INFO: Pod "agnhost" satisfied condition "running"
Mar  9 17:09:58.703: INFO: Creating service...
Mar  9 17:09:58.719: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=DELETE
Mar  9 17:09:58.723: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  9 17:09:58.723: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=OPTIONS
Mar  9 17:09:58.729: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  9 17:09:58.729: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=PATCH
Mar  9 17:09:58.731: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  9 17:09:58.732: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=POST
Mar  9 17:09:58.735: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  9 17:09:58.735: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=PUT
Mar  9 17:09:58.738: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  9 17:09:58.738: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=DELETE
Mar  9 17:09:58.742: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  9 17:09:58.742: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar  9 17:09:58.746: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  9 17:09:58.746: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=PATCH
Mar  9 17:09:58.750: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  9 17:09:58.750: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=POST
Mar  9 17:09:58.754: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  9 17:09:58.754: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=PUT
Mar  9 17:09:58.758: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  9 17:09:58.758: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=GET
Mar  9 17:09:58.761: INFO: http.Client request:GET StatusCode:301
Mar  9 17:09:58.761: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=GET
Mar  9 17:09:58.764: INFO: http.Client request:GET StatusCode:301
Mar  9 17:09:58.764: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=HEAD
Mar  9 17:09:58.766: INFO: http.Client request:HEAD StatusCode:301
Mar  9 17:09:58.766: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=HEAD
Mar  9 17:09:58.770: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  9 17:09:58.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6360" for this suite. 03/09/23 17:09:58.773
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":312,"skipped":5892,"failed":0}
------------------------------
â€¢ [2.107 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:56.672
    Mar  9 17:09:56.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename proxy 03/09/23 17:09:56.673
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:56.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:56.687
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar  9 17:09:56.690: INFO: Creating pod...
    Mar  9 17:09:56.697: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6360" to be "running"
    Mar  9 17:09:56.699: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250565ms
    Mar  9 17:09:58.703: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005720673s
    Mar  9 17:09:58.703: INFO: Pod "agnhost" satisfied condition "running"
    Mar  9 17:09:58.703: INFO: Creating service...
    Mar  9 17:09:58.719: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=DELETE
    Mar  9 17:09:58.723: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  9 17:09:58.723: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=OPTIONS
    Mar  9 17:09:58.729: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  9 17:09:58.729: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=PATCH
    Mar  9 17:09:58.731: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  9 17:09:58.732: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=POST
    Mar  9 17:09:58.735: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  9 17:09:58.735: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=PUT
    Mar  9 17:09:58.738: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  9 17:09:58.738: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar  9 17:09:58.742: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  9 17:09:58.742: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar  9 17:09:58.746: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  9 17:09:58.746: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar  9 17:09:58.750: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  9 17:09:58.750: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=POST
    Mar  9 17:09:58.754: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  9 17:09:58.754: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=PUT
    Mar  9 17:09:58.758: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  9 17:09:58.758: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=GET
    Mar  9 17:09:58.761: INFO: http.Client request:GET StatusCode:301
    Mar  9 17:09:58.761: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=GET
    Mar  9 17:09:58.764: INFO: http.Client request:GET StatusCode:301
    Mar  9 17:09:58.764: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/pods/agnhost/proxy?method=HEAD
    Mar  9 17:09:58.766: INFO: http.Client request:HEAD StatusCode:301
    Mar  9 17:09:58.766: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6360/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar  9 17:09:58.770: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  9 17:09:58.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6360" for this suite. 03/09/23 17:09:58.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:09:58.781
Mar  9 17:09:58.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 17:09:58.782
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:58.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:58.799
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-9735 03/09/23 17:09:58.803
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[] 03/09/23 17:09:58.819
Mar  9 17:09:58.823: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar  9 17:09:59.829: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9735 03/09/23 17:09:59.829
Mar  9 17:09:59.835: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9735" to be "running and ready"
Mar  9 17:09:59.839: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.42978ms
Mar  9 17:09:59.839: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:10:01.842: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006569455s
Mar  9 17:10:01.842: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  9 17:10:01.842: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[pod1:[100]] 03/09/23 17:10:01.845
Mar  9 17:10:01.854: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9735 03/09/23 17:10:01.854
Mar  9 17:10:01.860: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9735" to be "running and ready"
Mar  9 17:10:01.863: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188347ms
Mar  9 17:10:01.863: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:10:03.866: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006805854s
Mar  9 17:10:03.866: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  9 17:10:03.866: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[pod1:[100] pod2:[101]] 03/09/23 17:10:03.87
Mar  9 17:10:03.882: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/09/23 17:10:03.882
Mar  9 17:10:03.882: INFO: Creating new exec pod
Mar  9 17:10:03.886: INFO: Waiting up to 5m0s for pod "execpodlv7dm" in namespace "services-9735" to be "running"
Mar  9 17:10:03.890: INFO: Pod "execpodlv7dm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587772ms
Mar  9 17:10:05.893: INFO: Pod "execpodlv7dm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007138676s
Mar  9 17:10:05.893: INFO: Pod "execpodlv7dm" satisfied condition "running"
Mar  9 17:10:06.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar  9 17:10:07.044: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar  9 17:10:07.044: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:10:07.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.234.133 80'
Mar  9 17:10:07.207: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.234.133 80\nConnection to 10.110.234.133 80 port [tcp/http] succeeded!\n"
Mar  9 17:10:07.207: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:10:07.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar  9 17:10:07.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar  9 17:10:07.356: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:10:07.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.234.133 81'
Mar  9 17:10:07.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.234.133 81\nConnection to 10.110.234.133 81 port [tcp/*] succeeded!\n"
Mar  9 17:10:07.500: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9735 03/09/23 17:10:07.5
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[pod2:[101]] 03/09/23 17:10:07.517
Mar  9 17:10:07.530: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9735 03/09/23 17:10:07.53
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[] 03/09/23 17:10:07.544
Mar  9 17:10:07.556: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 17:10:07.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9735" for this suite. 03/09/23 17:10:07.582
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":313,"skipped":5911,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.806 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:09:58.781
    Mar  9 17:09:58.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 17:09:58.782
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:09:58.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:09:58.799
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-9735 03/09/23 17:09:58.803
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[] 03/09/23 17:09:58.819
    Mar  9 17:09:58.823: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Mar  9 17:09:59.829: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9735 03/09/23 17:09:59.829
    Mar  9 17:09:59.835: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9735" to be "running and ready"
    Mar  9 17:09:59.839: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.42978ms
    Mar  9 17:09:59.839: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:10:01.842: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006569455s
    Mar  9 17:10:01.842: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  9 17:10:01.842: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[pod1:[100]] 03/09/23 17:10:01.845
    Mar  9 17:10:01.854: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9735 03/09/23 17:10:01.854
    Mar  9 17:10:01.860: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9735" to be "running and ready"
    Mar  9 17:10:01.863: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188347ms
    Mar  9 17:10:01.863: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:10:03.866: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006805854s
    Mar  9 17:10:03.866: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  9 17:10:03.866: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[pod1:[100] pod2:[101]] 03/09/23 17:10:03.87
    Mar  9 17:10:03.882: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/09/23 17:10:03.882
    Mar  9 17:10:03.882: INFO: Creating new exec pod
    Mar  9 17:10:03.886: INFO: Waiting up to 5m0s for pod "execpodlv7dm" in namespace "services-9735" to be "running"
    Mar  9 17:10:03.890: INFO: Pod "execpodlv7dm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587772ms
    Mar  9 17:10:05.893: INFO: Pod "execpodlv7dm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007138676s
    Mar  9 17:10:05.893: INFO: Pod "execpodlv7dm" satisfied condition "running"
    Mar  9 17:10:06.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Mar  9 17:10:07.044: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar  9 17:10:07.044: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:10:07.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.234.133 80'
    Mar  9 17:10:07.207: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.234.133 80\nConnection to 10.110.234.133 80 port [tcp/http] succeeded!\n"
    Mar  9 17:10:07.207: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:10:07.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Mar  9 17:10:07.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar  9 17:10:07.356: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:10:07.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-9735 exec execpodlv7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.234.133 81'
    Mar  9 17:10:07.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.234.133 81\nConnection to 10.110.234.133 81 port [tcp/*] succeeded!\n"
    Mar  9 17:10:07.500: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9735 03/09/23 17:10:07.5
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[pod2:[101]] 03/09/23 17:10:07.517
    Mar  9 17:10:07.530: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9735 03/09/23 17:10:07.53
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9735 to expose endpoints map[] 03/09/23 17:10:07.544
    Mar  9 17:10:07.556: INFO: successfully validated that service multi-endpoint-test in namespace services-9735 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 17:10:07.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9735" for this suite. 03/09/23 17:10:07.582
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:10:07.588
Mar  9 17:10:07.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename deployment 03/09/23 17:10:07.59
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:07.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:07.606
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/09/23 17:10:07.612
Mar  9 17:10:07.613: INFO: Creating simple deployment test-deployment-r4qpq
Mar  9 17:10:07.625: INFO: deployment "test-deployment-r4qpq" doesn't have the required revision set
STEP: Getting /status 03/09/23 17:10:09.636
Mar  9 17:10:09.640: INFO: Deployment test-deployment-r4qpq has Conditions: [{Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 03/09/23 17:10:09.64
Mar  9 17:10:09.648: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 10, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 10, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 10, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 10, 7, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-r4qpq-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/09/23 17:10:09.648
Mar  9 17:10:09.651: INFO: Observed &Deployment event: ADDED
Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
Mar  9 17:10:09.651: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  9 17:10:09.651: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4qpq-777898ffcc" is progressing.}
Mar  9 17:10:09.651: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
Mar  9 17:10:09.652: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.652: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  9 17:10:09.652: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
Mar  9 17:10:09.652: INFO: Found Deployment test-deployment-r4qpq in namespace deployment-298 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  9 17:10:09.652: INFO: Deployment test-deployment-r4qpq has an updated status
STEP: patching the Statefulset Status 03/09/23 17:10:09.652
Mar  9 17:10:09.652: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  9 17:10:09.657: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/09/23 17:10:09.657
Mar  9 17:10:09.662: INFO: Observed &Deployment event: ADDED
Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
Mar  9 17:10:09.662: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  9 17:10:09.662: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4qpq-777898ffcc" is progressing.}
Mar  9 17:10:09.663: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
Mar  9 17:10:09.663: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  9 17:10:09.663: INFO: Observed &Deployment event: MODIFIED
Mar  9 17:10:09.663: INFO: Found deployment test-deployment-r4qpq in namespace deployment-298 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar  9 17:10:09.663: INFO: Deployment test-deployment-r4qpq has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  9 17:10:09.667: INFO: Deployment "test-deployment-r4qpq":
&Deployment{ObjectMeta:{test-deployment-r4qpq  deployment-298  f0bf9cfc-e95f-440a-bb2b-627a1c2f64de 120668 1 2023-03-09 17:10:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-09 17:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-09 17:10:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-09 17:10:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007710a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-r4qpq-777898ffcc",LastUpdateTime:2023-03-09 17:10:09 +0000 UTC,LastTransitionTime:2023-03-09 17:10:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  9 17:10:09.670: INFO: New ReplicaSet "test-deployment-r4qpq-777898ffcc" of Deployment "test-deployment-r4qpq":
&ReplicaSet{ObjectMeta:{test-deployment-r4qpq-777898ffcc  deployment-298  07c5d81c-7289-47e2-b13b-d1af9ac1cf86 120659 1 2023-03-09 17:10:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-r4qpq f0bf9cfc-e95f-440a-bb2b-627a1c2f64de 0xc007816490 0xc007816491}] [] [{kube-controller-manager Update apps/v1 2023-03-09 17:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0bf9cfc-e95f-440a-bb2b-627a1c2f64de\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 17:10:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007816538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  9 17:10:09.673: INFO: Pod "test-deployment-r4qpq-777898ffcc-qlr8r" is available:
&Pod{ObjectMeta:{test-deployment-r4qpq-777898ffcc-qlr8r test-deployment-r4qpq-777898ffcc- deployment-298  67001f14-3e84-4c94-8689-c7e39d29bbe3 120658 0 2023-03-09 17:10:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:dee8e153ee673e3929a772c414052938837b5cdadfe29854e0782f648dc56840 cni.projectcalico.org/podIP:10.244.42.222/32 cni.projectcalico.org/podIPs:10.244.42.222/32] [{apps/v1 ReplicaSet test-deployment-r4qpq-777898ffcc 07c5d81c-7289-47e2-b13b-d1af9ac1cf86 0xc007816900 0xc007816901}] [] [{kube-controller-manager Update v1 2023-03-09 17:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07c5d81c-7289-47e2-b13b-d1af9ac1cf86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 17:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 17:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k77hk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k77hk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.222,StartTime:2023-03-09 17:10:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 17:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4a45d0ad5ceb18d3c67f2ca2a36572a4f7259dbc52c9c9b3cf4eac11393997cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  9 17:10:09.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-298" for this suite. 03/09/23 17:10:09.677
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":314,"skipped":5914,"failed":0}
------------------------------
â€¢ [2.093 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:10:07.588
    Mar  9 17:10:07.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename deployment 03/09/23 17:10:07.59
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:07.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:07.606
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/09/23 17:10:07.612
    Mar  9 17:10:07.613: INFO: Creating simple deployment test-deployment-r4qpq
    Mar  9 17:10:07.625: INFO: deployment "test-deployment-r4qpq" doesn't have the required revision set
    STEP: Getting /status 03/09/23 17:10:09.636
    Mar  9 17:10:09.640: INFO: Deployment test-deployment-r4qpq has Conditions: [{Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 03/09/23 17:10:09.64
    Mar  9 17:10:09.648: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 10, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 10, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 10, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 10, 7, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-r4qpq-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/09/23 17:10:09.648
    Mar  9 17:10:09.651: INFO: Observed &Deployment event: ADDED
    Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
    Mar  9 17:10:09.651: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
    Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  9 17:10:09.651: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4qpq-777898ffcc" is progressing.}
    Mar  9 17:10:09.651: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  9 17:10:09.651: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
    Mar  9 17:10:09.652: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.652: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  9 17:10:09.652: INFO: Observed Deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
    Mar  9 17:10:09.652: INFO: Found Deployment test-deployment-r4qpq in namespace deployment-298 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  9 17:10:09.652: INFO: Deployment test-deployment-r4qpq has an updated status
    STEP: patching the Statefulset Status 03/09/23 17:10:09.652
    Mar  9 17:10:09.652: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  9 17:10:09.657: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/09/23 17:10:09.657
    Mar  9 17:10:09.662: INFO: Observed &Deployment event: ADDED
    Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
    Mar  9 17:10:09.662: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4qpq-777898ffcc"}
    Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  9 17:10:09.662: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  9 17:10:09.662: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:07 +0000 UTC 2023-03-09 17:10:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4qpq-777898ffcc" is progressing.}
    Mar  9 17:10:09.663: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
    Mar  9 17:10:09.663: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-09 17:10:08 +0000 UTC 2023-03-09 17:10:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4qpq-777898ffcc" has successfully progressed.}
    Mar  9 17:10:09.663: INFO: Observed deployment test-deployment-r4qpq in namespace deployment-298 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  9 17:10:09.663: INFO: Observed &Deployment event: MODIFIED
    Mar  9 17:10:09.663: INFO: Found deployment test-deployment-r4qpq in namespace deployment-298 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar  9 17:10:09.663: INFO: Deployment test-deployment-r4qpq has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  9 17:10:09.667: INFO: Deployment "test-deployment-r4qpq":
    &Deployment{ObjectMeta:{test-deployment-r4qpq  deployment-298  f0bf9cfc-e95f-440a-bb2b-627a1c2f64de 120668 1 2023-03-09 17:10:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-09 17:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-09 17:10:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-09 17:10:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007710a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-r4qpq-777898ffcc",LastUpdateTime:2023-03-09 17:10:09 +0000 UTC,LastTransitionTime:2023-03-09 17:10:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  9 17:10:09.670: INFO: New ReplicaSet "test-deployment-r4qpq-777898ffcc" of Deployment "test-deployment-r4qpq":
    &ReplicaSet{ObjectMeta:{test-deployment-r4qpq-777898ffcc  deployment-298  07c5d81c-7289-47e2-b13b-d1af9ac1cf86 120659 1 2023-03-09 17:10:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-r4qpq f0bf9cfc-e95f-440a-bb2b-627a1c2f64de 0xc007816490 0xc007816491}] [] [{kube-controller-manager Update apps/v1 2023-03-09 17:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0bf9cfc-e95f-440a-bb2b-627a1c2f64de\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-09 17:10:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007816538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  9 17:10:09.673: INFO: Pod "test-deployment-r4qpq-777898ffcc-qlr8r" is available:
    &Pod{ObjectMeta:{test-deployment-r4qpq-777898ffcc-qlr8r test-deployment-r4qpq-777898ffcc- deployment-298  67001f14-3e84-4c94-8689-c7e39d29bbe3 120658 0 2023-03-09 17:10:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:dee8e153ee673e3929a772c414052938837b5cdadfe29854e0782f648dc56840 cni.projectcalico.org/podIP:10.244.42.222/32 cni.projectcalico.org/podIPs:10.244.42.222/32] [{apps/v1 ReplicaSet test-deployment-r4qpq-777898ffcc 07c5d81c-7289-47e2-b13b-d1af9ac1cf86 0xc007816900 0xc007816901}] [] [{kube-controller-manager Update v1 2023-03-09 17:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07c5d81c-7289-47e2-b13b-d1af9ac1cf86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-09 17:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-09 17:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.42.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k77hk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k77hk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:tt-test-el8-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-09 17:10:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.230.140,PodIP:10.244.42.222,StartTime:2023-03-09 17:10:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-09 17:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4a45d0ad5ceb18d3c67f2ca2a36572a4f7259dbc52c9c9b3cf4eac11393997cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.42.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  9 17:10:09.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-298" for this suite. 03/09/23 17:10:09.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:10:09.689
Mar  9 17:10:09.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replicaset 03/09/23 17:10:09.69
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:09.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:09.708
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar  9 17:10:09.719: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  9 17:10:14.722: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/09/23 17:10:14.722
STEP: Scaling up "test-rs" replicaset  03/09/23 17:10:14.722
Mar  9 17:10:14.730: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/09/23 17:10:14.73
W0309 17:10:14.738789      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  9 17:10:14.740: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
Mar  9 17:10:14.751: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
Mar  9 17:10:14.763: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
Mar  9 17:10:14.776: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
Mar  9 17:10:15.876: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 2, AvailableReplicas 2
Mar  9 17:10:15.905: INFO: observed Replicaset test-rs in namespace replicaset-9212 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  9 17:10:15.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9212" for this suite. 03/09/23 17:10:15.909
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":315,"skipped":5982,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.225 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:10:09.689
    Mar  9 17:10:09.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replicaset 03/09/23 17:10:09.69
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:09.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:09.708
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar  9 17:10:09.719: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  9 17:10:14.722: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/09/23 17:10:14.722
    STEP: Scaling up "test-rs" replicaset  03/09/23 17:10:14.722
    Mar  9 17:10:14.730: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/09/23 17:10:14.73
    W0309 17:10:14.738789      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  9 17:10:14.740: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
    Mar  9 17:10:14.751: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
    Mar  9 17:10:14.763: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
    Mar  9 17:10:14.776: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 1, AvailableReplicas 1
    Mar  9 17:10:15.876: INFO: observed ReplicaSet test-rs in namespace replicaset-9212 with ReadyReplicas 2, AvailableReplicas 2
    Mar  9 17:10:15.905: INFO: observed Replicaset test-rs in namespace replicaset-9212 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  9 17:10:15.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9212" for this suite. 03/09/23 17:10:15.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:10:15.916
Mar  9 17:10:15.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 17:10:15.918
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:15.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:15.932
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-d88c5f3f-c21b-4292-9d8c-5fc047e0f482 03/09/23 17:10:15.935
STEP: Creating a pod to test consume configMaps 03/09/23 17:10:15.939
Mar  9 17:10:15.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237" in namespace "configmap-5116" to be "Succeeded or Failed"
Mar  9 17:10:15.948: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527894ms
Mar  9 17:10:17.952: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006244794s
Mar  9 17:10:19.952: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006535111s
STEP: Saw pod success 03/09/23 17:10:19.952
Mar  9 17:10:19.952: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237" satisfied condition "Succeeded or Failed"
Mar  9 17:10:19.954: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 17:10:19.96
Mar  9 17:10:19.969: INFO: Waiting for pod pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237 to disappear
Mar  9 17:10:19.971: INFO: Pod pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 17:10:19.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5116" for this suite. 03/09/23 17:10:19.975
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":316,"skipped":6026,"failed":0}
------------------------------
â€¢ [4.064 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:10:15.916
    Mar  9 17:10:15.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 17:10:15.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:15.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:15.932
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-d88c5f3f-c21b-4292-9d8c-5fc047e0f482 03/09/23 17:10:15.935
    STEP: Creating a pod to test consume configMaps 03/09/23 17:10:15.939
    Mar  9 17:10:15.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237" in namespace "configmap-5116" to be "Succeeded or Failed"
    Mar  9 17:10:15.948: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527894ms
    Mar  9 17:10:17.952: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006244794s
    Mar  9 17:10:19.952: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006535111s
    STEP: Saw pod success 03/09/23 17:10:19.952
    Mar  9 17:10:19.952: INFO: Pod "pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237" satisfied condition "Succeeded or Failed"
    Mar  9 17:10:19.954: INFO: Trying to get logs from node tt-test-el8-003 pod pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 17:10:19.96
    Mar  9 17:10:19.969: INFO: Waiting for pod pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237 to disappear
    Mar  9 17:10:19.971: INFO: Pod pod-configmaps-f17e83d0-71aa-4d5b-9315-7f5f9a902237 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 17:10:19.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5116" for this suite. 03/09/23 17:10:19.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:10:19.981
Mar  9 17:10:19.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 17:10:19.982
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:19.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:19.995
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/09/23 17:10:20.002
Mar  9 17:10:20.009: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7119" to be "running and ready"
Mar  9 17:10:20.012: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.563513ms
Mar  9 17:10:20.012: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:10:22.015: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005992644s
Mar  9 17:10:22.015: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  9 17:10:22.015: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 03/09/23 17:10:22.018
Mar  9 17:10:22.023: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7119" to be "running and ready"
Mar  9 17:10:22.025: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340125ms
Mar  9 17:10:22.026: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:10:24.029: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006364006s
Mar  9 17:10:24.030: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar  9 17:10:24.030: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/09/23 17:10:24.032
STEP: delete the pod with lifecycle hook 03/09/23 17:10:24.045
Mar  9 17:10:24.051: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  9 17:10:24.055: INFO: Pod pod-with-poststart-http-hook still exists
Mar  9 17:10:26.055: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  9 17:10:26.059: INFO: Pod pod-with-poststart-http-hook still exists
Mar  9 17:10:28.056: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  9 17:10:28.060: INFO: Pod pod-with-poststart-http-hook still exists
Mar  9 17:10:30.056: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  9 17:10:30.059: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  9 17:10:30.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7119" for this suite. 03/09/23 17:10:30.063
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":317,"skipped":6037,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.090 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:10:19.981
    Mar  9 17:10:19.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/09/23 17:10:19.982
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:19.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:19.995
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/09/23 17:10:20.002
    Mar  9 17:10:20.009: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7119" to be "running and ready"
    Mar  9 17:10:20.012: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.563513ms
    Mar  9 17:10:20.012: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:10:22.015: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005992644s
    Mar  9 17:10:22.015: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  9 17:10:22.015: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 03/09/23 17:10:22.018
    Mar  9 17:10:22.023: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7119" to be "running and ready"
    Mar  9 17:10:22.025: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340125ms
    Mar  9 17:10:22.026: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:10:24.029: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006364006s
    Mar  9 17:10:24.030: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar  9 17:10:24.030: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/09/23 17:10:24.032
    STEP: delete the pod with lifecycle hook 03/09/23 17:10:24.045
    Mar  9 17:10:24.051: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  9 17:10:24.055: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  9 17:10:26.055: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  9 17:10:26.059: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  9 17:10:28.056: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  9 17:10:28.060: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  9 17:10:30.056: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  9 17:10:30.059: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  9 17:10:30.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7119" for this suite. 03/09/23 17:10:30.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:10:30.073
Mar  9 17:10:30.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubectl 03/09/23 17:10:30.074
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:30.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:30.089
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 03/09/23 17:10:30.092
Mar  9 17:10:30.092: INFO: namespace kubectl-5806
Mar  9 17:10:30.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 create -f -'
Mar  9 17:10:30.896: INFO: stderr: ""
Mar  9 17:10:30.896: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/09/23 17:10:30.896
Mar  9 17:10:31.900: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 17:10:31.900: INFO: Found 0 / 1
Mar  9 17:10:32.899: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 17:10:32.899: INFO: Found 1 / 1
Mar  9 17:10:32.899: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  9 17:10:32.902: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  9 17:10:32.902: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  9 17:10:32.902: INFO: wait on agnhost-primary startup in kubectl-5806 
Mar  9 17:10:32.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 logs agnhost-primary-v79rs agnhost-primary'
Mar  9 17:10:32.978: INFO: stderr: ""
Mar  9 17:10:32.978: INFO: stdout: "Paused\n"
STEP: exposing RC 03/09/23 17:10:32.978
Mar  9 17:10:32.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  9 17:10:33.067: INFO: stderr: ""
Mar  9 17:10:33.067: INFO: stdout: "service/rm2 exposed\n"
Mar  9 17:10:33.071: INFO: Service rm2 in namespace kubectl-5806 found.
STEP: exposing service 03/09/23 17:10:35.079
Mar  9 17:10:35.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  9 17:10:35.170: INFO: stderr: ""
Mar  9 17:10:35.170: INFO: stdout: "service/rm3 exposed\n"
Mar  9 17:10:35.174: INFO: Service rm3 in namespace kubectl-5806 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  9 17:10:37.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5806" for this suite. 03/09/23 17:10:37.184
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":318,"skipped":6065,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.115 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:10:30.073
    Mar  9 17:10:30.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubectl 03/09/23 17:10:30.074
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:30.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:30.089
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 03/09/23 17:10:30.092
    Mar  9 17:10:30.092: INFO: namespace kubectl-5806
    Mar  9 17:10:30.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 create -f -'
    Mar  9 17:10:30.896: INFO: stderr: ""
    Mar  9 17:10:30.896: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/09/23 17:10:30.896
    Mar  9 17:10:31.900: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 17:10:31.900: INFO: Found 0 / 1
    Mar  9 17:10:32.899: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 17:10:32.899: INFO: Found 1 / 1
    Mar  9 17:10:32.899: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  9 17:10:32.902: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  9 17:10:32.902: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  9 17:10:32.902: INFO: wait on agnhost-primary startup in kubectl-5806 
    Mar  9 17:10:32.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 logs agnhost-primary-v79rs agnhost-primary'
    Mar  9 17:10:32.978: INFO: stderr: ""
    Mar  9 17:10:32.978: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/09/23 17:10:32.978
    Mar  9 17:10:32.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar  9 17:10:33.067: INFO: stderr: ""
    Mar  9 17:10:33.067: INFO: stdout: "service/rm2 exposed\n"
    Mar  9 17:10:33.071: INFO: Service rm2 in namespace kubectl-5806 found.
    STEP: exposing service 03/09/23 17:10:35.079
    Mar  9 17:10:35.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=kubectl-5806 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar  9 17:10:35.170: INFO: stderr: ""
    Mar  9 17:10:35.170: INFO: stdout: "service/rm3 exposed\n"
    Mar  9 17:10:35.174: INFO: Service rm3 in namespace kubectl-5806 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  9 17:10:37.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5806" for this suite. 03/09/23 17:10:37.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:10:37.189
Mar  9 17:10:37.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 17:10:37.19
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:37.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:37.206
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8402 03/09/23 17:10:37.209
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-8402 03/09/23 17:10:37.214
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8402 03/09/23 17:10:37.218
Mar  9 17:10:37.222: INFO: Found 0 stateful pods, waiting for 1
Mar  9 17:10:47.225: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/09/23 17:10:47.226
Mar  9 17:10:47.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 17:10:47.379: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 17:10:47.379: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 17:10:47.379: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 17:10:47.382: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  9 17:10:57.388: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 17:10:57.388: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 17:10:57.404: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar  9 17:10:57.404: INFO: ss-0  tt-test-el8-003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  }]
Mar  9 17:10:57.404: INFO: 
Mar  9 17:10:57.404: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  9 17:10:58.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994021283s
Mar  9 17:10:59.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99013541s
Mar  9 17:11:00.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985769782s
Mar  9 17:11:01.421: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981530517s
Mar  9 17:11:02.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977625702s
Mar  9 17:11:03.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973189541s
Mar  9 17:11:04.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968595609s
Mar  9 17:11:05.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964291504s
Mar  9 17:11:06.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.185863ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8402 03/09/23 17:11:07.443
Mar  9 17:11:07.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 17:11:07.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  9 17:11:07.595: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 17:11:07.595: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  9 17:11:07.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 17:11:07.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  9 17:11:07.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 17:11:07.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  9 17:11:07.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  9 17:11:07.895: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  9 17:11:07.895: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  9 17:11:07.895: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  9 17:11:07.898: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar  9 17:11:17.903: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 17:11:17.903: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 17:11:17.903: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/09/23 17:11:17.903
Mar  9 17:11:17.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 17:11:18.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 17:11:18.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 17:11:18.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 17:11:18.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 17:11:18.200: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 17:11:18.200: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 17:11:18.200: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 17:11:18.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  9 17:11:18.346: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  9 17:11:18.346: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  9 17:11:18.346: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  9 17:11:18.346: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 17:11:18.349: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  9 17:11:28.356: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 17:11:28.356: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 17:11:28.356: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  9 17:11:28.366: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar  9 17:11:28.366: INFO: ss-0  tt-test-el8-003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  }]
Mar  9 17:11:28.366: INFO: ss-1  tt-test-el8-004  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  }]
Mar  9 17:11:28.366: INFO: ss-2  tt-test-el8-003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  }]
Mar  9 17:11:28.366: INFO: 
Mar  9 17:11:28.366: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  9 17:11:29.369: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.996660805s
Mar  9 17:11:30.372: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993191843s
Mar  9 17:11:31.376: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98974923s
Mar  9 17:11:32.379: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986412532s
Mar  9 17:11:33.382: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983201022s
Mar  9 17:11:34.386: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.979352793s
Mar  9 17:11:35.390: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975357893s
Mar  9 17:11:36.394: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.971312219s
Mar  9 17:11:37.398: INFO: Verifying statefulset ss doesn't scale past 0 for another 967.26663ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8402 03/09/23 17:11:38.398
Mar  9 17:11:38.402: INFO: Scaling statefulset ss to 0
Mar  9 17:11:38.411: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 17:11:38.413: INFO: Deleting all statefulset in ns statefulset-8402
Mar  9 17:11:38.415: INFO: Scaling statefulset ss to 0
Mar  9 17:11:38.423: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 17:11:38.425: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 17:11:38.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8402" for this suite. 03/09/23 17:11:38.44
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":319,"skipped":6080,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.255 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:10:37.189
    Mar  9 17:10:37.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 17:10:37.19
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:10:37.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:10:37.206
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8402 03/09/23 17:10:37.209
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-8402 03/09/23 17:10:37.214
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8402 03/09/23 17:10:37.218
    Mar  9 17:10:37.222: INFO: Found 0 stateful pods, waiting for 1
    Mar  9 17:10:47.225: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/09/23 17:10:47.226
    Mar  9 17:10:47.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 17:10:47.379: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 17:10:47.379: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 17:10:47.379: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 17:10:47.382: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  9 17:10:57.388: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 17:10:57.388: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 17:10:57.404: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
    Mar  9 17:10:57.404: INFO: ss-0  tt-test-el8-003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  }]
    Mar  9 17:10:57.404: INFO: 
    Mar  9 17:10:57.404: INFO: StatefulSet ss has not reached scale 3, at 1
    Mar  9 17:10:58.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994021283s
    Mar  9 17:10:59.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99013541s
    Mar  9 17:11:00.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985769782s
    Mar  9 17:11:01.421: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981530517s
    Mar  9 17:11:02.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977625702s
    Mar  9 17:11:03.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973189541s
    Mar  9 17:11:04.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968595609s
    Mar  9 17:11:05.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964291504s
    Mar  9 17:11:06.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.185863ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8402 03/09/23 17:11:07.443
    Mar  9 17:11:07.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 17:11:07.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  9 17:11:07.595: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 17:11:07.595: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  9 17:11:07.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 17:11:07.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  9 17:11:07.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 17:11:07.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  9 17:11:07.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  9 17:11:07.895: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  9 17:11:07.895: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  9 17:11:07.895: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  9 17:11:07.898: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Mar  9 17:11:17.903: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 17:11:17.903: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 17:11:17.903: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/09/23 17:11:17.903
    Mar  9 17:11:17.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 17:11:18.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 17:11:18.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 17:11:18.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 17:11:18.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 17:11:18.200: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 17:11:18.200: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 17:11:18.200: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 17:11:18.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=statefulset-8402 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  9 17:11:18.346: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  9 17:11:18.346: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  9 17:11:18.346: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  9 17:11:18.346: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 17:11:18.349: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar  9 17:11:28.356: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 17:11:28.356: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 17:11:28.356: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  9 17:11:28.366: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
    Mar  9 17:11:28.366: INFO: ss-0  tt-test-el8-003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:37 +0000 UTC  }]
    Mar  9 17:11:28.366: INFO: ss-1  tt-test-el8-004  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  }]
    Mar  9 17:11:28.366: INFO: ss-2  tt-test-el8-003  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:11:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-09 17:10:57 +0000 UTC  }]
    Mar  9 17:11:28.366: INFO: 
    Mar  9 17:11:28.366: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar  9 17:11:29.369: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.996660805s
    Mar  9 17:11:30.372: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993191843s
    Mar  9 17:11:31.376: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98974923s
    Mar  9 17:11:32.379: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986412532s
    Mar  9 17:11:33.382: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983201022s
    Mar  9 17:11:34.386: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.979352793s
    Mar  9 17:11:35.390: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975357893s
    Mar  9 17:11:36.394: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.971312219s
    Mar  9 17:11:37.398: INFO: Verifying statefulset ss doesn't scale past 0 for another 967.26663ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8402 03/09/23 17:11:38.398
    Mar  9 17:11:38.402: INFO: Scaling statefulset ss to 0
    Mar  9 17:11:38.411: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 17:11:38.413: INFO: Deleting all statefulset in ns statefulset-8402
    Mar  9 17:11:38.415: INFO: Scaling statefulset ss to 0
    Mar  9 17:11:38.423: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 17:11:38.425: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 17:11:38.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8402" for this suite. 03/09/23 17:11:38.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:11:38.445
Mar  9 17:11:38.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename taint-multiple-pods 03/09/23 17:11:38.446
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:11:38.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:11:38.461
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar  9 17:11:38.464: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  9 17:12:38.503: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Mar  9 17:12:38.506: INFO: Starting informer...
STEP: Starting pods... 03/09/23 17:12:38.506
Mar  9 17:12:38.722: INFO: Pod1 is running on tt-test-el8-003. Tainting Node
Mar  9 17:12:38.931: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2458" to be "running"
Mar  9 17:12:38.933: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613113ms
Mar  9 17:12:40.938: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007040483s
Mar  9 17:12:40.938: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar  9 17:12:40.938: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2458" to be "running"
Mar  9 17:12:40.941: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.626701ms
Mar  9 17:12:40.941: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar  9 17:12:40.941: INFO: Pod2 is running on tt-test-el8-003. Tainting Node
STEP: Trying to apply a taint on the Node 03/09/23 17:12:40.941
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 17:12:40.954
STEP: Waiting for Pod1 and Pod2 to be deleted 03/09/23 17:12:40.957
Mar  9 17:12:47.315: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  9 17:13:07.380: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 17:13:07.393
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Mar  9 17:13:07.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2458" for this suite. 03/09/23 17:13:07.401
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":320,"skipped":6085,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.962 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:11:38.445
    Mar  9 17:11:38.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename taint-multiple-pods 03/09/23 17:11:38.446
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:11:38.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:11:38.461
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Mar  9 17:11:38.464: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  9 17:12:38.503: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Mar  9 17:12:38.506: INFO: Starting informer...
    STEP: Starting pods... 03/09/23 17:12:38.506
    Mar  9 17:12:38.722: INFO: Pod1 is running on tt-test-el8-003. Tainting Node
    Mar  9 17:12:38.931: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2458" to be "running"
    Mar  9 17:12:38.933: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613113ms
    Mar  9 17:12:40.938: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007040483s
    Mar  9 17:12:40.938: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar  9 17:12:40.938: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2458" to be "running"
    Mar  9 17:12:40.941: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.626701ms
    Mar  9 17:12:40.941: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar  9 17:12:40.941: INFO: Pod2 is running on tt-test-el8-003. Tainting Node
    STEP: Trying to apply a taint on the Node 03/09/23 17:12:40.941
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 17:12:40.954
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/09/23 17:12:40.957
    Mar  9 17:12:47.315: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar  9 17:13:07.380: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/09/23 17:13:07.393
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 17:13:07.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-2458" for this suite. 03/09/23 17:13:07.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:13:07.409
Mar  9 17:13:07.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 17:13:07.41
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:07.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:07.443
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 03/09/23 17:13:07.446
Mar  9 17:13:07.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: mark a version not serverd 03/09/23 17:13:16.294
STEP: check the unserved version gets removed 03/09/23 17:13:16.313
STEP: check the other version is not changed 03/09/23 17:13:20.211
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:13:28.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6998" for this suite. 03/09/23 17:13:28.563
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":321,"skipped":6114,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.158 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:13:07.409
    Mar  9 17:13:07.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename crd-publish-openapi 03/09/23 17:13:07.41
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:07.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:07.443
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 03/09/23 17:13:07.446
    Mar  9 17:13:07.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: mark a version not serverd 03/09/23 17:13:16.294
    STEP: check the unserved version gets removed 03/09/23 17:13:16.313
    STEP: check the other version is not changed 03/09/23 17:13:20.211
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:13:28.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6998" for this suite. 03/09/23 17:13:28.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:13:28.569
Mar  9 17:13:28.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename pod-network-test 03/09/23 17:13:28.57
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:28.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:28.584
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1775 03/09/23 17:13:28.587
STEP: creating a selector 03/09/23 17:13:28.587
STEP: Creating the service pods in kubernetes 03/09/23 17:13:28.587
Mar  9 17:13:28.587: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  9 17:13:28.608: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1775" to be "running and ready"
Mar  9 17:13:28.613: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.53918ms
Mar  9 17:13:28.613: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:13:30.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008618779s
Mar  9 17:13:30.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:32.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00892764s
Mar  9 17:13:32.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:34.616: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008106084s
Mar  9 17:13:34.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:36.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008955429s
Mar  9 17:13:36.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:38.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008877169s
Mar  9 17:13:38.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:40.616: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007999839s
Mar  9 17:13:40.616: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:42.616: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007973962s
Mar  9 17:13:42.616: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:44.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008468611s
Mar  9 17:13:44.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:46.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008519903s
Mar  9 17:13:46.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:48.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008278863s
Mar  9 17:13:48.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  9 17:13:50.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008697447s
Mar  9 17:13:50.617: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  9 17:13:50.617: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  9 17:13:50.619: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1775" to be "running and ready"
Mar  9 17:13:50.622: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.489239ms
Mar  9 17:13:50.622: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  9 17:13:50.622: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 03/09/23 17:13:50.625
Mar  9 17:13:50.629: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1775" to be "running"
Mar  9 17:13:50.632: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264116ms
Mar  9 17:13:52.636: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006115051s
Mar  9 17:13:52.636: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  9 17:13:52.638: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar  9 17:13:52.638: INFO: Breadth first check of 10.244.42.245 on host 100.100.230.140...
Mar  9 17:13:52.641: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.203:9080/dial?request=hostname&protocol=udp&host=10.244.42.245&port=8081&tries=1'] Namespace:pod-network-test-1775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 17:13:52.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 17:13:52.642: INFO: ExecWithOptions: Clientset creation
Mar  9 17:13:52.642: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.203%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.42.245%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  9 17:13:52.722: INFO: Waiting for responses: map[]
Mar  9 17:13:52.722: INFO: reached 10.244.42.245 after 0/1 tries
Mar  9 17:13:52.722: INFO: Breadth first check of 10.244.88.206 on host 100.100.231.104...
Mar  9 17:13:52.730: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.203:9080/dial?request=hostname&protocol=udp&host=10.244.88.206&port=8081&tries=1'] Namespace:pod-network-test-1775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  9 17:13:52.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
Mar  9 17:13:52.731: INFO: ExecWithOptions: Clientset creation
Mar  9 17:13:52.731: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.203%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.88.206%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  9 17:13:52.815: INFO: Waiting for responses: map[]
Mar  9 17:13:52.815: INFO: reached 10.244.88.206 after 0/1 tries
Mar  9 17:13:52.815: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  9 17:13:52.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1775" for this suite. 03/09/23 17:13:52.82
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":322,"skipped":6135,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.256 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:13:28.569
    Mar  9 17:13:28.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename pod-network-test 03/09/23 17:13:28.57
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:28.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:28.584
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1775 03/09/23 17:13:28.587
    STEP: creating a selector 03/09/23 17:13:28.587
    STEP: Creating the service pods in kubernetes 03/09/23 17:13:28.587
    Mar  9 17:13:28.587: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  9 17:13:28.608: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1775" to be "running and ready"
    Mar  9 17:13:28.613: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.53918ms
    Mar  9 17:13:28.613: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:13:30.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008618779s
    Mar  9 17:13:30.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:32.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00892764s
    Mar  9 17:13:32.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:34.616: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008106084s
    Mar  9 17:13:34.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:36.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008955429s
    Mar  9 17:13:36.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:38.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008877169s
    Mar  9 17:13:38.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:40.616: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007999839s
    Mar  9 17:13:40.616: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:42.616: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007973962s
    Mar  9 17:13:42.616: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:44.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008468611s
    Mar  9 17:13:44.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:46.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008519903s
    Mar  9 17:13:46.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:48.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008278863s
    Mar  9 17:13:48.617: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  9 17:13:50.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008697447s
    Mar  9 17:13:50.617: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  9 17:13:50.617: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  9 17:13:50.619: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1775" to be "running and ready"
    Mar  9 17:13:50.622: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.489239ms
    Mar  9 17:13:50.622: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  9 17:13:50.622: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 03/09/23 17:13:50.625
    Mar  9 17:13:50.629: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1775" to be "running"
    Mar  9 17:13:50.632: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264116ms
    Mar  9 17:13:52.636: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006115051s
    Mar  9 17:13:52.636: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  9 17:13:52.638: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Mar  9 17:13:52.638: INFO: Breadth first check of 10.244.42.245 on host 100.100.230.140...
    Mar  9 17:13:52.641: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.203:9080/dial?request=hostname&protocol=udp&host=10.244.42.245&port=8081&tries=1'] Namespace:pod-network-test-1775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 17:13:52.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 17:13:52.642: INFO: ExecWithOptions: Clientset creation
    Mar  9 17:13:52.642: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.203%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.42.245%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  9 17:13:52.722: INFO: Waiting for responses: map[]
    Mar  9 17:13:52.722: INFO: reached 10.244.42.245 after 0/1 tries
    Mar  9 17:13:52.722: INFO: Breadth first check of 10.244.88.206 on host 100.100.231.104...
    Mar  9 17:13:52.730: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.42.203:9080/dial?request=hostname&protocol=udp&host=10.244.88.206&port=8081&tries=1'] Namespace:pod-network-test-1775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  9 17:13:52.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    Mar  9 17:13:52.731: INFO: ExecWithOptions: Clientset creation
    Mar  9 17:13:52.731: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.42.203%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.88.206%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  9 17:13:52.815: INFO: Waiting for responses: map[]
    Mar  9 17:13:52.815: INFO: reached 10.244.88.206 after 0/1 tries
    Mar  9 17:13:52.815: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  9 17:13:52.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1775" for this suite. 03/09/23 17:13:52.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:13:52.826
Mar  9 17:13:52.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 17:13:52.828
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:52.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:52.843
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 03/09/23 17:13:52.846
Mar  9 17:13:52.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239" in namespace "projected-1710" to be "Succeeded or Failed"
Mar  9 17:13:52.855: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297616ms
Mar  9 17:13:54.859: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006211105s
Mar  9 17:13:56.860: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007622148s
STEP: Saw pod success 03/09/23 17:13:56.86
Mar  9 17:13:56.860: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239" satisfied condition "Succeeded or Failed"
Mar  9 17:13:56.863: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239 container client-container: <nil>
STEP: delete the pod 03/09/23 17:13:56.877
Mar  9 17:13:56.887: INFO: Waiting for pod downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239 to disappear
Mar  9 17:13:56.890: INFO: Pod downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  9 17:13:56.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1710" for this suite. 03/09/23 17:13:56.894
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":323,"skipped":6159,"failed":0}
------------------------------
â€¢ [4.072 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:13:52.826
    Mar  9 17:13:52.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 17:13:52.828
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:52.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:52.843
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 03/09/23 17:13:52.846
    Mar  9 17:13:52.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239" in namespace "projected-1710" to be "Succeeded or Failed"
    Mar  9 17:13:52.855: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297616ms
    Mar  9 17:13:54.859: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006211105s
    Mar  9 17:13:56.860: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007622148s
    STEP: Saw pod success 03/09/23 17:13:56.86
    Mar  9 17:13:56.860: INFO: Pod "downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239" satisfied condition "Succeeded or Failed"
    Mar  9 17:13:56.863: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239 container client-container: <nil>
    STEP: delete the pod 03/09/23 17:13:56.877
    Mar  9 17:13:56.887: INFO: Waiting for pod downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239 to disappear
    Mar  9 17:13:56.890: INFO: Pod downwardapi-volume-446e6d4f-7a04-4b4d-9a0d-d97150786239 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  9 17:13:56.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1710" for this suite. 03/09/23 17:13:56.894
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:13:56.898
Mar  9 17:13:56.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename daemonsets 03/09/23 17:13:56.9
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:56.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:56.914
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Mar  9 17:13:56.930: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 17:13:56.935
Mar  9 17:13:56.938: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:13:56.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 17:13:56.941: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 17:13:57.945: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:13:57.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 17:13:57.948: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 17:13:58.945: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:13:58.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 17:13:58.948: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 03/09/23 17:13:58.958
STEP: Check that daemon pods images are updated. 03/09/23 17:13:58.968
Mar  9 17:13:58.971: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  9 17:13:58.971: INFO: Wrong image for pod: daemon-set-lpdvn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  9 17:13:58.975: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:13:59.979: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  9 17:13:59.982: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:14:00.980: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  9 17:14:00.983: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:14:01.979: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  9 17:14:01.979: INFO: Pod daemon-set-zkpp2 is not available
Mar  9 17:14:01.985: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:14:02.983: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:14:03.979: INFO: Pod daemon-set-69gkj is not available
Mar  9 17:14:03.983: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 03/09/23 17:14:03.983
Mar  9 17:14:03.987: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:14:03.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 17:14:03.989: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 17:14:04.994: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:14:04.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 17:14:04.998: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/09/23 17:14:05.011
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8664, will wait for the garbage collector to delete the pods 03/09/23 17:14:05.011
Mar  9 17:14:05.070: INFO: Deleting DaemonSet.extensions daemon-set took: 5.798022ms
Mar  9 17:14:05.171: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.039771ms
Mar  9 17:14:07.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 17:14:07.575: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  9 17:14:07.577: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"121962"},"items":null}

Mar  9 17:14:07.579: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"121962"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  9 17:14:07.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8664" for this suite. 03/09/23 17:14:07.592
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":324,"skipped":6161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.698 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:13:56.898
    Mar  9 17:13:56.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename daemonsets 03/09/23 17:13:56.9
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:13:56.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:13:56.914
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Mar  9 17:13:56.930: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 17:13:56.935
    Mar  9 17:13:56.938: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:13:56.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 17:13:56.941: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 17:13:57.945: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:13:57.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 17:13:57.948: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 17:13:58.945: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:13:58.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 17:13:58.948: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 03/09/23 17:13:58.958
    STEP: Check that daemon pods images are updated. 03/09/23 17:13:58.968
    Mar  9 17:13:58.971: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  9 17:13:58.971: INFO: Wrong image for pod: daemon-set-lpdvn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  9 17:13:58.975: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:13:59.979: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  9 17:13:59.982: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:14:00.980: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  9 17:14:00.983: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:14:01.979: INFO: Wrong image for pod: daemon-set-b96lt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  9 17:14:01.979: INFO: Pod daemon-set-zkpp2 is not available
    Mar  9 17:14:01.985: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:14:02.983: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:14:03.979: INFO: Pod daemon-set-69gkj is not available
    Mar  9 17:14:03.983: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 03/09/23 17:14:03.983
    Mar  9 17:14:03.987: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:14:03.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 17:14:03.989: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 17:14:04.994: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:14:04.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 17:14:04.998: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/09/23 17:14:05.011
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8664, will wait for the garbage collector to delete the pods 03/09/23 17:14:05.011
    Mar  9 17:14:05.070: INFO: Deleting DaemonSet.extensions daemon-set took: 5.798022ms
    Mar  9 17:14:05.171: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.039771ms
    Mar  9 17:14:07.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 17:14:07.575: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  9 17:14:07.577: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"121962"},"items":null}

    Mar  9 17:14:07.579: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"121962"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 17:14:07.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8664" for this suite. 03/09/23 17:14:07.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:14:07.599
Mar  9 17:14:07.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 17:14:07.6
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:14:07.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:14:07.615
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/09/23 17:14:07.618
Mar  9 17:14:07.625: INFO: Waiting up to 5m0s for pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a" in namespace "emptydir-5989" to be "Succeeded or Failed"
Mar  9 17:14:07.627: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293062ms
Mar  9 17:14:09.631: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006320451s
Mar  9 17:14:11.632: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006715026s
STEP: Saw pod success 03/09/23 17:14:11.632
Mar  9 17:14:11.632: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a" satisfied condition "Succeeded or Failed"
Mar  9 17:14:11.634: INFO: Trying to get logs from node tt-test-el8-003 pod pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a container test-container: <nil>
STEP: delete the pod 03/09/23 17:14:11.64
Mar  9 17:14:11.648: INFO: Waiting for pod pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a to disappear
Mar  9 17:14:11.650: INFO: Pod pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 17:14:11.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5989" for this suite. 03/09/23 17:14:11.653
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":325,"skipped":6179,"failed":0}
------------------------------
â€¢ [4.062 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:14:07.599
    Mar  9 17:14:07.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 17:14:07.6
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:14:07.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:14:07.615
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/09/23 17:14:07.618
    Mar  9 17:14:07.625: INFO: Waiting up to 5m0s for pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a" in namespace "emptydir-5989" to be "Succeeded or Failed"
    Mar  9 17:14:07.627: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293062ms
    Mar  9 17:14:09.631: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006320451s
    Mar  9 17:14:11.632: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006715026s
    STEP: Saw pod success 03/09/23 17:14:11.632
    Mar  9 17:14:11.632: INFO: Pod "pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a" satisfied condition "Succeeded or Failed"
    Mar  9 17:14:11.634: INFO: Trying to get logs from node tt-test-el8-003 pod pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a container test-container: <nil>
    STEP: delete the pod 03/09/23 17:14:11.64
    Mar  9 17:14:11.648: INFO: Waiting for pod pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a to disappear
    Mar  9 17:14:11.650: INFO: Pod pod-1223d2a2-6815-4da6-aba8-c5fe4b27f96a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 17:14:11.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5989" for this suite. 03/09/23 17:14:11.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:14:11.661
Mar  9 17:14:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-preemption 03/09/23 17:14:11.663
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:14:11.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:14:11.676
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  9 17:14:11.690: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  9 17:15:11.721: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 03/09/23 17:15:11.724
Mar  9 17:15:11.746: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  9 17:15:11.752: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  9 17:15:11.769: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  9 17:15:11.774: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/09/23 17:15:11.774
Mar  9 17:15:11.774: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9873" to be "running"
Mar  9 17:15:11.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509217ms
Mar  9 17:15:13.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007669125s
Mar  9 17:15:15.784: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009109393s
Mar  9 17:15:17.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007666476s
Mar  9 17:15:19.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007211943s
Mar  9 17:15:21.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.007581857s
Mar  9 17:15:21.782: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  9 17:15:21.782: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9873" to be "running"
Mar  9 17:15:21.785: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.557876ms
Mar  9 17:15:21.785: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  9 17:15:21.785: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9873" to be "running"
Mar  9 17:15:21.787: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.533331ms
Mar  9 17:15:21.787: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  9 17:15:21.787: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9873" to be "running"
Mar  9 17:15:21.790: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.289486ms
Mar  9 17:15:21.790: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/09/23 17:15:21.79
Mar  9 17:15:21.795: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9873" to be "running"
Mar  9 17:15:21.799: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066037ms
Mar  9 17:15:23.803: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007944614s
Mar  9 17:15:25.804: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009619693s
Mar  9 17:15:27.802: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007793124s
Mar  9 17:15:27.802: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  9 17:15:27.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9873" for this suite. 03/09/23 17:15:27.819
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":326,"skipped":6185,"failed":0}
------------------------------
â€¢ [SLOW TEST] [76.190 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:14:11.661
    Mar  9 17:14:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-preemption 03/09/23 17:14:11.663
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:14:11.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:14:11.676
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  9 17:14:11.690: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  9 17:15:11.721: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 03/09/23 17:15:11.724
    Mar  9 17:15:11.746: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  9 17:15:11.752: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  9 17:15:11.769: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  9 17:15:11.774: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/09/23 17:15:11.774
    Mar  9 17:15:11.774: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9873" to be "running"
    Mar  9 17:15:11.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509217ms
    Mar  9 17:15:13.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007669125s
    Mar  9 17:15:15.784: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009109393s
    Mar  9 17:15:17.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007666476s
    Mar  9 17:15:19.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007211943s
    Mar  9 17:15:21.782: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.007581857s
    Mar  9 17:15:21.782: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  9 17:15:21.782: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9873" to be "running"
    Mar  9 17:15:21.785: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.557876ms
    Mar  9 17:15:21.785: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  9 17:15:21.785: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9873" to be "running"
    Mar  9 17:15:21.787: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.533331ms
    Mar  9 17:15:21.787: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  9 17:15:21.787: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9873" to be "running"
    Mar  9 17:15:21.790: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.289486ms
    Mar  9 17:15:21.790: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/09/23 17:15:21.79
    Mar  9 17:15:21.795: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9873" to be "running"
    Mar  9 17:15:21.799: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066037ms
    Mar  9 17:15:23.803: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007944614s
    Mar  9 17:15:25.804: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009619693s
    Mar  9 17:15:27.802: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007793124s
    Mar  9 17:15:27.802: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 17:15:27.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9873" for this suite. 03/09/23 17:15:27.819
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:15:27.854
Mar  9 17:15:27.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 17:15:27.856
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:15:27.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:15:27.869
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-102 03/09/23 17:15:27.872
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/09/23 17:15:27.887
STEP: creating service externalsvc in namespace services-102 03/09/23 17:15:27.888
STEP: creating replication controller externalsvc in namespace services-102 03/09/23 17:15:27.91
I0309 17:15:27.918626      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-102, replica count: 2
I0309 17:15:30.969747      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/09/23 17:15:30.972
Mar  9 17:15:30.991: INFO: Creating new exec pod
Mar  9 17:15:30.997: INFO: Waiting up to 5m0s for pod "execpodnnc7v" in namespace "services-102" to be "running"
Mar  9 17:15:31.001: INFO: Pod "execpodnnc7v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316142ms
Mar  9 17:15:33.006: INFO: Pod "execpodnnc7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008175795s
Mar  9 17:15:33.006: INFO: Pod "execpodnnc7v" satisfied condition "running"
Mar  9 17:15:33.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-102 exec execpodnnc7v -- /bin/sh -x -c nslookup clusterip-service.services-102.svc.cluster.local'
Mar  9 17:15:33.226: INFO: stderr: "+ nslookup clusterip-service.services-102.svc.cluster.local\n"
Mar  9 17:15:33.226: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-102.svc.cluster.local\tcanonical name = externalsvc.services-102.svc.cluster.local.\nName:\texternalsvc.services-102.svc.cluster.local\nAddress: 10.107.134.161\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-102, will wait for the garbage collector to delete the pods 03/09/23 17:15:33.227
Mar  9 17:15:33.286: INFO: Deleting ReplicationController externalsvc took: 5.1862ms
Mar  9 17:15:33.387: INFO: Terminating ReplicationController externalsvc pods took: 101.323603ms
Mar  9 17:15:35.806: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 17:15:35.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-102" for this suite. 03/09/23 17:15:35.822
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":327,"skipped":6217,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.974 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:15:27.854
    Mar  9 17:15:27.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 17:15:27.856
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:15:27.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:15:27.869
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-102 03/09/23 17:15:27.872
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/09/23 17:15:27.887
    STEP: creating service externalsvc in namespace services-102 03/09/23 17:15:27.888
    STEP: creating replication controller externalsvc in namespace services-102 03/09/23 17:15:27.91
    I0309 17:15:27.918626      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-102, replica count: 2
    I0309 17:15:30.969747      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/09/23 17:15:30.972
    Mar  9 17:15:30.991: INFO: Creating new exec pod
    Mar  9 17:15:30.997: INFO: Waiting up to 5m0s for pod "execpodnnc7v" in namespace "services-102" to be "running"
    Mar  9 17:15:31.001: INFO: Pod "execpodnnc7v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316142ms
    Mar  9 17:15:33.006: INFO: Pod "execpodnnc7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008175795s
    Mar  9 17:15:33.006: INFO: Pod "execpodnnc7v" satisfied condition "running"
    Mar  9 17:15:33.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-102 exec execpodnnc7v -- /bin/sh -x -c nslookup clusterip-service.services-102.svc.cluster.local'
    Mar  9 17:15:33.226: INFO: stderr: "+ nslookup clusterip-service.services-102.svc.cluster.local\n"
    Mar  9 17:15:33.226: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-102.svc.cluster.local\tcanonical name = externalsvc.services-102.svc.cluster.local.\nName:\texternalsvc.services-102.svc.cluster.local\nAddress: 10.107.134.161\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-102, will wait for the garbage collector to delete the pods 03/09/23 17:15:33.227
    Mar  9 17:15:33.286: INFO: Deleting ReplicationController externalsvc took: 5.1862ms
    Mar  9 17:15:33.387: INFO: Terminating ReplicationController externalsvc pods took: 101.323603ms
    Mar  9 17:15:35.806: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 17:15:35.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-102" for this suite. 03/09/23 17:15:35.822
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:15:35.831
Mar  9 17:15:35.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 17:15:35.832
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:15:35.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:15:35.849
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3488 03/09/23 17:15:35.852
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Mar  9 17:15:35.871: INFO: Found 0 stateful pods, waiting for 1
Mar  9 17:15:45.876: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/09/23 17:15:45.881
W0309 17:15:45.887800      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  9 17:15:45.893: INFO: Found 1 stateful pods, waiting for 2
Mar  9 17:15:55.899: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 17:15:55.899: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/09/23 17:15:55.905
STEP: Delete all of the StatefulSets 03/09/23 17:15:55.907
STEP: Verify that StatefulSets have been deleted 03/09/23 17:15:55.913
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 17:15:55.916: INFO: Deleting all statefulset in ns statefulset-3488
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 17:15:55.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3488" for this suite. 03/09/23 17:15:55.932
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":328,"skipped":6272,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.107 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:15:35.831
    Mar  9 17:15:35.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 17:15:35.832
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:15:35.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:15:35.849
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3488 03/09/23 17:15:35.852
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Mar  9 17:15:35.871: INFO: Found 0 stateful pods, waiting for 1
    Mar  9 17:15:45.876: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/09/23 17:15:45.881
    W0309 17:15:45.887800      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  9 17:15:45.893: INFO: Found 1 stateful pods, waiting for 2
    Mar  9 17:15:55.899: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 17:15:55.899: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/09/23 17:15:55.905
    STEP: Delete all of the StatefulSets 03/09/23 17:15:55.907
    STEP: Verify that StatefulSets have been deleted 03/09/23 17:15:55.913
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 17:15:55.916: INFO: Deleting all statefulset in ns statefulset-3488
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 17:15:55.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3488" for this suite. 03/09/23 17:15:55.932
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:15:55.939
Mar  9 17:15:55.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename init-container 03/09/23 17:15:55.94
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:15:55.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:15:55.96
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 03/09/23 17:15:55.963
Mar  9 17:15:55.963: INFO: PodSpec: initContainers in spec.initContainers
Mar  9 17:16:34.951: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8dc4a0be-8731-4161-9719-4314caba95fc", GenerateName:"", Namespace:"init-container-4232", SelfLink:"", UID:"978c8b2c-3fb5-49a0-8589-9a6b1c04331a", ResourceVersion:"122741", Generation:0, CreationTimestamp:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"963167153"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"3bcef21d0804962839f6af0d2c74061b63089b9452353e6db9375e458aea68b2", "cni.projectcalico.org/podIP":"10.244.42.228/32", "cni.projectcalico.org/podIPs":"10.244.42.228/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009619e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 9, 17, 15, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000961a10), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 9, 17, 16, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000961a40), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-g44k8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00651a5e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g44k8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g44k8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g44k8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003773430), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"tt-test-el8-003", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00064c930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037734d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037734f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0037734f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0037734fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012bfd40), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"100.100.230.140", PodIP:"10.244.42.228", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.42.228"}}, StartTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064cc40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064ce00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://bc79248aafaea1734c9031cc95b6ff3b62fbc57558dea2b835e74969b86296b1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00651a660), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00651a640), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc003773574)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  9 17:16:34.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4232" for this suite. 03/09/23 17:16:34.956
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":329,"skipped":6275,"failed":0}
------------------------------
â€¢ [SLOW TEST] [39.022 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:15:55.939
    Mar  9 17:15:55.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename init-container 03/09/23 17:15:55.94
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:15:55.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:15:55.96
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 03/09/23 17:15:55.963
    Mar  9 17:15:55.963: INFO: PodSpec: initContainers in spec.initContainers
    Mar  9 17:16:34.951: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8dc4a0be-8731-4161-9719-4314caba95fc", GenerateName:"", Namespace:"init-container-4232", SelfLink:"", UID:"978c8b2c-3fb5-49a0-8589-9a6b1c04331a", ResourceVersion:"122741", Generation:0, CreationTimestamp:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"963167153"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"3bcef21d0804962839f6af0d2c74061b63089b9452353e6db9375e458aea68b2", "cni.projectcalico.org/podIP":"10.244.42.228/32", "cni.projectcalico.org/podIPs":"10.244.42.228/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009619e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 9, 17, 15, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000961a10), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 9, 17, 16, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000961a40), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-g44k8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00651a5e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g44k8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g44k8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g44k8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003773430), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"tt-test-el8-003", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00064c930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037734d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037734f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0037734f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0037734fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012bfd40), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"100.100.230.140", PodIP:"10.244.42.228", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.42.228"}}, StartTime:time.Date(2023, time.March, 9, 17, 15, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064cc40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064ce00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://bc79248aafaea1734c9031cc95b6ff3b62fbc57558dea2b835e74969b86296b1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00651a660), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00651a640), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc003773574)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  9 17:16:34.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4232" for this suite. 03/09/23 17:16:34.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:34.964
Mar  9 17:16:34.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename security-context-test 03/09/23 17:16:34.966
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:34.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:34.98
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Mar  9 17:16:34.989: INFO: Waiting up to 5m0s for pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092" in namespace "security-context-test-6427" to be "Succeeded or Failed"
Mar  9 17:16:34.992: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.512235ms
Mar  9 17:16:36.996: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006737097s
Mar  9 17:16:38.996: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006683493s
Mar  9 17:16:38.996: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  9 17:16:38.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6427" for this suite. 03/09/23 17:16:39
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":330,"skipped":6303,"failed":0}
------------------------------
â€¢ [4.041 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:34.964
    Mar  9 17:16:34.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename security-context-test 03/09/23 17:16:34.966
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:34.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:34.98
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Mar  9 17:16:34.989: INFO: Waiting up to 5m0s for pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092" in namespace "security-context-test-6427" to be "Succeeded or Failed"
    Mar  9 17:16:34.992: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.512235ms
    Mar  9 17:16:36.996: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006737097s
    Mar  9 17:16:38.996: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006683493s
    Mar  9 17:16:38.996: INFO: Pod "busybox-user-65534-36b3caa7-efe2-47fc-a52f-fefc25086092" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  9 17:16:38.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6427" for this suite. 03/09/23 17:16:39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:39.006
Mar  9 17:16:39.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 17:16:39.007
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:39.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:39.022
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 03/09/23 17:16:39.025
Mar  9 17:16:39.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d" in namespace "downward-api-6544" to be "Succeeded or Failed"
Mar  9 17:16:39.035: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695565ms
Mar  9 17:16:41.041: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008563793s
Mar  9 17:16:43.039: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006957103s
STEP: Saw pod success 03/09/23 17:16:43.04
Mar  9 17:16:43.040: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d" satisfied condition "Succeeded or Failed"
Mar  9 17:16:43.042: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d container client-container: <nil>
STEP: delete the pod 03/09/23 17:16:43.057
Mar  9 17:16:43.065: INFO: Waiting for pod downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d to disappear
Mar  9 17:16:43.067: INFO: Pod downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  9 17:16:43.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6544" for this suite. 03/09/23 17:16:43.071
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":331,"skipped":6310,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:39.006
    Mar  9 17:16:39.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 17:16:39.007
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:39.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:39.022
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 03/09/23 17:16:39.025
    Mar  9 17:16:39.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d" in namespace "downward-api-6544" to be "Succeeded or Failed"
    Mar  9 17:16:39.035: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695565ms
    Mar  9 17:16:41.041: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008563793s
    Mar  9 17:16:43.039: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006957103s
    STEP: Saw pod success 03/09/23 17:16:43.04
    Mar  9 17:16:43.040: INFO: Pod "downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d" satisfied condition "Succeeded or Failed"
    Mar  9 17:16:43.042: INFO: Trying to get logs from node tt-test-el8-003 pod downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d container client-container: <nil>
    STEP: delete the pod 03/09/23 17:16:43.057
    Mar  9 17:16:43.065: INFO: Waiting for pod downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d to disappear
    Mar  9 17:16:43.067: INFO: Pod downwardapi-volume-1093e4fb-952e-4445-8cad-5b1c62e9767d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  9 17:16:43.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6544" for this suite. 03/09/23 17:16:43.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:43.079
Mar  9 17:16:43.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replicaset 03/09/23 17:16:43.08
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:43.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:43.094
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/09/23 17:16:43.096
STEP: Verify that the required pods have come up 03/09/23 17:16:43.1
Mar  9 17:16:43.103: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar  9 17:16:48.106: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/09/23 17:16:48.107
Mar  9 17:16:48.109: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/09/23 17:16:48.109
STEP: DeleteCollection of the ReplicaSets 03/09/23 17:16:48.112
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/09/23 17:16:48.118
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  9 17:16:48.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2110" for this suite. 03/09/23 17:16:48.13
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":332,"skipped":6356,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.058 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:43.079
    Mar  9 17:16:43.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replicaset 03/09/23 17:16:43.08
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:43.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:43.094
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/09/23 17:16:43.096
    STEP: Verify that the required pods have come up 03/09/23 17:16:43.1
    Mar  9 17:16:43.103: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar  9 17:16:48.106: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/09/23 17:16:48.107
    Mar  9 17:16:48.109: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/09/23 17:16:48.109
    STEP: DeleteCollection of the ReplicaSets 03/09/23 17:16:48.112
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/09/23 17:16:48.118
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  9 17:16:48.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2110" for this suite. 03/09/23 17:16:48.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:48.137
Mar  9 17:16:48.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 17:16:48.138
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:48.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:48.162
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 03/09/23 17:16:48.167
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 17:16:48.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9749" for this suite. 03/09/23 17:16:48.174
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":333,"skipped":6361,"failed":0}
------------------------------
â€¢ [0.042 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:48.137
    Mar  9 17:16:48.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 17:16:48.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:48.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:48.162
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 03/09/23 17:16:48.167
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 17:16:48.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9749" for this suite. 03/09/23 17:16:48.174
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:48.18
Mar  9 17:16:48.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename projected 03/09/23 17:16:48.182
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:48.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:48.195
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-7dfd5251-7276-4c13-8716-ce4ea6909344 03/09/23 17:16:48.198
STEP: Creating a pod to test consume configMaps 03/09/23 17:16:48.201
Mar  9 17:16:48.209: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23" in namespace "projected-3953" to be "Succeeded or Failed"
Mar  9 17:16:48.212: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.375987ms
Mar  9 17:16:50.215: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005719382s
Mar  9 17:16:52.216: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006855887s
STEP: Saw pod success 03/09/23 17:16:52.216
Mar  9 17:16:52.217: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23" satisfied condition "Succeeded or Failed"
Mar  9 17:16:52.219: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 17:16:52.224
Mar  9 17:16:52.236: INFO: Waiting for pod pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23 to disappear
Mar  9 17:16:52.239: INFO: Pod pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  9 17:16:52.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3953" for this suite. 03/09/23 17:16:52.242
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6382,"failed":0}
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:48.18
    Mar  9 17:16:48.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename projected 03/09/23 17:16:48.182
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:48.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:48.195
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-7dfd5251-7276-4c13-8716-ce4ea6909344 03/09/23 17:16:48.198
    STEP: Creating a pod to test consume configMaps 03/09/23 17:16:48.201
    Mar  9 17:16:48.209: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23" in namespace "projected-3953" to be "Succeeded or Failed"
    Mar  9 17:16:48.212: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.375987ms
    Mar  9 17:16:50.215: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005719382s
    Mar  9 17:16:52.216: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006855887s
    STEP: Saw pod success 03/09/23 17:16:52.216
    Mar  9 17:16:52.217: INFO: Pod "pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23" satisfied condition "Succeeded or Failed"
    Mar  9 17:16:52.219: INFO: Trying to get logs from node tt-test-el8-003 pod pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 17:16:52.224
    Mar  9 17:16:52.236: INFO: Waiting for pod pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23 to disappear
    Mar  9 17:16:52.239: INFO: Pod pod-projected-configmaps-a1e631b3-fcba-4cb9-be13-4f44efde8a23 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  9 17:16:52.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3953" for this suite. 03/09/23 17:16:52.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:52.249
Mar  9 17:16:52.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename dns 03/09/23 17:16:52.25
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:52.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:52.265
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/09/23 17:16:52.268
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/09/23 17:16:52.274
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/09/23 17:16:52.274
STEP: creating a pod to probe DNS 03/09/23 17:16:52.274
STEP: submitting the pod to kubernetes 03/09/23 17:16:52.275
Mar  9 17:16:52.284: INFO: Waiting up to 15m0s for pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d" in namespace "dns-1684" to be "running"
Mar  9 17:16:52.287: INFO: Pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.667955ms
Mar  9 17:16:54.291: INFO: Pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006888621s
Mar  9 17:16:54.291: INFO: Pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d" satisfied condition "running"
STEP: retrieving the pod 03/09/23 17:16:54.291
STEP: looking for the results for each expected name from probers 03/09/23 17:16:54.294
Mar  9 17:16:54.307: INFO: DNS probes using dns-1684/dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d succeeded

STEP: deleting the pod 03/09/23 17:16:54.307
STEP: deleting the test headless service 03/09/23 17:16:54.321
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  9 17:16:54.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1684" for this suite. 03/09/23 17:16:54.34
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":335,"skipped":6393,"failed":0}
------------------------------
â€¢ [2.096 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:52.249
    Mar  9 17:16:52.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename dns 03/09/23 17:16:52.25
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:52.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:52.265
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/09/23 17:16:52.268
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/09/23 17:16:52.274
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1684.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/09/23 17:16:52.274
    STEP: creating a pod to probe DNS 03/09/23 17:16:52.274
    STEP: submitting the pod to kubernetes 03/09/23 17:16:52.275
    Mar  9 17:16:52.284: INFO: Waiting up to 15m0s for pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d" in namespace "dns-1684" to be "running"
    Mar  9 17:16:52.287: INFO: Pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.667955ms
    Mar  9 17:16:54.291: INFO: Pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006888621s
    Mar  9 17:16:54.291: INFO: Pod "dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d" satisfied condition "running"
    STEP: retrieving the pod 03/09/23 17:16:54.291
    STEP: looking for the results for each expected name from probers 03/09/23 17:16:54.294
    Mar  9 17:16:54.307: INFO: DNS probes using dns-1684/dns-test-9e1c1a72-9620-4005-a56a-c9e8ca34d49d succeeded

    STEP: deleting the pod 03/09/23 17:16:54.307
    STEP: deleting the test headless service 03/09/23 17:16:54.321
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  9 17:16:54.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1684" for this suite. 03/09/23 17:16:54.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:54.347
Mar  9 17:16:54.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sched-pred 03/09/23 17:16:54.348
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:54.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:54.362
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  9 17:16:54.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  9 17:16:54.373: INFO: Waiting for terminating namespaces to be deleted...
Mar  9 17:16:54.375: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-003 before test
Mar  9 17:16:54.382: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.382: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 17:16:54.382: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.382: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 17:16:54.382: INFO: csi-node-driver-ggtjb from calico-system started at 2023-03-09 17:13:07 +0000 UTC (2 container statuses recorded)
Mar  9 17:16:54.382: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 17:16:54.382: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 17:16:54.382: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.382: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 17:16:54.382: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.382: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  9 17:16:54.382: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 17:16:54.382: INFO: 	Container e2e ready: true, restart count 0
Mar  9 17:16:54.382: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 17:16:54.382: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 17:16:54.382: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 17:16:54.382: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 17:16:54.382: INFO: 
Logging pods the apiserver thinks is on node tt-test-el8-004 before test
Mar  9 17:16:54.390: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container calico-apiserver ready: true, restart count 0
Mar  9 17:16:54.390: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  9 17:16:54.390: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container calico-node ready: true, restart count 0
Mar  9 17:16:54.390: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container calico-typha ready: true, restart count 0
Mar  9 17:16:54.390: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container calico-csi ready: true, restart count 0
Mar  9 17:16:54.390: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Mar  9 17:16:54.390: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container webhook ready: true, restart count 0
Mar  9 17:16:54.390: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container coredns ready: true, restart count 0
Mar  9 17:16:54.390: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container coredns ready: true, restart count 0
Mar  9 17:16:54.390: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  9 17:16:54.390: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  9 17:16:54.390: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  9 17:16:54.390: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  9 17:16:54.390: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
Mar  9 17:16:54.390: INFO: 	Container tigera-operator ready: true, restart count 3
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/09/23 17:16:54.39
Mar  9 17:16:54.399: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5365" to be "running"
Mar  9 17:16:54.401: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407254ms
Mar  9 17:16:56.405: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005971072s
Mar  9 17:16:56.405: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/09/23 17:16:56.407
STEP: Trying to apply a random label on the found node. 03/09/23 17:16:56.417
STEP: verifying the node has the label kubernetes.io/e2e-92ec29f0-9676-4674-a63d-b8d851e2ac42 42 03/09/23 17:16:56.426
STEP: Trying to relaunch the pod, now with labels. 03/09/23 17:16:56.43
Mar  9 17:16:56.435: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5365" to be "not pending"
Mar  9 17:16:56.437: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.34835ms
Mar  9 17:16:58.441: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.006599558s
Mar  9 17:16:58.442: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-92ec29f0-9676-4674-a63d-b8d851e2ac42 off the node tt-test-el8-003 03/09/23 17:16:58.444
STEP: verifying the node doesn't have the label kubernetes.io/e2e-92ec29f0-9676-4674-a63d-b8d851e2ac42 03/09/23 17:16:58.457
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  9 17:16:58.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5365" for this suite. 03/09/23 17:16:58.464
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":336,"skipped":6417,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:54.347
    Mar  9 17:16:54.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sched-pred 03/09/23 17:16:54.348
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:54.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:54.362
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  9 17:16:54.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  9 17:16:54.373: INFO: Waiting for terminating namespaces to be deleted...
    Mar  9 17:16:54.375: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-003 before test
    Mar  9 17:16:54.382: INFO: calico-node-wh8hs from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.382: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: calico-typha-7cd8bd454-gbhkv from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.382: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: csi-node-driver-ggtjb from calico-system started at 2023-03-09 17:13:07 +0000 UTC (2 container statuses recorded)
    Mar  9 17:16:54.382: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: kube-proxy-k95qd from kube-system started at 2023-03-09 03:22:11 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.382: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: sonobuoy from sonobuoy started at 2023-03-09 15:46:35 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.382: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: sonobuoy-e2e-job-975b039fb38f48d3 from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 17:16:54.382: INFO: 	Container e2e ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-gr4pp from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 17:16:54.382: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 17:16:54.382: INFO: 
    Logging pods the apiserver thinks is on node tt-test-el8-004 before test
    Mar  9 17:16:54.390: INFO: calico-apiserver-7c747f5cd5-b5vxx from calico-apiserver started at 2023-03-09 03:23:53 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container calico-apiserver ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: calico-kube-controllers-764fd57778-m668z from calico-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: calico-node-2bvv5 from calico-system started at 2023-03-09 03:23:20 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container calico-node ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: calico-typha-7cd8bd454-fdn6r from calico-system started at 2023-03-09 03:23:27 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: csi-node-driver-mmnqh from calico-system started at 2023-03-09 03:23:31 +0000 UTC (2 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container calico-csi ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: externalip-validation-webhook-76d97fbd6-96c5g from externalip-validation-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container webhook ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: coredns-7b86c745f6-dj4rw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: coredns-7b86c745f6-gblnw from kube-system started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container coredns ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: kube-proxy-hrgxt from kube-system started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: kubernetes-dashboard-5c84574c69-4r4nv from kubernetes-dashboard started at 2023-03-09 03:23:31 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: sonobuoy-systemd-logs-daemon-set-9f27e9f5d831441d-ctz6t from sonobuoy started at 2023-03-09 15:46:36 +0000 UTC (2 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  9 17:16:54.390: INFO: tigera-operator-7cbc46df58-t82qm from tigera-operator started at 2023-03-09 03:22:24 +0000 UTC (1 container statuses recorded)
    Mar  9 17:16:54.390: INFO: 	Container tigera-operator ready: true, restart count 3
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/09/23 17:16:54.39
    Mar  9 17:16:54.399: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5365" to be "running"
    Mar  9 17:16:54.401: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407254ms
    Mar  9 17:16:56.405: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005971072s
    Mar  9 17:16:56.405: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/09/23 17:16:56.407
    STEP: Trying to apply a random label on the found node. 03/09/23 17:16:56.417
    STEP: verifying the node has the label kubernetes.io/e2e-92ec29f0-9676-4674-a63d-b8d851e2ac42 42 03/09/23 17:16:56.426
    STEP: Trying to relaunch the pod, now with labels. 03/09/23 17:16:56.43
    Mar  9 17:16:56.435: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5365" to be "not pending"
    Mar  9 17:16:56.437: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.34835ms
    Mar  9 17:16:58.441: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.006599558s
    Mar  9 17:16:58.442: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-92ec29f0-9676-4674-a63d-b8d851e2ac42 off the node tt-test-el8-003 03/09/23 17:16:58.444
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-92ec29f0-9676-4674-a63d-b8d851e2ac42 03/09/23 17:16:58.457
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 17:16:58.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5365" for this suite. 03/09/23 17:16:58.464
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:16:58.473
Mar  9 17:16:58.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename downward-api 03/09/23 17:16:58.474
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:58.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:58.487
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 03/09/23 17:16:58.49
Mar  9 17:16:58.498: INFO: Waiting up to 5m0s for pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df" in namespace "downward-api-6644" to be "Succeeded or Failed"
Mar  9 17:16:58.500: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479119ms
Mar  9 17:17:00.504: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006109982s
Mar  9 17:17:02.504: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006612318s
STEP: Saw pod success 03/09/23 17:17:02.504
Mar  9 17:17:02.504: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df" satisfied condition "Succeeded or Failed"
Mar  9 17:17:02.507: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-71486e94-38ff-4056-a6c2-dca95baa94df container dapi-container: <nil>
STEP: delete the pod 03/09/23 17:17:02.515
Mar  9 17:17:02.525: INFO: Waiting for pod downward-api-71486e94-38ff-4056-a6c2-dca95baa94df to disappear
Mar  9 17:17:02.527: INFO: Pod downward-api-71486e94-38ff-4056-a6c2-dca95baa94df no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  9 17:17:02.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6644" for this suite. 03/09/23 17:17:02.531
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":337,"skipped":6441,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:16:58.473
    Mar  9 17:16:58.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename downward-api 03/09/23 17:16:58.474
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:16:58.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:16:58.487
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 03/09/23 17:16:58.49
    Mar  9 17:16:58.498: INFO: Waiting up to 5m0s for pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df" in namespace "downward-api-6644" to be "Succeeded or Failed"
    Mar  9 17:16:58.500: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479119ms
    Mar  9 17:17:00.504: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006109982s
    Mar  9 17:17:02.504: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006612318s
    STEP: Saw pod success 03/09/23 17:17:02.504
    Mar  9 17:17:02.504: INFO: Pod "downward-api-71486e94-38ff-4056-a6c2-dca95baa94df" satisfied condition "Succeeded or Failed"
    Mar  9 17:17:02.507: INFO: Trying to get logs from node tt-test-el8-003 pod downward-api-71486e94-38ff-4056-a6c2-dca95baa94df container dapi-container: <nil>
    STEP: delete the pod 03/09/23 17:17:02.515
    Mar  9 17:17:02.525: INFO: Waiting for pod downward-api-71486e94-38ff-4056-a6c2-dca95baa94df to disappear
    Mar  9 17:17:02.527: INFO: Pod downward-api-71486e94-38ff-4056-a6c2-dca95baa94df no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  9 17:17:02.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6644" for this suite. 03/09/23 17:17:02.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:02.536
Mar  9 17:17:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename container-probe 03/09/23 17:17:02.538
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:02.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:02.554
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c in namespace container-probe-8375 03/09/23 17:17:02.557
Mar  9 17:17:02.563: INFO: Waiting up to 5m0s for pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c" in namespace "container-probe-8375" to be "not pending"
Mar  9 17:17:02.565: INFO: Pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269501ms
Mar  9 17:17:04.568: INFO: Pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005351738s
Mar  9 17:17:04.568: INFO: Pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c" satisfied condition "not pending"
Mar  9 17:17:04.568: INFO: Started pod liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c in namespace container-probe-8375
STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 17:17:04.568
Mar  9 17:17:04.571: INFO: Initial restart count of pod liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c is 0
Mar  9 17:17:24.612: INFO: Restart count of pod container-probe-8375/liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c is now 1 (20.041548775s elapsed)
STEP: deleting the pod 03/09/23 17:17:24.612
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  9 17:17:24.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8375" for this suite. 03/09/23 17:17:24.628
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":338,"skipped":6446,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.096 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:02.536
    Mar  9 17:17:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename container-probe 03/09/23 17:17:02.538
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:02.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:02.554
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c in namespace container-probe-8375 03/09/23 17:17:02.557
    Mar  9 17:17:02.563: INFO: Waiting up to 5m0s for pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c" in namespace "container-probe-8375" to be "not pending"
    Mar  9 17:17:02.565: INFO: Pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269501ms
    Mar  9 17:17:04.568: INFO: Pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005351738s
    Mar  9 17:17:04.568: INFO: Pod "liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c" satisfied condition "not pending"
    Mar  9 17:17:04.568: INFO: Started pod liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c in namespace container-probe-8375
    STEP: checking the pod's current state and verifying that restartCount is present 03/09/23 17:17:04.568
    Mar  9 17:17:04.571: INFO: Initial restart count of pod liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c is 0
    Mar  9 17:17:24.612: INFO: Restart count of pod container-probe-8375/liveness-e5cff54f-dd35-4fea-bd9f-5fdd088ba56c is now 1 (20.041548775s elapsed)
    STEP: deleting the pod 03/09/23 17:17:24.612
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  9 17:17:24.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8375" for this suite. 03/09/23 17:17:24.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:24.639
Mar  9 17:17:24.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replicaset 03/09/23 17:17:24.64
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:24.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:24.656
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/09/23 17:17:24.662
STEP: Verify that the required pods have come up. 03/09/23 17:17:24.667
Mar  9 17:17:24.669: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  9 17:17:29.673: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/09/23 17:17:29.673
STEP: Getting /status 03/09/23 17:17:29.673
Mar  9 17:17:29.676: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/09/23 17:17:29.676
Mar  9 17:17:29.685: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/09/23 17:17:29.685
Mar  9 17:17:29.687: INFO: Observed &ReplicaSet event: ADDED
Mar  9 17:17:29.687: INFO: Observed &ReplicaSet event: MODIFIED
Mar  9 17:17:29.687: INFO: Observed &ReplicaSet event: MODIFIED
Mar  9 17:17:29.688: INFO: Observed &ReplicaSet event: MODIFIED
Mar  9 17:17:29.688: INFO: Found replicaset test-rs in namespace replicaset-5612 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  9 17:17:29.688: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/09/23 17:17:29.688
Mar  9 17:17:29.688: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  9 17:17:29.694: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/09/23 17:17:29.694
Mar  9 17:17:29.696: INFO: Observed &ReplicaSet event: ADDED
Mar  9 17:17:29.696: INFO: Observed &ReplicaSet event: MODIFIED
Mar  9 17:17:29.697: INFO: Observed &ReplicaSet event: MODIFIED
Mar  9 17:17:29.697: INFO: Observed &ReplicaSet event: MODIFIED
Mar  9 17:17:29.697: INFO: Observed replicaset test-rs in namespace replicaset-5612 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  9 17:17:29.697: INFO: Observed &ReplicaSet event: MODIFIED
Mar  9 17:17:29.697: INFO: Found replicaset test-rs in namespace replicaset-5612 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar  9 17:17:29.697: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  9 17:17:29.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5612" for this suite. 03/09/23 17:17:29.701
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":339,"skipped":6456,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.066 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:24.639
    Mar  9 17:17:24.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replicaset 03/09/23 17:17:24.64
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:24.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:24.656
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/09/23 17:17:24.662
    STEP: Verify that the required pods have come up. 03/09/23 17:17:24.667
    Mar  9 17:17:24.669: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  9 17:17:29.673: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/09/23 17:17:29.673
    STEP: Getting /status 03/09/23 17:17:29.673
    Mar  9 17:17:29.676: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/09/23 17:17:29.676
    Mar  9 17:17:29.685: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/09/23 17:17:29.685
    Mar  9 17:17:29.687: INFO: Observed &ReplicaSet event: ADDED
    Mar  9 17:17:29.687: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  9 17:17:29.687: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  9 17:17:29.688: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  9 17:17:29.688: INFO: Found replicaset test-rs in namespace replicaset-5612 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  9 17:17:29.688: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/09/23 17:17:29.688
    Mar  9 17:17:29.688: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  9 17:17:29.694: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/09/23 17:17:29.694
    Mar  9 17:17:29.696: INFO: Observed &ReplicaSet event: ADDED
    Mar  9 17:17:29.696: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  9 17:17:29.697: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  9 17:17:29.697: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  9 17:17:29.697: INFO: Observed replicaset test-rs in namespace replicaset-5612 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  9 17:17:29.697: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  9 17:17:29.697: INFO: Found replicaset test-rs in namespace replicaset-5612 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar  9 17:17:29.697: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  9 17:17:29.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5612" for this suite. 03/09/23 17:17:29.701
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:29.706
Mar  9 17:17:29.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename webhook 03/09/23 17:17:29.707
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:29.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:29.723
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/09/23 17:17:29.737
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:17:30.501
STEP: Deploying the webhook pod 03/09/23 17:17:30.508
STEP: Wait for the deployment to be ready 03/09/23 17:17:30.518
Mar  9 17:17:30.525: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/09/23 17:17:32.535
STEP: Verifying the service has paired with the endpoint 03/09/23 17:17:32.548
Mar  9 17:17:33.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/09/23 17:17:33.552
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/09/23 17:17:33.566
STEP: Creating a dummy validating-webhook-configuration object 03/09/23 17:17:33.58
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/09/23 17:17:33.588
STEP: Creating a dummy mutating-webhook-configuration object 03/09/23 17:17:33.598
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/09/23 17:17:33.606
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  9 17:17:33.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9349" for this suite. 03/09/23 17:17:33.626
STEP: Destroying namespace "webhook-9349-markers" for this suite. 03/09/23 17:17:33.631
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":340,"skipped":6460,"failed":0}
------------------------------
â€¢ [3.967 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:29.706
    Mar  9 17:17:29.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename webhook 03/09/23 17:17:29.707
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:29.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:29.723
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/09/23 17:17:29.737
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/09/23 17:17:30.501
    STEP: Deploying the webhook pod 03/09/23 17:17:30.508
    STEP: Wait for the deployment to be ready 03/09/23 17:17:30.518
    Mar  9 17:17:30.525: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/09/23 17:17:32.535
    STEP: Verifying the service has paired with the endpoint 03/09/23 17:17:32.548
    Mar  9 17:17:33.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/09/23 17:17:33.552
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/09/23 17:17:33.566
    STEP: Creating a dummy validating-webhook-configuration object 03/09/23 17:17:33.58
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/09/23 17:17:33.588
    STEP: Creating a dummy mutating-webhook-configuration object 03/09/23 17:17:33.598
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/09/23 17:17:33.606
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  9 17:17:33.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9349" for this suite. 03/09/23 17:17:33.626
    STEP: Destroying namespace "webhook-9349-markers" for this suite. 03/09/23 17:17:33.631
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:33.675
Mar  9 17:17:33.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename runtimeclass 03/09/23 17:17:33.676
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:33.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:33.694
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5642-delete-me 03/09/23 17:17:33.705
STEP: Waiting for the RuntimeClass to disappear 03/09/23 17:17:33.712
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  9 17:17:33.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5642" for this suite. 03/09/23 17:17:33.725
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":341,"skipped":6476,"failed":0}
------------------------------
â€¢ [0.056 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:33.675
    Mar  9 17:17:33.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename runtimeclass 03/09/23 17:17:33.676
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:33.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:33.694
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5642-delete-me 03/09/23 17:17:33.705
    STEP: Waiting for the RuntimeClass to disappear 03/09/23 17:17:33.712
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  9 17:17:33.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5642" for this suite. 03/09/23 17:17:33.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:33.733
Mar  9 17:17:33.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename watch 03/09/23 17:17:33.734
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:33.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:33.749
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/09/23 17:17:33.752
STEP: starting a background goroutine to produce watch events 03/09/23 17:17:33.755
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/09/23 17:17:33.755
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  9 17:17:36.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6368" for this suite. 03/09/23 17:17:36.588
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":342,"skipped":6491,"failed":0}
------------------------------
â€¢ [2.906 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:33.733
    Mar  9 17:17:33.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename watch 03/09/23 17:17:33.734
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:33.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:33.749
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/09/23 17:17:33.752
    STEP: starting a background goroutine to produce watch events 03/09/23 17:17:33.755
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/09/23 17:17:33.755
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  9 17:17:36.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6368" for this suite. 03/09/23 17:17:36.588
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:36.639
Mar  9 17:17:36.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename ingress 03/09/23 17:17:36.641
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:36.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:36.654
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/09/23 17:17:36.657
STEP: getting /apis/networking.k8s.io 03/09/23 17:17:36.662
STEP: getting /apis/networking.k8s.iov1 03/09/23 17:17:36.663
STEP: creating 03/09/23 17:17:36.664
STEP: getting 03/09/23 17:17:36.677
STEP: listing 03/09/23 17:17:36.679
STEP: watching 03/09/23 17:17:36.682
Mar  9 17:17:36.682: INFO: starting watch
STEP: cluster-wide listing 03/09/23 17:17:36.683
STEP: cluster-wide watching 03/09/23 17:17:36.685
Mar  9 17:17:36.685: INFO: starting watch
STEP: patching 03/09/23 17:17:36.686
STEP: updating 03/09/23 17:17:36.691
Mar  9 17:17:36.697: INFO: waiting for watch events with expected annotations
Mar  9 17:17:36.697: INFO: saw patched and updated annotations
STEP: patching /status 03/09/23 17:17:36.697
STEP: updating /status 03/09/23 17:17:36.702
STEP: get /status 03/09/23 17:17:36.71
STEP: deleting 03/09/23 17:17:36.712
STEP: deleting a collection 03/09/23 17:17:36.721
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Mar  9 17:17:36.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3016" for this suite. 03/09/23 17:17:36.734
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":343,"skipped":6493,"failed":0}
------------------------------
â€¢ [0.099 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:36.639
    Mar  9 17:17:36.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename ingress 03/09/23 17:17:36.641
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:36.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:36.654
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/09/23 17:17:36.657
    STEP: getting /apis/networking.k8s.io 03/09/23 17:17:36.662
    STEP: getting /apis/networking.k8s.iov1 03/09/23 17:17:36.663
    STEP: creating 03/09/23 17:17:36.664
    STEP: getting 03/09/23 17:17:36.677
    STEP: listing 03/09/23 17:17:36.679
    STEP: watching 03/09/23 17:17:36.682
    Mar  9 17:17:36.682: INFO: starting watch
    STEP: cluster-wide listing 03/09/23 17:17:36.683
    STEP: cluster-wide watching 03/09/23 17:17:36.685
    Mar  9 17:17:36.685: INFO: starting watch
    STEP: patching 03/09/23 17:17:36.686
    STEP: updating 03/09/23 17:17:36.691
    Mar  9 17:17:36.697: INFO: waiting for watch events with expected annotations
    Mar  9 17:17:36.697: INFO: saw patched and updated annotations
    STEP: patching /status 03/09/23 17:17:36.697
    STEP: updating /status 03/09/23 17:17:36.702
    STEP: get /status 03/09/23 17:17:36.71
    STEP: deleting 03/09/23 17:17:36.712
    STEP: deleting a collection 03/09/23 17:17:36.721
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Mar  9 17:17:36.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-3016" for this suite. 03/09/23 17:17:36.734
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:36.74
Mar  9 17:17:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename emptydir 03/09/23 17:17:36.741
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:36.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:36.755
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/09/23 17:17:36.758
Mar  9 17:17:36.765: INFO: Waiting up to 5m0s for pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f" in namespace "emptydir-3928" to be "Succeeded or Failed"
Mar  9 17:17:36.767: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669495ms
Mar  9 17:17:38.771: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006320788s
Mar  9 17:17:40.771: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006479093s
STEP: Saw pod success 03/09/23 17:17:40.771
Mar  9 17:17:40.771: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f" satisfied condition "Succeeded or Failed"
Mar  9 17:17:40.774: INFO: Trying to get logs from node tt-test-el8-003 pod pod-9d99e563-2098-4829-ab4e-024ec93f939f container test-container: <nil>
STEP: delete the pod 03/09/23 17:17:40.78
Mar  9 17:17:40.789: INFO: Waiting for pod pod-9d99e563-2098-4829-ab4e-024ec93f939f to disappear
Mar  9 17:17:40.792: INFO: Pod pod-9d99e563-2098-4829-ab4e-024ec93f939f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  9 17:17:40.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3928" for this suite. 03/09/23 17:17:40.795
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":344,"skipped":6497,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:36.74
    Mar  9 17:17:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename emptydir 03/09/23 17:17:36.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:36.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:36.755
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/09/23 17:17:36.758
    Mar  9 17:17:36.765: INFO: Waiting up to 5m0s for pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f" in namespace "emptydir-3928" to be "Succeeded or Failed"
    Mar  9 17:17:36.767: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669495ms
    Mar  9 17:17:38.771: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006320788s
    Mar  9 17:17:40.771: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006479093s
    STEP: Saw pod success 03/09/23 17:17:40.771
    Mar  9 17:17:40.771: INFO: Pod "pod-9d99e563-2098-4829-ab4e-024ec93f939f" satisfied condition "Succeeded or Failed"
    Mar  9 17:17:40.774: INFO: Trying to get logs from node tt-test-el8-003 pod pod-9d99e563-2098-4829-ab4e-024ec93f939f container test-container: <nil>
    STEP: delete the pod 03/09/23 17:17:40.78
    Mar  9 17:17:40.789: INFO: Waiting for pod pod-9d99e563-2098-4829-ab4e-024ec93f939f to disappear
    Mar  9 17:17:40.792: INFO: Pod pod-9d99e563-2098-4829-ab4e-024ec93f939f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  9 17:17:40.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3928" for this suite. 03/09/23 17:17:40.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:40.801
Mar  9 17:17:40.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename sysctl 03/09/23 17:17:40.802
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:40.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:40.818
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/09/23 17:17:40.82
STEP: Watching for error events or started pod 03/09/23 17:17:40.827
STEP: Waiting for pod completion 03/09/23 17:17:42.831
Mar  9 17:17:42.831: INFO: Waiting up to 3m0s for pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f" in namespace "sysctl-352" to be "completed"
Mar  9 17:17:42.834: INFO: Pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612248ms
Mar  9 17:17:44.837: INFO: Pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006467308s
Mar  9 17:17:44.837: INFO: Pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/09/23 17:17:44.84
STEP: Getting logs from the pod 03/09/23 17:17:44.84
STEP: Checking that the sysctl is actually updated 03/09/23 17:17:44.845
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  9 17:17:44.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-352" for this suite. 03/09/23 17:17:44.849
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":345,"skipped":6513,"failed":0}
------------------------------
â€¢ [4.052 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:40.801
    Mar  9 17:17:40.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename sysctl 03/09/23 17:17:40.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:40.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:40.818
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/09/23 17:17:40.82
    STEP: Watching for error events or started pod 03/09/23 17:17:40.827
    STEP: Waiting for pod completion 03/09/23 17:17:42.831
    Mar  9 17:17:42.831: INFO: Waiting up to 3m0s for pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f" in namespace "sysctl-352" to be "completed"
    Mar  9 17:17:42.834: INFO: Pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612248ms
    Mar  9 17:17:44.837: INFO: Pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006467308s
    Mar  9 17:17:44.837: INFO: Pod "sysctl-4b7da88e-6be7-4cbf-a816-d47626fbf85f" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/09/23 17:17:44.84
    STEP: Getting logs from the pod 03/09/23 17:17:44.84
    STEP: Checking that the sysctl is actually updated 03/09/23 17:17:44.845
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  9 17:17:44.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-352" for this suite. 03/09/23 17:17:44.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:44.855
Mar  9 17:17:44.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 17:17:44.856
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:44.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:44.869
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-ee749d92-fd26-4589-84ec-353e204f0484 03/09/23 17:17:44.873
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 17:17:44.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6775" for this suite. 03/09/23 17:17:44.878
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":346,"skipped":6527,"failed":0}
------------------------------
â€¢ [0.030 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:44.855
    Mar  9 17:17:44.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 17:17:44.856
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:44.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:44.869
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-ee749d92-fd26-4589-84ec-353e204f0484 03/09/23 17:17:44.873
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 17:17:44.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6775" for this suite. 03/09/23 17:17:44.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:44.886
Mar  9 17:17:44.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename init-container 03/09/23 17:17:44.888
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:44.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:44.905
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 03/09/23 17:17:44.908
Mar  9 17:17:44.908: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  9 17:17:48.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-113" for this suite. 03/09/23 17:17:48.275
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":347,"skipped":6540,"failed":0}
------------------------------
â€¢ [3.394 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:44.886
    Mar  9 17:17:44.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename init-container 03/09/23 17:17:44.888
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:44.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:44.905
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 03/09/23 17:17:44.908
    Mar  9 17:17:44.908: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  9 17:17:48.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-113" for this suite. 03/09/23 17:17:48.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:48.282
Mar  9 17:17:48.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename containers 03/09/23 17:17:48.283
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:48.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:48.295
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 03/09/23 17:17:48.298
Mar  9 17:17:48.305: INFO: Waiting up to 5m0s for pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770" in namespace "containers-4631" to be "Succeeded or Failed"
Mar  9 17:17:48.308: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645119ms
Mar  9 17:17:50.313: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008063694s
Mar  9 17:17:52.314: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00816883s
STEP: Saw pod success 03/09/23 17:17:52.314
Mar  9 17:17:52.314: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770" satisfied condition "Succeeded or Failed"
Mar  9 17:17:52.316: INFO: Trying to get logs from node tt-test-el8-003 pod client-containers-fcad51dc-5f71-4822-890e-426b1258a770 container agnhost-container: <nil>
STEP: delete the pod 03/09/23 17:17:52.322
Mar  9 17:17:52.332: INFO: Waiting for pod client-containers-fcad51dc-5f71-4822-890e-426b1258a770 to disappear
Mar  9 17:17:52.334: INFO: Pod client-containers-fcad51dc-5f71-4822-890e-426b1258a770 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  9 17:17:52.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4631" for this suite. 03/09/23 17:17:52.338
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":348,"skipped":6553,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:48.282
    Mar  9 17:17:48.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename containers 03/09/23 17:17:48.283
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:48.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:48.295
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 03/09/23 17:17:48.298
    Mar  9 17:17:48.305: INFO: Waiting up to 5m0s for pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770" in namespace "containers-4631" to be "Succeeded or Failed"
    Mar  9 17:17:48.308: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645119ms
    Mar  9 17:17:50.313: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008063694s
    Mar  9 17:17:52.314: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00816883s
    STEP: Saw pod success 03/09/23 17:17:52.314
    Mar  9 17:17:52.314: INFO: Pod "client-containers-fcad51dc-5f71-4822-890e-426b1258a770" satisfied condition "Succeeded or Failed"
    Mar  9 17:17:52.316: INFO: Trying to get logs from node tt-test-el8-003 pod client-containers-fcad51dc-5f71-4822-890e-426b1258a770 container agnhost-container: <nil>
    STEP: delete the pod 03/09/23 17:17:52.322
    Mar  9 17:17:52.332: INFO: Waiting for pod client-containers-fcad51dc-5f71-4822-890e-426b1258a770 to disappear
    Mar  9 17:17:52.334: INFO: Pod client-containers-fcad51dc-5f71-4822-890e-426b1258a770 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  9 17:17:52.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4631" for this suite. 03/09/23 17:17:52.338
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:17:52.342
Mar  9 17:17:52.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename replicaset 03/09/23 17:17:52.344
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:52.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:52.357
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar  9 17:17:52.360: INFO: Creating ReplicaSet my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96
Mar  9 17:17:52.366: INFO: Pod name my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96: Found 0 pods out of 1
Mar  9 17:17:57.370: INFO: Pod name my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96: Found 1 pods out of 1
Mar  9 17:17:57.370: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96" is running
Mar  9 17:17:57.370: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j" in namespace "replicaset-6664" to be "running"
Mar  9 17:17:57.372: INFO: Pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j": Phase="Running", Reason="", readiness=true. Elapsed: 2.525448ms
Mar  9 17:17:57.372: INFO: Pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j" satisfied condition "running"
Mar  9 17:17:57.372: INFO: Pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:52 +0000 UTC Reason: Message:}])
Mar  9 17:17:57.372: INFO: Trying to dial the pod
Mar  9 17:18:02.382: INFO: Controller my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96: Got expected result from replica 1 [my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j]: "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  9 17:18:02.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6664" for this suite. 03/09/23 17:18:02.386
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":349,"skipped":6553,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.049 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:17:52.342
    Mar  9 17:17:52.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename replicaset 03/09/23 17:17:52.344
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:17:52.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:17:52.357
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar  9 17:17:52.360: INFO: Creating ReplicaSet my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96
    Mar  9 17:17:52.366: INFO: Pod name my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96: Found 0 pods out of 1
    Mar  9 17:17:57.370: INFO: Pod name my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96: Found 1 pods out of 1
    Mar  9 17:17:57.370: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96" is running
    Mar  9 17:17:57.370: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j" in namespace "replicaset-6664" to be "running"
    Mar  9 17:17:57.372: INFO: Pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j": Phase="Running", Reason="", readiness=true. Elapsed: 2.525448ms
    Mar  9 17:17:57.372: INFO: Pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j" satisfied condition "running"
    Mar  9 17:17:57.372: INFO: Pod "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-09 17:17:52 +0000 UTC Reason: Message:}])
    Mar  9 17:17:57.372: INFO: Trying to dial the pod
    Mar  9 17:18:02.382: INFO: Controller my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96: Got expected result from replica 1 [my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j]: "my-hostname-basic-4d042be8-f211-4b0f-b26c-f609658c5d96-sv62j", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  9 17:18:02.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6664" for this suite. 03/09/23 17:18:02.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:18:02.393
Mar  9 17:18:02.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename statefulset 03/09/23 17:18:02.394
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:18:02.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:18:02.411
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1187 03/09/23 17:18:02.414
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 03/09/23 17:18:02.421
Mar  9 17:18:02.428: INFO: Found 0 stateful pods, waiting for 3
Mar  9 17:18:12.433: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 17:18:12.433: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 17:18:12.433: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/09/23 17:18:12.442
Mar  9 17:18:12.460: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/09/23 17:18:12.46
STEP: Not applying an update when the partition is greater than the number of replicas 03/09/23 17:18:22.479
STEP: Performing a canary update 03/09/23 17:18:22.479
Mar  9 17:18:22.498: INFO: Updating stateful set ss2
Mar  9 17:18:22.504: INFO: Waiting for Pod statefulset-1187/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 03/09/23 17:18:32.511
Mar  9 17:18:32.554: INFO: Found 2 stateful pods, waiting for 3
Mar  9 17:18:42.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 17:18:42.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  9 17:18:42.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/09/23 17:18:42.563
Mar  9 17:18:42.580: INFO: Updating stateful set ss2
Mar  9 17:18:42.586: INFO: Waiting for Pod statefulset-1187/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  9 17:18:52.612: INFO: Updating stateful set ss2
Mar  9 17:18:52.618: INFO: Waiting for StatefulSet statefulset-1187/ss2 to complete update
Mar  9 17:18:52.618: INFO: Waiting for Pod statefulset-1187/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  9 17:19:02.625: INFO: Deleting all statefulset in ns statefulset-1187
Mar  9 17:19:02.627: INFO: Scaling statefulset ss2 to 0
Mar  9 17:19:12.641: INFO: Waiting for statefulset status.replicas updated to 0
Mar  9 17:19:12.643: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  9 17:19:12.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1187" for this suite. 03/09/23 17:19:12.66
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":350,"skipped":6577,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.274 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:18:02.393
    Mar  9 17:18:02.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename statefulset 03/09/23 17:18:02.394
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:18:02.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:18:02.411
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1187 03/09/23 17:18:02.414
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 03/09/23 17:18:02.421
    Mar  9 17:18:02.428: INFO: Found 0 stateful pods, waiting for 3
    Mar  9 17:18:12.433: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 17:18:12.433: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 17:18:12.433: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/09/23 17:18:12.442
    Mar  9 17:18:12.460: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/09/23 17:18:12.46
    STEP: Not applying an update when the partition is greater than the number of replicas 03/09/23 17:18:22.479
    STEP: Performing a canary update 03/09/23 17:18:22.479
    Mar  9 17:18:22.498: INFO: Updating stateful set ss2
    Mar  9 17:18:22.504: INFO: Waiting for Pod statefulset-1187/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 03/09/23 17:18:32.511
    Mar  9 17:18:32.554: INFO: Found 2 stateful pods, waiting for 3
    Mar  9 17:18:42.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 17:18:42.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  9 17:18:42.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/09/23 17:18:42.563
    Mar  9 17:18:42.580: INFO: Updating stateful set ss2
    Mar  9 17:18:42.586: INFO: Waiting for Pod statefulset-1187/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  9 17:18:52.612: INFO: Updating stateful set ss2
    Mar  9 17:18:52.618: INFO: Waiting for StatefulSet statefulset-1187/ss2 to complete update
    Mar  9 17:18:52.618: INFO: Waiting for Pod statefulset-1187/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  9 17:19:02.625: INFO: Deleting all statefulset in ns statefulset-1187
    Mar  9 17:19:02.627: INFO: Scaling statefulset ss2 to 0
    Mar  9 17:19:12.641: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  9 17:19:12.643: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  9 17:19:12.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1187" for this suite. 03/09/23 17:19:12.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:19:12.668
Mar  9 17:19:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename job 03/09/23 17:19:12.67
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:19:12.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:19:12.685
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 03/09/23 17:19:12.688
STEP: Ensuring job reaches completions 03/09/23 17:19:12.692
STEP: Ensuring pods with index for job exist 03/09/23 17:19:20.696
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  9 17:19:20.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5480" for this suite. 03/09/23 17:19:20.703
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":351,"skipped":6596,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.040 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:19:12.668
    Mar  9 17:19:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename job 03/09/23 17:19:12.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:19:12.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:19:12.685
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 03/09/23 17:19:12.688
    STEP: Ensuring job reaches completions 03/09/23 17:19:12.692
    STEP: Ensuring pods with index for job exist 03/09/23 17:19:20.696
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  9 17:19:20.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5480" for this suite. 03/09/23 17:19:20.703
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:19:20.708
Mar  9 17:19:20.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename aggregator 03/09/23 17:19:20.71
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:19:20.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:19:20.723
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar  9 17:19:20.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/09/23 17:19:20.727
Mar  9 17:19:21.142: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  9 17:19:23.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:25.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:27.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:29.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:31.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:33.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:35.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:37.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:39.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:41.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:43.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:45.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  9 17:19:47.320: INFO: Waited 118.671704ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/09/23 17:19:47.368
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/09/23 17:19:47.37
STEP: List APIServices 03/09/23 17:19:47.376
Mar  9 17:19:47.383: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Mar  9 17:19:47.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2857" for this suite. 03/09/23 17:19:47.666
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":352,"skipped":6599,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.002 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:19:20.708
    Mar  9 17:19:20.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename aggregator 03/09/23 17:19:20.71
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:19:20.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:19:20.723
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar  9 17:19:20.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/09/23 17:19:20.727
    Mar  9 17:19:21.142: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar  9 17:19:23.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:25.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:27.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:29.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:31.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:33.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:35.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:37.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:39.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:41.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:43.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:45.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 9, 17, 19, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  9 17:19:47.320: INFO: Waited 118.671704ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/09/23 17:19:47.368
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/09/23 17:19:47.37
    STEP: List APIServices 03/09/23 17:19:47.376
    Mar  9 17:19:47.383: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Mar  9 17:19:47.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-2857" for this suite. 03/09/23 17:19:47.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:19:47.713
Mar  9 17:19:47.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename resourcequota 03/09/23 17:19:47.714
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:19:47.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:19:47.729
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 03/09/23 17:19:47.732
STEP: Counting existing ResourceQuota 03/09/23 17:19:52.735
STEP: Creating a ResourceQuota 03/09/23 17:19:57.739
STEP: Ensuring resource quota status is calculated 03/09/23 17:19:57.745
STEP: Creating a Secret 03/09/23 17:19:59.748
STEP: Ensuring resource quota status captures secret creation 03/09/23 17:19:59.758
STEP: Deleting a secret 03/09/23 17:20:01.763
STEP: Ensuring resource quota status released usage 03/09/23 17:20:01.768
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  9 17:20:03.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7165" for this suite. 03/09/23 17:20:03.777
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":353,"skipped":6610,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.069 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:19:47.713
    Mar  9 17:19:47.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename resourcequota 03/09/23 17:19:47.714
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:19:47.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:19:47.729
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 03/09/23 17:19:47.732
    STEP: Counting existing ResourceQuota 03/09/23 17:19:52.735
    STEP: Creating a ResourceQuota 03/09/23 17:19:57.739
    STEP: Ensuring resource quota status is calculated 03/09/23 17:19:57.745
    STEP: Creating a Secret 03/09/23 17:19:59.748
    STEP: Ensuring resource quota status captures secret creation 03/09/23 17:19:59.758
    STEP: Deleting a secret 03/09/23 17:20:01.763
    STEP: Ensuring resource quota status released usage 03/09/23 17:20:01.768
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  9 17:20:03.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7165" for this suite. 03/09/23 17:20:03.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:20:03.782
Mar  9 17:20:03.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename configmap 03/09/23 17:20:03.783
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:20:03.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:20:03.797
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-196527e7-3080-47dc-82dc-ef017ebfdb83 03/09/23 17:20:03.803
STEP: Creating the pod 03/09/23 17:20:03.807
Mar  9 17:20:03.814: INFO: Waiting up to 5m0s for pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98" in namespace "configmap-1609" to be "running and ready"
Mar  9 17:20:03.817: INFO: Pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21909ms
Mar  9 17:20:03.817: INFO: The phase of Pod pod-configmaps-49587794-c80e-4224-a541-89358549df98 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:20:05.821: INFO: Pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98": Phase="Running", Reason="", readiness=true. Elapsed: 2.006790925s
Mar  9 17:20:05.821: INFO: The phase of Pod pod-configmaps-49587794-c80e-4224-a541-89358549df98 is Running (Ready = true)
Mar  9 17:20:05.821: INFO: Pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-196527e7-3080-47dc-82dc-ef017ebfdb83 03/09/23 17:20:05.839
STEP: waiting to observe update in volume 03/09/23 17:20:05.843
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  9 17:21:16.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1609" for this suite. 03/09/23 17:21:16.156
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":354,"skipped":6615,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.378 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:20:03.782
    Mar  9 17:20:03.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename configmap 03/09/23 17:20:03.783
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:20:03.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:20:03.797
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-196527e7-3080-47dc-82dc-ef017ebfdb83 03/09/23 17:20:03.803
    STEP: Creating the pod 03/09/23 17:20:03.807
    Mar  9 17:20:03.814: INFO: Waiting up to 5m0s for pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98" in namespace "configmap-1609" to be "running and ready"
    Mar  9 17:20:03.817: INFO: Pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21909ms
    Mar  9 17:20:03.817: INFO: The phase of Pod pod-configmaps-49587794-c80e-4224-a541-89358549df98 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:20:05.821: INFO: Pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98": Phase="Running", Reason="", readiness=true. Elapsed: 2.006790925s
    Mar  9 17:20:05.821: INFO: The phase of Pod pod-configmaps-49587794-c80e-4224-a541-89358549df98 is Running (Ready = true)
    Mar  9 17:20:05.821: INFO: Pod "pod-configmaps-49587794-c80e-4224-a541-89358549df98" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-196527e7-3080-47dc-82dc-ef017ebfdb83 03/09/23 17:20:05.839
    STEP: waiting to observe update in volume 03/09/23 17:20:05.843
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  9 17:21:16.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1609" for this suite. 03/09/23 17:21:16.156
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:21:16.161
Mar  9 17:21:16.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename proxy 03/09/23 17:21:16.162
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:16.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:16.176
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/09/23 17:21:16.192
STEP: creating replication controller proxy-service-dt88g in namespace proxy-5035 03/09/23 17:21:16.193
I0309 17:21:16.200099      22 runners.go:193] Created replication controller with name: proxy-service-dt88g, namespace: proxy-5035, replica count: 1
I0309 17:21:17.251252      22 runners.go:193] proxy-service-dt88g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0309 17:21:18.251910      22 runners.go:193] proxy-service-dt88g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0309 17:21:19.253022      22 runners.go:193] proxy-service-dt88g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  9 17:21:19.255: INFO: setup took 3.076174066s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/09/23 17:21:19.255
Mar  9 17:21:19.265: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.034623ms)
Mar  9 17:21:19.265: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.241154ms)
Mar  9 17:21:19.265: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.311814ms)
Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.059884ms)
Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 10.053715ms)
Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 10.116143ms)
Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.99772ms)
Mar  9 17:21:19.267: INFO: (0) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 11.288243ms)
Mar  9 17:21:19.267: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 11.57759ms)
Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 13.494943ms)
Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 13.639983ms)
Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 13.651181ms)
Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 13.789509ms)
Mar  9 17:21:19.271: INFO: (0) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 15.383792ms)
Mar  9 17:21:19.271: INFO: (0) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 15.388982ms)
Mar  9 17:21:19.272: INFO: (0) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 15.884255ms)
Mar  9 17:21:19.278: INFO: (1) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.672071ms)
Mar  9 17:21:19.279: INFO: (1) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.626872ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.619764ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 6.90296ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 8.741959ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.987873ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 9.553386ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 8.832024ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 8.041786ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.293454ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.458005ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.221934ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 8.584926ms)
Mar  9 17:21:19.282: INFO: (1) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.91611ms)
Mar  9 17:21:19.282: INFO: (1) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.539459ms)
Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.705673ms)
Mar  9 17:21:19.286: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 4.28208ms)
Mar  9 17:21:19.286: INFO: (2) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 4.464474ms)
Mar  9 17:21:19.288: INFO: (2) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.14543ms)
Mar  9 17:21:19.288: INFO: (2) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.446157ms)
Mar  9 17:21:19.288: INFO: (2) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 6.691955ms)
Mar  9 17:21:19.290: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 8.669492ms)
Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 9.045276ms)
Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.304241ms)
Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 9.338048ms)
Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.385672ms)
Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.733245ms)
Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.142757ms)
Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.108002ms)
Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.09902ms)
Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.427504ms)
Mar  9 17:21:19.293: INFO: (2) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 10.906169ms)
Mar  9 17:21:19.296: INFO: (3) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 3.548918ms)
Mar  9 17:21:19.297: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 4.567697ms)
Mar  9 17:21:19.297: INFO: (3) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.618993ms)
Mar  9 17:21:19.298: INFO: (3) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 4.890434ms)
Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.205171ms)
Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 6.5084ms)
Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.590295ms)
Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.620562ms)
Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.602815ms)
Mar  9 17:21:19.300: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.416027ms)
Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.818299ms)
Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.224407ms)
Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.253137ms)
Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 9.331225ms)
Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.310105ms)
Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.481163ms)
Mar  9 17:21:19.308: INFO: (4) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 6.176617ms)
Mar  9 17:21:19.309: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 6.160791ms)
Mar  9 17:21:19.309: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.09896ms)
Mar  9 17:21:19.309: INFO: (4) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.388584ms)
Mar  9 17:21:19.310: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.998142ms)
Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 9.225961ms)
Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 8.967357ms)
Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.257998ms)
Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 9.692131ms)
Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.593664ms)
Mar  9 17:21:19.313: INFO: (4) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.395ms)
Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.461648ms)
Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.308058ms)
Mar  9 17:21:19.313: INFO: (4) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.196451ms)
Mar  9 17:21:19.313: INFO: (4) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 9.850854ms)
Mar  9 17:21:19.314: INFO: (4) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 11.311834ms)
Mar  9 17:21:19.320: INFO: (5) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 5.636883ms)
Mar  9 17:21:19.320: INFO: (5) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.85337ms)
Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 6.954142ms)
Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.929198ms)
Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 6.994014ms)
Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.92452ms)
Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.157887ms)
Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 7.228958ms)
Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.833566ms)
Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.880766ms)
Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 8.057946ms)
Mar  9 17:21:19.323: INFO: (5) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.092809ms)
Mar  9 17:21:19.323: INFO: (5) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.841802ms)
Mar  9 17:21:19.323: INFO: (5) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.110206ms)
Mar  9 17:21:19.324: INFO: (5) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.014764ms)
Mar  9 17:21:19.324: INFO: (5) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 9.011777ms)
Mar  9 17:21:19.329: INFO: (6) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 4.352568ms)
Mar  9 17:21:19.330: INFO: (6) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.099914ms)
Mar  9 17:21:19.330: INFO: (6) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.884873ms)
Mar  9 17:21:19.331: INFO: (6) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.693397ms)
Mar  9 17:21:19.331: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.990968ms)
Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.665847ms)
Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.285965ms)
Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.252257ms)
Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.439602ms)
Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 6.879466ms)
Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.248034ms)
Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 7.981503ms)
Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.342831ms)
Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 8.203979ms)
Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.164084ms)
Mar  9 17:21:19.334: INFO: (6) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.419753ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.075528ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.184081ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 6.002617ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 5.467457ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.79255ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 5.450608ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.206771ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 5.880056ms)
Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 6.458134ms)
Mar  9 17:21:19.341: INFO: (7) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 6.462302ms)
Mar  9 17:21:19.342: INFO: (7) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 7.786471ms)
Mar  9 17:21:19.342: INFO: (7) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 7.686031ms)
Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.801149ms)
Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 7.590159ms)
Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.030569ms)
Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.804421ms)
Mar  9 17:21:19.348: INFO: (8) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 4.961584ms)
Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 8.732356ms)
Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 8.80977ms)
Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.22774ms)
Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.469503ms)
Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.555011ms)
Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 10.137555ms)
Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 10.172357ms)
Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.285353ms)
Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 10.366985ms)
Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 10.33315ms)
Mar  9 17:21:19.354: INFO: (8) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.497384ms)
Mar  9 17:21:19.354: INFO: (8) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.777178ms)
Mar  9 17:21:19.354: INFO: (8) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.818568ms)
Mar  9 17:21:19.355: INFO: (8) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 11.630799ms)
Mar  9 17:21:19.356: INFO: (8) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 12.477138ms)
Mar  9 17:21:19.360: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.116852ms)
Mar  9 17:21:19.364: INFO: (9) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.447685ms)
Mar  9 17:21:19.364: INFO: (9) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 8.307758ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 8.697065ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.056187ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 8.965348ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.147982ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 9.12615ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.536998ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 9.605221ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 9.456758ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.257586ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.385283ms)
Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.427007ms)
Mar  9 17:21:19.366: INFO: (9) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.178192ms)
Mar  9 17:21:19.366: INFO: (9) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.924335ms)
Mar  9 17:21:19.372: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.012873ms)
Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.356223ms)
Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.044414ms)
Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.155712ms)
Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.109143ms)
Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 7.073181ms)
Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 7.160856ms)
Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 7.390345ms)
Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.921671ms)
Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.881685ms)
Mar  9 17:21:19.375: INFO: (10) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.52834ms)
Mar  9 17:21:19.375: INFO: (10) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.81523ms)
Mar  9 17:21:19.378: INFO: (10) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 11.809787ms)
Mar  9 17:21:19.379: INFO: (10) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 12.16235ms)
Mar  9 17:21:19.379: INFO: (10) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 12.363195ms)
Mar  9 17:21:19.379: INFO: (10) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 12.585293ms)
Mar  9 17:21:19.399: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 20.179495ms)
Mar  9 17:21:19.401: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 22.345638ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 22.403444ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 22.464083ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 22.636708ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 22.787686ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 22.956535ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 23.397393ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 23.393232ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 23.500001ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 23.291185ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 23.608064ms)
Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 23.370403ms)
Mar  9 17:21:19.403: INFO: (11) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 23.758278ms)
Mar  9 17:21:19.403: INFO: (11) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 24.154427ms)
Mar  9 17:21:19.403: INFO: (11) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 24.073562ms)
Mar  9 17:21:19.408: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.790709ms)
Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 8.768499ms)
Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.36171ms)
Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.405837ms)
Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.848897ms)
Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 9.901913ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.849987ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 9.708947ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.803904ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 10.024483ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.256943ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 10.398397ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.864318ms)
Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.808147ms)
Mar  9 17:21:19.415: INFO: (12) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 11.088868ms)
Mar  9 17:21:19.415: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 10.568949ms)
Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.336471ms)
Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 5.940351ms)
Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.02647ms)
Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 6.48341ms)
Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.791727ms)
Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 6.61934ms)
Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.051052ms)
Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.032999ms)
Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.572966ms)
Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.340638ms)
Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 8.418014ms)
Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.380766ms)
Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.60728ms)
Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 8.598172ms)
Mar  9 17:21:19.424: INFO: (13) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 8.688062ms)
Mar  9 17:21:19.424: INFO: (13) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.670406ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 7.004245ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 7.040944ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 7.362131ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.372416ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.255325ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.180214ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.181766ms)
Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 7.380838ms)
Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.585511ms)
Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.954964ms)
Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.10659ms)
Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.30352ms)
Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 8.177201ms)
Mar  9 17:21:19.434: INFO: (14) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.952347ms)
Mar  9 17:21:19.434: INFO: (14) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.23089ms)
Mar  9 17:21:19.434: INFO: (14) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 10.294109ms)
Mar  9 17:21:19.441: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.967078ms)
Mar  9 17:21:19.441: INFO: (15) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.719804ms)
Mar  9 17:21:19.442: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 7.268829ms)
Mar  9 17:21:19.444: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.087627ms)
Mar  9 17:21:19.445: INFO: (15) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.075849ms)
Mar  9 17:21:19.445: INFO: (15) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.972093ms)
Mar  9 17:21:19.445: INFO: (15) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.359382ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.835861ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 10.64358ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 10.000037ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 10.587388ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.361223ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 10.539475ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.723719ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 10.679892ms)
Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 11.022761ms)
Mar  9 17:21:19.450: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 3.046492ms)
Mar  9 17:21:19.450: INFO: (16) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 3.444726ms)
Mar  9 17:21:19.451: INFO: (16) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 4.679553ms)
Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 4.959705ms)
Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.981109ms)
Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 5.162813ms)
Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.202172ms)
Mar  9 17:21:19.454: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.232425ms)
Mar  9 17:21:19.454: INFO: (16) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.197053ms)
Mar  9 17:21:19.454: INFO: (16) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.415763ms)
Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 9.478664ms)
Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.347345ms)
Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.555073ms)
Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.66034ms)
Mar  9 17:21:19.457: INFO: (16) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.68993ms)
Mar  9 17:21:19.457: INFO: (16) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 9.781307ms)
Mar  9 17:21:19.465: INFO: (17) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.791383ms)
Mar  9 17:21:19.467: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 9.716299ms)
Mar  9 17:21:19.467: INFO: (17) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.740547ms)
Mar  9 17:21:19.467: INFO: (17) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 10.661111ms)
Mar  9 17:21:19.468: INFO: (17) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 11.626864ms)
Mar  9 17:21:19.468: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 11.64911ms)
Mar  9 17:21:19.469: INFO: (17) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 11.833903ms)
Mar  9 17:21:19.469: INFO: (17) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 12.517707ms)
Mar  9 17:21:19.469: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 12.436371ms)
Mar  9 17:21:19.470: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 13.389494ms)
Mar  9 17:21:19.470: INFO: (17) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 13.654017ms)
Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 14.219978ms)
Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 14.286873ms)
Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 14.348079ms)
Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 14.860426ms)
Mar  9 17:21:19.472: INFO: (17) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 15.033198ms)
Mar  9 17:21:19.477: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 5.581325ms)
Mar  9 17:21:19.478: INFO: (18) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.880868ms)
Mar  9 17:21:19.478: INFO: (18) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.792544ms)
Mar  9 17:21:19.484: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 12.346947ms)
Mar  9 17:21:19.484: INFO: (18) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 12.550153ms)
Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 12.537573ms)
Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 12.687458ms)
Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 12.996866ms)
Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 13.129926ms)
Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 13.109015ms)
Mar  9 17:21:19.498: INFO: (18) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 26.629966ms)
Mar  9 17:21:19.498: INFO: (18) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 26.321525ms)
Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 26.404676ms)
Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 26.437557ms)
Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 26.62723ms)
Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 26.678381ms)
Mar  9 17:21:19.504: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.311101ms)
Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 17.349719ms)
Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 17.671332ms)
Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 18.187715ms)
Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 18.352304ms)
Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 18.323981ms)
Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 18.17343ms)
Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 18.239525ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 18.563563ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 18.681711ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 18.736811ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 18.838793ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 18.867611ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 18.810715ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 18.952597ms)
Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 19.239852ms)
STEP: deleting ReplicationController proxy-service-dt88g in namespace proxy-5035, will wait for the garbage collector to delete the pods 03/09/23 17:21:19.518
Mar  9 17:21:19.581: INFO: Deleting ReplicationController proxy-service-dt88g took: 8.656459ms
Mar  9 17:21:19.682: INFO: Terminating ReplicationController proxy-service-dt88g pods took: 100.876461ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  9 17:21:21.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5035" for this suite. 03/09/23 17:21:21.988
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":355,"skipped":6617,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.835 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:21:16.161
    Mar  9 17:21:16.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename proxy 03/09/23 17:21:16.162
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:16.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:16.176
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/09/23 17:21:16.192
    STEP: creating replication controller proxy-service-dt88g in namespace proxy-5035 03/09/23 17:21:16.193
    I0309 17:21:16.200099      22 runners.go:193] Created replication controller with name: proxy-service-dt88g, namespace: proxy-5035, replica count: 1
    I0309 17:21:17.251252      22 runners.go:193] proxy-service-dt88g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0309 17:21:18.251910      22 runners.go:193] proxy-service-dt88g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0309 17:21:19.253022      22 runners.go:193] proxy-service-dt88g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  9 17:21:19.255: INFO: setup took 3.076174066s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/09/23 17:21:19.255
    Mar  9 17:21:19.265: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.034623ms)
    Mar  9 17:21:19.265: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.241154ms)
    Mar  9 17:21:19.265: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.311814ms)
    Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.059884ms)
    Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 10.053715ms)
    Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 10.116143ms)
    Mar  9 17:21:19.266: INFO: (0) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.99772ms)
    Mar  9 17:21:19.267: INFO: (0) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 11.288243ms)
    Mar  9 17:21:19.267: INFO: (0) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 11.57759ms)
    Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 13.494943ms)
    Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 13.639983ms)
    Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 13.651181ms)
    Mar  9 17:21:19.269: INFO: (0) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 13.789509ms)
    Mar  9 17:21:19.271: INFO: (0) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 15.383792ms)
    Mar  9 17:21:19.271: INFO: (0) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 15.388982ms)
    Mar  9 17:21:19.272: INFO: (0) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 15.884255ms)
    Mar  9 17:21:19.278: INFO: (1) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.672071ms)
    Mar  9 17:21:19.279: INFO: (1) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.626872ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.619764ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 6.90296ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 8.741959ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.987873ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 9.553386ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 8.832024ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 8.041786ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.293454ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.458005ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.221934ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 8.584926ms)
    Mar  9 17:21:19.282: INFO: (1) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.91611ms)
    Mar  9 17:21:19.282: INFO: (1) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.539459ms)
    Mar  9 17:21:19.281: INFO: (1) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.705673ms)
    Mar  9 17:21:19.286: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 4.28208ms)
    Mar  9 17:21:19.286: INFO: (2) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 4.464474ms)
    Mar  9 17:21:19.288: INFO: (2) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.14543ms)
    Mar  9 17:21:19.288: INFO: (2) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.446157ms)
    Mar  9 17:21:19.288: INFO: (2) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 6.691955ms)
    Mar  9 17:21:19.290: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 8.669492ms)
    Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 9.045276ms)
    Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.304241ms)
    Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 9.338048ms)
    Mar  9 17:21:19.291: INFO: (2) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.385672ms)
    Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.733245ms)
    Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.142757ms)
    Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.108002ms)
    Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.09902ms)
    Mar  9 17:21:19.292: INFO: (2) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.427504ms)
    Mar  9 17:21:19.293: INFO: (2) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 10.906169ms)
    Mar  9 17:21:19.296: INFO: (3) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 3.548918ms)
    Mar  9 17:21:19.297: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 4.567697ms)
    Mar  9 17:21:19.297: INFO: (3) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.618993ms)
    Mar  9 17:21:19.298: INFO: (3) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 4.890434ms)
    Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.205171ms)
    Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 6.5084ms)
    Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.590295ms)
    Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.620562ms)
    Mar  9 17:21:19.299: INFO: (3) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.602815ms)
    Mar  9 17:21:19.300: INFO: (3) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.416027ms)
    Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.818299ms)
    Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.224407ms)
    Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.253137ms)
    Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 9.331225ms)
    Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.310105ms)
    Mar  9 17:21:19.302: INFO: (3) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.481163ms)
    Mar  9 17:21:19.308: INFO: (4) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 6.176617ms)
    Mar  9 17:21:19.309: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 6.160791ms)
    Mar  9 17:21:19.309: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.09896ms)
    Mar  9 17:21:19.309: INFO: (4) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.388584ms)
    Mar  9 17:21:19.310: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.998142ms)
    Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 9.225961ms)
    Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 8.967357ms)
    Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.257998ms)
    Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 9.692131ms)
    Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.593664ms)
    Mar  9 17:21:19.313: INFO: (4) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.395ms)
    Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.461648ms)
    Mar  9 17:21:19.312: INFO: (4) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.308058ms)
    Mar  9 17:21:19.313: INFO: (4) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.196451ms)
    Mar  9 17:21:19.313: INFO: (4) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 9.850854ms)
    Mar  9 17:21:19.314: INFO: (4) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 11.311834ms)
    Mar  9 17:21:19.320: INFO: (5) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 5.636883ms)
    Mar  9 17:21:19.320: INFO: (5) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.85337ms)
    Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 6.954142ms)
    Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.929198ms)
    Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 6.994014ms)
    Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.92452ms)
    Mar  9 17:21:19.321: INFO: (5) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.157887ms)
    Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 7.228958ms)
    Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.833566ms)
    Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.880766ms)
    Mar  9 17:21:19.322: INFO: (5) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 8.057946ms)
    Mar  9 17:21:19.323: INFO: (5) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.092809ms)
    Mar  9 17:21:19.323: INFO: (5) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.841802ms)
    Mar  9 17:21:19.323: INFO: (5) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.110206ms)
    Mar  9 17:21:19.324: INFO: (5) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.014764ms)
    Mar  9 17:21:19.324: INFO: (5) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 9.011777ms)
    Mar  9 17:21:19.329: INFO: (6) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 4.352568ms)
    Mar  9 17:21:19.330: INFO: (6) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.099914ms)
    Mar  9 17:21:19.330: INFO: (6) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.884873ms)
    Mar  9 17:21:19.331: INFO: (6) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.693397ms)
    Mar  9 17:21:19.331: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.990968ms)
    Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.665847ms)
    Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.285965ms)
    Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.252257ms)
    Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.439602ms)
    Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 6.879466ms)
    Mar  9 17:21:19.332: INFO: (6) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.248034ms)
    Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 7.981503ms)
    Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.342831ms)
    Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 8.203979ms)
    Mar  9 17:21:19.333: INFO: (6) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.164084ms)
    Mar  9 17:21:19.334: INFO: (6) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.419753ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.075528ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.184081ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 6.002617ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 5.467457ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.79255ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 5.450608ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.206771ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 5.880056ms)
    Mar  9 17:21:19.340: INFO: (7) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 6.458134ms)
    Mar  9 17:21:19.341: INFO: (7) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 6.462302ms)
    Mar  9 17:21:19.342: INFO: (7) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 7.786471ms)
    Mar  9 17:21:19.342: INFO: (7) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 7.686031ms)
    Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.801149ms)
    Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 7.590159ms)
    Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.030569ms)
    Mar  9 17:21:19.343: INFO: (7) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.804421ms)
    Mar  9 17:21:19.348: INFO: (8) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 4.961584ms)
    Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 8.732356ms)
    Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 8.80977ms)
    Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 9.22774ms)
    Mar  9 17:21:19.352: INFO: (8) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.469503ms)
    Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.555011ms)
    Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 10.137555ms)
    Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 10.172357ms)
    Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.285353ms)
    Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 10.366985ms)
    Mar  9 17:21:19.353: INFO: (8) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 10.33315ms)
    Mar  9 17:21:19.354: INFO: (8) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.497384ms)
    Mar  9 17:21:19.354: INFO: (8) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.777178ms)
    Mar  9 17:21:19.354: INFO: (8) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.818568ms)
    Mar  9 17:21:19.355: INFO: (8) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 11.630799ms)
    Mar  9 17:21:19.356: INFO: (8) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 12.477138ms)
    Mar  9 17:21:19.360: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.116852ms)
    Mar  9 17:21:19.364: INFO: (9) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.447685ms)
    Mar  9 17:21:19.364: INFO: (9) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 8.307758ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 8.697065ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.056187ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 8.965348ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.147982ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 9.12615ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.536998ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 9.605221ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 9.456758ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.257586ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.385283ms)
    Mar  9 17:21:19.365: INFO: (9) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.427007ms)
    Mar  9 17:21:19.366: INFO: (9) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.178192ms)
    Mar  9 17:21:19.366: INFO: (9) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.924335ms)
    Mar  9 17:21:19.372: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.012873ms)
    Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 6.356223ms)
    Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.044414ms)
    Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.155712ms)
    Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.109143ms)
    Mar  9 17:21:19.373: INFO: (10) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 7.073181ms)
    Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 7.160856ms)
    Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 7.390345ms)
    Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.921671ms)
    Mar  9 17:21:19.374: INFO: (10) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.881685ms)
    Mar  9 17:21:19.375: INFO: (10) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.52834ms)
    Mar  9 17:21:19.375: INFO: (10) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.81523ms)
    Mar  9 17:21:19.378: INFO: (10) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 11.809787ms)
    Mar  9 17:21:19.379: INFO: (10) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 12.16235ms)
    Mar  9 17:21:19.379: INFO: (10) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 12.363195ms)
    Mar  9 17:21:19.379: INFO: (10) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 12.585293ms)
    Mar  9 17:21:19.399: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 20.179495ms)
    Mar  9 17:21:19.401: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 22.345638ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 22.403444ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 22.464083ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 22.636708ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 22.787686ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 22.956535ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 23.397393ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 23.393232ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 23.500001ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 23.291185ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 23.608064ms)
    Mar  9 17:21:19.402: INFO: (11) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 23.370403ms)
    Mar  9 17:21:19.403: INFO: (11) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 23.758278ms)
    Mar  9 17:21:19.403: INFO: (11) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 24.154427ms)
    Mar  9 17:21:19.403: INFO: (11) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 24.073562ms)
    Mar  9 17:21:19.408: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.790709ms)
    Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 8.768499ms)
    Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.36171ms)
    Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 9.405837ms)
    Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.848897ms)
    Mar  9 17:21:19.413: INFO: (12) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 9.901913ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.849987ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 9.708947ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.803904ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 10.024483ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.256943ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 10.398397ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.864318ms)
    Mar  9 17:21:19.414: INFO: (12) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.808147ms)
    Mar  9 17:21:19.415: INFO: (12) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 11.088868ms)
    Mar  9 17:21:19.415: INFO: (12) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 10.568949ms)
    Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.336471ms)
    Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 5.940351ms)
    Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.02647ms)
    Mar  9 17:21:19.421: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 6.48341ms)
    Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 6.791727ms)
    Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 6.61934ms)
    Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.051052ms)
    Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.032999ms)
    Mar  9 17:21:19.422: INFO: (13) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.572966ms)
    Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 8.340638ms)
    Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 8.418014ms)
    Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 8.380766ms)
    Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.60728ms)
    Mar  9 17:21:19.423: INFO: (13) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 8.598172ms)
    Mar  9 17:21:19.424: INFO: (13) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 8.688062ms)
    Mar  9 17:21:19.424: INFO: (13) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.670406ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 7.004245ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 7.040944ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 7.362131ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 7.372416ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.255325ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.180214ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.181766ms)
    Mar  9 17:21:19.431: INFO: (14) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 7.380838ms)
    Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.585511ms)
    Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 7.954964ms)
    Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 8.10659ms)
    Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 8.30352ms)
    Mar  9 17:21:19.432: INFO: (14) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 8.177201ms)
    Mar  9 17:21:19.434: INFO: (14) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.952347ms)
    Mar  9 17:21:19.434: INFO: (14) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.23089ms)
    Mar  9 17:21:19.434: INFO: (14) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 10.294109ms)
    Mar  9 17:21:19.441: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.967078ms)
    Mar  9 17:21:19.441: INFO: (15) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 6.719804ms)
    Mar  9 17:21:19.442: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 7.268829ms)
    Mar  9 17:21:19.444: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 9.087627ms)
    Mar  9 17:21:19.445: INFO: (15) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.075849ms)
    Mar  9 17:21:19.445: INFO: (15) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.972093ms)
    Mar  9 17:21:19.445: INFO: (15) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 10.359382ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 9.835861ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 10.64358ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 10.000037ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 10.587388ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 10.361223ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 10.539475ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 10.723719ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 10.679892ms)
    Mar  9 17:21:19.446: INFO: (15) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 11.022761ms)
    Mar  9 17:21:19.450: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 3.046492ms)
    Mar  9 17:21:19.450: INFO: (16) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 3.444726ms)
    Mar  9 17:21:19.451: INFO: (16) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 4.679553ms)
    Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 4.959705ms)
    Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 4.981109ms)
    Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 5.162813ms)
    Mar  9 17:21:19.452: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.202172ms)
    Mar  9 17:21:19.454: INFO: (16) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.232425ms)
    Mar  9 17:21:19.454: INFO: (16) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 7.197053ms)
    Mar  9 17:21:19.454: INFO: (16) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 7.415763ms)
    Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 9.478664ms)
    Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 9.347345ms)
    Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 9.555073ms)
    Mar  9 17:21:19.456: INFO: (16) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 9.66034ms)
    Mar  9 17:21:19.457: INFO: (16) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 9.68993ms)
    Mar  9 17:21:19.457: INFO: (16) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 9.781307ms)
    Mar  9 17:21:19.465: INFO: (17) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 7.791383ms)
    Mar  9 17:21:19.467: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 9.716299ms)
    Mar  9 17:21:19.467: INFO: (17) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 10.740547ms)
    Mar  9 17:21:19.467: INFO: (17) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 10.661111ms)
    Mar  9 17:21:19.468: INFO: (17) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 11.626864ms)
    Mar  9 17:21:19.468: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 11.64911ms)
    Mar  9 17:21:19.469: INFO: (17) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 11.833903ms)
    Mar  9 17:21:19.469: INFO: (17) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 12.517707ms)
    Mar  9 17:21:19.469: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 12.436371ms)
    Mar  9 17:21:19.470: INFO: (17) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 13.389494ms)
    Mar  9 17:21:19.470: INFO: (17) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 13.654017ms)
    Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 14.219978ms)
    Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 14.286873ms)
    Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 14.348079ms)
    Mar  9 17:21:19.471: INFO: (17) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 14.860426ms)
    Mar  9 17:21:19.472: INFO: (17) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 15.033198ms)
    Mar  9 17:21:19.477: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 5.581325ms)
    Mar  9 17:21:19.478: INFO: (18) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.880868ms)
    Mar  9 17:21:19.478: INFO: (18) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 5.792544ms)
    Mar  9 17:21:19.484: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 12.346947ms)
    Mar  9 17:21:19.484: INFO: (18) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 12.550153ms)
    Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 12.537573ms)
    Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 12.687458ms)
    Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 12.996866ms)
    Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 13.129926ms)
    Mar  9 17:21:19.485: INFO: (18) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 13.109015ms)
    Mar  9 17:21:19.498: INFO: (18) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 26.629966ms)
    Mar  9 17:21:19.498: INFO: (18) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 26.321525ms)
    Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 26.404676ms)
    Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 26.437557ms)
    Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 26.62723ms)
    Mar  9 17:21:19.499: INFO: (18) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 26.678381ms)
    Mar  9 17:21:19.504: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:162/proxy/: bar (200; 5.311101ms)
    Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:460/proxy/: tls baz (200; 17.349719ms)
    Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:1080/proxy/rewriteme">test<... (200; 17.671332ms)
    Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:1080/proxy/rewriteme">... (200; 18.187715ms)
    Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname2/proxy/: bar (200; 18.352304ms)
    Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname1/proxy/: foo (200; 18.323981ms)
    Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:160/proxy/: foo (200; 18.17343ms)
    Mar  9 17:21:19.517: INFO: (19) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname1/proxy/: tls baz (200; 18.239525ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/services/https:proxy-service-dt88g:tlsportname2/proxy/: tls qux (200; 18.563563ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:443/proxy/tlsrewritem... (200; 18.681711ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/: <a href="/api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft/proxy/rewriteme">test</a> (200; 18.736811ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/http:proxy-service-dt88g-vhhft:162/proxy/: bar (200; 18.838793ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/https:proxy-service-dt88g-vhhft:462/proxy/: tls qux (200; 18.867611ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/services/proxy-service-dt88g:portname2/proxy/: bar (200; 18.810715ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/pods/proxy-service-dt88g-vhhft:160/proxy/: foo (200; 18.952597ms)
    Mar  9 17:21:19.518: INFO: (19) /api/v1/namespaces/proxy-5035/services/http:proxy-service-dt88g:portname1/proxy/: foo (200; 19.239852ms)
    STEP: deleting ReplicationController proxy-service-dt88g in namespace proxy-5035, will wait for the garbage collector to delete the pods 03/09/23 17:21:19.518
    Mar  9 17:21:19.581: INFO: Deleting ReplicationController proxy-service-dt88g took: 8.656459ms
    Mar  9 17:21:19.682: INFO: Terminating ReplicationController proxy-service-dt88g pods took: 100.876461ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  9 17:21:21.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5035" for this suite. 03/09/23 17:21:21.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:21:21.997
Mar  9 17:21:21.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename subpath 03/09/23 17:21:21.999
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:22.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:22.016
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/09/23 17:21:22.019
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-d4bk 03/09/23 17:21:22.027
STEP: Creating a pod to test atomic-volume-subpath 03/09/23 17:21:22.027
Mar  9 17:21:22.035: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d4bk" in namespace "subpath-8718" to be "Succeeded or Failed"
Mar  9 17:21:22.039: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259686ms
Mar  9 17:21:24.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 2.007004352s
Mar  9 17:21:26.044: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 4.008656987s
Mar  9 17:21:28.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 6.007808302s
Mar  9 17:21:30.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 8.007248071s
Mar  9 17:21:32.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 10.006947227s
Mar  9 17:21:34.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 12.00786436s
Mar  9 17:21:36.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 14.007975025s
Mar  9 17:21:38.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 16.007522004s
Mar  9 17:21:40.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 18.006816523s
Mar  9 17:21:42.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 20.006912909s
Mar  9 17:21:44.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=false. Elapsed: 22.007836291s
Mar  9 17:21:46.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007944968s
STEP: Saw pod success 03/09/23 17:21:46.043
Mar  9 17:21:46.043: INFO: Pod "pod-subpath-test-configmap-d4bk" satisfied condition "Succeeded or Failed"
Mar  9 17:21:46.046: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-configmap-d4bk container test-container-subpath-configmap-d4bk: <nil>
STEP: delete the pod 03/09/23 17:21:46.054
Mar  9 17:21:46.064: INFO: Waiting for pod pod-subpath-test-configmap-d4bk to disappear
Mar  9 17:21:46.066: INFO: Pod pod-subpath-test-configmap-d4bk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d4bk 03/09/23 17:21:46.066
Mar  9 17:21:46.066: INFO: Deleting pod "pod-subpath-test-configmap-d4bk" in namespace "subpath-8718"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  9 17:21:46.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8718" for this suite. 03/09/23 17:21:46.072
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":356,"skipped":6624,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.080 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:21:21.997
    Mar  9 17:21:21.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename subpath 03/09/23 17:21:21.999
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:22.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:22.016
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/09/23 17:21:22.019
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-d4bk 03/09/23 17:21:22.027
    STEP: Creating a pod to test atomic-volume-subpath 03/09/23 17:21:22.027
    Mar  9 17:21:22.035: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d4bk" in namespace "subpath-8718" to be "Succeeded or Failed"
    Mar  9 17:21:22.039: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259686ms
    Mar  9 17:21:24.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 2.007004352s
    Mar  9 17:21:26.044: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 4.008656987s
    Mar  9 17:21:28.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 6.007808302s
    Mar  9 17:21:30.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 8.007248071s
    Mar  9 17:21:32.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 10.006947227s
    Mar  9 17:21:34.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 12.00786436s
    Mar  9 17:21:36.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 14.007975025s
    Mar  9 17:21:38.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 16.007522004s
    Mar  9 17:21:40.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 18.006816523s
    Mar  9 17:21:42.042: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=true. Elapsed: 20.006912909s
    Mar  9 17:21:44.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Running", Reason="", readiness=false. Elapsed: 22.007836291s
    Mar  9 17:21:46.043: INFO: Pod "pod-subpath-test-configmap-d4bk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007944968s
    STEP: Saw pod success 03/09/23 17:21:46.043
    Mar  9 17:21:46.043: INFO: Pod "pod-subpath-test-configmap-d4bk" satisfied condition "Succeeded or Failed"
    Mar  9 17:21:46.046: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-configmap-d4bk container test-container-subpath-configmap-d4bk: <nil>
    STEP: delete the pod 03/09/23 17:21:46.054
    Mar  9 17:21:46.064: INFO: Waiting for pod pod-subpath-test-configmap-d4bk to disappear
    Mar  9 17:21:46.066: INFO: Pod pod-subpath-test-configmap-d4bk no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-d4bk 03/09/23 17:21:46.066
    Mar  9 17:21:46.066: INFO: Deleting pod "pod-subpath-test-configmap-d4bk" in namespace "subpath-8718"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  9 17:21:46.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8718" for this suite. 03/09/23 17:21:46.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:21:46.079
Mar  9 17:21:46.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename daemonsets 03/09/23 17:21:46.08
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:46.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:46.095
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 03/09/23 17:21:46.11
STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 17:21:46.115
Mar  9 17:21:46.118: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:21:46.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 17:21:46.121: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
Mar  9 17:21:47.125: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:21:47.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  9 17:21:47.128: INFO: Node tt-test-el8-004 is running 0 daemon pod, expected 1
Mar  9 17:21:48.126: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:21:48.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 17:21:48.129: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/09/23 17:21:48.131
Mar  9 17:21:48.145: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  9 17:21:48.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  9 17:21:48.149: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/09/23 17:21:48.149
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/09/23 17:21:48.159
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5357, will wait for the garbage collector to delete the pods 03/09/23 17:21:48.159
Mar  9 17:21:48.217: INFO: Deleting DaemonSet.extensions daemon-set took: 4.771761ms
Mar  9 17:21:48.318: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.779428ms
Mar  9 17:21:50.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  9 17:21:50.721: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  9 17:21:50.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"124931"},"items":null}

Mar  9 17:21:50.726: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"124931"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  9 17:21:50.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5357" for this suite. 03/09/23 17:21:50.738
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":357,"skipped":6639,"failed":0}
------------------------------
â€¢ [4.666 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:21:46.079
    Mar  9 17:21:46.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename daemonsets 03/09/23 17:21:46.08
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:46.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:46.095
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 03/09/23 17:21:46.11
    STEP: Check that daemon pods launch on every node of the cluster. 03/09/23 17:21:46.115
    Mar  9 17:21:46.118: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:21:46.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 17:21:46.121: INFO: Node tt-test-el8-003 is running 0 daemon pod, expected 1
    Mar  9 17:21:47.125: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:21:47.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  9 17:21:47.128: INFO: Node tt-test-el8-004 is running 0 daemon pod, expected 1
    Mar  9 17:21:48.126: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:21:48.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 17:21:48.129: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/09/23 17:21:48.131
    Mar  9 17:21:48.145: INFO: DaemonSet pods can't tolerate node tt-test-el8-001 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  9 17:21:48.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  9 17:21:48.149: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/09/23 17:21:48.149
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/09/23 17:21:48.159
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5357, will wait for the garbage collector to delete the pods 03/09/23 17:21:48.159
    Mar  9 17:21:48.217: INFO: Deleting DaemonSet.extensions daemon-set took: 4.771761ms
    Mar  9 17:21:48.318: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.779428ms
    Mar  9 17:21:50.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  9 17:21:50.721: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  9 17:21:50.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"124931"},"items":null}

    Mar  9 17:21:50.726: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"124931"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  9 17:21:50.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5357" for this suite. 03/09/23 17:21:50.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:21:50.745
Mar  9 17:21:50.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename services 03/09/23 17:21:50.746
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:50.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:50.761
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-7369 03/09/23 17:21:50.764
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[] 03/09/23 17:21:50.777
Mar  9 17:21:50.782: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar  9 17:21:51.789: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7369 03/09/23 17:21:51.789
Mar  9 17:21:51.795: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7369" to be "running and ready"
Mar  9 17:21:51.798: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171533ms
Mar  9 17:21:51.798: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:21:53.802: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00657596s
Mar  9 17:21:53.802: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  9 17:21:53.802: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[pod1:[80]] 03/09/23 17:21:53.805
Mar  9 17:21:53.813: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/09/23 17:21:53.813
Mar  9 17:21:53.813: INFO: Creating new exec pod
Mar  9 17:21:53.817: INFO: Waiting up to 5m0s for pod "execpodqnvd4" in namespace "services-7369" to be "running"
Mar  9 17:21:53.819: INFO: Pod "execpodqnvd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438952ms
Mar  9 17:21:55.823: INFO: Pod "execpodqnvd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006180766s
Mar  9 17:21:55.823: INFO: Pod "execpodqnvd4" satisfied condition "running"
Mar  9 17:21:56.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  9 17:21:56.996: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  9 17:21:56.996: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:21:56.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.25.12 80'
Mar  9 17:21:57.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.25.12 80\nConnection to 10.99.25.12 80 port [tcp/http] succeeded!\n"
Mar  9 17:21:57.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7369 03/09/23 17:21:57.163
Mar  9 17:21:57.169: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7369" to be "running and ready"
Mar  9 17:21:57.172: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843563ms
Mar  9 17:21:57.172: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:21:59.176: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007162858s
Mar  9 17:21:59.177: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  9 17:21:59.177: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[pod1:[80] pod2:[80]] 03/09/23 17:21:59.179
Mar  9 17:21:59.188: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/09/23 17:21:59.189
Mar  9 17:22:00.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  9 17:22:00.344: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  9 17:22:00.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:22:00.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.25.12 80'
Mar  9 17:22:00.501: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.25.12 80\nConnection to 10.99.25.12 80 port [tcp/http] succeeded!\n"
Mar  9 17:22:00.501: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7369 03/09/23 17:22:00.501
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[pod2:[80]] 03/09/23 17:22:00.516
Mar  9 17:22:00.529: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/09/23 17:22:00.529
Mar  9 17:22:01.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  9 17:22:01.677: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  9 17:22:01.677: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  9 17:22:01.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.25.12 80'
Mar  9 17:22:01.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.25.12 80\nConnection to 10.99.25.12 80 port [tcp/http] succeeded!\n"
Mar  9 17:22:01.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7369 03/09/23 17:22:01.83
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[] 03/09/23 17:22:01.844
Mar  9 17:22:01.852: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  9 17:22:01.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7369" for this suite. 03/09/23 17:22:01.881
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":358,"skipped":6644,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.140 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:21:50.745
    Mar  9 17:21:50.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename services 03/09/23 17:21:50.746
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:21:50.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:21:50.761
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-7369 03/09/23 17:21:50.764
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[] 03/09/23 17:21:50.777
    Mar  9 17:21:50.782: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Mar  9 17:21:51.789: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7369 03/09/23 17:21:51.789
    Mar  9 17:21:51.795: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7369" to be "running and ready"
    Mar  9 17:21:51.798: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171533ms
    Mar  9 17:21:51.798: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:21:53.802: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00657596s
    Mar  9 17:21:53.802: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  9 17:21:53.802: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[pod1:[80]] 03/09/23 17:21:53.805
    Mar  9 17:21:53.813: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/09/23 17:21:53.813
    Mar  9 17:21:53.813: INFO: Creating new exec pod
    Mar  9 17:21:53.817: INFO: Waiting up to 5m0s for pod "execpodqnvd4" in namespace "services-7369" to be "running"
    Mar  9 17:21:53.819: INFO: Pod "execpodqnvd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438952ms
    Mar  9 17:21:55.823: INFO: Pod "execpodqnvd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006180766s
    Mar  9 17:21:55.823: INFO: Pod "execpodqnvd4" satisfied condition "running"
    Mar  9 17:21:56.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  9 17:21:56.996: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  9 17:21:56.996: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:21:56.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.25.12 80'
    Mar  9 17:21:57.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.25.12 80\nConnection to 10.99.25.12 80 port [tcp/http] succeeded!\n"
    Mar  9 17:21:57.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-7369 03/09/23 17:21:57.163
    Mar  9 17:21:57.169: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7369" to be "running and ready"
    Mar  9 17:21:57.172: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843563ms
    Mar  9 17:21:57.172: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:21:59.176: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007162858s
    Mar  9 17:21:59.177: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  9 17:21:59.177: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[pod1:[80] pod2:[80]] 03/09/23 17:21:59.179
    Mar  9 17:21:59.188: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/09/23 17:21:59.189
    Mar  9 17:22:00.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  9 17:22:00.344: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  9 17:22:00.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:22:00.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.25.12 80'
    Mar  9 17:22:00.501: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.25.12 80\nConnection to 10.99.25.12 80 port [tcp/http] succeeded!\n"
    Mar  9 17:22:00.501: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7369 03/09/23 17:22:00.501
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[pod2:[80]] 03/09/23 17:22:00.516
    Mar  9 17:22:00.529: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/09/23 17:22:00.529
    Mar  9 17:22:01.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  9 17:22:01.677: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  9 17:22:01.677: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  9 17:22:01.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2615781702 --namespace=services-7369 exec execpodqnvd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.25.12 80'
    Mar  9 17:22:01.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.25.12 80\nConnection to 10.99.25.12 80 port [tcp/http] succeeded!\n"
    Mar  9 17:22:01.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-7369 03/09/23 17:22:01.83
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7369 to expose endpoints map[] 03/09/23 17:22:01.844
    Mar  9 17:22:01.852: INFO: successfully validated that service endpoint-test2 in namespace services-7369 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  9 17:22:01.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7369" for this suite. 03/09/23 17:22:01.881
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:22:01.886
Mar  9 17:22:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename subpath 03/09/23 17:22:01.887
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:01.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:01.903
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/09/23 17:22:01.906
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-mzjm 03/09/23 17:22:01.914
STEP: Creating a pod to test atomic-volume-subpath 03/09/23 17:22:01.914
Mar  9 17:22:01.921: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mzjm" in namespace "subpath-1669" to be "Succeeded or Failed"
Mar  9 17:22:01.924: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679518ms
Mar  9 17:22:03.928: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007390527s
Mar  9 17:22:05.930: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 4.008865329s
Mar  9 17:22:07.928: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 6.007453503s
Mar  9 17:22:09.931: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 8.010111401s
Mar  9 17:22:11.928: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 10.007410841s
Mar  9 17:22:13.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 12.008648122s
Mar  9 17:22:15.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 14.008556595s
Mar  9 17:22:17.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 16.007952763s
Mar  9 17:22:19.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 18.007714151s
Mar  9 17:22:21.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 20.007847993s
Mar  9 17:22:23.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=false. Elapsed: 22.008066651s
Mar  9 17:22:25.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007982318s
STEP: Saw pod success 03/09/23 17:22:25.929
Mar  9 17:22:25.929: INFO: Pod "pod-subpath-test-secret-mzjm" satisfied condition "Succeeded or Failed"
Mar  9 17:22:25.932: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-secret-mzjm container test-container-subpath-secret-mzjm: <nil>
STEP: delete the pod 03/09/23 17:22:25.939
Mar  9 17:22:25.947: INFO: Waiting for pod pod-subpath-test-secret-mzjm to disappear
Mar  9 17:22:25.949: INFO: Pod pod-subpath-test-secret-mzjm no longer exists
STEP: Deleting pod pod-subpath-test-secret-mzjm 03/09/23 17:22:25.949
Mar  9 17:22:25.950: INFO: Deleting pod "pod-subpath-test-secret-mzjm" in namespace "subpath-1669"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  9 17:22:25.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1669" for this suite. 03/09/23 17:22:25.956
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":359,"skipped":6655,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.074 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:22:01.886
    Mar  9 17:22:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename subpath 03/09/23 17:22:01.887
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:01.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:01.903
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/09/23 17:22:01.906
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-mzjm 03/09/23 17:22:01.914
    STEP: Creating a pod to test atomic-volume-subpath 03/09/23 17:22:01.914
    Mar  9 17:22:01.921: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mzjm" in namespace "subpath-1669" to be "Succeeded or Failed"
    Mar  9 17:22:01.924: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679518ms
    Mar  9 17:22:03.928: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007390527s
    Mar  9 17:22:05.930: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 4.008865329s
    Mar  9 17:22:07.928: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 6.007453503s
    Mar  9 17:22:09.931: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 8.010111401s
    Mar  9 17:22:11.928: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 10.007410841s
    Mar  9 17:22:13.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 12.008648122s
    Mar  9 17:22:15.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 14.008556595s
    Mar  9 17:22:17.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 16.007952763s
    Mar  9 17:22:19.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 18.007714151s
    Mar  9 17:22:21.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=true. Elapsed: 20.007847993s
    Mar  9 17:22:23.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Running", Reason="", readiness=false. Elapsed: 22.008066651s
    Mar  9 17:22:25.929: INFO: Pod "pod-subpath-test-secret-mzjm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007982318s
    STEP: Saw pod success 03/09/23 17:22:25.929
    Mar  9 17:22:25.929: INFO: Pod "pod-subpath-test-secret-mzjm" satisfied condition "Succeeded or Failed"
    Mar  9 17:22:25.932: INFO: Trying to get logs from node tt-test-el8-003 pod pod-subpath-test-secret-mzjm container test-container-subpath-secret-mzjm: <nil>
    STEP: delete the pod 03/09/23 17:22:25.939
    Mar  9 17:22:25.947: INFO: Waiting for pod pod-subpath-test-secret-mzjm to disappear
    Mar  9 17:22:25.949: INFO: Pod pod-subpath-test-secret-mzjm no longer exists
    STEP: Deleting pod pod-subpath-test-secret-mzjm 03/09/23 17:22:25.949
    Mar  9 17:22:25.950: INFO: Deleting pod "pod-subpath-test-secret-mzjm" in namespace "subpath-1669"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  9 17:22:25.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1669" for this suite. 03/09/23 17:22:25.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:22:25.962
Mar  9 17:22:25.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename watch 03/09/23 17:22:25.963
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:25.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:25.977
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/09/23 17:22:25.979
STEP: creating a watch on configmaps with label B 03/09/23 17:22:25.98
STEP: creating a watch on configmaps with label A or B 03/09/23 17:22:25.982
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/09/23 17:22:25.983
Mar  9 17:22:25.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125154 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 17:22:25.987: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125154 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/09/23 17:22:25.987
Mar  9 17:22:25.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125155 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 17:22:25.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125155 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/09/23 17:22:25.993
Mar  9 17:22:25.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125156 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 17:22:25.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125156 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/09/23 17:22:26
Mar  9 17:22:26.004: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125157 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 17:22:26.004: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125157 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/09/23 17:22:26.004
Mar  9 17:22:26.007: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125158 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 17:22:26.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125158 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/09/23 17:22:36.008
Mar  9 17:22:36.014: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125189 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  9 17:22:36.014: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125189 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  9 17:22:46.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3644" for this suite. 03/09/23 17:22:46.019
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":360,"skipped":6685,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.062 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:22:25.962
    Mar  9 17:22:25.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename watch 03/09/23 17:22:25.963
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:25.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:25.977
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/09/23 17:22:25.979
    STEP: creating a watch on configmaps with label B 03/09/23 17:22:25.98
    STEP: creating a watch on configmaps with label A or B 03/09/23 17:22:25.982
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/09/23 17:22:25.983
    Mar  9 17:22:25.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125154 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 17:22:25.987: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125154 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/09/23 17:22:25.987
    Mar  9 17:22:25.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125155 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 17:22:25.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125155 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/09/23 17:22:25.993
    Mar  9 17:22:25.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125156 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 17:22:25.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125156 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/09/23 17:22:26
    Mar  9 17:22:26.004: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125157 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 17:22:26.004: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3644  5635f3cb-6ff9-47bd-b007-0182d8fe9103 125157 0 2023-03-09 17:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/09/23 17:22:26.004
    Mar  9 17:22:26.007: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125158 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 17:22:26.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125158 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/09/23 17:22:36.008
    Mar  9 17:22:36.014: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125189 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  9 17:22:36.014: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3644  df02bdbf-dfbb-4080-9df3-6746421ec78f 125189 0 2023-03-09 17:22:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-09 17:22:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  9 17:22:46.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3644" for this suite. 03/09/23 17:22:46.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:22:46.025
Mar  9 17:22:46.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename kubelet-test 03/09/23 17:22:46.026
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:46.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:46.04
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar  9 17:22:46.049: INFO: Waiting up to 5m0s for pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900" in namespace "kubelet-test-6510" to be "running and ready"
Mar  9 17:22:46.051: INFO: Pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625317ms
Mar  9 17:22:46.052: INFO: The phase of Pod busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900 is Pending, waiting for it to be Running (with Ready = true)
Mar  9 17:22:48.056: INFO: Pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900": Phase="Running", Reason="", readiness=true. Elapsed: 2.006857547s
Mar  9 17:22:48.056: INFO: The phase of Pod busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900 is Running (Ready = true)
Mar  9 17:22:48.056: INFO: Pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  9 17:22:48.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6510" for this suite. 03/09/23 17:22:48.067
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":361,"skipped":6699,"failed":0}
------------------------------
â€¢ [2.050 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:22:46.025
    Mar  9 17:22:46.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename kubelet-test 03/09/23 17:22:46.026
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:46.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:46.04
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar  9 17:22:46.049: INFO: Waiting up to 5m0s for pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900" in namespace "kubelet-test-6510" to be "running and ready"
    Mar  9 17:22:46.051: INFO: Pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625317ms
    Mar  9 17:22:46.052: INFO: The phase of Pod busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900 is Pending, waiting for it to be Running (with Ready = true)
    Mar  9 17:22:48.056: INFO: Pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900": Phase="Running", Reason="", readiness=true. Elapsed: 2.006857547s
    Mar  9 17:22:48.056: INFO: The phase of Pod busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900 is Running (Ready = true)
    Mar  9 17:22:48.056: INFO: Pod "busybox-readonly-fse06e7daf-ef78-431f-84b2-a7ade9a3c900" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  9 17:22:48.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6510" for this suite. 03/09/23 17:22:48.067
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/09/23 17:22:48.075
Mar  9 17:22:48.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
STEP: Building a namespace api object, basename gc 03/09/23 17:22:48.077
STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:48.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:48.093
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/09/23 17:22:48.095
STEP: Wait for the Deployment to create new ReplicaSet 03/09/23 17:22:48.1
STEP: delete the deployment 03/09/23 17:22:48.608
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/09/23 17:22:48.614
STEP: Gathering metrics 03/09/23 17:22:49.13
Mar  9 17:22:49.155: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
Mar  9 17:22:49.157: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.558576ms
Mar  9 17:22:49.157: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
Mar  9 17:22:49.158: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
Mar  9 17:22:49.228: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  9 17:22:49.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8222" for this suite. 03/09/23 17:22:49.232
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":362,"skipped":6703,"failed":0}
------------------------------
â€¢ [1.162 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/09/23 17:22:48.075
    Mar  9 17:22:48.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2615781702
    STEP: Building a namespace api object, basename gc 03/09/23 17:22:48.077
    STEP: Waiting for a default service account to be provisioned in namespace 03/09/23 17:22:48.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/09/23 17:22:48.093
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/09/23 17:22:48.095
    STEP: Wait for the Deployment to create new ReplicaSet 03/09/23 17:22:48.1
    STEP: delete the deployment 03/09/23 17:22:48.608
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/09/23 17:22:48.614
    STEP: Gathering metrics 03/09/23 17:22:49.13
    Mar  9 17:22:49.155: INFO: Waiting up to 5m0s for pod "kube-controller-manager-tt-test-el8-001" in namespace "kube-system" to be "running and ready"
    Mar  9 17:22:49.157: INFO: Pod "kube-controller-manager-tt-test-el8-001": Phase="Running", Reason="", readiness=true. Elapsed: 2.558576ms
    Mar  9 17:22:49.157: INFO: The phase of Pod kube-controller-manager-tt-test-el8-001 is Running (Ready = true)
    Mar  9 17:22:49.158: INFO: Pod "kube-controller-manager-tt-test-el8-001" satisfied condition "running and ready"
    Mar  9 17:22:49.228: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  9 17:22:49.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8222" for this suite. 03/09/23 17:22:49.232
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Mar  9 17:22:49.239: INFO: Running AfterSuite actions on all nodes
Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Mar  9 17:22:49.239: INFO: Running AfterSuite actions on node 1
Mar  9 17:22:49.239: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  9 17:22:49.239: INFO: Running AfterSuite actions on all nodes
    Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Mar  9 17:22:49.239: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  9 17:22:49.239: INFO: Running AfterSuite actions on node 1
    Mar  9 17:22:49.239: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.083 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5771.619 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h36m12.007158115s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

